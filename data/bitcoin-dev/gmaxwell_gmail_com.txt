
@_date: 2011-08-03 10:48:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] DNS seeds returning gone peers 
You can't "massively increase" the number of available connection
slots without risking running nodes on lower memory systems (e.g. VMs)
out of memory.
Moreover, 125 slots should be more than enough.  We need to figure out
why it isn't.

@_date: 2011-08-04 16:08:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double spend detection to speed up 
But they can be trivially generated on demand, and potentially result
in unbounded flooding.
Even if you carefully don't duplicate an announcement I can easily
generate an unlimited number of double-spends for the network to
flood. The normal anti-DDOS logic doesn't work because there can be no
additional proof-of-workish costs for the double spend (they'd share
whatever anti-ddos fees the first txn had).
This is somewhat soluble, I guess. Rather than NAK the transaction the
way it would work is propagating conflicts on each of the conflicted
inputs.  "I've seen at least two transactions recently trying to spend
input X, here is proof: (two txn IDs)". Even if there are more spends
of that input you don't need to hear about them, knowing about two
spends of an input is enough to consider that input (and perhaps all
inputs with an identical script to that one) temporarily suspect.
Though it would have to be done input by input.
This might be an interesting feature if not for the fact that the
software already waits a fair number of confirms before considering
something confirmed. Of course, a sybil can just filter these messages
diminishing their usefulness.
I suppose I could add this as a (7) to this list:

@_date: 2011-08-04 16:35:53
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Discussion related to pull 349 and pull 
It's not just a matter of mining power, it's also a question of
propagation.  Matt and I tried to perform a non-standard transaction
weeks ago and weren't able to get in mined after many hours. (we
eventually double spent the input with a normal transaction in order
to make it go away, interestingly one point about non-propagating txn
is that they're extra vulnerable to double spending by a standard txn,
as the non-standard one won't preclude the propagation of the standard
on maturity/bugfixes is probably going to delay your full patch, but
the IsStandard part is uncontroversial and could go in quickly.
Based on that I think it would be very useful to split 319 into two
pull requests: one which does the IsStandard change, and one which
adds the full escrow feature set.  This way when the escrow patch does
enter the mainline client it will be meet up with a network which is
happy to handle its transactions.
(and people who are eager to use escrow can use modified clients on
the main network before that point in time)
Ah for some reason I thought your current code did not always produce
the shortest sequence.
If so, I see no reason to block on the other pull.

@_date: 2011-08-04 18:18:40
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double spend detection to speed up 
Except for the fact that such a party is a DOS attack on the network
which is already short on functioning listeners.  I don't have much
doubt that people doing the "connect to everyone" are already causing
harm. There are some nodes in .ru/.ua which aggressively connect to me
(instant reconnects if I hang up on them) which have never passed me a
transaction in all my available logs.
Alerts scale better? both can have a place in the ecosystem, they're
actually complementary: Alerts are vulnerable to filtering by sibyl
attackers but they have deeper network penetration and where filtering
doesn't prevent them you don't need a connection to hear them.

@_date: 2011-08-05 17:23:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double spend detection to speed up 
I'm sure many people would be interested in patches that solve the
~O(N) peak memory usage with additional connections.

@_date: 2011-08-18 12:46:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] From the forums: one-confirmation attack 
At the same time, if this causes people to wait less than the 6 blocks
that the software currently waits for before leaving unconfirmed
status then that would be sad.
Simply waiting a number of blocks is an excellent metric and provides
robustness against almost all attack patterns in a way that various
one-off-heuristics can not as it equates to _real difficulty_ (and
real expense (hashing computation, loss of income on the orphaned
blocks)) in a way that can't be faked.
A few weeks back when there was some rumor going around that mybitcoin
lost coin based on some kind of one confirmation attack I described on
IRC a similar attack pattern which had a useful improvement:
* The attacker runs many widely distributed sybil nodes (e.g. using
botnet drones as simple tcp proxies to appear at many addresses). He
takes advantage of the fact the bitcoin won't connect to /16s that
have already connected to it to further isolate nodes.
* By creating normal looking probe transactions which his own nodes
won't forward he detects network partitions which he is able to
create. He searches for a cut which causes there to be at least two
partitions which contain significant mining power.
* He creates two accounts at MoronBank. He doesn't even bother
identifying MoronBank's node. MoronBank will be in one partition or
another. He makes deposits in both partitions, and conflicting
transactions in the opposite partitions, while carefully filtering out
these transactions from crossing the boundary.
(Notably, the network doesn't appear partitioned to everyone else now
because he's still forwarding blocks and transactions unrelated to his
attack? it only becomes visible once some of his evil transactions are
* After the funds show up in MoronBank he withdraws and drops the partitioning.
Only if he has difficulty getting MoronBank into the smaller partition
does he need to bother locating it and attacking it directly.
The bad thing about this attack is that it doesn't require the
attacker to have any hash power at all: he captures miners as
unwilling (or willing but plausibly deniable) participants. The lost
income from orphaned blocks is externalized to the victimized miners
(and since most pools don't pay orphaned blocks out of pocket a pool
operator would be inclined to help out).
The good thing about it is that it's killed dead by nodes adding in a
few manually configured peerings, they don't even really need to be
trusted: You just need to trust that they don't all go to a single
bad-guy conspiracy. At a minimum all major miners should be fully
Unfortunately, We don't currently have software for this as addnode
doesn't worry about keeping the links up... and the major pools also
don't seem to be interested in participating.

@_date: 2011-08-18 13:27:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] From the forums: one-confirmation attack 
Not that helpful. In that attack pattern the attacker can release the
block and the conflicting transaction at the same time (to different
nodes, of course). They can also inject the conflicting transaction
into many places in the network at once.

@_date: 2011-08-24 11:45:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
It's a good tool which we should have in our tool-belt.
Though it's a bit of when you are a hammer all problems are nails.
This issue can also be addressed by things like external private key
protectors.  But someone would have to build one.
Someone might be more inclined to build such a thing if the software
had good support for tracking public keys without private keys, and
generating unsigned transactions for export to the device for signing.
Regardless, it might be useful to contact the authors.
I agree.
One way of doing this would be to have an address which hashes an
ordered concatenation of many addresses (perhaps plus a length
argument). To redeem you provide the public keys which are signing,
plus the addresses which aren't signing, and the receiver validates.
If it can be done, then yes, I agree it would be worth forking the chain.
This _feels_ like something which could and should be done with the
existing (but disabled opcodes).
It's not exclusive, however, with a long N-address address type for
multisig destinations.  We could support that _now_ and defer the
'compressed version' until after people have experience with this
usage.  The only cost would be supporting this address type forever,
which isn't that bad.
It's also important to note that incompatibility wouldn't be complete:
The only limit is that old clients couldn't send funds to escrow
addresses? which is an issue no matter how you encode the information.

@_date: 2011-08-24 12:46:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
Too early for that.
The quantized scheme limits the amount of difficulty skew miners can
create by lying about timestamps to about a half a percent. A rolling
window with the same time constant would allow much more skew.
Increasing precision I would agree with but, sadly, causing people to
need more than 64 bit would create a lot of bugs.
infinite numerator + denominator is absolutely completely and totally
batshit insane. For one, it has weird consequences that the same value
can have redundant encodings.
Most importantly, it suffers factor inflation: If you spend inputs
1/977 1/983 1/991 1/997 the smallest denominator you can use for the
output 948892238557.
Not to mention that the idiots writing financial software can only
barely manage to not use radix-2 floating point on everything. Asking
them to use arbitrary rational numbers with mixed radix will never
Please lets not make bitcoin _less_ trustworthy.
The 100 block maturity on generated coins is good. The generation from
an orphaning is lost forever like the losing side of a double spend,
but far far worse... because orphaning happens all the time on its own
without any malice.
I agree it's obnoxious that you can't pad your generation payouts
without creating more transactions, but I don't see a solution for
that. Repeat the addresses... make up for it by increasing your payout

@_date: 2011-08-24 13:19:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
On Wed, Aug 24, 2011 at 1:07 PM, Rick Wesson
Can you provide a reference to this 'demand' a post by Luke isn't
enough to support the claim of demand.
We're not at maximum size right now (thankfully).
We don't know what the network dynamics would look like at that
traffic level. So how could we competently say what the right metrics
would be to get the right behavior there?  Thats what I meant by too

@_date: 2011-08-24 16:29:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
This works fine for ECC.  But it requires that the composite key
signer has simultaneous access to all the key-parts, so it doesn't
solve the "my PC has malware" problem.

@_date: 2011-08-25 14:31:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
the customer to bypass the clerk and have 3 key addresses, or could we
just leave it to the/a client to implement the multisign transaction
after the money has been received - as a transfer to a safe? This
would greatly simplify the problem and cover the vast majority of use
cases. Not covered in this is huge single transfers where the intruder
of a single key system finds it profitable to reveal their intrusion
by grabbing the entire wallet.
Obviously these things don't need to be hard coupled, since they're
useful independently.   But I don't agree with the premise that being
able to pay directly into an escrow using an address isn't essential
at least as an eventual feature.
The bank analogy falls down because in our threat model people are
replacing the bank teller with a convincing facsimile (malware turning
your computer against you).  Funds can be stolen in a microsecond, so
any exposure isn't good.
Again, I'm not arguing to delay anything? just pointing out that the
ability to have usable addresses (they can be long) that encode a
couple escrow destination.
Perhaps just an address type that can encode any payment script?  User
provides the inputs, sets the outputs plus and additional outputs, and
signs. Client refuses to pay to an address if the resulting
transaction fails IsStandard.
3-of-3 in particular seems somewhat important to me (group trust
accounts).  I'd really rather not drop use cases unless we're not
confident that they can't be tested sufficiently simply because it'll
just mean another cycle of testing later someday to test them and,
honestly, a more uphill argument as the usecases get narrower and
I'll spend some cycles testing whatever cases make it in.

@_date: 2011-08-25 16:29:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New standard transaction types: time to 
On Thu, Aug 25, 2011 at 4:10 PM, Pieter Wuille
So 187 bytes in base-58? I think pretty darn good for a destination
with 6 keys and complicated rules. I like that a lot.

@_date: 2011-12-16 12:48:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd: [BIP 15] Aliases 
How can one construct a zero-trust (or nearly zero trust) namecoin
resolver without having a copy of the ever growing complete namecoin
block chain?
The bitcoin lite node mechanism will not work because a peer could
return stale records or no-result and you would have no evidence of
their deception.  (In the case of lite bitcoin nodes, telling you
about old transactions is harmless because you control your own

@_date: 2011-12-17 14:28:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Protocol extensions 
I strongly advise people who are not me to use this sort of scheme, so
that I may enjoy the benefits of robbing you blind.
.... But really, saying "some sort of DHT" without basically
presenting a working implementation that demonstrates the feasibility
of solving the very difficulty attack resistance problems these
schemes have basically triggers my time-wasting-idiot filter.  (Or
likewise, presenting a fixed network structure that would have a nice
small and easily identifiable min-cut...)
I don't doubt I'm completely alone in this,  though perhaps I'm more
of a jerk about it.   Even if your actual proposal might have some
merit you should be aware that every fool who has operated a
bittorrent client has heard of "DHT" and, although they may not even
understand what a hash table is, many have no reservation going around
suggesting them for _every_ distributed systems problem. Want to scale
matrix multiples? DHT! Want to validate bitcoin blocks? DHT! Network
syncup slow (because It's bound on validation related local IO)? DHT!
I suggest people solve the real problems first, then worry what name
to give the solutions. ;)
To address gavin's tragedy of the commons concern, one useful feature
would being able to mutually authenticate a peer... then full nodes
could pick and choose which lite nodes they're willing to do (a lot
of) hard work for. This would also be valuable because some modes of
lite operation require non-zero trust of the full node being queried.

@_date: 2011-12-17 18:46:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Pubkey addresses 
Would introduce yet another address type that services will have to cope with.
No currently deployed sofware knows how to spend it.
No currently deployed software knows how to receive it.
All pay-to-pubkey schemes (point compressed or otherwise) shift
storage to TXN _output_ scripts which are the least prunable place, so
for nodes which are pruning any pay to pubkey scheme will result in
more storage than pay to address.
Ignoring pruning, pay-to-address + key recovery is quite a bit smaller
than pay-to-compressed pubkey.
The downsides to op-eval2+recovery were the lack of software, but
we're in an equal boat with this.
Excitement over key recovery fell was diminished when it was pointed
out that it only saves space in input scripts which wasn't so
important because they're quickly prunable.  If you accept that
pruning will someday be common on many nodes then you should prefer
pay to address (since its smallest in that case).  If you assume they
won't be, you should prefer pay to address plus key recovery (since
its the smallest without pruning).
Pay to non-compressed pubkey is smaller than
pay-to-address-without-recovery assuming you don't prune, and its more
deployable because nodes can already recieve it.  It's larger if you
do prune, and it's larger than recovery either way.  Pay-to-compressed
has all the disadvantages, it still larger than recovery and doesn't
have the advantage of already deployed software.
Sorry to be curt? I'm a little irritated that discussion on recovery
in OP_EVAL was dropped because "input script size doesn't matter
because of pruning" and now people are talking about adding another
address type which creates seriously bloated transactions where there
is pruning, because its slightly smaller in the no-pruning case (and
again, still not as small for key recovery).

@_date: 2011-12-19 17:29:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP language on normative behavior 
I've been arguing with Luke-JR on IRC about the interpenetration of
BIP_0014?  Gavin's recent commit uses the same version string for the
GUI interface and the daemon mode.
Luke believes this is a _violation_ of BIP_0014 and an error in
judgement on Gavin's part, and a failure to conform to the community
adopted standard. I believe Luke is mistaken: that BIP_0014 actually
don't have mandatory requirements for what you put in the version
field and even if it did, that they are in fact the same software and
should have the same name.
I don't think an agreement is likely on the second point, but the
first point highlights some ambiguity in the interpretation of BIP
language. E.g. What is permitted vs encouraged vs required.
There is well established standard language for this purpose:
I strongly recommend that all BIPs be written using the RFC2119
keywords where appropriate.

@_date: 2011-06-30 22:07:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.3.24 
*Flood fix
I think this is important, slow bringups are problematic and I think
the flood disconnects have been contributing to network partitioning
(you'll disconnect nodes that have the full blockchain but keep ones
that don't), adding to the partitioning problems cause elsewhere.
I've been running it for a couple hours on a large public node which
was seeing frequent flood disconnects, and it seems to be working
fine. No more flood disconnects.
Syncing a local node to it (no a not terribly fast core) now takes
34.5 minutes (I new bringup on the same system a few days ago hadn't
synced in over an hour).
Increasing the nLimit in sipa's code from 500 to 5000 reduced the
syncup time for me by about 1.5 minutes, almost all of the speedup
being in the early blocks.  Since it has the 5MB limit now I don't see
much reason for a large per block limit.
I've been using this for a while, we need more dnsseed roots but I see
no reason not to turn it on now.
Lfnet now reports 32843.  Presumably there are more bitcoin users than
that, because not all use IRC. 32843*8 = 262744 listening sockets.
Meaning, assuming a nice equal distribution we need 2189 listening
nodes to support the network? but the real distribution will be
somewhat uneven due to bad luck and the /16 limit.
Matt has estimated that there are around 4000 stable listening nodes.
Linear extrapolation from the two day lfnet growth leave us running
out of sockets in a little more than a month. While it won't all break
if it runs out, since we don't strictly need 8 connections, it's still
not good.
I think getting more listening nodes is a somewhat urgent matter as a result.
I'd also like suggest updating the checkpoint in 0.3.24. Difficulty
has increased almost 17x since the highest one currently in there. A
rather large number of parties could mine pretty nice forks at 1/16th
the current difficulty for nodes that they've sibyled.

@_date: 2011-07-01 12:23:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Useful bitcoin patches... 
The multithreaded RPC stuff will need very aggressive testing to make
sure it doesn't expose race conditions elsewhere in the code.
E.g. you don't want to lose change from a send because some txn called
getnewaddress concurrently and there was a bug. So far the
multithreaded RPC patches have pretty much only been run by miners...
who have a different rpc profile than everyone else.
(and the MT RPC that I've been using only multhreaded getwork?)
Gah. No.
The 'hub mode' is not good. We're already low on sockets network wide,
adding a built in DDOS mode flag to bitcoin that makes nodes
aggressively connect to lots of neighbors is a bad idea. People will
ignorantly enable it thinking they are adding resources to the network
when they are really consuming much much more.
I have a big fast node with a higher connection limit and the flood
fixes and I'm currently seeing 596 inbound connections right now. This
suggests the situation is already a lot worse than the rough numbers
using lfnet connection counts suggested.
Miners, concerned with fast block propagation, should manually addnode
each other. We should fix the addnode logic so that it reserve
connection slots for addnoded nodes and tries to keep connecting to
them (or, alternatively, add a peernode flag for that behavior)
currently addnode is oneshot.
There is a lot of room for longer term improvements to the connection
and forwarding logic, and I have a couple interesting ones I'm running
on my nodes, but we don't really have any good way to test and
validate changes, so I'm hesitant to publish them.

@_date: 2011-07-01 13:59:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.3.24 
I also don't like upnp, but I strongly support it being on because
leaving it off is not really an alternative.
IMO a forum poll is the wrong tool to use to decide if bitcoin keeps
working or not. ;) If the alternative were upnp vs some other way to
reasonably increase the number of listeners... e.g. "upnp always vs
upnp only if there has been no inbound connections in X minutes" that
would be another matter.
The bitcoin/bitcoind difference seems confusing to me, since when
someone complains about connectivity I'll have to remember to ask
which they are using... but enabling it for the gui is probably
sufficient in terms of network health.
But it'll probably happen anyways: I imagine most bitcoind users build
locally and don't bother installing the upnp library. I know I don't.
Yea, listening at all is more interesting than upnp? though almost any
harm that listening can cause can also be caused by outgoing
connections since the protocol is symmetric.
(e.g. if you have an exploit, you don't need to connect to people, you
can just sibyl attack the network and exploit people who connect to
you? not quite as effective but I think enough that UPNP isn't a great
additional risk)
If you want to talk about worrying people about security:  The IRC
connections seriously set off alarm bells, especially when someone
looks and sees something indistinguishable from a botnet in IRC.  It's
been blocked by major ISP multiple times. So, until we get IRC
disabled nothing else is really all that significant from a security
hebe-geebies perspective.
Well, users who don't like it can still disable listening? which is
healthier for the network right now than leaving listening on but not
actually working.
We can fix the incentive structure somewhat: We should give preference
in the form of preferred forwarding from/to to nodes that we've
connected to vs connected to us, potentially improved relay rules. Not
only does this given an incentives to listen (faster txn processing,
hearing about blocks earlier) but it also would reduce the
effectiveness of some DOS attacks.   Not something for 0.3.24,

@_date: 2011-07-01 20:51:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.3.24 
The fact that this will -rc before full release softens my concern some.
I did a lot of semi-automated testing of cwallet+crypto (in the
encrypted and non-encrypted states) which I really don't want to redo
for cwallet alone.

@_date: 2011-07-04 16:59:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Encrypted Wallet Backward Compatibility 
Rewriting the old one before erasing it and replacing it with a
placeholder might increase the chances that the old unencrypted keying
material was not left on disk.

@_date: 2011-07-04 18:30:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Encrypted Wallet Backward Compatibility 
Silently breaking them, not so much.
Or do you think people are going to notice that they've started
backing up a zero byte file?

@_date: 2011-07-07 13:45:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Suggestion for enhancements to getblock 
It _could_ be done another way, with a protocol change:

@_date: 2011-07-12 19:40:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] overall bitcoin client code quality 
Objectively, your believes have only the weight of the electrons they are
printed on, so long as you're talking and not coding.
I don't mean that as an insult? I'm sure many people value your ideas
but when you disagree with someone who is actually coding you'll
eventually lose every time.  Talk is cheap.
(And I'm guilty of this too? but aware of my lacking commits I'm
certainly not going to expect anyone to listen to _coding style_ advice.
 I try to keep my comments to crap I can measure and speculate about.)
Certainly no modern SCM has major issues with merge conflicts due to
shared files.
Bitcoin is a _tiny_ piece of software... on the order of 20kloc. It's a
a scale where someone competent can read it in a day and have a basic
overall understanding of it in a few.
This fact makes the aesthetics talk seem like pointless shed-painting
especially coming from people who are yet doing substantial work.
The proposal about reimplementing parts as libraries and the switching
to them after validating them is a fine one.  I suggest you do it.
Having multiple work on such projects would not be wasted effort,
as we'd all learn from the competition in designs/APIs/and targets for
comparative testing.
The interesting logic, however, is not net.cpp... because nothing too
awful happens if peers get confused and drop their connections here
and there. The critical logic is the blockchain validation logic. Which
must make absolutely identical decisions on all nodes and which has a
lot of corner cases which are difficult to test and might expose
behavioral differences.
There is also a lot of neat functionality in the scripts which is
currently disabled because of a lack of confidence about the
security of that code.
So not only are new, clean, secondary implementations of this logic
needed, but good automatic testing shims which can find
inconsistencies between implementations.
(Testing rigs are often an excellent area of work for people new to
a project.)

@_date: 2011-07-17 04:01:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Wallet encryption migration 
On many (most?) modern Unix file systems writing zeros just once is
not sufficient because the data won't be written in place, but
multiple writes aren't any better.
Moving the keypool addresses aside so they won't be used sounds like a
good idea.
The lamest thing is that there is no way for wallet to be
born-encrypted. So the only way to prevent a leak is to build the
wallet initially on a ramdisk or the like, then move it over after
encrypting it.
At least luke-jr's (2) would make the key leak on a new wallet
inconsequential? since all keys in it are keypool keys at that point.
So I really think it ought to be done.

@_date: 2011-07-23 19:39:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Discussion related to pull 349 and pull 319 
Pull 349 (
implements a pretty nice implementation of multiple signature escrowed
transactions. Especially with clearcoin gone I think that this is
something we ought to have sooner rather than later.
I've tested it on a private network and it appears to work pretty well.
It probably needs more testing and discussion before it is actually
added to the client, but one challenge is that because it requires a
new transaction type it won't be deployable until _after_ an updated
isStandard is widely used in the network.
So I think that makes a good argument for separating out the
IsStandard part of the patch and getting it out in 0.4.
Unfortunately, the patch exposes an issue with multisig validation: If
I understand it correctly, the problem is that due to redundancy in
 the script length coding opcodes it's possible to code a script
multiple ways. The signature validation code creates new template
scripts in order to evaluate signatures for one output, and the code
in bitcoin is not careful to code the new script the same way the
original one was coded, causing the signature validation to fail when
something used OP_PUSHDATA when a direct length could have been used.
Pull 349 ( contains one
candidate fix for this: Excluding the length opcodes from the
This fix carries a risk of creating differences in how nodes validate
transactions leading to lasting forks. (e.g. Old clients will reject a
block which new clients would have accepted).  I do also wonder about
strange effects arising from multiple valid TXN which are identical in
meaning but have different hashes, but I guess thats already possible
in a number of different ways.
Another way of fixing this would be to just define that OP_PUSHDATA*
_cannot_ be used to push the smaller lengths which could be more
efficiently encoded with the direct length opcodes.  I think this
would have the benefit of being consistent with the current behavior
and carry no severe split risk.
Yet way of fixing it would be to change out the templating code works
to make sure it codes the template the same way the original was
coded. This seems tricky to implement to me, tricky to validate, but
it would potentially be beneficial if this same class of problem
exists for things other than the length coding.

@_date: 2011-07-28 14:49:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Forking personal "vanity" versions... 
There is a 'vanity version' that removes the use of libdb or somehow
magically requires you to not have its development headers installed
on your system?

@_date: 2011-10-03 00:53:51
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Supermajority mining votes for valid->invalid 
It is possible to made changes to the distributed algorithm which make
previously valid txn invalid without necessarily creating any lasting
chain splits.  This has been proposed for the addition of the eval
opcode by using one of the existing NOPs.
One challenge is that if transactions are emitted which are invalid
under the new scheme but valid under the old after the block height
that the rule is coded to take effect and a super-majority of miners
are not yet upgraded the upgrade may cause a long reorganization and
serious disruption.
Here I explain one possible way of avoiding this.
Upgraded nodes get the following rules:
(0) Never forward or mine a txn which would be invalid under the new rule.
(1) Apply old behavior before height X unconditionally.
    (X set far enough in the future to get reasonable deployment by
large miners)
(2) Begin applying the new rule only after the first point in the chain
    after X when none of the last Y blocks have contained an invalid transaction
    under the new rules.
After the software has been released members of the bitcoin community then
begin _intentionally_ transmitting transactions which are invalid under
the new rules. (What would have been an attack under simplest deployment plan)
By setting Y high enough that all major miners have a chance to mine
in the window,
this actually becomes an effective vote for the change by miners with
a stochastic
super-majority threshold.  All nodes are able to exactly determine at what block
the election has completed because it is an objective fact of the chain.
With this scheme the new encoding will only become active when enough mining
capacity supports it (or at least helpfully refuses to mine the who class
of transactions) so that a large reorganization will not happen due to
incompatible blocks during deployment.
This could be further enhanced with conflicting block discouragement (e.g.
refusing to extend or forward a rules violating block until it is burred)
but I think this scheme is sufficient without that, and that this is generally
superior to discouragement for this purpose.

@_date: 2011-10-03 01:39:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Supermajority mining votes for 
(4) is a nice idea.
I was hoping to avoid (3) simply because for any one of these upgrades
hopefully 95% of the network is neutral wrt the change because they
won't mine either form of the transactions.
The active statement has the benefit that it constitutes a proof: You
know with specific confidence (based on the window size) how likely a
fork of length X will be if a newly invalid transaction is announced
at the time of the activation.

@_date: 2011-10-05 01:23:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Transaction Delivery and Storage 
However you want. The P2P network isn't all the essential to bitcoin.
With patches you can import and export transactions to text from a
regular client.
The distributed algorithm and the block chain are far more essential
to bitcoin than the p2p network is, you could interconnect all bitcoin
systems with morse code operators running spark gap transmitters with
nary an IP packet in sight and it would still be bitcoin.

@_date: 2011-10-05 08:50:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] vtxPrev 
They don't need 7 blocks to maturity and respendability. The software
will attempt to use older inputs when available but if not it will use
what it has.  It's also prone to respending its own outputs quickly
because it reasonably trusts that it won't doublespend its own
And, yes, if there is a deep split then its possible that inputs might
have been spent differently in the new split. But it's not especially
likely. Retransmitting one of your own txn's parents if its dropped
but not yet impossible sounds prudent to me.

@_date: 2011-10-25 10:49:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Detecting OP_EVAL scriptPubKeys that are 
If anyone finds that solution unsatisfying, consider? It's already the
case that I could take one of your disclosed public keys and create an
infinite series of secondary keys out of it for which only you could
decode, and the only way for you to find them in the blockchain would
be to have performed the same procedure and made a note of the
addresses you're watching for.
... or really, more simply I could generate a private key on your
behalf and send funds there. ("What do you mean you didn't get the
funds? I sent them to the private key defined by the cryptographic
hash of the lyrics of your favorite song!")
So it's already the case that if I didn't get your address from you
(or through a negotiation with you), I can't expect you to receive

@_date: 2011-10-26 10:03:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Detecting OP_EVAL scriptPubKeys that are 
The additional material _IS_ then part of the private key. It's not
something seperate. Its something you need to know in order to author
the address.  This was fundamentally my argument. Not that you could
hide information, but that information was already hidden.
Right now under conventional uses I can't identify all the
transactions that land in your wallet, because I don't know the keys
it contains. With the proposal it's the same situation.
These projects will be able to use the _same_ procedure to extract the
identifying information. Except now instead of
ripemd160(sha256(pubkey)) it will be more like ripemd160(sha256([some
extra bytes generated by the wallet holder]||pubkey)) that you
extract.  If the former is not a problem for these applications, why
is the latter?

@_date: 2011-10-27 05:08:53
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Detecting OP_EVAL scriptPubKeys that are 
(taking this a bit out of order)
script is part of the cryptographic key.
Which you must.
I can see no functional difference than if you said of the current
system "except if you also have knowledge of the final 32 bits of the
ECC private key".
I don't see any reason to expect clients to identify funds without
knowing the information required? it's impossible. I mean, sure, you
_could_ bruteforce the final 32 bits of your private key? or you could
attempt to try the cartesian product of every key you have with every
key seen in the block chain for finding an op_eval script. But thats
unworkable, unnecessary, equally bad for all client types, and not
being suggested.  Under either system a coin is not yours unless you
know all of the right bits? knowing some is not good enough.
Could you suggest how else we could gain the advantages of op_eval
without it?   How can I secure my wallet under whatever scheme I like?
create a trust that requires multiparty signoff? and securely have
senders pay into it without expecting them all to handle some rare and
complicated procedure for sending to me? (Or a burdensome address
which serializes a script and a large amount of data into hundreds of
characters, and which still may be unable to represent the rules I
wish to have govern my account? and which the sender might mutate?
e.g. twiddling the threshold counts? and cause me great
On the basis of the discussion here I now oppose checkmultisig as a
standard transaction type. (Sorry, I'm not trying to be a jerk if it
came off that way, I'm not opposing it simply because you support it:)
The advantage I saw of having it was faster deployment for the
explicit escrow cases that don't need to encode the payment rules in
an address (as is needed for wallet security and trusts)... but it
seems to me that there is a serious misunderstanding that there is a
bijection between hash160s and public keys, and one between ECC
private keys and spendable transactions, and that this bijection is
desirable or even essential to bitcoin.
I'm concerned that this misunderstanding will moot the flexibility of
the script system because every script that doesn't look like a direct
mapping of hash160->pubkey->payee will be regarded as _broken_? not
just useless to one app or another which could have simply chosen not
to generate those addresses? but actually incompatible with bitcoin,
as is basically being argued here? or, keeping in mind that people can
freely mine non-standard transactions, could this result in tools
which are rendered insecure by unexpected transaction types? Will a
system that thinks HASH160 = IDENTITY recognize that a script which
also requires an additional secret key on the stack is unspendable?
Keeping checkmultisig alone as a standard transaction, when it's
functionally a redundant subset of OP_EVAL  (and inferior because it
reduces the txn you can place in a block) could only further that
misunderstanding. :-/

@_date: 2011-09-05 09:30:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Adding a pong message to the protocol 
I'd rather see effort spent on the root issues, e.g. having nodes
gauge their own suitability (working inbound port, reasonably current
block chain, etc) before becoming advertised listeners.
Or more importantly? figuring some way of setting up network
simulations which could be used to actually _validate_ proposed
changes in this area.
I, or many other people, could spout endlessly about attractive
sounding network enhancements (e.g. move-to-front peer prioritization,
tweaks to peer selection, etc.) but it's all just arm waving without a
way to measure it, and the real network is far too slow to upgrade
(and important) to test things in situ while testnet is far too small
and unlike the real network for useful testing.

@_date: 2011-09-14 16:09:00
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Difficulty adjustment / time issues 
Perhaps better thing to do is to also delay the _forwarding_ of these
blocks _and_ blocks that extend them, until extended one more time.
This policy, if adopted by the forwarding nodes (who really shouldn't
care for much other than the overall health of bitcoins) puts miners
at risk if they don't run the augmented extension policy.
Though I generally agree with Luke that we should just fix the root
cause even though it forks the chain. Not for his reasons (I don't
give a crap about the burden on _one_ pool operator? the rest cope
with bitcoind scaling fine without excessive dependance on ntime
rolling),  but simply because not fixing it makes the bitcoin security
model harder to explain and analyze.
"Here is a vulnerability, but its offset by this workaround" is
inferior to "the system is secure against this kind of attack".

@_date: 2011-09-14 17:51:30
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Difficulty adjustment / time issues 
I'm generally opposed to doing "too much" at once in this kind of change.
Some changes, like this one, are completely uncontroversial (except
some argument about having fork causing change at all) where some have
more complicated social/economic impacts (the block size being among
them, though probably not the worst).
Moreover, the longer we go between such changes the more the cost is
perceived to be infinite. Better to take one per year, with six months
of gap between implementation, and give everyone the right
expectations than to have prolonged arguments due to our inexperience
that only get trumped by emergency changes.
General network health and user security _requires_ periodic upgrades
in any case, and will for the foreseeable future. The whole notion
that old versions will _stop working_ would be a pretty good thing at
this point in bitcoin's existence, judging by the high number of
pre-.24 listeners still reported.

@_date: 2011-09-15 10:21:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Request review: drop misbehaving peers 
On Thu, Sep 15, 2011 at 10:06 AM, Gavin Andresen
Fail hard, log the reason locally. Problem becomes tractable. Also,
for any problem big enough to cause a network outage the issue won't
be reproducibility.
I support the imposition of txn rules? otherwise the dropping is
nearly pointless due to the hole that any attack can just take the
form of junk txn? but you must be super careful that an attack can't
be transitive: There should be nothing I can give a node that it will
forward on that will make that node's peers drop it. (and this needs
to remain true while forwarding rules evolve)
So, I'd suggest that you'd only drop on transactions that would
invalidate a block if included in it but the problem there is that
double spends meet that criteria. Better would, perhaps be something
like "would invalidate a block if included; except that double spends
after the last checkpoint are allowed, and nodes should not forward
any txn until they are current with their last checkpoint"
(That bit of complexity is to reduce exposure where a new node gets
hit with double spends that its yet too stupid to reject, and it
forwards them onto its friendly peers who then hang up on it thus
prolonging its period of ignorance? in general care needs to be taken
to avoid hanging up on nodes that are just too young to know better)
It would be fine to hang up on any garbage data: something is
obviously wrong. I'd be hesitant to ban on a single instance of it,
it's rare but happens. (e.g. see

@_date: 2011-09-19 09:03:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.4.x stable branch 
I think the primary concern that they are attempting to address there
is providing a stable base bitcoind for miners, banks, and webservices
to apply their patches on top of.
Right now, if they want to keep up with development they are stuck
forward porting against often disruptive changes as just about
everyone running something of importance needs some patch or another
so you have people who are clearly in the know like Luke and tcatm
trailing development on some of their systems by many months.
This isn't healthy for the network.
I'm not convinced a bugfixes only branch will help much: Even bug fixes
will disrupt local fixes, and testing and supervising your upgrade usually
takes more effort than the forward porting.
I'd rather see more effort put into mainlining the changes people are
carrying sooner and restructuring code to better accommodate patches
which aren't suitable for mainline.  This will also encourage people
to make the fixes they're running publicly available, rather than
just keeping them private for competitive advantage.

@_date: 2011-09-19 11:06:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.4.x stable branch 
Bug fixes also introduce bugs. Considering the fairly small number of new
features added, I'd take a bet that most of the more recently introduced
bugs were the result of fixes not features.

@_date: 2011-09-27 16:23:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Newly introduced DoS 
Might be better to have a global flag that indicates when the node
thinks its current with the network (this could have other UI impacts,
like letting the user know if they send and their connectivity looks
non-current), and only enforce this check when the node believes that
its current.
Currency could be
=height>last_checkpoint&âŠ¤_timestamp>now()-safe_amount;  with
safe_amount to be high enough that it's very unlikely to be falsely
triggered by an improbably long gap.

@_date: 2011-09-30 12:32:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Multisignature transations 
The ease of omitting useful cases is why I was strongly supporting the
full RPN boolean validation, even though it's harder to get good
testing confidence.

@_date: 2011-09-30 13:29:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Multisignature transations 
I think 2 of 3 is a _far_ more useful example than (a or b), ?it is
the prototype for a normal escrow transaction., and still only results
in three address and at most two signatures like the (A and B) or C
You can also replicate the functionality of (a or b) in a hashish and
inefficient sort of way with two of three by simply using a public
known key as one of the roles.

@_date: 2012-08-13 11:07:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP: Custom Services 
I'm not opposed to that logic.  But for cases where an introduction mechanism
will be needed... it would be awfully good to have one, and I do think that
there is harm in making people think that simple services negotiation will
actually work for their needs for cases where a separate p2p network is

@_date: 2012-12-03 10:02:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
Pieter already commented on this, but it's so important it must be
said twice because everyone developing software for Bitcoin must
understand and internalize it:
Bitcoin is not a democracy, it's certainly not a democracy of miners.
Every full node independently and autonomously validates the rules of
the system without the influence of other participants.
Unfortunately, there is no universally consistent way to evaluate the
temporal ordering of transactions independently known? and none likely
to ever exist? and a digital currency requires ordering to resolve
double spends. Because of this Bitcoin must compromise the autonomy of
its validation slightly: It uses a computational majority to determine
transaction ordering. But only ordering!
This is essential because if all the rules were subject to the whim of
a computing majority the system would be far less trustworthy.  The
economic incentives which keep the mining participants honest depend
on the value of defection being as limited as possible.
So, no? you can't achieve by what you want with miners. Any miner
which applied your rules would instantly stop mining from the
perspective of Bitcoin users. As a miner myself, I welcome my
competition adopting your proposal :P.  You're looking for a hard fork
of the system.  Such a change must be supported by ~all users, and so
it must be something which has near universal consensus that it is
essential.  I think it's not essential? though I agree that better
UTXO set  size management would have been a useful component if it had
been in the origin economic promise of the system?  and I already know
that some participants take a principled position that views changes
to the mere spendability of outputs as _theft_.
Your proposal is also more economically hazardous than necessary: By
paying unmoved coins to miners you create a substantial incentive for
miners to delay processing transactions in the hopes that they expire
first.  There is also some risk that the return of large coins from
the past after the currency has substantially deflated would be
extremely economically disruptive.
As far as your concern? as opposed to the mechanism? I share it.  But
it's important to note that the source of most of the problem
transactions is a single source, and a rather unusual one that defies
the normal anti-spamming economic incentives by attracting mentally
ill people to subsidize pay for the bloating transactions, which are
already penalized.  I believe this specific issue can be adequately
addressed primarily through a three fold process:
(1) Make client software aggressive about sweeping up dust inputs:
"Any time a transaction is created that has change keep adding in
extra inputs? smallest to largest? until an additional one would
increase the cost of the transaction by 0.0001 BTC or more"  ? the
only major complication is doing this without concurrently harming
privacy which is why it's not done yet in the reference client.
(2) Change the default relay and mining rules to further penalize
transactions with very small outputs.  Making sure that its
practically possible to create inexpensive transactions right now is
very important for the long term success of the system while the small
size of the system makes it unattractive to use, but I don't believe
that applies for dust outputs.
(3) Change the default relay and mining rules to further incentive
reductions in the UTXO set size.  This would make the actions of (1)
save the participants funds instead of just being an altruistic
behavior that most do because its a default.
It might also be useful for client software to incorporate a "destroy
wallet" button for people with wallets that only have dust remaining
to send the private keys off to something of community benefit (e.g.
bitcoin foundation, the faucet, or the developers of that that client)
for recovery so that they don't perpetually bloat the UTXO set.
I expect that these actions would substantially address your concerns,
and even if they do not I believe that they would be the most basic
prerequisites for any kind of argument that something more drastic
(especially something that some would could consider theft!) is

@_date: 2012-12-03 10:07:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
It's part of their messaging system. Every losing play results in a
new 1e-8 output being created.

@_date: 2012-12-03 10:51:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
Yea, the obvious case would look for exact matches?  but because sane
software pays change to fresh addresses I expect this to trigger an
unfortunately infrequent amount of time.
Why do you bother with the 5 input limit?  The way I'd handle this in
the reference client is as a pure post-processing step. E.g. take any
w/ change transaction formed and try adding small inputs in a greedy
fashion until the next would change the fees. Do you see a reason not
to do this?
A next step up in infrequency is to use input taint tracking
information to gather up coins from all inputs which are already
crosslinked.  In the reference client, which doesn't avoid
crosslinking, this would likely be quite effective but I worry about
having an O(N^2)-ish algorithm as part of coin selection. And so I
think it would require maintaining in the wallet the cross link
history for each address rather than building it on the fly. This
seems like a lot of changes for a relatively modest optimization.
Another possibility would be to not apply the privacy rule to very
small inputs or to addresses which have only ever received a very
small sum total. But I don't know how to define very small in a robust
way, and I think that the privacy behavior of the software being
"inconsistent" from the users perspective would be somewhat
unfortunate.  Perhaps a setting for the value considered very small
for this purpose which defaults to the relay MINFEE?   (And also
include larger outputs when they're address matches).
The the problem with this is this:  Say I have an address 1GMaxwellFOO
 that everyone knows belongs to me. Someone who wants to identify all
my transactions sends me a constant spray of 1e-8 inputs to
1GMaxwellFOO.  If the address association is ignored (even for only
very small inputs) then all my transactions become rapidly
identifiable.    Privacy, of at least a basic form, is an important
element of the system, if it's not preserved than bitcoin is inferior
to traditional value transfer systems in an additional way.
(And FWIW, I've seen self-appointed sluths on IRC trying to catch
trouble makers by paying tiny amounts to their extortion addresses;
with the incorrect expectation that it would taint their other
transactions. So even when it doesn't usually work people have tried
using this to attack people)

@_date: 2012-12-03 15:14:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
While thats a fine thing? and a feature that I'd personally use? its
not one that I expect to have a real measurable impact on the overall
network behavior.
For this kind of minutia especially, defaults are all powerful and
must be the best they can be. :)

@_date: 2012-12-03 16:42:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
This brings up an additional point.  If we're mutually trusting
parties (or secured by some kind of external mechanism), and you've
given me a payment which I haven't broadcast for confirmation? and
later we make another transactions I should be able to offer you the
original unconfirmed txn and ask if you'd instead be willing to write
a replacement that combines both payments.

@_date: 2012-12-04 13:17:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
I strongly believe that if community leads with client software which
is not a full _capable_ node (e.g. which can begin life as a SPV node
but at least eventually become full if the system resources permit)
then Bitcoin will fail, or at least fail to be anything but the
world's most inefficient centralized payment system.  Obviously SPV
nodes are excellent tools for getting bitcoin into less capable
systems, but they aren't a general replacement for the software the
participants in Bitcoin run.
? Because the properties promised by the system can not be upheld if
there is only a fairly small number of self selecting nodes enforcing
the rules. If we wanted a system where its security against theft,
denial of service, and non-inflation were governed by the consensus of
{mtgox,blockchain.info, deepbit, bitpay, slush, btcguild, bitminter}
we could have something infinitely more scalable by just using
something OT like with a simple O(N) consensus between these parties.
No disrespect intended to any of these services? but a system whos
rules were only enforced at the good graces of a small number of
interested parties is not what the users of bitcoin signed up for.
People obviously care about supporting the goals and security of a the
system they use but actions speak louder than words.  If a
non-validating node is promoted then we're telling people that it's
not important that many people run them.  If running a full node
requires using different software (with a different interface) or a
much more painful initialization than another promoted option then it
will be correctly perceived as costly. If people perceive it to be
both costly and not important then rational participants will not run
it. The result will be fragile to non-existent security, where
dishonest or exploitative parties benefit from running all the full
nodes until they start ripping people off and shift the equilibrium
just a little towards running costly nodes.
It sounds to me that you're insisting that you're asking people who
oppose degrading our recommendations to commit to a costly rushed
development timeline. I think this is a false choice.
There is no set timeline for the adoption of Bitcoin? man has survived
eons without Bitcoin just fine? and there are many practical reasons
why slow adoption is beneficial, including reducing the harm users
experience from growing pains.  By allowing things to mature at their
own pace we can preserve the principles that make the system valuable.
If the new user experience is sufficiently bad (and I agree it's bad,
esp with the current release versions of Bitcoin-Qt) then that should
justify more support of work that improves it without compromising the
system. If it's not bad enough to apply those resources, then it's not
bad enough to justify compromising it: as this sort of change is hard
to reverse.

@_date: 2012-12-04 14:36:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
:(
No, not really. Somewhat easier due to some structural changes, but it
still needs to invent and get consensus on a normative data structure
and people need to write implementations of the required operations on
it (implementations probably required to prove performance for
consensus).  We still have to sort through the tradeoff of making a
_single_ data structure the normative merkle tree representation for
the UTxO set to the preclusion of other implementations? including
ones which are  asymptotically faster, such as a straight hash table.
There are also issues that need to be sorted out like key structure?
the most useful index for validation is txid:vout keyed, but Alan
wanted 'address' prefixed, which is not friendly for validation but
enables robust query by address? a query that the referce normal
bitcoin software doesn't even optionally support right now.  Any
disagreements on this point must be hammed out because the structure
would be normative.
The above said, that is all good stuff too. And I do thing starting
fast with reduced security (be it to SPV+ or SPV) is a good idea.

@_date: 2012-12-04 16:41:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
Marketing initiatives have limited windows.  This matters, perhaps,
when you're some VC pumping cash into a startup with the hopes of
being the next stockmarket pump and dump darling.  Outside of that
people use whatever they use because it works for them.
And by the numbers Linux desktops are more common than they've ever
been? and certainly Linux kernel _systems_ half the people I know have
one in their pocket and its hard to go more than a few hours without
touching one.  To some extent the "Year of the Linux desktop" is a bit
like the "Year of being able to turn lead into gold" ... we can turn
lead into gold now, but the particle accelerators, atomic power, and
atomic weapons enabled by the same technology are far more interesting
due to the particle realities of this. So we didn't get the ubiquitous
Linux desktop: We got the ubiquitious Linux server, the ubiquitous
Linux-kernel smart phone, the ubiquitous Linux television, media
player, HVAC controller, etc. instead.
Desktops? well, that didn't meet people's hopes though I think not for
the lack of marketing on the part of Linux, but because Apple stepped
up and produced middle ground products that attracted a larger
audience. Especially as MSFT dropped the ball. They did some things
better, had a running start, and had a non open source software
business model which made reaping rewards easier.
But I don't see how any of this has anything to do with Bitcoin...
Except for the point that if Bitcoin doesn't become the money system
everyone uses and instead becomes the money system infrastructure all
the systems people use depend on? just as Linux has with the desktop,
where it might not be on the desktop but its in router firmware, cloud
servers, and just about everything else? I wouldn't consider that much
of a loss.
Bitcoin already missed its first? and perhaps only? fad window in any
case. Today people say "Bitcoin? Thats still around? I thought it got
hacked". ... thanks to compromised centralized services.
Every man and his dog? Perhaps not.  But as many as can? probably so.
If we depend on the organic need for full nodes to overcome cost and
effort to run one there will always be major incentives to let someone
else do that, and the system would have its equilibrium right on the
brink of insecurity. Perhaps worse, since insecurity is most obvious
retrospectively. Security doesn't make for a good market force.
Tor is a distributed but controlled, by a small number of directory
authority operators, system.
It is a good system. But it has a trust model which is categorically
weaker than the one in Bitcoin.  If you want something where a
majority of a dozen signing keys? hopefully in the hands of trusted
parties? can decide the state of the system you can produce someting
far superior to Bitcoin? something that gives near instant
non-reversable transactions, something that gives good client security
without the complexity of a SPV node, etc.
But that isn't Bitcoin.
And yet every tor user? if the have the bandwidth available can be a
full internal relay and the software nags them to do it (and also nags
them to act as invisible bridges for blocking avoidance), and every
user is technically able to run an exit (though they don't bludgeon
users to do that, because of the legal/political/technical issues
involved).  To do any of this doesn't require a user to switch to
different software, and the tor project has previously opposed client
only software.
It's less different than you make it out to be? but it _is_ different.
  Bitcoin is a distributed currency. The value of bitcoin comes from
the soundness of its properties and from the persistence of its
security. If the integrity of the distributed ledger is disrupted the
damage produced, both in funds stolen and in undermining the
confidence of the system, can be irreversible. Because Bitcoin's value
comes from confidence in Bitcoin and not from the specific
functionality of Bitcoins (they're random numbers that sit on your
disk) even if the ledger isn't actually compromised but people
reasonably believe it could be compromised that undermines the value.
 Tor, on the other hand, is a functioning system whos value depends on
its current usefulness, and not the past or future security.
Compare in your mind? Say everyone just found out that at block
420,000 Bitcoin would stop enforcing signature correctness or block
subsidy values (and this wasn't going to be fixed), and you also found
out that one year from now Tor would hand over their sites, source
code repositories, and directory authority keys to Iran (and you have
no suspicion that they already had done so).   How fast would you stop
using Tor vs how fast would to sell whatever coins you could?
I don't think we really can send such a message.  Thanks just the same
as asking for donations, not completely unsuccessful but not easy to
make successful either.  You're arguing for people running distinct
software which has no capability to be a full node, and changing what
they're doing in order to support the network. This maximizes the
cost, because in addition to the real cost the user must take a
switching cost too, and deemphasizes investing in keeping the full
node software as usable because 'oh just run a lite node if the full
is too slow'.

@_date: 2012-12-04 19:27:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
There is no fundamental completion between taking what actions we can
to maximize the decentralization of the network and making the
software maximally friendly and painless to get started with and use.
It's possible? not even deep rocket science? to create software that
accommodates both.
And because of this, I don't think it's acceptable to promote
solutions which may endanger the decentralization that makes the
system worthwhile in the first place.  If the current experience is so
poor that you'd even consider talking about promoting directions which
reduce its robustness then thats evidence that it would be worth
finding more resources to make the experience better without doing
anything the that reduces the model, even if you've got an argument
that maybe we can get away with it.  If there isn't interest in
putting in more resources to make these improvements then maybe the
issue isn't as bad as we think it is?
Absolutely? and yet that has nothing to do with promoting software to
users which only consumes without directly contributing and which
doesn't even have the capability to do so even if the user wants to
(or much less, is indifferent).

@_date: 2012-12-04 21:54:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
Not so? a moderately fast multicore desktop machine can keep up with
the maximum possible validation rate of the Bitcoin network and the
bandwidth has a long term maximum rate of about 14kbit/sec? though
you'll want at least ten times that for convergence stability and the
ability feed multiple peers.
Here are the worst blocks testnet3 (which has some intentionally
constructed maximum sized blocks),E31230 :
(with the new parallel validation code)
- Verify 2166 txins: 250.29ms (0.116ms/txin)
- Verify 3386 txins: 1454.25ms (0.429ms/txin)
- Verify 5801 txins: 575.46ms (0.099ms/txin)
- Verify 6314 txins: 625.05ms (0.099ms/txin)
Even the slowest one _validates_ at 400x realtime. (these measurements
are probably a bit noisy? but the point is that its fast).
(the connecting is fast too, but thats obvious with such a small database)
Although I haven't tested leveldb+ultraprune with a really enormous
txout set or generally with sustained maximum load? so there may be
other gaffs in the software that get exposed with sustained load, but
they'd all be correctable. Sounds like some interesting stuff to test
with on testnet fork that has the POW test disabled.
While syncing up a behind node can take a while? keep in mind that
you're expecting to sync up weeks of network work in hours. Even
'slow' is quite fast.
Thats not generally concern for me. There are a number of DOS attack
risks... But attacker linear DOS attacks aren't generally avoidable
and they don't persist.
Of the class of connectedness concerns I have is that a sybil attacker
could spin up enormous numbers of nodes and then use them to partition
large miners.  So, e.g. find BitTaco's node(s) and the nodes for
miners covering 25% hashpower and get them into a separate partition
from the rest of the network. Then they give double spends to that
partition and use them to purchase an unlimited supply of digitally
delivered tacos? allowing their captured miners to build an ill fated
fork? and drop the partition once the goods are delivered.
But there is no amount of full nodes that removes this concern,
especially if you allow for attackers which have compromised ISPs.
It can be adequately addressed by a healthy darknet of private
authenticated peerings between miners and other likely targets. I've
also thrown out some ideas on using merged mined node IDs to make some
kinds of sybil attacks harder ... but it'll be interesting to see how
the deployment of ASICs influences the concentration of hashpower? it
seems like there has already been a substantial move away from the
largest pools. Less hashpower consolidation makes attacks like this
less worrisome.
Yes, I said so specifically.  But the fact that people are flapping
their lips here instead of testing the bitcoin-qt git master which is
an 1-2 order of magnitude improvement suggests that perhaps I'm wrong
about that.  Certainly the dearth of people testing and making bug
reports suggests people don't actually care that much.
No. The "question" that I'm concerned with is do we promote lite nodes
as equally good option? even for high end systems? remove the
incentive for people to create, improve, and adopt more useful full
node software and forever degrade the security of the system.
The current software patches plus parallelism can sync on a fast
system with luck network access (or a local copy of the data) in under
an hour.
This is no replacement for start as SPV, but nor are handicapped
client programs a replacement for making fully capable ones acceptably
Making the all the software painless for users is a great goal? and
one I share.  I still maintain that it has nothing to do with
promoting less capable and secure software to users.

@_date: 2012-12-04 22:23:37
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] String-based Hierarchical Deterministic 
As Pieter pointed out recently? it's not (realistically) possible to
blindly iterate through strings.  This means your proposal loses the
backup recoverablity property which is part the point of a
deterministic wallet:  If you have a backup prior to a new string name
being established you must also have a reliable backup of the string
as well.
Of course, if you're backing up the strings then you can also backup a
map equating the hdwallet indexes to your strings, and in the event of
a catastrophic loss where you are only left with the original ultimate
root you lose no coins (only metadata) with the BIP32 scheme. If,
instead, we have your scheme and the backup of strings is incomplete
then some or all assigned coin may be lost forever.
Your extended hierarchy of multiplers also makes me uncomfortable.
BIP32 uses a HMAC in its construction to obtain strongly unstructured

@_date: 2012-12-04 22:50:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] String-based Hierarchical Deterministic Keys 
If you have the the full extended secret there then you can spend
along the chain? but just the plain ecdsa secret by itself is not
enough to spend anything but that address itself.
Or have I misunderstood you here?
Sadly that construction has no ability to separate address generation
from spending? an important element for merchant applications.  Not
just for their own own distinguishing of transactions but because the
use of fresh addresses is essential to the limited privacy properties
of the Bitcoin system.
I called that a type-1 deterministic wallet in some old forum post
where I wrote about the different derivation schemes as opposed to the
point combining type-2 construction. The hope in BIP32 was that we
could get away just using a single one.

@_date: 2012-12-21 08:20:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Multiwallet support 
How about a rpc like "usewallet  "  that simply
generalizes all the rpcs?
And instead of explicitly deactivating rpcs that don't make sense,
simply have them return an error.  Or, for example,  sendtoaddress on
a watching wallet should actually return an unsigned raw transaction
and a wallet specific message that tells you where to find the private
I think it's desirable to not break compatibility but for this kind of
feature compatibility should not get in the way of doing it right.

@_date: 2012-12-22 13:45:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Testnet3 difficulty transition problem? 
Not so? but what you're actually seeing is that difficult change is
relative to the prior block's difficulty. E.g. if the penultimate
block in the difficulty cycle is under the special rule the difficulty
change will be relative to 1.
(I had intentionally avoided triggering that test case when adding the
timewarp attack to the testnet chain in case we had wanted to fix it
prior to testnet3's release? I guess I should have added it sooner in
order to catch the bitcoinj misbehavior!)

@_date: 2012-02-01 09:59:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Announcement: libcoin 
Very interesting. Do you know where this speedup came from?  It's not
typical for straight refactors that don't change datastructures and
the like to see such big speedups.
I see you have commented out code that disables fsync, which was my
first guess since I get big speedups from doing similar things.

@_date: 2012-02-02 12:12:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development]  Announcement: libcoin 
> The libcoin/bitcoind client downloads the
entire block chain 3.5 times faster than the bitcoin/bitcoind client.
This is less than 90 minutes on a modern laptop!
I'm guessing that you benchmarked this against the version you forked
from rather than the current reference client?
If so? I suspect your speedup was almost entirely because you removed
the secure allocator and as a result fixed the mlock performance bug
[ as a side effect. On
some systems the mlock issue makes a very big difference (on other
systems not so much).
In any case, I finally got libbitcoin built and I'm disappointed to
report that in the same time it takes the reference client to fully
sync, libbitcoin only made it to height 138k (of course, because the
time is mostly spent late in the chain 138k is not very far along? I'm
guessing it's going to take libbitcoin 3x-4x longer all said)
I assume the reason it's actually slower is because it's CPU bound on
ECDSA checks, which are skipped in bitcoin in blocks up to the highest
hardcoded checkpoint. ?Without that difference I suspect libbitcoin
would be about the same speed? maybe a little faster because of the
other changes you mentioned (though, e.g. lock profiling shows hardly
any contention during sync).
I don't doubt your rpc performance is a lot better. There is a
longstanding pull request for async rpc for the reference client that
hasn't been merged.

@_date: 2012-02-02 12:36:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Announcement: libcoin 
It ended up taking almost exactly twice as long, FWIW.

@_date: 2012-02-02 12:46:36
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Announcement: libcoin 
(and Gah: forgive the  autocompletion  of my fingers: I'm apparently
unable to type the word coin without prefacing it with bit)  *libcoin*
not libbitcoin.

@_date: 2012-02-03 08:45:30
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.5.2 tag in github ?? 
No developer politics are involved? the cuts to the stable versions
have always been done from Luke's repository simply because he
maintains it.

@_date: 2012-01-02 17:41:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] does "stubbing" off Merkle trees reduce 
Er, no?  if a node controls the private keys for a transaction, and
that transaction makes it into the chain then it can safely assume
that its unspent (at least once its buried a few blocks into the
chain).  This is the essence of a SPV node.
What it can't do is perform this function for txn which aren't its
own. Though the system could be extended in a compatible manner to
make this possible:

@_date: 2012-01-16 03:12:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] bitcoin.org SOPA/PIPA blackout 
Very few people actually care if they can load that particular URL ...
if you were talking about the forums it might matter more.   It also
might make sense to run some informative popup, except people are
going to be seeing them all over the internet on higher traffic sites.
E.g.:

@_date: 2012-01-17 01:15:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] bitcoin.org SOPA/PIPA blackout 
In addition to the concerns about internet freedom and domain name
system filtering which are against the interests of bitcoin users and
the bitcoin system generally, SOPA contains new requirements for
payment networks which may adversely impact Bitcoin services
businesses and limit their ability to do business in the US and other
places where similar legislation is adopted.  There are many millions
of potential Bitcoin users in the US, so US law matters for our
ecosystem even though far from all Bitcoin users are in the US
(21) PAYMENT NETWORK PROVIDER-
            (A) IN GENERAL- The term `payment network provider' means
an entity that directly or indirectly provides the proprietary
services, infrastructure, and software to effect or facilitate a
debit, credit, or other payment transaction.
                (i) PREVENTING AFFILIATION- A payment network provider
shall take technically feasible and reasonable measures, as
expeditiously as possible, but in any case within 5 days after being
served with a copy of the order, or within such time as the court may
order, designed to prevent, prohibit, or suspend its service from
completing payment transactions involving customers located within the
United States or subject to the jurisdiction of the United States and
the payment account--
                    (I) which is used by the foreign infringing site,
or portion thereof, that is subject to the order; and
                    (II) through which the payment network provider
would complete such payment transactions.
If you really want to go for the more extreme interpretation, it's not
hard to conclude that the Bitcoin system itself is a "payment network"
by the definition under the act, and if so in theory the AG's office
could? without due process? order miners and mining pools located in
the US to, for example, not process transactions containing the well
known addresses of targeted infringing sites (e.g. The Wikileaks
donation address).  Though I personally think this is far out.
I also think that other people will covered the SOPA/PIPA awareness
(e.g. Wikipedia is shutting down for 24 hours) more than we could
possibly do with our own resources.
But this attitude of it being someone elses problem? I think thats
nonsense. We live in _one world_, one world which is getting smaller
every day.  The value of a network?or of a economy? comes from the
number of potential connections it can make. One reason Bitcoin is
good is because it deconstructs some of the old barriers and anything
that risks imposing new ones is a threat to us all.
So, don't participate because bitcoin.org's help would be so small as
to be pointless? sure.  But because it doesn't matter? hardly.

@_date: 2012-01-29 00:19:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Quote on BIP 16 
...  No it's not a mistake.  P2SH _prevents_ needing long addresses.
Lets unpack the acronym "pay to script _hash_".  Hashes only need to
be 128-256 bits in size or so to have acceptable security, so you
don't need something longer than that for paying to a hash.
Note that gavin is saying 70 characters, not bytes.
Without some form of P2SH then only way for you to make a personal
choice of asking people to pay to a two-factor protected account or
two a multiparty trust that manages the finances of an organization
is using some form of "P2S", pay-to-script.
In other words, you'd have to have an address that encodes a full
script specification for the sender to pay to,  instead of just
encoding its hash.  As a result these addresses would be much longer
(and potentially very long).
The minimum size of a two address involving encoded script would be on
that order, but they get bigger quite quickly if you add more options
to the script (actually 70 sounds quite small, it should be more like
100 for a minimum two pubkey script).
In addition to the unworkability of very long addresses as described
by gavin (amusingly I am unable to copy and paste the quoted example
in one go) a P2S solution has several problems which you might
consider more or less important:
(1) They are highly vulnerable to invisible substitution.  E.g. I can
trivially take a P2S address, change one or two characters and get a
script which is redeemable by anyone.  With P2SH you have to do
computation which is exponential in the number of unchanged digits to
get a look alike address.
(2) The sender is fully responsible for fees related to the enlarged
transactions. Even if _you're_ willing to take the txn-processing time
and fee burden of a 30 person joint trust address,  random e-commerce
sites will not be and will randomly reject your addresses.
(3) They create another input vector for non-trivial data which must
be inspected and validated, potentially presenting an attack surface.
(4) They leave the complicated (long) release rules in the transaction
outputs.  When a transaction is mined we can't be sure if it will ever
be redeemed. The outputs are unprunable.   In a future world where
many nodes prune output space is far more important than input space
and it would make sense to require more fees for it because we're
never sure how long it would need to be stored (making it an
attractive target for someone who wants to make Bitcoin unusable by
spamming it with worthless data).  P2SH reduces output sizes to the
absolute minimum without inflating the total data size.

@_date: 2012-01-29 03:14:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Quote on BIP 16 
Be careful not to conflate multisig _addresses_  and P2S with multisig
output scripts in general.
Of the issues I raised only the size of the potentially unprunable
transaction outputs is an argument against multisig outputs which
aren't getting packed up in addresses.
Things like negotiated escrow arrangements can work okay either way.
I think P2SH is still better for these for two reasons: Reasonable
anti-spam behavior by network participant may make it hard to make
large output scripts (see above), but this isn't an issue yet...  and
P2S(H) lets you use a separate escrow-maker tool for clients paying
into escrow without any knowledge or support of escrow transactions in
that client. This uncoupling is important both for general "feature
velocity" as well as providing a uniform feature set across bitcoin
services (e.g. you negotiate paying someone via escrow, you use a tool
to make a mutually agreed escrow configuration, but your funds are in
MTGOX? no issue if P2SH is widely used).

@_date: 2012-01-30 21:57:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] CAddrMan: Stochastic IP address manager 
S?? would we remove it or leave it deactivated as a fallback users can turn on?
I have two different thoughts about IRC depending on the answer.
I think it's important that we have more mechanisms then just DNS and
hardcoded seednodes.
This is important because the mechanisms we have are all pretty
subject to blocking. Now? before you say it? Bitcoin isn't intended to
be blocking resistant (combine it with Tor and Tor anti-censorship
tools) but by making blocking a bit harder we discourage people from
even trying, even if we're not seriously in the anti-blocking
business? and it gives bitcoin users more confidence because there is
a bit less FUD  "What if your ISP blocks it?? It uses DNS! Someone
might take away the domains! SOPA PIPI ACTA CIPA Alakazam".
Is the fact that users can addnodes / addr.txt enough of an
alternative to address this?   _If so_, then removing it is a good
idea.  I volunteer to maintain a multi-channel joining node for the
foreseeable future to avoid letting old clients get partitioned
(several people need to do this).
An area where I think our mechanisms are inadequate absent IRC is
announcing new nodes. I had a new listener up for over a week recently
and was basically getting no inbound until I enabled IRC.   I
volunteer to do some measurement of this (e.g. bring up some nodes
with no irc and find out how long until sipa hears about them).  If
DNS seeds are slow to learn about new nodes we may need to add a
simple UDP announcement feature.
In any case, I hadn't been thinking that we would completely remove
IRC? I was expecting us to keep IRC around but turned off.
In particular I think it may be a little risky to turn off IRC at the
same time as deploying addrman, because if addrman has unexpected bad
behavior IRC is what may keep things going.  Obviously it should be
well tested enough to feel confident, but belt-and-suspenders is the
way to go.
If we do keep in the long run I think it's important to _fix_ IRC.
Right now it has some really stupid behavior which is highly
*/who only returns a few nodes, and because most idlers aren't
actually working (no port forward) it's usually for there to be only a
few that work. (I've never seen zero, but I've seen 1).
*Other than who we only learn about nodes when they join. But the
stable long lived nodes we need to hear about seldom rejoin. Nonuseful
windows boxes go up and down a lot.
*Nodes sit in a single channel forever. There are 100 of them.
Especially with fewer clients on line nodes may be sitting alone with
no correctly working nodes with them.
*Nodes recently seen on IRC are highly promoted in the peer selection.
So, here is an updated irc.cpp which I've been running (in various
versions) for a while:
It does the following things:
* Only stays connected for a half hour
* If its sure its not listening it uses a random nick so people won't
try to connect
* Reconnects if it needs more connections
* If the node is actually listening (evidence by actual incoming
connections) it reconnects on its own every 1-2 hours and joins two
channels at random rather than one.
(it doesn't change peer selection? It's hard to be confident of the
impact of that change. I think addrman makes it less of an issue)
I've only not submitted it as a pull request because I haven't had a
chance to test to my standards, and because I felt unsure about the
future of IRC.
I feel strongly that if we're going to keep IRC as a backup we should
fix it. If we're not going to bother then thats fine? but I think we
need to think carefully if we're doing enough for bootstraping (with
the points I made) without it.
Certainly getting it off by default would be a good move. The botnet
allegations are horrible.

@_date: 2012-01-31 02:17:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] CAddrMan: Stochastic IP address manager 
Meh, careful not to mixup addrman created issues with preexisting ones
simply related to the number of connections vs the number of nodes.
Even absent addressman someone who can spin up a large multiple of the
current nodes as tcp forwarders to a system they control can capture
all of a nodes outbound connections.
Increasing the number of outbound connections is a very bad solution
to this problem: It invites a tragedy of the commons: you get the
"best" security by setting your number as high as it will let you. Who
doesn't want security?   Meanwhile we've come pretty close to running
out of open listening ports already in the past.
There is a much more scalable improvement for those concerned about
the sybil attack (I say those concerned because a sybil attack is not
that fatal in bitcoin? checkpoints prevent a total fantasy chain, it's
mostly  but not entirely a DOS risk)...
The solution is to addnode a couple of (ideally) trusted nodes, or
failing the availability of trusted nodes, a few that you think are
unlikely to be mutually cooperating against you.
A single connection to the 'good' network kills isolation attacks
dead, so a couple carefully selected outbound connections its a more
secure remedy and one which doesn't explode the network.

@_date: 2012-01-31 09:59:26
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 20 Rejected, process for BIP 21N 
Mixed bag of worms there, even ignoring what people have already
implemented? if you make it use satoshis people who are working with
things at COIN scale are inevitably going to end up multiplying
numbers stored as radix-2 floating point to get satoshis and then are
going to be confused when it comes out "wrong".
Using decimal numbers at least lets them treat the values as strings
and avoid arithmetic that will end up confusing them.

@_date: 2012-01-31 10:02:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 20 Rejected, process for BIP 21N 
Lots and lots of people do.  Go place a sell order on mtgox for
per BTC and look at the awesome doublemax trade it actually stores for

@_date: 2012-01-31 12:45:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP16/17 replacement 
I think you've been deceived by people who have some interest in
promoting this as some sort of big controversy, or perhaps just
confused by the general level of noise.
The differences between BIP16/BIP17 are technically obscure, everyone
who is well versed in the issue (with the potential exception of
Luke). There is broad consensus among the involved technically minded
parties over just about all of it.
Luke has been maintaining an opinion tracker page:
reflecting the views of core developers and people who've been
technically involved enough to have an informed opinion.
There is always a different color that the shed could be painted.
Expecting absolute consensus on the _best_ way forward is an
unreasonable standard, especially if you're going to invite the
opinions of many people.
Depending on how you count we have considered a good two dozen options
in this space?  Starting with the OP_CAT key combinations many months
back, and including many variants of the current ideas. The BIPs only
represent the "final" surviving ideas.
In particular, BIP16 was the isolated consensus path forward that came
out of the discussions about the concerns that BIP12 was too
computationally powerful? I don't think I can identify any particular
person as the author of the BIP16 idea.  At the the time BIP16 became
a BIP only Luke was actively objecting to it.
Though his hard work and tireless (...unstoppable dogmatic) promotion
he's managed to build a workable alternative, and it now has some
support other than himself.  This, however, doesn't constitute a
material schism.
As always, asbestos underwear is required.
It does not, in fact? Yes, it requires a client update to make use of
the new functionality, but old nodes will happily continue to validate
things.  It's hard to express how critical this is distinctly.
Bitcoin is, predominantly, a zero-trust system. Nodes don't trust that
things were done right, the validate them for themselves.
A breaking change of the kind you suggest is not something that would
be considered lightly, and this is certainly not justified for this.
If we ever were to scrap the system, I think we very much would do
something like what you describe here... and as much has been
(see "Elimination of output scripts")
But, to be clear, this stuff is pretty much fantasy. I'm doubtful that
it will ever happen, doubtful that we can get the kind of development
resources required to pull off a true breaking change in a way that
people would actually trust upgrading to? at least not before a time
that the system is simply too big to make that kind of change.

@_date: 2012-07-06 13:19:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Pruning in the reference client: 
Pieter's performance numbers are a bit conservative if anything?
profiles on ultraprune[1] show that the reference client's wasting of
a lot of time in redundant serialization and hashing operations is
the major time sink. Once thats cleared up it should be quite a
bit faster
[1] In particular,  if the BDB indexing in ultraprune is replaced with
a hash committed tree structure who's root is committed in the
blockchain you then have a txout commitment scheme.
Ultraprune is most of the messy structural work for that. The
rest is mostly storage differences.

@_date: 2012-07-06 16:10:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 34: Block v2, Height in Coinbase 
It's important to note that bip 30 doesn't prevent duplication, it
just prevents the identified really evil outcome of the duplication.
There was discussion on doing the height _before_ that, but the
realization that the rewrites were a real vulnerability made it urgent
and rolling out the height will require time while the bip30 change
could be deployed more quickly.

@_date: 2012-07-09 09:46:02
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Wallet for Android 
I agree too.  Not that being open is _any_ guarantee, ideally we'd
want standards
of review and testing, but thats a bit much to ask for right now.

@_date: 2012-07-09 10:00:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Wallet for Android 
I've reverted these additions to the page, nothing personal but?
At the moment I'm strongly opposed to including any non-reviewable
client options (including centrally operated web services) on the
page, and I think this need to be discussed along with establishing

@_date: 2012-07-09 10:12:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Wallet for Android 
Er, to be clear, I left the android software in because the source is
available (And I'm told its had some review).
I removed the proprietary software section the plug for the
blockchain.info webservices, and the demotion of the armory client.
As far as criteria goes, I don't think we should list anything with a
security model weaker than SPV unless users can practically operate
their own servers. ?and even that I'm a little uneasy with, because
most people will use the defaults. Ideally even thin clients would
have a near SPV security model, just without the bandwidth. But since
the alternative for thin clients is centralized web services the lower
standard will probably have better net results for now.
Nor do I think we should list anything which can't currently be
subjected to independent review of the whole stack (e.g. including the
server components in thinclients, unless the server is untrusted). In
the future this should be raised to there existing actual evidence of
third party review.

@_date: 2012-07-09 12:04:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Random order for clients page 
If you had authored this as a pull request rather than making the
change unilaterally I would have recommended leaving it so the
reference client was always first. I also would have suggested that it
use JS randomization instead of jekyll in order to get more even
coverage, though I think thats a more minor point.
Some people were concerned when this page was created that it would
just be a source of useless disputes.  I think its becoming clear that
this is the case. I think the cost of dealing with this page is
starting to exceed the benefit it provides and we should probably
consider removing it.

@_date: 2012-07-09 13:46:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Random order for clients page 
JS randomization doesn't imply needing JS to view the page. It implies
needing JS to see it in random order.  You could also combine it with
the server-side randomization if you care about non-js being non
random, though I don't think it matters.
As others have pointed out I don't generally think the randomization
is good in principle, but if its done it should at least achieve its
I'll let other people speak for themselves, but I did consult others
before reverting your last batch of changes.
More generally, we have pull requests in order to get some peer review
of changes.  Everyone should use them except for changes which are
urgent or trivially safe.  (Presumably everyone with access knows how
to tell if their changes are likely to be risky or controversial)
I'm strongly supportive diversity in the Bitcoin network, and some alt
client developers can speak to the positive prodding I've given them
towards becoming more complete software. If I've said anything that
suggests otherwise I'd love to be pointed to it in order to clarify my
Unfortunately none of the primary alternatives are yet complete, the
network would be non-function if it consisted entirely of multibit or
electrum nodes (and as you've noted armory uses a local reference
client as its 'server').  The distinction between multiple kinds of
clients in terms of security and network health are subtle and can be
difficult to explain even to technical users and so until something
changes there the reference client needs to be the option we lead
with. People should us it unless their use-case doesn't match. When it
does they'll know it and they'll be looking. We don't need to make one
of those recommendations a primary option.
I like the proposals of moving this stuff to the Wiki as the wiki
already contains tons of questionable (and sometimes contradictory)
advice and so there is less expectation that placement there implies
any vetting.

@_date: 2012-07-09 14:48:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Random order for clients page 
Yes, because I reverted eight commits to it by you because they were
clearly controversial, including the proprietary clients section and
You went on to add the randomization, again without a pull request
and, as seen here, its somewhat controversial.
Luke originally authored the multiple clients page. It sounded like it
could be useful and I made some recommendations for it too.  I'm
concerned that it's not working out that well. Thus "we should
probably consider".  Perhaps that came off as too strong.  If I really
pushing for that I'd submit it as a pull request. (and everyone,
including the people you listed, could comment)
I think the fact that we can just remove it if we can't agree on it is
a useful point to the discussion.  For the site to be a neutral
resource it should be conservatively operated and if sometimes being
neutral, safe, and conservative gets in the way of being complete then
we should choose those other things over completeness. There are a
great many other resources available, bitcoin.org will never contain
all the relevant knowledge.
Crazy. I have considerable evidence to the contrary, in fact. The wiki
is widely used and promoted as the primary community memory.
I certainly didn't agree with that suggestion because I thought it
wouldn't get seen. I found it agreeable because it would reflect the
lower degree of consensus we apparently have about listing the page.
I've used multibit, armory, and electrum (though not for some time). I
shed painted the electrum determinstic wallet stuff pretty extensively
when it was first created, and I think the wordlist seed stuff was my
I'd like to invite you to point your electrum client against a server
I operate.  I will then happily agree with you that it is more secure:
because the bitcoin I rob from you will soothe my pain at the loss of
this "debate".  Sound like a deal?
I think you're exaggerating the features there, and simultaneously
underplaying the fact that clients doesn't actually participate in the
bitcoin protocol, don't provide the security promises of bitcoin, and
basically leave us with a centralized system (if thats all we had).
It's a worthwhile part of the ecosystem, I agree.
There has been QR integration in bitcoin-qt for some time. ::shrugs::
I don't really understand why you're arguing features here: Yes the
other clients are great things. I never said they weren't.  They are
not, however, complete alternatives to the reference client yet.
Please stop putting words in my mouth. I certainly don't think that.

@_date: 2012-07-09 23:05:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Random order for clients page 
I'm generally not a fan of feature matrixes, they encourage "checkbox
decision making"? which is seldom very good for the decider, though
it's much loved by the marketing department that puts together the
matrix.  But just becase something is loved by marketing departments
for its ability to set the agenda in variously biased ways doesn't
mean its a great thing to emulate.
Take the matrix Luke linked to for example[1].  Now imagine that we
tunnel MyBitcoin from a year ago and drop it into that table.  It
would have every light green, except 'encryption' (which wouldn't have
been green for bitcoin-qt then either). It would basically be the
dominant option by the matrix comparison, and this is without any
lobbying to get MyBitcoin specific features (like their shopping chart
interface) added, not to mention the "_vanishes with everyone's
money_" feature.
I don't think I'm being unreasonable to say that if you could drop in
something that retrospectively cost people a lot into your decision
matrix and it comes out on top you're doing something wrong.
In tables like this significant differences like "a remote hacker can
rob you" get reduced to equal comparison with "chrome spoiler",  and
it further biases development motivations towards features that make
nice bullets (even if they're seldom used) vs important infrastructure
which may invisibly improve usage every day or keeps the network
secure and worth having.  "Of course I want the fastest startup! Why
would I choose anything else?" "What do you mean all my bitcoin is
gone because the four remaining full nodes were taken over and reorged
it all?"
I wouldn't expect any really important features which don't have
complicated compromises attached to them to be omitted from all
clients for all that long.
Basically matrixes make bad decision making fast, and by making it
fast it's more attractive than careful decision making that always
takes time.  The text is nice because it contextualizes the complete
feature set and helps you understand why different clients exist, what
problems they attempt to solve, and what compromises they make. ...
without making the unrealistic demand of the user they they know how
to fairly weigh the value of technical and sometimes subtle issues.
[1]

@_date: 2012-07-23 03:35:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Scalability issues 
Please fix your software stack. Something is wrong with your system
and I doubt it has much to do with bitcoin. A full sync here takes
something like an hour.
There are certainly lots of scalability things going on, but there is
no cause for concern for regular hardware being unable to _keep up_
without a hardforking change to the protocol first.

@_date: 2012-07-26 08:50:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin script opcode counts 
The OP_DEPTH are all screwups in P2Pool blocks.
( the software was making tiny payments to scriptpubkey 'script' due
to a bug, and it went unnoticed for a long time because it was assumed
that it was just some p2sh user intentionally being stupid )

@_date: 2012-07-27 00:59:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Scalability issues 
I now have an 1.8 ghz p3 celeron (128k cache) which should be
substantially slower than your machine, running vintage 2.6.20 linux.
Unfortunately I forgot to turn on timestamp logging so I don't know
how long it took to sync the chain, but it was less than two days as
that was the span between when I checked on it. It's staying current
just fine.
Again, I encourage you to investigate your software configuration.

@_date: 2012-07-27 02:56:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Scalability issues 
The file system is using twofish-cbc-essiv:sha256, apparently.  (I
went and dug up a mothballed machine of mine because of your post).
And I agree, encrypting everything is a good practice? I once got a
disk back from RMA where only the first sectors were zeroed and the
rest had someone elses data, since then I've encrypted everything
because you can't wipe a dead drive.
I'd love to know precisely what Bitcoin is doing thats making your
machine so unhappy... but your configuration is uncommon for bitcoin
nodes in many distinct ways so it's not clear where to start.

@_date: 2012-07-30 16:32:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 22 - getmemorypool 
It actually took place on this list:
It just took some IRC prodding to get Luke to move in the direction of
breaking it up to get the optional parts that Pieter objected to out
of the spec.
Perhaps other people missed it too... so discussing it more sounds
fine if anyone objects.

@_date: 2012-06-11 11:39:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bootstrapping full nodes post-pruning 
I'm not a fan of anything which introduces unauditable single source
material.  "Trust us" is a bad place to be because it would greatly
increase the attractiveness of compromising developers.
If we wanted to go the route of shipping pruned chains I'd prefer to
have a deterministic process to produce archival chains and then start
introducing commitments to them in the blockchain or something like
that.   Then a client doing a reverse header sync[1] would bump into a
commitment for an archival chain that they have and would simply stop
syncing and use the archival chain for points before that.
This would leave it so that the distribution of the software could
still be audited.
More generally we should start doing something with the service
announcements so that full nodes that don't have enough bandwidth to
support a lot of syncing from new nodes can do so without turning off
[1]

@_date: 2012-06-11 16:43:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bootstrapping full nodes post-pruning 
HAHAHA.   Have you consider doing comedy full time?
Actual BDB files are absolutely not deterministic. Nor is the raw
blockchain itself currently, because blocks aren't always added in the
same order (plus they get orphans in them)
But the serious inter-version compatibility problems as well as poor
space efficiency make BDB a poor candidate for read only pruned
The binaries distributed by bitcoin.org are all already compiled
deterministically and validated by multiple independent parties.  In
the future there will be a downloader tool (e.g. for updates) which
will automatically check for N approvals before accepting an update,
even for technically unsophisticated users.
This will produce a full chain of custody which tracks the actual
binaries people fetch to specific source code which can be audited, so
substitution attacks will at least in theory always be detectable. Of
course, you're left with Ken Thompson's compiler attack but even that
can be substantially closed.

@_date: 2012-06-15 12:53:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development]  Near-term scalability 
[I originally sent an earlier version of this message to Mike off
list, but I figure it's worth adding to the public discussion]
By itself letting the size float has non-trivial existential risk. ?A
Bitcoin with expensive transactions due to competition for space in
blocks can be front-ended with fast payment systems and still provide
the promised decentralized currency. Bitcoin with a very large
blockchain and blocks does not. ?It would do the bitcoin users no good
to increase the transaction volume while concurrently making Bitcoin
more or less pointless over the alternatives.
Scalability must be improved, we can unite on that opinion. ?But
scalability can't come at the expense of what made Bitcoin worth
having in the first place.
Fortunately it appear to be possible to greatly increase the
scalability without compromising on keeping the costs of operating a
fully validating node very low, ?for example Pieter's experimentation
with txout+txid indexing (for the 'flip the chain' proposals)
indicates that the data required right now to validate further
transactions is only about 85MiB? and that would be somewhat smaller
with compression and with clients which intentionally try to reduce
the set of unspent transactions. ? Commitments to these indexes in the
chain would allow almost-full validating nodes with fairly limited
resources.  (Almost-full meaning they would not validate the history
long before they started, they'd trusted header difficulty for that. They
could still mine and otherwise act as full nodes).
Achieving scalability improvements without breaking the radical
decentralization will be a lot harder than just improving scalability
but it's effort that is justified if the scalability is actually
How much decentralization is needed in the end?  That isn't clear? "As
much as possible" should generally be the goal.  Modern currencies
aren't controlled by single parties but by tens of thousands of
parties locked in economic, legal, and political compromise that
limits their control.  In Bitcoin the traditional controls that keep
parties honest are non-existent and if they were just directly applied
we'd potentially lose the properties that make Bitcoin distinct and
useful (e.g. make all miners mine only with FED permission and you
just have a really bandwidth inefficient interface to the dollar).
Instead we have aggressive decentralization and autonomous rule
Mike pointed out that  "Before he left Satoshi made a comment saying
he used to think Bitcoin would need millions of nodes if it became
really popular, but in the end he thought it could do fine with just
tens of thousands"    I'm not so sure? and I think the truth is in
between.  Tens of thousands of nodes? run by a self-selecting bunch of
people who reap the greatest rewards from controlling the validation
of Bitcoin, who by that criteria necessarily have a lot in common with
each other and perhaps not with the regular users? could easily be an
outcome where control is _less_ publicly vested than popular
government controlled currencies.   We probably don't need the raw
numbers of nodes, but we need a distribution of ownership and a
distribution of interest (e.g. not a system by bankers for bankers) of
those nodes which I think can only be achieved by making them cheap to
operate and having a lot more than we actually need. ? though not so
much that it has to run on every laptop.
The core challenge is that the only obvious ways to justify the cost
of maintaining expensive validation infrastructure is because you
intend to manipulate the currency using it or because you intend to
prevent other people from manipulating the currency.  The latter
motivation is potentially subject to a tragedy of the commons? you
don't need to run a full validating node as long as 'enough' other
people do, and enough is a nice slippery slope to zero.   Right now
just the random computers I? some random geek? had at home prior to
Bitcoin could store over a hundred years of max size blocks and
process the maximum rate of transactions.   With the costs so low
there isn't any real question about a consolidation of validation
making Bitcoin pointless.  You could probably increase the scale 10x
without breaking that analysis  but beyond that unless the
cost-per-scale goes down a highly consolidated future seems likely.
40 years from now why would people use Bitcoin over centralized
private banknotes like paypal or democratic government controlled
Perhaps Bitcoin transaction could transition to being more of the
same? controlled by a consortium of banks, exchanging gigabyte blocks
over terabit ethernet, but I think that would be sad.  An alternative
which was autonomous and decentralized even if the transactions were
somewhat slow or costly would be excellent competition for everything
else, and it's something I think man kind ought to have.

@_date: 2012-06-15 14:55:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Near-term scalability 
I would take the last block I solved and use it to write a transaction
to nowhere which which gave all 50 BTC out in fee.  This pays for as
many transactions in the block as I like for any value of X you want
to choose.
You should read the bitcointalk forums more often: variants on that
idea are frequently suggested and dismantled. There is a lot of noise
there but also a lot of ideas and knowing what doesn't work is good

@_date: 2012-06-16 19:39:00
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys 
ACK.  Hopefully no one will mine these before we can merge denying
them into another rule change. But if they do, oh well.

@_date: 2012-06-17 06:17:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.6.x - detachdb in wrong place 
It isn't inside the ifdef in bitcoin git master.
(For future reference this sort of request is probably best opened as
an issue in the github issue tracker instead of posted to the list).

@_date: 2012-06-17 17:52:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.6.x - detachdb in wrong place 
The latter is Luke's backports of security and stability fixes to
otherwise unmaintained old versions.

@_date: 2012-06-17 20:07:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.6.x - detachdb in wrong place 
0.6.2 is very widely used, more so than the other acceptably updated backports.
I can't speak for anyone but myself:
I don't run master on wallets with large amounts of (non-testnet) coin
in them, except for a few times when I needed access to this feature
or that or just in a isolated capacity for testing.  In any use with
real wallets I'd be sure to have good backups that never touched the
new code.
We have at various times had bugs in master that would corrupt wallets
(though IIRC not too severely) and have bugs that would burn coin both
in mining and in transactions (though again, I think not too
severely).  My caution is not due to the risk being exceptionally
great but just because there is probably no remedy if things go wrong,
this caution is magnified by the fact that we don't currently have
enough testing activity on master.
Testnet exists so that people can test without fear of losing a lot of
funds and with the 0.7.0(git master) testnet reboot it should be more
usable than it has been.   It would be very helpful if anyone offering
bitcoin services would setup parallel toy versions of your sites on
testnet? it would bring more attention to your real services, it would
give you an opportunity to get more testing done of your real
services, it would show some more commitment to software quality, and
it would let you take a more active role in advancing bitcoin
development by doing a little testing yourself that you couldn't do on
your production systems.

@_date: 2012-06-17 20:24:27
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Block preview for faster relaying 
Right now we're seeing cases where block propagation is sometimes
taking minutes.
This doesn't cause much of a problem for general Bitcoin users but for
miners its problematic because it potentially increases the risk for
There are probably many contributing factors which can be improved
here but one of the most obvious is that nodes fully validate blocks
before relaying them. The validation is IO intensive and can currently
take a minute alone on sufficiently slow nodes with sufficiently large
blocks and larger blocks require more data to be transmitted.  Because
this slowness is proportional to the size of the block this risks
creating mismatched incentives where miners are better off not mining
(many) transactions in order to maximize their income.
The validation speed can and should be improved but there is at least
one short term improvement that can be made at the protocol level:
Make it possible to relay blocks to other nodes before fully
validating them.
This can be reasonable secure because basic validation (such as the
difficulty, previous block identity, and timestamps) can be done first
so an attacker would need to burn enormous amounts of computing power
just to make very modest trouble with it... and it's a change which
would be beneficial even after any other performance improvements were
Luke has been working on a patch for this:
One aspect of it that I wanted to see more comments on was the use of
a new message for the preview-blocks instead of just announcing them
like normal. The reason for this is two-fold: To prevent existing full
nodes from blacklisting nodes sending a bad preview block due to the
existing misbehavior checks, otherwise an attacker could burn one
block to partition the network,  and also so that SPV nodes which
aren't able to fully validate the block themselves can opt-out or at
least know that the data is not yet validated by the peer.
I don't see any better way to address this but I thought other people
might have comments.

@_date: 2012-06-19 13:59:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Ultimate Blockchain Compression w/ 
This is why good comprehensive tests and a well specified algorithim
are important. The tree update algorithm would be normative in that
scheme. Worrying that implementers might get it wrong would be like
worrying that they'd get SHA256 wrong.
Provable libJudy trees. Oh boy.

@_date: 2012-06-21 18:02:27
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Ultimate Blockchain Compression w/ 
We're talking about commitments to the state of _unspent_ transactions
which would allow ~memoryless nodes to engage in full validation
without having to trust anything with the help of some untrusted
non-memoryless peers.  Also, talking about being able to securely
initialize new pruned nodes (not memoryless but reduced memory)
without exposing them to the old history of the chain. In both cases
this is possible without substantially degrading the full node
security model (rule violations prior to where they begin are only
undetectable with a conspiracy of the entire network).
But it requires a new data structure for managing these trees of
unspent transactions in a secure, scalable, and DOS resistant manner.
Fortunately there are lots of possibilities here.
Yes. Though this is obviously not an ultra short term thing.

@_date: 2012-06-24 14:03:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Enforcing inflation rules for SPV clients 
I also proposed this on this list (see the response in the tree
datastructures thread) along with more elaboration on IRC. If multiple
people are coming up with it thats a good sign that it it might
actually be viable. :)
I was going for a slightly different angle and pointing out that the
proofs would mean that a node doing validation with TxOUT tree which
hasn't personally wittnessed the complete history of Bitcoin actually
has basically the same security? including resistance to miners
creating fake coin in the past? as a full node today because in order
to get away with a lie every single node must conspire: It's adequate
that only one honest node wittness the lie because once it has the
proof information is hard to suppress.
To save people from having to dig through the public IRC logs for what
I wrote there:
--- Day changed Thu Jun 21 2012
15:10 < gmaxwell> etotheipi_: amiller: an interesting point with all
this txout tree stuff is that if you join the network late and just
trust that the history is correct based on the headers, any other node
who has witnessed a rule violation in the past can prepare a small
message which you would take to be conclusive proof of a rule
violation and then ignore that chain.
15:11 < gmaxwell> e.g. if someone doublespends I just take the
conflicting transactions out and the segments connecting them to the
chain... and show them to you. And without trusting me you can now
ignore the entire child chain past that point.
15:13 < gmaxwell> This fits nicely with the Satoshi comment "It takes
advantage of the nature of information being easy to spread but hard
to stifle" ...  it would be safe to late-join a txout tree chain,
because if there is only a single other honest node in the world who
was around long enough to wittness the cheating, he could still tell
you and it would be as good as if you saw it yourself.
15:17 < gmaxwell> (this is akin to the provable doublespend alert
stuff we talked about before, but applied to blocks)

@_date: 2012-06-26 20:14:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Tor hidden service support 
Perhaps some argument to add blocks to the IsRoutable check is in
order?  Then people who use overlay networks that are actually
routable but which use otherwise private space can just add the
relevant blocks.
Note that while the hidden service support in bitcoin uses a
compatible IPv6 mapping with onioncat,  it is _not_ onioncat, does not
use onioncat, does not need onioncat, and wouldn't benefit from
onioncat.  The onioncat style advertisement is used because our
protocol already relays IPv6 addresses. The connections are regular
tor hidden service connections, not the more-risky and low performance
ip in tcp onioncat stuff.

@_date: 2012-03-01 09:27:53
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
I am not following you here, can you explain what you're thinking?
Because this would make it impossible for nodes to prune the vaules.
They'd all forever have to keep a set of all the coinbase hashes in
order to perform the test. The height-in-coinbase BIP will make
duplicates effectively impossible to create, which is a much more
clean behavior.

@_date: 2012-03-06 19:05:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd: Proposal for a new opcode 
I believe I understand what the opcode does directly? it just
validates an opaque signautre. I don't understand how it enables
anonymous transactions.
Can you spell this out for me?
In particular I don't see why it is not, from the perspective of the
blockchain, isomorphic to a hash locked transaction.   (This
equivalence is more obvious when you think about how lamport
signtures turn simple hashing into a one time signature).

@_date: 2012-03-21 15:54:30
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd: Proposal for a new opcode 
Here is an alternative protocol:
N parties wish to purchase equal amounts of Bitcoin without the
exchange being able to link their future transactions, they each put
the relevant amount of gold/whatever up at the exchange.
The exchange provides the exchanges public key, and the user provides
a public key for signing.   Externally the N participants agree on a
collection of non-cooperating mixers (the mixers may actually just be
the participants themselves, independent third parties, etc).   Each
participant generates a new bitcoin address, and encrypts it with the
the public keys of the the exchange and all the mixers using an
appropriate communicative homorophic scheme (or just a layers stack of
regular encryption keys).  The participants then combine their
encrypted addresess into a block and hand it off to the mixing chain.
Each mixer randomizes the order and decrypts all the messages with its
At the end of the chain the exchange does the final decryption and
presents a list of addresses to the involved users.  Users validate
that their address is in the set and sign the entire set.  Once all
involved users have signed, the exchange pays.
This requires no changes to the Bitcoin system and could be trivially
implemented by anyone interested.  It provides anonymity which is
strong so long as any one of the mixers is uncompromised.  It has very
low overhead.   It is not directly resistant to disruption, but if
participation in an identified round requires a key provided by the
exchange, abusive users can be detected and excluded.
Have I explained this clearly enough? I could probably implement the
whole system it if its unclear.
Can you contrast this with your proposal for me?

@_date: 2012-03-21 20:49:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal for a new opcode 
If the final step fails (someone says their address is missing) you
challenge the mixes to disclose half of their correspondences. You can
then prove which (if any) mixes defected.
Why I didn't bother elaborating is ... I think you can even avoid the
fancy protocol where you must take care to only disclose alternating
halves at each mix because the addresses are throwaway: If the it
fails in the final stage everyone publishes _everything_ and the
cheater is instantly and provably identified and can be excluded from
the next attempt which is then performed using totally new addresses
and the disclosed addresses are never used.  Care would need to be
taken to avoid fake-failures (e.g. the exchange says 'it fails'
triggering disclosure then sending anyways? but the participants could
prove this cheating and stop using the exchange), I think there isn't
much risk there if the participants are themselves the mixes.  I need
to think this through a bit more.
It's not something I thought about? In general the P2SH tends to be
a superset of other schemes, e.g. you can do a signature to prove you
access to a private key, then you can show someone a script using that
key to show control of a P2SH address.
There are lot of interesting things you can do with bitcoin if you can
construct (potentially interactive) proofs for knowing the preimages of hashes.

@_date: 2012-05-02 09:30:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] new bitcoin.org clients page 
What computer is the initial start time 24-hours+ now?   On normal
systems initial sync-up now takes a couple hours.  It could be slower,
of course, if you have the bad luck to end up with unresponsive peers?
but that will also make the SPV nodes slow.
Better to be conservative I agree, but calling it a dozen times longer
than I'd expect is perhaps a bit much.
Refine refine refine.

@_date: 2012-05-02 09:38:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] new bitcoin.org clients page 
Bitcoin-qt is translated into a pretty broad set of languages (now? I
cant tell you how many of them are _good_). Listing language just
under multibit makes it sound like a distinguishing characteristic.
Might it be useful to add two info lines to each entry:  One with the
language codes it supports (ISO 639 please, not flags),  and another
line with operating system support? (perhaps not, they're all
win/mac/linux, enh?)   These are both things which are particular
suitable to clear objective enumeration.

@_date: 2012-05-24 21:00:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Punishing empty blocks? 
It's important to understand the motivations before acting? otherwise
you'll fail to do anything useful.
E.g. if they're empty because some miners want to drive up fees or
fight against the rapidly increasing blockchain size there isn't much
you can do there.
If they're empty because they're mined by botnets which don't have a
local copy of the chain in order to load their victims less (and avoid
central pooling) then you want something like
If they're produced by people who think they gain a mining speed
advantage by not including them then then we need education? dropping
their blocks won't help much: we've seen miners go a month with 100%
of their blocks being orphaned.

@_date: 2012-05-29 11:33:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Punishing empty blocks? 
In the last 2016 blocks, as I write this, there are only 35 1 txn blocks.
This is about 1.73%, which wouldn't be surprising just from timing
alone.  Moreover, a fair amount (I didn't measure the percentage)
appear to be mined by Eligius? Luke does some clever pre-computation
of the hash tree for faster distribution right after new blocks.
Resources expended on fancy (and potentially risky) techno-economic
hacks to discourage empty blocks would probably be better spent
writing very fast transaction tree generating code.
Can we kill this thread now?

@_date: 2012-11-08 08:07:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] IRC meeting agenda, 18:00 UTC Thursday 
I agree on getting the bloom filtering stuff in for 0.8, though I
don't think it'll need any marketing at all? ultraprune speaks for
itself. :P
I'm also concerned about overselling it for miners and merchants when
the ultraprune stuff is such a major change.
Since the current changes will just need a lot of testing and soaking
time which is pretty much independent of RPC and GUI changes it might
be unfortunate to feature freeze those things and then have a long
delay just on QA for the other stuff.  I do think we need to talk
about what we think we need to o to get what we have now ready for

@_date: 2012-11-15 18:45:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum security model concerns 
Just to close the loop on this: I finally got in touch with Thomas on
IRC and walked over the security issues I brought up here, plus a
number of other ones.
He took the concerns seriously and rapidly redesigned big swaths of
electrum to eliminate the issues structurally.  Electrum no longer a
classical thin client it is now a slightly watered down
simplified-payment-validation node with generally the same security
properties as other SPV nodes. Its network behavior leaves it somewhat
more vulnerable to isolation and compromise by a high hash power
attacker, because it does not (yet) make an effort to make sure it's
really on the longest chain. It is also more vulnerable to transaction
hiding (a DOS attack) for similar reasons.  But this is still a
massive improvement.  The UI was also changed and the confirmation
status of payments is no longer hidden.
There are still things to improve? both in the client and the security
communication to users. But I wanted to leave a note that it's come a
long way and that I now feel confident that any remaining issues will
be resolved.

@_date: 2012-11-26 18:32:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
This is messy.   It's important to people to know that their cert will
be accepted by ~everyone because non-acceptance looks like malice.  If
the cert system is actually to provide value then false positives need
to be low enough that people can start calling in law enforcement,
computer investigators, etc.. every time a cert failure happens.
Otherwise there is little incentive for an attacker to not _try_.
Obviously the state of the world with browsers is not that good... but
in our own UAs we can do better and get closer to that.
Would you find it acceptable if something supported a static whitelist
plus a OS provided list minus a user configured blacklist and the
ability for sophisticated users to disable the whitelist?
This way people could trust that if their cert is signed via one on
the whitelist they'll work for ALL normal users.. and the UI can have
very strong behavior that protects people (e.g. no 'click here to
disable all security because tldr' button)... but advanced users who
can deal with sorting out failure can still have complete control
including OS based control.

@_date: 2012-11-26 19:16:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
Thats the CA model generally. It _is_ a distributed-centralized model
in practice.
Because the list is not identical (and of course, couldn't be without
centralizing control of all OSes :P ) meaning that the software has to
be setup in a way where false-positive authentication failures are a
common thing (terrible for user security) or merchants have to waste a
bunch of time, probably unsuccessfully, figuring out what certs work
sufficiently 'everwhere' and likely end up handing over extortion
level fees to the most well established CAs that happen to be included
on the oldest and most obscure things.
Taking? say? the intersection of Chrome, Webkit, and Firefox's CA list
as of the first of the year every year and putting the result on a
whitelist would be a possible nothing-up-my-sleeve approach which is
not as limited as having some users subject to the WinXP cert list,
which IIRC is very limited (but not in a way that improves security!).
Uhh.  Really?   Well, I agree with you that they should be (I
unsuccessfully lobbied browser vendors to make self-signed https on
http URLs JustWork and simply hide all user visible evidence of
security), but the really nasty warnings on those sites undermines the
security of the sites _and_ of other HTTPS sites because it conditions
users to click ignore-ignore-ignore. I don't think they are all that
One thing which I think will be hard for us in this discussion is
being sensitive to the (quite justified!) concerns that the current CA
system is absolute rubbish, both terrible for security, usability, and
an unreasonable barrier to entry relative to the provided security?
without allowing the discussion to be usurped by everyone's pet
replacement, which there are a great many of with varying feasibility
and security.
Perhaps we should agree to talk about everything _except_ that first?

@_date: 2012-11-26 21:47:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
I see that draft-stanish-x-iso4217-a3 is not standards track, is there
a reason for this?
It also doesn't appear to address ~any of the the targeted items here.
Is there another draft I should be looking for which has more overlap
with the discussion here?

@_date: 2012-10-02 18:52:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment protocol thoughts 
SSL itself (as opposed to using the certs as you suggest) is not
non-reputablable, so it's not enough for the below concerns
To their prospective customer base.  "I can prove to the public that
you ripped me off" is protective, even if there isn't formal direct
remedy available.

@_date: 2012-10-02 22:02:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Testing Project 
Perhaps a bit bluntly here? but since you seem to be rather boldly
insisting on getting paid:
With all of this testing where can I find the issues you've uncovered?
 Searching on your name/email in the issue tracker reports nothing,
likewise I can't find anything in my email (beyond abstract discussion
of testing).

@_date: 2012-10-06 12:37:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum security model concerns 
I'm concerned about how the particular security model of electrum is
being described; or rather? not being described.  The electrum website
appears to have no security discussion beyond platitudes like "Secure:
Your private keys are not shared with the server. You do not have to
trust the server with your money.", "No scripts: Electrum does not
download any script at runtime. A compromised server cannot compromise
your client."
Claims like "You do not have to trust the server with your money" are
factually incorrect.
What I would expect is a proper discussion, like "Understanding the
bitcoinj security model":
  (which I don't
agree with completely? as it makes some claims which are known to be
false? wrt detecting double spends, but it does give a reasonable
overview),  and avoidance of broad claims which will result in
misunderstandings that result in users engaging in riskier behaviors
which they could avoid if they better understood the security of the
software they're running.

@_date: 2012-10-08 23:22:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum security model concerns 
Electrum also has a daemon for merchants. Considering the dislike of
Java that exist reflexively in much of the non-java community and the
greater ease of deployment and the integration of type-2 split key
management,  I wouldn't be surprised if it became quite popular
quickly especially if the status quo of failing to disclose and
discuss the security limitations of the client continues.
What I've found is that even fairly sophisticated bitcoin participants
are actually unaware of the security implications? not just of thin
clients architecturally but of electrum specifically.  I think even
you may find my findings of the latter a bit surprising.
Generally for thin clients? a lying server can make clients think
they've received confirmed payments they haven't, and unless the
client is constructed to be a bit less thin a lying server can lie
about input values and cause think clients to spend large values to
fees. Servers can also monitor clients and deanonymize them and
selectively deny service to particular clients or transactions. Thin
clients must trust their servers to be available, and to not perform
these attacks. Users can use tools like tor to reduce the privacy
attacks, but doing so inhibits having a trust relationship to protect
against the other attacks. And none of these attacks leave
cryptographic proof of their existence, so a victim can't convince the
public of a server's treachery. Us experts know about these risks, but
I don't think the general users do.
But thats not the limit of it?  It seems some people believe Electrum
does majority quorum between servers, complicating attacks arising
from the fact that today users virtually never have a reason to trust
their server operators.  This isn't true? it connect to one at a time.
(And sibyl attacks would make that pretty weak protection even if it
did that, as someone could use a a botnet to run tens of thousands of
'servers' (really proxies)).
Beyond that the protocol between the clients and servers is
unauthenticated cleartext JSON in TCP.  So any network advisory with
access to the network near the server has the same power to attack as
the server operator... and one near the client has the same power to
attack as the sum of all the server operators.  A passive attacker
near the client has full deanonymization power.
But I don't even know if any of these limitations matter much?  The
electrum client instantly displays unconfirmed transactions and allows
users to spend them.  The default user interface gives _no_ indication
that the payment is unconfirmed. There is a "pro" mode, that shows
'processing' for unconfirmed transactions... but it looks as final as
it ever will be once it gets a single confirm. Only the most cautious
and well informed users would open the pro interface and right click
and select details to see the count? and even then there is no
guidance on what numbers are good (beyond '1').  So I suspect people
can probably rob typical electrum users (including electrum running
merchants) without actually using any of the above.
When a thin client is willing to provide arbitrary features like
showing unconfirmed payments and simplified UI without regard to
security it removes the functional advantage of running more secure
software like SPV and various degrees of full node... the only
motivation is security, and it's not much of a motivation when the
risks aren't even disclosed.
...and I haven't even gotten into delving into what kind of attacks
are possible due to deeper implementation specifics.
But I do share your view that people will migrate to stronger client
models in the future? but I don't agree that it will be due to those
clients improving (though they will improve), it will be because
people will know that they provide better security and will choose
them for that reason.
My only question is will they know this because we as a community and
the authors of the thin clients provided clear explanations and
appropriate caution, or will it be because they're getting robbed
blind, producing a bunch of bad press for thin clients in particular
and Bitcoin generally?

@_date: 2012-10-09 20:03:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] On bitcoin testing 
The mention of testnet3 here reminds me to make a point:  Confirmation
bias is a common problem for software testing? people often over test
the success cases and under-test the failure cases.  This is certainly
the case in Bitcoin: For example, testnet3+the packaged tests test all
the branches inside the interior script evaluation engine _except_ the
rejection cases.
For us failure cases can be harder to package up (e.g. can't be placed
in testnet) but Matt's node-simulation based tester provides a good
example of how to create a data driven test set that tests both
failure cases and dynamic behavior (e.g. reorgs).
Testing of failure cases is absolutely critical for testing of
implementation compatibility: The existence of a difference in what
gets rejected in a widely deployed alternative node could result in an
utterly devastating network split.
Generally every test of something which must succeeded should be
matched by a test of something that must fail. Personally, I like to
test the boundary cases? e.g. if something has an allowed range of
[0-8], I'll test -1,0,8,9 at a minimum. Though reasoning trumps rules
of thumb.
Confirmation bias is another reason why it's important to have a more
diverse collection of testers than the core developers.  People who
work closely with the software have strong expectations of how the
software should work and are less likely to test crazy corner cases
because they "know" the outcome, sometimes erroneously.
To reinforce Jeff's list of different approaches: I've long found that
each mechanism of software testing has diminishing returns the more of
it you apply. So you're best off using as many different approaches a
little rather than spending all your resources going as deep as
possible with any one approach.
There are also some kind of testing which are synergistic: Almost all
testing is enhanced enormously by combining it with valgrind because
it substantially lowers the threshold of issue detection substantially
(e.g. detecting bogus memory accesses which are _currently_ causing a
crash for you but could). If I could only test one of "with valgrind"
or "without" I'd test with every time.  Sadly valgrind doesn't exist
on windows and it's rather slow. Dr. Memory
( may be an alternative on Windows,
and there is work to port ASAN to GCC so it may be possible to mingw
ASAN builds in not too long.
I've also found that any highly automatable testing (coded data
driven, unit, and fuzz testing) combines well with diverse
compilation, e.g. building on as many system types and architectures?
including production irrelevant ones? as possible in the hopes that
some system specific quark make a bug easier to detect.

@_date: 2012-10-10 11:23:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum security model concerns 
I tried in IRC and got no response. These messages are copying the
only contact email address I could find.
Yes, so do a lot of people. It doesn't.
There is a middle ground: You can not hide it without explaining it.
AFAICT we don't see ~any questions about the reference client waiting
for six confirmations before saying confirmed.
There have been a great many circulated on the network. People don't
report all losses? e.g. we've never seen a report from those who've
burned hundreds of bitcoins in fees on transactions.
I think this is very hard because this matter is rapidly politicized.
There are some in the community who will instantly allege misconduct
when there is a mis-agreement.
Basically: No one sane should want the job, and anyone who wants
should on no account be allowed to have it.
At this point I think we also will get better results communicating
among technical people in order to get the development focus adjusted
in a way that mitigates those risks that can be mitigated and those
cautions that can be offered offered.
After all, if the Electrum project is _unwilling_ to disclose the
limitations of their implementation and security model on their own
site, even after having them pointed out then someone updating
Bitcoin.org to include them will be politically contentious.  I want
to make sure that we've followed all reasonable avenues before going
that route? first I attempted informally on IRC, now I've brought the
discussion here... instead of, e.g. starting the process to remove it
from the bitcoin.org clients page.
I agree, thats why I started this thread.

@_date: 2012-10-14 18:33:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Hosting of compiled bitcoin client 
The only restrictions I'm aware of are the EAR restrictions on the
export of cryptography.
These are generally not applicable to us for two reasons. One is that
we only use cryptography for authentication, which is explicitly
The other is that since Bernstein vs US
( there has
been absolutely no enforcement attempts against open source projects
as the precedent creating holding there makes it clear that these
regulations cannot inhibit the publication of source code.
Perhaps someone could make a little noise about binaries, but it would
be pure pretext: Especially since with the deterministic build process
we use anyone can produce bit-identical binaries (thus allowing builds
by untrusted third partities to be just as trustworthy as the official
This made me laugh. It's hard to find places with better effective law
for most online and internet things.  Many places copy the US's
statutes (either cargo culting, or as part of treaty compliance) but
do so without also copying our legislative history which is
US munitions regulations exactly, but has no analog of Bernstein v. US
to limit the government's power.
Unfortunately sourceforce was rather vague about what regulations they
believe they're enforcing:
So unless someone has already done it, I'll get in touch with the EFF
and find out if they're aware of any particular precautions we should
take here.

@_date: 2012-10-20 14:34:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Public key and signature malleability 
I strongly support heading down this path. Malleability has produced a
steady trickle of surprising outcomes. In addition to the problems we
already know about and expect there may be additional security or DOS
problems that arise from allowing these.

@_date: 2012-10-25 12:56:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Thanks for taking the time to write this up.
I still don't understand what purpose the apparently gratuitous
inefficiency of constantly resending common tree fragments. There are
many points of complexity in this protocol? handling premature
disconnections without missing blocks, the actual implementation of
the hash functions for the filter, validation of the hash tree, etc.
Presumably these components will just get implemented a few times in
some carefully constructed library code, so I don't see an
implementation complexity argument here? except the fact that it isn't
what Matt has implemented so far.
The current design can cause massive overhead compared to pulling an
unfiltered block should a filter be somewhat overboard and also makes
this filtering useless for applications which would select a small but
not tiny subset of the transactions (e.g. 10%).
Also, it's not mentioned in the page? but the hash function used is
not cryptographically strong,  so what prevents a complexity (well,
bandwidth in this case) attack?  someone could start using txids and
txouts that collide with the maximum number of other existing txouts
in order to waste bandwidth for people.  Is this avenue of attack not
a concern?

@_date: 2012-10-25 13:01:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Sorry for the rapid additional comment, but I should also have
mentioned that the in efficiency is at odds with the privacy argument
for the particular mechanism... if the filter is not set at least
somewhat too broadly then it will uniquely identify the user. The
inefficiency, however, argues for setting the filter as narrowly as
possible because there is much more bandwidth used for a wider filter
than would be otherwise.

@_date: 2012-10-26 10:17:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Because I can potentially waste bandwidth of all nodes forever (well as long
as users are still scanning blocks with my transactions in them) with O(1) work.
Though I'm not sure how much of a threat is vs just paying 1e-8 btc to lots of
addresses which would only be less bad by some constant factor as worse.
I guess I should try to attack it and see how bad the pollution I can construct
should be. (offline, of course)

@_date: 2012-10-26 10:34:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
============================== START ==============================
Which happens and is a concern. Altcoins have been attacked on things
we fixed. For example, litecoin nodes were being run out of disk space
through addr.dat flooding.
I think we've been generally fortunate that the level of griefing is
low (though not non-existent).  But part of the reason its been low is
that it's probably harder to DOS attack bitcoin than you believe. In
the reference client a lot of work has gone in to removing attacks
with sublinear cost for the attackers.
That people aren't attacking much now is not an argument to accept a
new vulnerability much less a _normative_ vulnerability in the
That it's no big deal even attacked would be a fine argument to me, so
I'll go try to convince myself of that.
Please don't put that kind of black helicopter junk in my mouth. I
agree with you the point that these aren't a source of concern for me.

@_date: 2012-09-10 11:14:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
Why does this focus on actually sending the hash tree?  The block
header + transaction list + transactions a node doesn't already know
(often just the coinbase) is enough.

@_date: 2012-09-10 16:00:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
As you know there is a hard protocol limit of 1MB.
If you're going to talk about doing that you are screwing with the
core economic promises of the system. (in particular, removing the cap
eliminates the only armwave we have for long term security).  But in
any case, removing it requires a complete and totally incompatible
hardfork, and at that point you can do whatever you want with the
protocol. Changing how blocks are fetched is almost incidental to the
number of other things that would be changed.  I don't think it makes
sense to design for that especially when something far simpler (as you
pointed out) is prudent for the design of bitcoin.

@_date: 2012-09-11 15:42:32
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
Someone can do that just by pipelining the one at a time requests.
How much bandwidth do you think you could save over that?
I don't see what value this provides.  For protecting against the
future you might as well suggest uploading x86 code which gets
executed to select transactions. "Protects against the future".  Can
you clarify some more about exactly how you think it would help?
It's sometimes desirable to be more general rather than more special
case when it's costless... but having couple extra p2p protocol
messages to implement, test for interop, guard against vulnerability,
etc. isn't costless... and should be justified with concrete benefits.
it's not clear to me how your proposal is really all that useful for
very large blocks: I looks like it would lot of bytes sending
redundant tree data.
And what if? _Bitcoin_ blocksizes can't be any larger.  Some future
incompatible system? well perhaps. But we're working on the protocol
for bitcoin now.
The finite size? and ultimately the contention for space it causes? is
the only thing will creates non-trivial fees. Without the fees there
is no honest economic motivation to mine with adequate computing power
to provide security (lots of dishonest motivations? e.g. applying
control over the currency exist), you'd just have a race to the
bottom, given unbounded block sizes it is always rational for
decentralized to include any transaction with a fee even if it is very
small? otherwise the next rational solver is just going to include it.
Bitcoin gets its value through scarcity. There are two kinds of
scarcity that are economically important, scarcity of the coins? there
will never be more than 21 million? and scarcity of the block space
which, as the protocol is defined and enforced by every node can not
be more than 1MB. The latter scarcity is what makes the security model
economically sane.
Fortunately, its perfectly possible to make transactions denominated
in bitcoin outside of the blockchain, and in a secure and distributed
manner that respects the principles that make bitcoin attractive, but
with information hiding that improves privacy, transaction speed, and
scalability. See, e.g. the good work being done by Open transactions
to create distributed cryptographic banks.  So blockchain scarcity
itself doesn't prevent Bitcoin from being a one world currency
(something which isn't at all sane no matter how big you make the
blocks if you don't allow for other modes of transaction processing?
who the heck wants to possibly wait an hour to get a 1 confirm
be slower (or expensive to make merely slow; Bitcoin is incapable of
reliable fast confirmations)? thats the nature the stochastic
consensus and the fee based security support.  You could instead
imagine a future where bitcoin's security came by collusion by major
financial cartels and governments, and where fees aren't important....
 But I reject that future, it's a perfectly viable one, but why bother
with Bitcoins in the first place? To make some early adopters a little
bit of money starting off the next big centrally controlled fiat?
I can't say for sure if the 1MB limit will stay exactly as is forever,
as I expect the economics work with any limit out of a fairly broad
range that is low enough to both make the space seriously scarce and
low enough that 'inexpensive' (e.g. privately owned) hardware can
continue to audit it to preserve the decentralized security,  and the
economic importance of the size limit is more subtle than the
inflation resistance... but I know that changing it is precisely as
technically difficult as changing the 21 million limit: all Bitcoin
node software must be replaced with incompatible software, and I
believe it would be just as economically risky? if not more so? if
done wrong, as at least inflation would have a easily understood
direct dillution effect while inadequate security would potentially
make all Bitcoin worthless.  As such I don't think it's even worth
discussing until there is an urgent demand to clarify the tradeoffs...
Should the block size ever be increased the message format used for
relaying the larger blocks will be the smallest of the issues being

@_date: 2012-09-11 19:22:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
On Tue, Sep 11, 2012 at 5:48 PM, Matthew Mitchell
But you only need to request the transactions you don't have. Most of
time you should already have almost all of the transactions.
You can fetch transactions from multiple peers with just a simple
mechanism that gives you the headers plus the txn list. And if you
want ArgumentAdSomethingElse, thats what bittorrent does too: the
torrent file contains the list of block hashes, and you get it from
one place.
Because there is no motivation not to set them to zero, if you don't
someone else will.  Right now the income from fees is hardly relevant,
and the ability to drive more non-existant because there isn't enough
load to create scarcity.
Because it can't resist inflation. You have to trust that the banks
won't conspire to their mutual benefit to inflate the base currency.
OT can make it so a 'bank' (which is really a distributed collection
of nodes, not a single point of trust) can trivially prove how much
"gold certificates" it has issued, but you also need to prove how much
'gold' exists and which keys hold it, and for that you need a _global_
consensus; which bitcoin provides...
If you don't like
Well, Bitcoin gives you no certainty that any particular transaction
will be confirmed at all, ever; so perhaps best not to overstate it
too much. But yes, Bitcoin is great. ... but all that greatness
depends on there being a way to fund enough computation so that
attacks are too costly to be justified and that the cost of
maintaining a system to fully validate the system's rules (e.g. that
the miners aren't mining duplicate txns to create inflation for
themselves) is low enough that it will naturally enormously
distributed so that a conspiracy is effectively impossible.  Otherwise
everything consolidates down to a few meganodes and the attractive
properties are all gone.
OT's issuers can prove how much bitcoin they hold on the blockchain
(by nothing more sophisticated than signmessage) and they can prove
how many tokens they've issued against it.
And I didn't mean to suggest OT as a unique solution. Another path is
ripple, the idea of which is a sort of a p2p hawala where you have
pairwise trust and debt. It can allow you to circulate around tokens
between a community of users and only settle infrequently (as
determined by your level of trust, the debt involved, and the cost of
the bitcoin transaction) against bitcoin.
They already exist, in crappy centralized form? e.g. look at mtgox
codes and user to user instant transfers; and bitcoin isn't useless.
Plus some extra system of some kind is the _only_ way to securely
irreversible transactions which are reliably fast, so it's not like
there is any real prospect of using bitcoin directly for all kinds of
uses at scale. (yes, blocks are 'only' 10 minutes apart on average,
but if you care about fast, you care about e.g. the 99% not the
Bitcoin scales fine.  It is not a singular replacement for everything
you can imagine it being a replacement for, however, or at least not a
good replacement.  The fact that you could conceivably make it
directly scale up to handle e.g. the volume of all the credit network
doesn't make that a good idea. It would still be a very poor
replacement for a credit network (slow transactions; which can't be
fixed by tweaking some parameters, the bitcoin blockchain consensus
algorithm has infinite convergence time when the block time falls
below the hash-power-weighed latency), and that kind of scaling would
absolutely ruin the decentralization, making it so only large states
and megabanks could run full nodes, and even at that level it couldn't
match the worldwide volume of cash transactions or 'internal' money
transactions (like money moving around on all the poker tables in the
world).   It's like someone made the mistake of saying the floor wax
is edible (linseed oil) and now you complain that its a crappy desert
topping. :P
Maybe people will ultimately agree to raise the block sizes, but I
expect and hope that they'll only do so when it is entirely
uncontroversial that doing so won't significantly degrade the
decentralization (certainly not the case today: a large portion of the
network appears to have trouble keeping up with large blocks right
now, though upcoming software improvements will help enormously), or
the mining economics.   And yes, of course, you schedule the change
for the future, but as you note that it doesn't solve the problem of
people opposing it.

@_date: 2012-09-13 11:16:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
On Thu, Sep 13, 2012 at 10:05 AM, Matthew Mitchell
Sorry, I'm still not seeing what the value is.  How is the tree level
useful to anyone?  If you did want to get only parts of the
transaction list, why not just ranges from the lowest level?

@_date: 2012-09-23 16:44:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Large backlog of transactions building up? 
Right, this disconnect is why simple scalar measures of mempool size
aren't terribly informative.
There are bursts of weird transactions (e.g. someone was flooding zero
value txn a few weeks ago; before that there were some enormous series
of double-spend induced orphans), and other sustained loads that quite
a few miners are intentionally excluding.
Sounds good? my only concern is that nodes will repeat their own
transactions but not the unconfirmed parents. So being more aggressive
can turn otherwise valid transactions into orphans.
Would there be value in an archive-mempool which is only checked when
you receive an orphan transaction?
I would point out that you can't _KNOW_ a txn will disappear. Someone
else could happily reannounce it. (I know you know this; but it's good
to be clear on that point when we talk about it!)

@_date: 2012-09-25 13:52:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Large backlog of transactions building up? 
This is discussion about transactions which are not in the chain yet.
The double spending transaction is not stored? which is, in fact, the
problem which creates these huge chain. When a transaction depending
on the doublespend is received we do not know its parent (because we
dropped it because it was a rule violation) so we keep it around as an
orphan hoping its parent arrives.
The software could maintain a cache of rejected txids to consult for
orphan txn's parents, but it would need to be dropped any time there
is a reorg so I don't know how useful it would be.

@_date: 2012-09-26 22:29:30
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Testing Project 
Jenkins is excellent at cycling through tests,  while additional
external tools may bring some value they're not required.  It's also
essential to automate all tests that we really care are run? with our
small active development group and volunteer contributors the only
tests we can count on being run are the automated ones. Automated
tests included with the software? or at least the source? are also the
only way to have a good chance of catching gnarly platform
I think more than talking about testing I think we need is actual
testing. Code coverage from the current tests (e.g. bitcoin-test and a
testnet sync) is very unimpressive, and while coverage isn't some
magical silver bullet and does not, by itself, mean the tests are good
flaws in uncovered code can't be detected by the tests.  We also lack
simple testing cycle documentation for people interested in testing
manually to walk through, etc. I think all the meta discussion is not
very useful until we actually have more substance to put into it.
Otherwise I fear we're just building an airport by painting stripes
and waiting for the planes to land...
If someone wants to help and would like a list of some of the testing
I think would be useful, ping me off-list and I can blast some
suggestions. But I assume that anyone who actually wants to work on
this isn't short of ideas, and at this point "work on what interests
you, report what interesting thing you accomplish or discover" is
probably a perfectly fine level of coordination.

@_date: 2013-04-05 02:52:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A mining pool at 46% 
The estimates on there may be a bit lossy.
The whole fixation on "51" as a magic number is a bit confused? I'll
say more below.
None of the pools listed there are meaningfully decentralized?  before
Luke whines, in theory the ones supporting GBT could be if used in a
way that no one actually uses them.  P2Pool is decentralized based on
the same technology as Bitcoin itself, but it's certainly not as point
and click easy as a centralized pool.
That is correct.
Though I'd point out? the major pool ops all seem to be great folks
who care about the future of Bitcoin? and the continued success of
their very profitable businesses: a 50% mining pool with a 3% fee
rakes in 54 BTC per _day_.
The more likely threat isn't that pool owners do something bad: It's
that their stuff gets hacked (again) or that they're subjected to
coercion. ... and the attacker either wants to watch the (Bitcoin)
world burn, or after raiding the pool wallet can't exploit it further
except via blockchain attacks.
That makes no sense. A centralized pool is the miner, the remote
workers are just doing whatever computation it tells them to do.
Certainly these remote workers might switch to another pool if they
knew something bad was happening... but evidence suggests that this
takes days even when the pool is overtly losing money.  Miners have
freely dumped all their hashpower on questionable parties (like the
infamous pirate40) with nary a question as to what it would be used
for when they were paid a premium for doing so.  It seems even those
with large hardware investments are not aware of or thinking carefully
about the risks.
It's important to know exactly what kind of threat you're talking
about?  someone with a large amount of hash-power can replace
confirmed blocks with an alternative chain that contains different
transactions. This allows them to effectively reverse and respend
their own transactions? clawing back funds that perhaps had already
triggered irreversible actions.
This doesn't require some magic "51%"? its just that when a miner has
enough (long enough might be years if you're talking really close to
50% and he gets unlucky). Likewise, someone with a sustained
supermajority could deny all other blocks? but that attack's damage
stops when they lose the supermajority or go away.
More interesting is this:  An attacker with only 40% of the hashpower
can reverse six confirmations with a success rate of ~50%. There is
source for computing this at the end of the Bitcoin paper.   I did a
quick and really lame conversion of his code JS so you can play with
it in a browser:

@_date: 2013-04-05 03:02:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A mining pool at 46% 
As an aside and a clarification? P2pool works great with FPGAs, and
one of the largest FPGA farms I've heard of uses it.  But it doesn't
work well the old BFL FPGA miners? because they have insane latency.
Likewise it doesn't currently work well with Avalon, again because of
insane latency.   P2pool uses a 10 second sharechain in order to give
miners low variance but that means that if you have a several second
miner you'll end up subsidizing all the faster p2pool users somewhat.
It was basically stable with the network until ASICminer came online
mining on BTCguild mostly and the first avalons started to ship, and
then the network went up 10TH in a couple weeks (and now 15TH) while
P2Pool stayed mostly constant.
ForrestV (the author and maintainer of the P2pool software) would love
to work on making Avalon and other higher latency devices first class
supported on P2Pool, but he doesn't have one? and frankly, all the
people who have them aren't super eager to fuss around with a 5BTC/day
revenue stream, especially since the avalon firmware (and its internal
copy of cgminer) itself has a bunch of quirks and bugs that are still
getting worked out... and I do believe that p2pool helps reduce
concerns around mining pool centralization. ... but I think as a
community we don't always do a great job at supporting people who work
on infrastructure? even just making sure to get them what they need to
keep giving us free stuff?, we just assume they're super rich Bitcoin
old hands, but that often isn't true.

@_date: 2013-04-05 10:42:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Integration testing for BitCoin 
I prefer to call them system tests.
We use a system called blocktester that Matt Corallo wrote,
It's based on BitcoinJ and works by simulating a peer against a
slightly instrumented copy of Bitcoin(d/-qt) (modified to avoid
computationally expensive mining).  The tests simulates many
complicated network scenarios and tests the boundaries of many
(hopefully all) the particular rules of the blockchain validation
protocol.  We can use these tests to compare different versions of the
reference software to each other and to bitcoinj (or other full node
implementations) as well as comparing them to our abstract
understanding of what we believe the rules of the protocol to be.
These tests are run as part of the automated tests on every proposed
patch to the reference software. Via a robot called pulltester which
comments on github requests and produces logs like this:
Pulltester also performs automatic code coverage measurements.
Additionally, we run a public secondary test bitcoin network called
'testnet', which can be accessed by anyone by starting the reference
software with testnet=1.  Testnet operates the same as the production
network except it allows mining low difficulty blocks to prevent it
going for long times without blocks, and some of the protective
relaying rules against "non standard" transaction types are disabled.
Most of this testing work has been centered around validating the
blockchain behavior because thats what has serious systemic risk.
Measuring the json rpc behavior is strictly less interesting, though
interesting too.

@_date: 2013-04-09 12:25:37
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] On-going data spam 
On Tue, Apr 9, 2013 at 7:39 AM, Caleb James DeLisle
I stuffed the testnet chain full of the EICAR test string and it
hasn't triggered for anyone? it seems that (most?) AV tools do not
scan big binary files of unknown type.. apparently.
If we encounter a case where they do we can implement storage
scrambling: E.g. every node picks a random word and all their stored
data is xored with it.

@_date: 2013-04-09 19:53:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] =?utf-8?q?To_prevent_arbitrary_data_storage?= 
(1) Define a new address type, P2SH^2 like P2SH but is instead
H(H(ScriptPubKey)) instead of H(ScriptPubKey). A P2SH^2 address it is
a hash of a P2SH address.
(2) Make a relay rule so that to relay a P2SH^2  you must include
along the inner P2SH address.  All nodes can trivially verify it by
hashing it.
(2a) If we find that miners mine P2SH^2 addresses where the P2SH
wasn't relayed (e.g. they want the fees) we introduce a block
discouragement rule where a block is discouraged if you receive it
without receiving the P2SH^2 pre-images for it.
With this minor change there is _no_ non-prunable location for users
to cram data into except values.  (and the inefficiency of cramming
data into values is a strong deterrent in any case)
The same thing could also be done for OP_RETURN PUSH value outputs
used to link transactions to data. Make the data be a hash, outside of
the txn include the preimage of the hash.

@_date: 2013-04-09 20:58:40
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 
Correct. It doesn't prevent value commitment (which is actually what
we need for our currency use), just data storage.
And as Peter points out? you could try to store a little data, but
that has a computational cost which is exponential in the amount of
data stored per output. ... like storing data in values, thats awkward
enough to not be especially problematic, I expect.

@_date: 2013-04-10 00:15:26
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 
Oh wow, thats actually a quite good thing? it's a property I know I've
lamented before that I didn't know how to get.

@_date: 2013-04-13 14:58:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Who is creating non-DER signatures? 
Without knowing how they're getting created it's hard to say what the
damage is...  are they being created by people using old cached JS
transaction generators? If so? the harm is insignificant. Are they
being created by hardware wallets with the keys baked inside that
can't be changed?  If so? the harm would be more significant.
I think the latter is unlikely right now? but if the network doesn't
stop relaying these transactions it seems inevitable.
In all cases these transactions can be currently be mutated to an
acceptable form? the malleability being one of the arguments for
removing support for non-canonical encodings.  So we could easily post
a transaction normalizer tool that someone with unrelayable
transactions could pass their transactions through to fix them, even
without coming to the developers for help.

@_date: 2013-04-24 09:37:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP21 bitcoin URIs and HTML5 
Yup.  Corrections are fine, esp ones which are not gratuitously incompatible.

@_date: 2013-04-28 12:50:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
X most recent is special for two reasons:  It meshes well with actual demand,
and the data is required for reorganization.
So whatever we do for historic data, N most recent should be treated
But I also agree that its important that  be splittable into ranges
because otherwise when having to choose between serving historic data
and? say? 40 GB storage, a great many are going to choose not to serve
historic data... and so nodes may be willing to contribute 4-39 GB storage
to the network there will be no good way for them to do so and we may end
up with too few copies of the historic data available.
As can be seen in the graph, once you get past the most recent 4000
blocks the probability is fairly uniform... so "N most recent" is not a
good way to divide load for the older blocks. But simple ranges? perhaps
quantized to groups of 100 or 1000 blocks or something? would work fine.
This doesn't have to come in the first cut, however? and it needs new
addr messages in any case.

@_date: 2013-04-28 20:36:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
I think this is not a great idea on a couple levels?
Least importantly, our own experience with tracker-less torrents on
the bootstrap files that they don't work very well in practice? and
thats without someone trying to DOS attack it.
More importantly, I think it's very important that the process of
offering up more storage not take any more steps. The software could
have user overridable defaults based on free disk space to make
contributing painless. This isn't possible if it takes extra software,
requires opening additional ports.. etc.  Also means that someone
would have to be constantly creating new torrents, there would be
issues with people only seeding the old ones, etc.
It's also the case that bittorrent is blocked on many networks and is
confused with illicit copying. We would have the same problems with
that that we had with IRC being confused with botnets.
We already have to worry about nodes finding each other just for basic
operation. The only addition this requires is being able to advertise
what parts of the chain they have.
Using Bitcoin to bootstrap the Bittorrent DHT would probably make it
more reliable, but then again it might cause commercial services that
are in the business of poisoning the bittorrent DHT to target the
Bitcoin network.
Integration also brings up the question of network exposed attack surface.
Seems like it would be more work than just adding the ability to add
ranges to address messages. I think we already want to revise the
address message format in order to have signed flags and to support
I2P peers.

@_date: 2013-08-04 23:41:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Preparing for the Cryptopocalypse 
Lamport signatures (and merkle tree variants that allow reuse) are
simpler, faster, trivially implemented, and intuitively secure under
both classical and quantum computation (plus unlikely some proposed QC
strong techniques they're patent clear).  They happen to be the only
digital signature scheme that you really can successfully explain to
grandma (even for values of grandma which are not cryptographers).
They have poor space/bandwidth usage properties, which is one reason
why Bitcoin doesn't use them today, but as far as I know the same is
so for all post-QC schemes.
The problems are intimately related, but under the best understanding
ECC (with suitable parameters) ends up being the maximally hard case
of that problem class.   I do sometimes worry about breakthroughs that
give index-calculus level performance for general elliptic curves,
this still wouldn't leave it any weaker than RSA but ECC is typically
used with smaller keys.

@_date: 2013-08-15 19:26:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 32.5 
I am wondering if we shouldn't have a BIP32 addendum which makes the
following signing related recommendations:
(1) Recommend a specific deterministic DSA derandomization procedure
(a deterministic way to generate the DSA nonce), presumably one based
on HMAC-SHA512 (since BIP32 uses that construct) or SHA256 in the
style of RFC 6979.
DSA systems being compromised due to poor randomness at runtime is not
new. It effected other systems before it effected Bitcoin systems,
it's not a new problem and it's not going away.  It's difficult to
tell if an implementation is correct or not.
Use of a fully deterministic signature  would allow for complete test
vectors in signing and complete confidence that there is no random
number related weakness in a signing implementation.
In particular, with relevance to our ecosystem a maliciously modified
difficult to audit hardware wallet could be leaking its keys material
via its signatures. Even without producing insecure K values it could
use the choice of K to leak a couple bits of an encrypted root key
with every signature, and allow the malicious party to recover the
keys by simply observing the network. Making the signatures
deterministic would make this kind of misbehavior practically
We wouldn't be alone in making this change, in general industry is
moving in this direction because it has become clear that DSA is a
hazard otherwise.
The primary arguments in most spaces against derandomizing DSA are
FIPS conformance (irrelevant for us) and reasonable concerns about the
risks of using a (less) reviewed cryptographic construct. With
widespread motion towards derandomized DSA this latter concern is less
of an issue.
Libcrypt has also implemented derandomized DSA in git. The ed25519
signature system of DJB, et. al. also uses a similar derandomization.
An alternative is implementing a still random construct where K is
some H(message||key||random) which should remain secure even where the
randomness is poor, but this loses the advantage of being able to
externally verify that an implementation is not leaking information.
OpenSSL development has implemented a form of this recently.
See also: (2) Recommends a procedure for using only even S values in signatures,
eliminating this source of mutability in transactions.
This can be accomplished via post-processing of existing signatures,
but since it requires bignum math it is usually preferable to
implement it along with signing.  I believe someday this will become a
network requirement for Bitcoin, but regardless it makes sense to
implement it as a best practice sooner rather than later.

@_date: 2013-08-16 07:56:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
There are whole countries who access the internet from single IP
addresses. There are major institution with hundreds or even thousands
of hosts that could be running Bitcoin who are visible to the public
internet as a single IP address (/single subnet).  Most tor traffic
exits to the internet from a dozen of the largest exits, common
local-network configurations have people addnode-ing local hosts from
many systems on a subnet, etc.
Prioritizing the availability of inbound slots based on source IP is
reasonable and prudent, but it does not have almost zero drawbacks.
Outright limiting is even worse.
As a protective measure its also neigh useless for IPv6 connected
hosts and hidden service hosts.  It's also ineffective at attacks
which exhaust your memory, cpu, IO, or bandwidth without trying to
exhaust your sockets.
So I am not opposed to prioritizing based on it (e.g. when full pick
an inbound connection to drop based on criteria which includes network
mask commonality), but I would not want to block completely based on

@_date: 2013-08-18 23:38:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] CoinWitness: Really Really ultimate 
I've posted a somewhat blue-skies idea on troll^wBitcointalk that some
here might find interesting:

@_date: 2013-08-19 13:16:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: remove "getwork" RPC from 
They have been, resulting in a replacement called "getblocktemplate"
which (presumably) almost everyone talking to bitcoin(d|-qt) has been
using for a long time.
I think removing the ability to mine in the stock package would be
regrettable, but to be honest we already don't have it for the
mainnet. I think we should do as Jeff suggests and remove getwork. But
I think we should also package along a proper getblocktemplate miner
to remove any doubt that we're providing a full network node here.  (I
note that the choice of miner is also easy:  Regardless of people's
preferences which way or another, AFAIK only luke's bfgminer stuff can
mine directly against bitcoin getblocktemplate with no pool in the
middle.  It also supports a huge variety of hardware, and a superset
of our target platforms)

@_date: 2013-08-19 13:23:16
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: remove "getwork" RPC from 
I am naughty and should clarify.  I had ass.u.me.d that Jeff's patch
also removed the internal CPU miner, because doing so is necessary for
actually getting rid of most of the getwork code. It doesn't actually.
Though this doesn't change the fact that the internal miner is mostly
a pretext for integrated mining.  Since it only really works on
testnet it also means our testnet testing using it is not a good test
of the actual production software.  I'd rather remove the internal
miner too, getting rid of the extra code and complexity, and package
up a GBT miner which would actually be usable on the mainnet.

@_date: 2013-08-19 14:07:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: remove "getwork" RPC from 
[Aside: When replying to the digest, please try to trim it]
It appears that we will soon be at a hashrate where all the desktop
CPUs in the world couldn't really make a dent in it... certainly not
desktop cpus using the slow integrated cpu miner, which is much slower
than external optimized cpu miners.
But this is why I suggest packaging up a modern mining tool that
supports CPU/GPU/FPGA/ASIC mining against a current bitcoind. Doing so
would reduce the difference between testnet and mainnet, and provide
an actually useful tool for contributing directly.
Though again, I note, that Jeff's patch doesn't actually remove the
integrated miner (I think it should?).  Just the getwork support for
external miners which don't use getblocktemplate... and if you're
going to download one of those you could go download bfgminer instead.

@_date: 2013-08-20 01:35:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 32.5 
Looks like we're in the midst of another DSA duplicated K disaster.
(Now, blockchain.info mywallet)
I talked to Pieter about this some earlier today and he sounded pretty
positive. I'll go ahead and start on an actual BIP document for it.

@_date: 2013-08-22 23:55:32
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Way to tell that transaction was issued 
It's feasible to do such things but I believe highly undesirable.
You're taking data which is inherently only of short term interest to
a single party in the whole world (the receiver) and enlarging the
transaction and increasing the effective transaction fees while
forcing (say) a hundred thousand other parties to spend effort
transmitting it, processing it, and storing it for all time.
While doing so you also leak to the whole world? who would have
previously had no way or reason to know? who the identity of one of
the parties in the transaction is in a strong cryptographically
non-reputable way... which then lowers the privacy of everyone in the
transaction graph region of that transaction since some coercive force
could send some ninjas out to bust some kneecaps of the identified
party until they tell them where those coins came from and where they
went. If you observe section 10 of Bitcoin.pdf you can see that
privacy in Bitcoin is based _exclusively_ on using pseudonymous
identities on every transaction. If you break that, you remove privacy
from Bitcoin, leaving it competitive disadvantage to centeralized
payment systems, which all provide pretty good basic privacy (against
most criminals and nosy neighbors) as a core feature.
Instead: You can simply perform this transaction using the payment
protocol, which could provide along all sorts of additional metadata
including signatures from the relevant parties.  By doing this, only
the parties that need to learn something learn something: privacy is
preserved and bloat is avoided.
If the payment protocol is too heavy handed for you, simply giving the
user a signmessaged txid can show a promise to pay for a transaction
without highly public communication.

@_date: 2013-12-08 08:51:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
Having control of the site gives you a cert regardless, as several CAs
will issue a cert to anyone who can make a http page appear at a
specific URL at the domain when requested via the CA over http.
It really is darn near pretextual security in this kind case? only
protecting you against attacks near the client, not the server? but as
Wladimir says, it's expected and I don't see how it would be a harm.
The revocation argument is somewhat interesting, especially since any
such site should use HSTS or otherwise a downgrade attack is trivial.

@_date: 2013-12-08 11:25:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
You're managing to argue against SSL. Because it actually provides
basically protection against an attacker who can actively intercept
traffic to the server. Against that threat model SSL is clearly? based
on your comments? providing a false sense of security.
We _do_ have protection that protect against that? the pgp signature,
but they are far from a solution since people do not check that.
(I'm not suggesting we shouldn't have it, I'm suggesting you stop
arguing SSL provides protection it doesn't before you manage to change
my mind!)

@_date: 2013-12-08 12:40:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
I am opposed to Bitcoin Foundation having control of Bitcoin.org, and
I think it would be foolish of the foundation to accept it were it

@_date: 2013-12-08 12:50:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
Sadly this isn't true: There are (many) CAs which will issue a
certificate (apparently sometime within minutes, though last
certificate I obtained took a couple hours total) to anyone who can
respond to http (not https) requests on behalf of the domain from the
perspective of the CA.
This means you can MITM the site, pass all traffic through except the
HTTP request from the CA, and start intercepting once the CA has
signed your certificate. This works because the CA does nothing to
verify identity except check that the requester can control the site.
If you'd like to me to demonstrate this attack for you I'd be willing?
I can provide a proxy that passes on :80 and :443, run your traffic
through it and I'll get a cert with your domain name.
I'm sorry for the tangent here? I think this sub-discussion is really
unrelated to having Bitcoin.org behind SSL? but "someone is wrong on
the internet", and its important to know that SSL hardly does anything
to reduce the need to check the offline signatures on the binaries.

@_date: 2013-12-08 13:09:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
My concern isn't a matter of trustworthyness, it's a matter of too
many eggs in one basket (especially a basket with potentially poor
jurisdictional locality).  The current control of the domain has
proven reasonably trustworthy, and if there is a concern for funding
our own server stuff that can be easily handled (e.g. if need be, I'd
pay for it myself, without being in control of it).
Also, in terms of effective lobbying/advocacy I worry that the
foundation would be unable to do an effective job if its saddled with
the belief that its in control of Bitcoin ("Why don't you just make
every transaction {...}": the answer is because its a decentralized
system and no one can unilaterally change it in ways its users would
hate, but it becomes complicated. It's crisper when its clear that
diverse and independant parties are in control of the popular

@_date: 2013-12-08 13:14:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
Godaddy and many other CA's are verified from nothing other than a
http fetch, no email involved.
As I said, I'm willing to demonstrate if you have a domain.
You can, once you've obtained a certificate.
As I warned before, you're making my reconsider my position about the
downloads being SSL. If people are so convinced that SSL provides
protection it does not that even with an explanation and and an offer
to demonstrate then perhaps providing SSL will reduce people's
... the _only_ reason I don't yet hold that position now is that I
know objectively that almost no one tests the signatures.
My understanding is that the domain is already controlled by more than
one person. You're not the first person to think of these things. :)

@_date: 2013-12-09 07:07:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.6 release candidate 1 
It has not been released. It's queued for announcement. We were
waiting for another independant gitian build before sending out the

@_date: 2013-12-09 07:25:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.6 release candidate 1 
Because it takes time to put the files up, propagate to mirrors, check
by multiple people that the downloads work an the signatures pass.
Every release comes with a release announcement, you'll know its out
when you see the release announcement.
The alternative is that announcements go out and the files are not
correct, or hide the files in the release pipeline and allow less
public participation in the release workflow. I think that would be

@_date: 2013-12-16 10:45:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fees UI warning 
I'd add a third: make structured key-management possible, e.g.
At a minimum:  Users should be able to hit a "retire keys / keys
possibly compromised" button, which creates a new seed, forces the
user to make a backup (and allows more than one), then switches to the
new seed and moves all their coins.
"We didn't say it couldn't be done?  We said don't do it!"
Part of the challenge here is that the service does a number of things
people _really_ shouldn't be doing? things so dangerous that I
certainly won't do them? and as a result to not use the site turn into
big education efforts rather than just "use this other thing (that
also does the wrong headed thing you want to do)".

@_date: 2013-12-17 14:48:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] RFC: MERGE transaction/script/process for 
We already automatically merge forks that we become aware of simply by
pulling in all the novel non-conflicting transactions the fork
contains and including them in our next blocks.

@_date: 2013-12-20 11:48:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
A couple very early comments? I shared some of these with you on IRC
but I thought I'd post them to make them more likely to not get lost.
Whats a VARCHAR()  A zero terminated string?  A length prefixed
string? How is the length encoded?  Hopefully not in a way that has
redundancy, since things that don't survive a serialization round trip
is a major trap.
Is the 'middle' the best place for the extradata? Have you
contemplated the possibility that some applications might use midstate
On that general subject, since the structure here pretty much always
guarantees two compression function invocations. SHA512/256 might
actually be faster in this application.
Re: using sha256 instead of sha256^2, we need to think carefully about
the implications of Merkle-Damgard generic length extension attacks.
It would be unfortunately to introduce them here, even though they're
currently mostly theoretical for sha256.
WRT hash function performance, hash functions are so ludicrously fast
(and will be more so as processors get SHA2 instructions) that the
performance of the raw compression function would hardly ever be a
performance consideration unless you're using a slow interpreted
language (... and that sounds like a personal problem to me). So I
don't think CPU performance should be a major consideration in this
What I do think should be a consideration is the cost of validating
the structure under a zero-knowledge proof. An example application is
a blind proof for a SIN or a proof of how much coin you control... or
even a proof that a block was a correctly validated one, and in these
cases additional compression function calls are indeed pretty
expensive. But they're not the only cost, any conditional logic in the
hash tree evaluation is expensive, and particular, I think that any
place where data from children will be combined with a variable offset
(especially if its not word aligned) would potentially be rather
I'm unconvinced about the prefix tree compressed applications, since
they break compact update proofs.  If we used them in the Bitcoin
network they could only be used for data where all verifying nodes had
all their data under the tree. I think they add a lot of complexity to
the BIP (esp from people reading the wrong section), so perhaps they
should be split into another document?
In any case, I want to thank you for talking the time to write this
up. You've been working on this stuff for a while and I think it will
be lead to useful results, even if we don't end up using how it was
originally envisioned.

@_date: 2013-12-31 05:48:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
Please cut it out with the snake oil pedaling. This is really over the
top. You're invoking the NSA as the threat here? Okay. The NSA can
trivially compromise an HTTPS download site: even ignoring the CA
insecurity, and government run CAs certificate authorities issue CA
certs to random governments and corporations for dataloss prevention
purposes. Not to mention unparalleled access to exploits.
The downloads are protected by something far stronger than SSL
already, which might even have a chance against the NSA. Actual
signatures of the downloads with offline keys.
I'm all pro-SSL and all that, but you are? piece by piece? really
convincing me that it produces an entirely false sense of security
which is entirely unjustified.

@_date: 2013-12-31 06:18:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
The site was actually moved onto a dedicated server temporarily and it
melted down under the load. I wouldn't call that no progress.
Perhaps I wasn't clear on the point I was making Drak's threat model
is not improved in the slightest by SSL. It would be improved by
increasing the use of signature checking, e.g. by making it easier.
Flat out misinformation never improves security.

@_date: 2013-02-06 08:45:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
I asked for permissions to unlock it but haven't heard back? will prod.

@_date: 2013-02-11 11:21:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Blockchain as root CA for payment protocol 
It seems you're not very well informed about what namecoin does? it's
a multiple namespace key-value store. And, as Peter pointed out? a
non-parasitic system can have exactly the same POW hashpower. Namecoin
chose a model which made it so that namecoin could survive even if
Bitcoin failed, but you don't have to.
I strongly recommend you listen to Peter and Luke? externalizing the
costs of your services onto people who do not care about them is not
going to produce good results for anyone. It's possible to accomplish
what you want to accomplish without taking resources from the users of
the Bitcoin currency without their consent? and you have people here
who are willing to help you figure out how.

@_date: 2013-02-12 07:49:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Incorporating block validation rule 
You misunderstand what BIP_0034 is doing? it's not gauging consensus,
it's making sure that the change is safe to enforce. This is a subtle
but important difference.  The mechanism happens to be the same, but
we're not asking for anyone's approval there? the change is needed to
make Bitcoin as secure as people previously believed it to be, there
have been no serious alternatives tendered. As far as I can tell the
proposal has always had universal agreement from anyone who's thought
about it.  The only open question was if it was safe to deploy, and
thats what that process solves.
Bitcoin is not a democracy? it quite intentionally uses the consensus
mechanism _only_ the one thing that nodes can not autonomously and
interdependently validate (the ordering of transactions). This
protects the users of Bitcoin by making most of the system largely
nonvolatile "constitutional" rules instead of being controlled by
popular whim where 'two wolves may vote to have the one sheep for
dinner'. If it were possible to run the whole thing autonomously it
would be, but alas...
Even if you accept the premise that voting is a just way of making
decisions (it isn't; it's just often the least unjust when something
must be done), mining is not a particularly just way of voting:
'Hashpower isn't people', and currently the authority to control the
majority of the hashpower is vested in a only a half dozen people.
Moreover, the incentives to abuse hashpower are sharply curtailed by
its limited authority (all one can do with it is reorder
transactions... which is powerful but still finite) and allowing
arbitrary rule changes would vastly increase that power.
There are some more subtle issues? if the acceptance of chain B
depends on if you've seen orthogonal chains A or A' first then there
can be a carefully timed announcement of A and A' which forever
prevents global convergence (thanks to a finite speed of light an
attacker can make sure some nodes see A first, some A').  If a rule
change can be reorged out, then it's not really a rule? any actual
rule prohibits otherwise valid blocks that violate it (and without
this distinction you might as well implement the 'rule' as miner
preferences). Additionally there is the very hard software engineering
QA problem for a sufficiently complex rule language? script isn't
turing complete and look at all the issues it has had.
In summary? this sort of thing, which has come up before, is
technically interesting and fun to think about but it would make for
substantial engineering challenges and would not be obviously
compatible with the economic motivations which make Bitcoin secure nor
would it be morally compatible with the social contract embedded in
the system today.

@_date: 2013-02-13 07:42:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Incorporating block validation rule 
In one sense it already is decided? there is a protocol rule
implementing a hard maximum, and soft rules for lower targets.  If
it's to be changed it would only be by it being obvious to almost
everyone that it should _and_ must be.  Since, in the long run,
Bitcoin can't meet its security and decenteralization promises without
blockspace scarcity to drive non-trivial fees and without scaling
limits to keep it decenteralized? it's not a change that could be made
more lightly than changing the supply of coin.
I hope that should it become necessary to do so that correct path will
be obvious to everyone, otherwise there is a grave risk of undermining
the justification for the confidence in the immutability of any of the
rules of the system.

@_date: 2013-02-13 16:28:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Incorporating block validation rule 
The cost to whom?  This is important because the cost of validating
blocks is borne by all the participants in Bitcoin? or at least all
the participants who haven't given up on the decenteralized trustless
stuff and are simply trusting someone else.   Even a small cost
becomes large when hundreds of thousands.
And perhaps you don't lament people delegating their trust to large
entities? but keep in mind: Bitcoin was created for the express
purpose of creating a money system which didn't require trust because
it was based on cryptographic proof? mathematical law? instead of
politics and human law.  Take that away and you have a really poorly
understood inefficient system operated by entities which are less
trustworthy and rightfully entitled to authority than the ones
operating the established major currencies.
Thats absolutely true? but I don't know that it's relevant in this case.
They can? but doing so would radically undermine Bitcoin.
A refresher:
If you combine digital signatures with simple transaction rules you
can have a purely autonomous monetary system based entirely on math.
It would be perfect, anonymous, scalable ...  except for the problem
of double spending.  To solve double spending the participants must
agree on which of a set of duplicated payments is one the
authoritative one. Coming to this agreement is fundamentally hard just
at the basics of physics? a result of relativity is that observers
will perceive events in different orders depending on the observer's
and the events relative locations. If no observer is privileged (a
decenteralized system) you have to have a way of reaching a consensus.
This kind of efficient consensus we need? which which participants can
join or enter at any time, which doesn't require exponential
communication, and which is robust against sock-puppet participants?
was long believed to be practically impossible.  Bitcoin solved the
problem by using hashcash to vote? because real resources were forever
expended in the process the sock-puppet problem is solved.  But the
vote only works if everyone can see the results of it: We assume that
the majority of hashpower isn't a dishonest party, and that honest
nodes can't be prevented from hearing the honest history. Nodes choose
then rules-valid history that has the most work (votes) expended on
it... but they can only choose among what they know of.  As Satoshi,
 "[Bitcoin] takes advantage of the nature of information being
easy to spread but hard to stifle".
The requirement for everyone to hear the history doesn't get talked
about much? at least with reasonably sized blocks and today's
technical and common political climates the assumption that
information is easy to spread but hard to stifle is a very sound one.
It's a good thing, because this assumption is even more important than
the hash-power honesty assumption (a malicious party with a simple
majority of hashpower is much weaker than one who can partition the
network).  ... but that all goes out the window if communicating
blocks is costly enough that the only way to sustain it is to
jealously guard and charge for access/forwarding.
The consequence of such a change is that the Bitcoin consensus
algorithm would be handicapped. How long must you wait before you know
that the history you have won't get replaced by a more authoritative
one?  Today an hour or two seems relatively solid.  In a world with
non-uniform block forwarding perhaps it takes days? if ever? before
any participant is confident that there isn't a better history
All doubly so if the bookkeeping required for this payment ends up
necessitating additional transactions and adds to the load.
[This is also the flaw in the 'Red Balloons' paper, making
transactions a dozen times longer just to attach credit for forwarding
doesn't seem wise compared to keep transactions so cheap to transmit
that even a small number of altruists make the equilibrium state be
Large miners would obviously locate and connect to each other. Even
enormous blocks are no problems for big industrial players.
Don't want to pay the cost to get their big blocks from them?  Your
loss:  If you don't take their blocks and they constitute the longest
history, you'll be believing the wrong history until such a time as
you wise up and pay the piper.  Your transactions will be reversed and
you'll lose money.
You can hypothesize some cartel behavior external to the rules of the
system? where by some consensus mechanism (????) some super large mass
of participants agree to reject blocks according 'extrajudicial
rules', some rule existing outside of Bitcoin itself? but there must
be a consensus because rejecting blocks by yourself only gets you
ripped off.
I don't see how this works? it basically embeds another hard consensus
problem (what is the criteria for blocks to be valid?) inside our
solution to a hard consensus problem (which are the best valid
blocks?),  but doesn't benefit from the same incentive structure?
locally-greedy miners obviously want to produce the largest blocks
possible? and in hashpower consensus non-miners don't have a voice.
That might be acceptable for ordering, but now you're deciding on the
rules of the system which all non-trusting participants must validate.
You could instead solve that consensus problem with politically
stipulated regulation or industry cartels, or good old-fashion kneecap
busting or whathave you. But then Bitcoin loses the transparency and
determinism that make it worthwhile.
I sure hope to hear something better than that.
This is basically the gap:   Right now I could afford hardware that
could process multiple gigabyte blocks? maybe it only costs as much as
a small house which is not an insane cost for a large business. But
the cost would be decidedly non-negligible and it would be rational
for me to let someone else take it. Applied to everyone, you end up
with a small number of the most vested parties doing all the
validation, and so they have full ability to manipulate like today's
central banks.
For a great many to perform validation? keeping the system honest and
decentralized as it was envisioned? without worrying about the cost
requires that the cost be almost unnoticeable. A tiny fraction of what
some industrial player? who profit from consolidation and
manipulation? could easily handle.  I'm skeptical about the system
internally self-regulating the size because of what gets called
"evaporative cooling" in social sciences? the cost goes up, some
people cross their "hey, I'm better off if I externalize the cost of
keeping Bitcoin secure by not participating" boundary and lose their
There is probably some equilibrium where Bitcoin is compromised
frequently enough that more validators spin up (and ignore past rule
violations which can't be undone without economic Armageddon), and eat
the costs even though there is an insane amount of freeloading going
on.  The trustworthiness of today's monetary systems suggests to me
that if there is an equilibrium point here it isn't a very trustworthy
one.  There is an even stronger equilibrium state available too: don't
use Bitcoin at at all.  If you want a system which is dominated by
political whim and expedience and large industrial players and is, as
a result, only somewhat trustworthy you can just use government issued
currencies? they're well established and have a lot less overhead than
this decentralized stuff.
(And generally? Security makes for a terrible market, security is
naturally a lemon market. The need is only clear in hindsight. In our
case it would be one with an enormous freeloading problem)
Our current anti-spam one is primarily an economic one? transactions
prioritized based on fee per KB in scarce blocks or priority (another
scarce commodity), the only really non-very-economic part is the
very-small-output heuristic.  I would argue that our economic
anti-spam mechanisms are currently failing at their job:  Various
parties are engaging in transaction patterns with near pessimal
efficiency? using a dozen (? sometimes thousands) of transactions
where one or two would be adequate. This isn't limited to just one or
two sites? many parties are using inefficient transaction patterns?
creating externalized costs on all future Bitcoin users?, simply
because there is hardly any incentive not to.
Though much discussion among technical people, no one has come up with
any reparametrizations that seem likely to achieve the desired
incentive alignment in the near term.  Of all the elements of the
anti-spam policy, it seems to me that the least economic? the minimum
output size? is actually the most effective (most spam suppression
relative to efficient usage suppression), especially as we move to
focusing on the UTXO set size. (The minimum output value requirement
discourages the creation of UTXOs which will never be economically
rational to redeem).

@_date: 2013-02-13 17:02:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Incorporating block validation rule 
With all I wrote on the gloom side? I thought I should elaborate how I
think that would work, assuming that my gloom isn't convincingly
It's the year 2043? the Y2038 problem is behind us and everyone is
beginning to forget how terrible it turned out to be?  By some amazing
chance Bitcoin still exists and is widely used.  Off-chain system like
fidelity bonded banks are vibrant and widely used providing scalable
instant and completely private transactions to millions of people.
Someone posts to the infrequently used IETF Bitcoin working group with
a new draft? It points out that the transaction load is high enough
that even with a 100x increase in block size completion for fees would
hardly be impacted and that? because computers are 2^20 times faster
per unit cost than they were in 2013? and networks had made similar
gains, so even a common wristwatch (the personal computer embedded in
everyone's wrist at birth) could easily keep up with 100 megabyte
blocks.... so the size should be increased as of block 2,047,500.
The only objections are filed by some bearded hippy at the museum of
internet trolling (their authentic reconstruction of Diablo-D3's
desktop exhibit couldn't keep up), and by some dictatorship who again
insists that their communist PeoplesCoin should be used instead? the
usual suspects.  And so, after a couple years of upgrades, it is so.
Or perhaps more likely? it would get revised along side a hardforking
cryptosystem upgrade (e.g. replacing sha256 in the hash trees with
SHA-4-512), thus amortizing out all the migration costs...
The trickiness and risk of changing it? of economic problems, of the
risk of undermining trust in the immutability of the system's rules?
only exists if there is genuine, considered, and honest controversy
about the parameters.  At the moment any increase would be sure to be
controversial: common hardware and networks would not obviously keep
up with our current maximum size, and our current transaction load
doesn't produce a usable fees market.  This cannot remain true

@_date: 2013-02-13 19:38:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Incorporating block validation rule 
Then perhaps I totally misunderstood what you were suggesting.  I
believed you were saying blocksize would be controlled by people
having to pay to receive and pay to have blocks forwarded.
The only fee-or-cost they're worrying about is their own marginal
costs.  This says nothing about the externalized cost of the hundreds
of thousands of other nodes which also must validate the block they
produce, many of which are not miners? if we are well distributed? and
thus don't have any way to monetize fees.  And even if they are all
miners for some reason,  if these fees are paying the ever growing
validation/storage costs what expenditure is left for the proof of
work that makes Bitcoin resistant to reversal?
If the cost is soaked up by validation/forwarding then the capacity to
run a validating node ends up being the barrier to entry and
difficulty would be very low... which sounds fine until you realize
that an attacker doesn't have validation costs, and that selfish
("optimally rational") miners could just eschew validation (who cares
if you lose some blocks to invalidity if you're producing them so much
cheaper than the honest players?).
What I want is for economics to dictate a positive outcome. They can
do this how the system is currently constructed where the economics of
using the system are clearly aligned with securing it.

@_date: 2013-02-23 16:56:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] How small blocks make delibrate orphan 
It's come up a number of times in the past that when there is no
subsidy we might expect slow convergence as miners try to orphan each
other instead in order to fee snipe.   What Peter pointed out here
that I had not previously considered and find interesting is was that
if there is a sufficient backlog (or nlocktime immature) of
transactions with fees beyond the maximum block size the incentive to
orphan blocks to take their fees is greatly reduced or eliminated.

@_date: 2013-07-17 16:04:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] SPV bitcoind? (was: Introducing 
This, however, reduces the node to SPV security of the past history.
Particularly for a wallet client? as opposed to a miner or what have
you? if you are willing to accept SPV security you could simply be an
SPV client.
(I like committed UTXO trees, and I believe I was the first person to
suggest them? but I think it's good to not over-hype what they do!)

@_date: 2013-07-23 13:14:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Linux packaging letter 
Ah, this is not entirely in sync with the last (mostly minor)
copyedits that had been signed off by Gavin, Pieter, Jgarzik, and I...
but that appears to be a smaller issue than the fact that the whole
thing is now being rewritten by "anonymous beaver" and friends.

@_date: 2013-07-23 13:50:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Linux packaging letter 
Some people/person went and actually started making substantive edits
to the text.
The text it's rolled back to is missing the last copyedits from last night too.
The text that had been ACKed last night was a3e52973,  available at
As far as the PGP goes?
I think using the PGP is good: it's making use of the right tools,
avoids issues like we just had where people go changing the content
after names had been affixed,  shows solidarity with people building
security infrastructure that our ecosystem depends on.  If you only
use it occasionally then its easy for someone to strip it when it _is_
needed and disguise that just as regular non-use.  It's my general
view that for people working in our domain basic competence and use of
these tools, even when they kinda stink, is a kind of civic hygiene.
At the same time it's not urgent. It's poorly used by people and will
be ignored by most but packagers are the most frequent users of it
that I've encountered.  Fortunately, it's harmless in any case.
If people are interested in offering PGP signatures of it:
wget gpg --clearsign 20130723-linux-distribution-packaging-and-bitcoin.md
and post the little signature asc. The result composes nicely:

@_date: 2013-07-23 17:50:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Linux packaging letter 
It's "portable" to anything that can run the relevant VMs.  Uh
provided you don't mind cross compiling everything from an unbuntu VM.
 It certainly would be nice if the trusted-computing-base for gitian
were a bit smaller, thats an area for long term improvement for sure.
It may need some massaging. The tor project is beginning to use the
same infrastructure, so this could be usefully conserved work.
Likewise expanding the supported output targets would be great? though
in the case of Bitcoin this is bounded by resources to adequately QA
builds on alternative targets.
In some cases packages solving bugs is problematic for Bitcoin.
This is something that it seems to take a whiteboard to explain, so I
apologize for the opacity of simple email here.
whole bunch of computers world wide to reach a bit identical agreement
on the content of a database, subject to a whole pile of rules, in the
face of potentially malicious input, without any trusted parties at
all (even the guy you got the software from, assuming you have the
resources to audit it).
I'll walk through a simple example:
Say Bitcoin used a backing database which had an unknown a bug where
any item with a key that begins with 0xDEADBEEF returns not found when
queried, even if its in the DB. Once discovered, any database library
would want to fix that quickly and they'd fix it in a point release
without reservation. They might not even release note that particular
fix it if went along with some others, it could even be fixed
Now say that we have a state where half the Bitcoin network is running
the old buggy version, and half is running the fixed version.  Someone
creates a transaction with ID 0xDEADBEEF...  and then subsequently
spends the output of that transaction. This could be by pure chance or
it could be a malicious act.
To half the network that spending transaction looks like someone
spending coin from nowhere, a violation of the rules.  The consensus
would then fork, effectively partitioning the network.  On each fork
any coin could be spent twice, and the fork will only be resolvable by
one side or the other abandoning their state (generally the more
permissive side would need to be abandoned because the permissive one
is tolerant of the restrictive one's behavior) by manually downgrading
or patching software.  As a result of this parties who believed some
of their transactions were safely settled would find them reversed by
people who exploited the inconsistent consensus.
To deploy such a fix in Bitcoin without creating a risk for
participants we need to make a staged revision of the network protocol
rules:  There would be a protocol update that fixed the database bug
_and_ explicitly rejected 0xDEADBEEF transactions until either some
far out future date or until triggered by quorum sensing (or both).
The users of Bitcoin would all be advised that they had to apply
fixes/workaround by the switchover point or be left out of service or
vulnerable. At designated time / quorum nodes would simultaneously
switch to the new behavior.  (Or in some cases, we'd just move the
'bug' into the Bitcoin code so that it can be fixed in the database,
and we'd then just keep it forever, depending on how harmful it was to
Bitcoin, a one if 4 billion chance of having to rewrite a transaction
wouldn't be a big deal)
We've done these organized updates to solve problems (as various flaws
in Bitcoin itself can present similar consensus risks) several times
with great success, typical time horizons spanning for months to
years.  But it cannot work if the behavior is changed out from under
the software.
Fortunately, if the number of users running with an uncontrolled
consensus relevant inconsistent behavior is small the danger is only
to themselves (and, perhaps, their customers). I'm not happy to see
anyone get harmed, but it's better if its few people than many. This
is part of the reason that it's a "linux packaging letter", since for
Bitcoin the combination of uncoordinated patching and non-trivial
userbases appears to be currently unique to GNU/Linux systems.  Though
indeed, the concerns do apply more broadly.
My understanding is that gentoo is actually able to handle this (and
does, for Bitcoin)? and really I presume just about everything else
could with enough effort. I certainly wouldn't ask anyone else to do
that.  If you're really getting into the rathole of building separate
libraries just for Bitcoin the value of packaging it goes away.
Running a complete set of tests is a start? though the unit tests are
not and cannot be adequate. There is a full systems testing harnesses
which should be used on new platforms.  Even that though isn't really
adequate, as it is currently infeasible to even achieve complete test
coverage in things like cryptographic libraries and database
This is an area where both the Bitcoin software ecosystem and the
greater art of large scale software validation need to mature. You
won't hear anyone applauding the fact that harmless looking bugfixes
in leveldb, boost, or openssl could be major doom event makers
We're not crazy folks who insist on using formally undefined behavior
and argue that it should never be changed out from under us. When
there is a known risk we will boil the oceans to close it even if we
think that the world would be more 'proper' some other way,  but for
known-unknowns and unknown-unknowns we can only adopt a conservative
approach and try to do our best.
One of the middle term things we did was internally integrated our
validation database library (leveldb).  Since we _know_ that its a
consistency critical component, and a part of our system that is
especially difficult to validate, integrating it meant removing a lot
of risk and allowed it to be upgraded with an eye on the
Bitcoin-specific consequences.  Unfortunately distributions have been
patching Bitcoin to unbundle it.  Checking versions isn't adequate
because, at least in other packages, some distributors frequently
backport fixes or apply novel fixes which may not even be shared with
Other considerations may drive us out of external dependencies for
many of the consensus parts of Bitcoin. E.g. Pieter has writen an
ECDSA library for our specific ECC curve which does signature
validation >6x faster than OpenSSL (but it isn't obviously
upstreamable due to some differing requirements for constant time
operations), at some point we may need to adopt a backing database
that is able to produce authentication proofs, etc.  Certainly
additional clever tests will make undiscovered surprising behavior
less likely, though figuring out how to get the tests actually run if
they take two hours and use 20GB of disk space is a challenge.
... but today we need to work with what we have, which is fragile in
some atypical ways.  Part of that is making an effort to make sure
that anyone who might create a big footgun event has some idea of the
concern space.
Configure time?  At the moment Bitcoin is built with a straight
forward makefile (though there is a switch to autotools in the
BE isn't currently supported (and I believe this is well documented in
the package).  Fixing this would be nice, patches accepted.   There
was an amusing incident a while back where a distributor was refusing
to take an update that added unit tests because they revealed failures
on BE, nevermind that the application itself instantly failed on BE
and never worked there. I believe that has since been resolved.
I _believe_  (and hope) we've been very accommodating system specific
fixes, even for systems we don't formally support.
I personally believe that portable software is better software.
Portability forces you to dust out nasty cobwebs, reveals dependency
on dangerous undefined behavior, encourages intelligent abstractions
and appropriate testing, and it invites contributions from more hands
and eyes? I don't care if you use a weird OS: I just want you for your
code and your bug-reports.  So even if we don't consider a platform
worth supporting in any rigorous way, we should still be open to fixes
and build support.
... although we're typical very much resource bound on testing. Our
upstreaming pipeline is often somewhat slow. But it's slow because we
are serious about review, even of trivial changes. Being slow is no
reason to not submit, even if you make a decision to not block on it
(though, if you're doing that you should make the decision in full
knowledge of the potential implications). Like all things stepping up
and being willing to do the work goes a long way to getting things

@_date: 2013-07-23 20:19:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Linux packaging letter 
Bummer, because this was a explicit consideration while writing it and
a concern several people had with the initial draft Mike did.
We're very much aware that upstreams frequently cry (wolf) at the
mutilation of their unique and precious snowflake.
The intention was that second paragraph acknowledging the many good
motivations for the existing norms and the third paragraph talking
about consensus systems would address these concerns? showing that we
aren't totally clueless, and pointing out that we have an actually
unusual situation. In intermediate drafts they were longer and more
elaborate, but we were struggling against length and trying to avoid
delving into a highly technical discussion which would lose anyone who
wasn't already very interested.
We also compromised on an initial approach of "please don't package
this at all" to "please understand first", in part at the protest of
our gentoo package (which also bundles leveldb but hard locks it to an
exact version in the package system with exact build flags, which is a
sophisticated compromise which might not generalize to other
distributors) maintainer (uh, Luke-Jr, not exactly the most
representative sample).
As a first step it's at least important to know that there is a
concern here shared by a bunch of people. Helping talk people through
understanding it is part of the job here.  I certainly didn't expect
the discussion to stop with the letter but getting it out there is a
way to start the discussion and make it more likely that we have it
again with the next packager who comes around.
I guess the first priority though is avoiding gratuitously offending
people.  Can anyone point out any specific tweaks that would reduce
initial bristling?
Oh be nice. If any of this were easy it would all be _done_ already. :)
There is naturally some tension when people with different priorities
and backgrounds interact, ... I've seen a lot of upstreams run into
disagreements with packagers the result is usually better for

@_date: 2013-07-23 21:07:37
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Endianness (was: Linux packaging letter) 
Not really. The software was originally written to write out memory
order to and from the wire, which is part of why the protocol is LE
everywhere, so fixing that much is pretty typical endianness fixes.
There is an extra kink in that almost everything Bitcoin sends and
receives is an authenticated data structure? the stuff gets hashed for
authentication.  So that simply swizzling the byte order on
immediately on input isn't enough because sometimes you'll go on to
hash that data and it can't be in memory order for that.
Luke gave an initial crack at it a long time ago:
But it wasn't enough yet.
Seems like its just enough of an undertaking that absent a really good
reason to care about it no real progress in fixing it is happening.

@_date: 2013-07-23 21:09:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Endianness (was: Linux packaging letter) 
*before someone corrects me, it's not LE everywhere (I meant
"manywhere" :P)? there is just enough BE to keep you on your toes. :P

@_date: 2013-07-24 12:35:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Linux packaging letter 
Maybe!  A widespread consensus failure causes people to lose money
even absent malice. How much depends on a bunch of details, including
the luck of attackers.
The total ramifications are as much social as they are technical so
it's hard to reason over the outcomes beyond "at a minimum, it's not
A really bad splitting event could results in large amounts of Bitcoin
being stolen through reversals. Obviously the system itself would keep
on ticking once the issue was resolved... but if millions of dollars
at recent prices in coins were stolen,  would people want to keep
using it?
The most dire outcomes are (very?) unlikely, but they're not necessary
to recognize that risk mitigation is important.
It's good to be careful here just to avoid the bad outcomes we are
sure will happen (because we've experienced them before):   Hundreds
of dollars worth of coin income 'lost' per minute to miners on the
losing side of a 50/50 fork, hours long disruption of the lives of
dozens of people in the Bitcoin technical ecosystem (many of whom are
volunteer OSS developers), hours of disruption (no payments processed)
to Bitcoin users and businesses.  These are the best case outcomes in
a substantial non-transient hard forking event.
I think one of the challenges in talking about this stuff is correctly
framing these risks.  Bitcoin is a novel technology that lacks a lot
of the recourse that other systems have? No Bitcoin central bank to
create a bit of inflation to paper over a glitch,  eliminating those
kinds of centralized "fixes" is much of the point, after all?  so with
the idea of starry eyed people taking out second mortgages on their
kids kidneys to buy up coin clearly in my mind I do think it's
important to be clear about the full range of risk:  It's _possible_
that due to some amazing sequences of technical screwups that by next
week most everyone could consider Bitcoin worthless. I think it's
important to be frank about those risks.  ... but it's also not good
to be chicken little, calling doom on anyone who wants to change the
color of the GUI. :P   Navigating it is hard, and generally I'd prefer
that if there is any misunderstanding people overestimate the risks a
little? so long as things stay in the realm of the possible? rather
than underestimate them.

@_date: 2013-07-29 23:50:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BitMail - p2p Email 0.1. beta 
Keep safe everyone:
A number of apparent sock accounts has been posting about what appears
to be the same software under the name "goldbug" for a couple days

@_date: 2013-06-27 04:04:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double-Spending Fast Payments in Bitcoin 
It would be kind if your paper cited the one of the prior discussions
of this transaction pattern:
E.g. (I think there are a couple others)
The family of transaction patterns you describe is one of the ones I
specifically cite as an example of why taking non-reversible actions
on unconfirmed transactions is unsafe (and why most of the Bitcoin
community resources) council the same.  You can get similar patterns
absent changes in the IsStandard rule through a number of other means.
 One obvious one is through concurrent announcement: You announce
conflicting transactions at the same time to many nodes and one
excludes another.  By performing this many times and using chains of
unconfirmed transactions and seeing which family your victim observes
you can create input mixes that are only accepted by very specific
subsets of the network.

@_date: 2013-06-27 09:13:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double-Spending Fast Payments in Bitcoin 
On Thu, Jun 27, 2013 at 9:03 AM, Arthur Gervais
It works just the same for dust based or any other criteria that makes
transactions non-standard? including the double spending working if
the second transaction is sent minutes after. Exactly the same code is
executed and the same behavior observed for any case of a non-standard
transaction being used to achieve inconsistent forwarding.
That is great and I'm certainly glad to see people doing that.
Though take care it that your focus on signature encoding differences
doesn't create a misunderstanding. This isn't only an issue with these
particular versions: There is always mining and relay behavior
inhomogeneity in the network. The level of inhomogeneity changes over
time? I believe its greatest when new reference client software that
changes IsStandard but it is never zero as there are large miners with
customized acceptance rules (also mempool state also creates
inhomogeneity). The greater inhomogeneity results in higher success
rates which may be important since some service could conceivable only
be profitable exploited with a high enough success rate.

@_date: 2013-06-27 10:56:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
Being able to promote a fast SPV desktop wallet would be great!
I went through an cycle of testing on multibit after I saw some
complaints when it went up on the page before without at lot of
discussion. There were a number of issues with it at the time, in
particular the frequent deadlocks? though Mike was saying that those
should be fixed.
I see some of the the other things that were concerning for me at the
time are still uncorrected though, e.g. no proxy support (so users
can't follow our recommended best practices of using it with Tor),
that it reuses addresses (esp for change), that it doesn't clearly
distinguish confirmation level. It also make repeated https
connections to 141.101.113.245? (I'm not seeing the IP in the source,
and it doesn't have a useful reverse dns entry, so I can't tell what
its for).  Is there any timeframe for changing any of this stuff?

@_date: 2013-06-27 11:41:56
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
Without validation listening isn't currently very useful. :( Maybe it
could be somewhat more with some protocol additions.

@_date: 2013-03-03 12:02:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Secure download 
While I think that it's silly that we don't have a HTTPS (only!) page,
it should be noted that an HTTPS page is in no way a replacement for
GPG, sadly:  Anyone who can MITM the server to the whole internet can
trivially obtain a fraudulent cert with only moderate cost and time.
(The reason for this is that (many? most? all?) CAs verify authority
by having you place a file at some HTTP path on the domain in
question. Effectively the current CA model only prevents those from
intercepting who cannot intercept the traffic generally. Basically
only helps with the evil hotspot/tor_exit problem.)

@_date: 2013-03-12 05:38:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
Locks are only mostly related to block size, once I heard what was
happening I was unsurprised the max sized test blocks hadn't triggered
until nodes start dying en-masse.
Scaremongering much? Egads.
And ... if you aren't aware that you're making a change ???

@_date: 2013-03-12 06:06:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Changing the fee on already sent 
I _strongly_  prefer this method of addressing this concern.  I think
you've hit the required requirements: pay at least all the same
inputs, increase fee by at least the min_relay_tx_fee*size.
The discussion we had on IRC where some were proposing fast expiration
would practically lower the security of Bitcoin.
While there is complexity and testing required here, getting full
branch coverage of this code would be fairly straight forward.  Doing
this correctly will be easier than child-pays-for-parent and although
it does not replace child-pays-for-parent (the two techniques are
complimentary in my view) it would reduce the urgency of getting
child-pays-for-parent included.

@_date: 2013-03-12 11:09:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Some PR preparation 
Some but not much.  If someone flooded a bunch of duplicate
concurrently announcing both spends to as many nodes as they could
reach they would almost certainly gotten some conflicts into both
chains. Then both chains would have gotten >6 confirms. Then one chain
would pop and anyone on the popped side would see >6 confirm
transactions undo.
This attack would not require any particular resources, and only
enough technical sophistication to run something like pynode to give
raw txn to nodes at random.
The biggest barriers against it were people being uninterested in
attacking (as usual for all things) and there not being many (any?)
good targets who hadn't shut down their deposits.  They would have to
have accepted deposits with <12 confirms and let you withdraw. During
the event an attacker could have gotten  of their deposit-able funds.
There were circulating double-spends during the fork (as were visible
on blockchain.info). I don't know if any conflicts made it into the
losing chain, however. It's not too hard to check to see what inputs
were consumed in the losing fork and see if any have been consumed by
different transactions now.
I agree it would be good to confirm no one was ripped off, even though
we can't say there weren't any attempts.

@_date: 2013-03-13 06:14:00
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
I'm not a fan of the two stages, your before block 262144 part sounds
fine to me, though I thought the safe number was closer to 5000.
Perhaps 4911?
The goal here is to pick something which is _absolutely sure_ to be
less than what pre-0.8 accepts (so that its is just a soft fork), but
it need not be needlessly smaller than that.
I think we can accept some small risk of "backport" clients getting
stuck after large reorgs after there has been sufficient upgrade time.
 Performance reasons mean that its very likely no one will be mining
on those nodes by then, and so if they get stuck they'll just need to
manually unstick them. Difficulty is high enough that its unlikely
anything important will remain stuck long enough for a malicious party
to exploit them by mining blocks on the stuck fork.
By allowing that risk you halve the complexity of your change by not
requiring two hard forks.  The 'never make' half of it would probably
be fine.
As far as the size change, that should be a separate process after
we've proven the ability to make a hardforking change with something
low risk/low controversy like this, and only after someone has
actually shown that the software is stable under those conditions lest
we get another issue like we have now where the increase in block
target from 500k/250k to 1MB by a miner exposed inadequate testing.

@_date: 2013-03-13 08:18:36
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
I beg people to not derail discussion about fixing things with
discussion of other controversial changes.
Luke-jr, any chance in getting you to revise your proposal to narrow
the scope to things that don't need serious debate?

@_date: 2013-03-13 12:30:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
In our common language a hardfork is a rule difference that can cause
irreconcilable failure in consensus; it's not some political change or
some change in the user's understanding or something fuzzy like that.
Please don't creep the definitions... but arguments over definitions
are silly.  If you really object to calling the causes consensus
failure thing something else okay, then suggest a name, but whatever
its called thats what we're talking about here.
Your proposal of having a hardfork but only on the mining nodes has
coordination problems. What happens if we don't know how to contact a
majority of the hashpower to get them to turn off their special
validation code?  This is especially a concern because it's not
unlikely that in a few months there may be solo miners with tens of
TH/s... already we have a single party with nearly a majority, though
at the moment they happen to be mining on the largest couple pools.
Far better to have this special code just triggered on a deadline,
which can be widely advertised as "you must upgrade to 0.7.4 or >0.8.1
before this time" and then all switch at once... and then we
demonstrate the viability of a general mechanism that doesn't depend
on poor miner decentralization.

@_date: 2013-03-13 13:24:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
On Wed, Mar 13, 2013 at 1:10 PM, Matthew Mitchell
The development community backports critical fixes which makes
updating instead of upgrading possible, but that still is not free.
Many people are carrying patches against Bitcoin which require
integration and time for testing? even if its just an update. Small
behavior changes can still break things for the users. For example, a
major mining pool lost well over 1000 BTC when upgrading to 0.8
because the reindex interacted poorly with their pool server software
and caused them to pay people 25 BTC per share, an update or upgrade
is just a risky even whos risk can be minimized if its done at your
own pace.
Sometimes when there is a vulnerability what people will do is isolate
their production nodes from the internet using upgraded nodes, so they
avoid touching the production systems. Other times the vulnerability
is only a DOS attack so they ignore it unless the attack happens, or
only applies to something else they don't care about.
Another point is that if everyone instantly upgrades in response to
developers claim that an urgent is needed (as opposed to implementing
other workarounds) then the security of the system much more obviously
reduces to the ability to compromise a developer? something no one
should want. When roll outs take time there is more time for review to
catch things, fewer nodes harmed by an introduced flaw, etc.

@_date: 2013-03-13 14:15:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] On fork awareness Was: 0.8.1 ideas 
The development of two chains was maximally bad because the state was
irreconcilable without the manual intervention. The only reason you're
saying that it was only the false confirms that were bad is because
manual intervention resolved the worse badness. :)  A whole bunch of
people spending coins that can only exist in the different exclusive
chains would have been very bad indeed.
Not reliably, you will only hear of a competing chain if some of your
peers have accepted it. If your peers were all pre-0.8 or all 0.8 you
only would have heard of one chain.
Relaying non-primary chains has some obvious and subtle challenges?
Obviously you need some way of preventing it from being a DOS vector,
but thats not a fundamental issue? you could rate limit by only
relaying chains which are close to the current best in sum difficulty?
just a software engineering one.
A more subtle issue is it that it's not in a node's self-interest to
tell peers about a chain it thinks is invalid: it wants its peers on
_its_ view of the consensus, not some other one.  Though perhaps this
could be addressed by only relaying headers for non-primary chains.

@_date: 2013-03-13 14:27:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
It does warn? if its heard the fork and its on the lower difficulty
side. Extending that to also alert if its on the winning side and the
fork is long enough might be wise, though I have a little concern that
it'll be mistaken to be more dependable than it would be.

@_date: 2013-03-15 12:52:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 0.8.1 ideas 
No one thinks its controversial to remove it or that it's a good thing
to have? only that its technically somewhat complicated and risky to
remove it.

@_date: 2013-03-23 12:18:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Upcoming network event: block v2 lock-in 
You're both right: BIP34's addition of the height is implemented in
the coinbase generator, so for almost everyone the Bitcoind version is
not very relevant for that.  Rejection of invalid chains, however, is
in the Bitcoin daemon.  Upgrading bitcoind alone should be sufficient
to prevent the creation of forks, though if miners would like to avoid
losing income they must update _both_ so that they don't create
invalid blocks or mine on invalid forks.

@_date: 2013-03-23 18:22:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A bitcoin UDP P2P protocol extension 
You tunnel it!
You could do worse to have a data stream that looks like WEBRTC traffic?
In some ways SCTP is a pretty optimal transport for Bitcoin like messaging.

@_date: 2013-03-25 14:10:53
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Key retirement and key compromise 
That is quite drastic enough, as it requires adding more perpetual
data that must remain in fast lookup for all validating nodes (the set
of revoked 'addresses').
Keep in mind that this is only improvement for what is a usually
inadvisable usage of Bitcoin to begin with... you should not be
reusing addresses.

@_date: 2013-05-06 10:42:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
SSL doesn't actually provide non-repudiation. We actually want
non-repudiation. I want to be able to prove to others that some node
deceived me.
(there are a number of other arguments I could make against SSL, but
that one is probably sufficient? or rather, it's an argument that we
should have some way of cheaply getting non-reputable signatures
regardless of the transport)
Also look into torchat, which bundles a special tor build and runs a
hidden service.
Because of services like Blockchain.info attacking the casual privacy
users not using their webwallet service I've been thinking that even
for clients that don't normally use tor their own transaction
announcements should probably be made by bringing up a connection over
tor and announcing. But thats another matter...
I've switched to running on tor exclusively for my personal node (yay
dogfooding) and I've found it to connect and sync up very fast most of
the time. The biggest slowdown appears to be the our timeout on the
tor connections is very high and so if it gets unlucky on the first
couple attempts it can be minutes before it gets a connection. We're
short on onion peers and I sometimes get inbound connections before I
manage to get an outbound.

@_date: 2013-05-06 11:01:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
Because if you just want bitcoin p2p over SSL... just start up stunnel
on another port. Done. You've still solved nothing about the problem
of discovery issue.
That isn't so. If a node is reliably rogue I can go manually gather
evidence and people can manually take action against it.  Consider the
DNSseeds, right now fraud proofs really wouldn't matter? the limited
amount of trust put in those things is based not on "oh no, nodes will
ignore you in the future if you're bad", it's based on the ability of
misconduct to sully the operator's reputation.
But without non-repudiation the ability to tie reputation to good
behavior is fairly limited especially if they perform targeted
attacks. "Wasn't me"
Instead? I'd argue that non-repudiation is always useful when there is
trust. It's things like fidelity bonds? a trust generator that depend
on automatic enforcement? that are only useful with fraud proofs.
Yea, indeed, per-node keys are useful for a bunch of things. Care is
needed to avoid problems like deanonymizing use over tor with them.

@_date: 2013-05-06 11:25:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
Uh.  It currently costs about 2016*25*$120 = six million dollars to
reduce the difficulty in your isolated fork by a factor of 4.
To reduce it by a factor of 1000 (what would be required to make a
parallel fork that you could maintain in realtime with a single avalon
device) the cost is  sum(2016*25/4^n*120,n,0,ceil(log4(1000))) or
about eight million dollars.
Surely you can think of attacks on Bitcoin which are less expensive
than eight million dollars. :P
Protecting against that? making sure any such attack has to start from
a high difficulty? is, in my opinion, the biggest continued
justification for checkpoints.
They are signed.
No, it doesn't. It has centrally controlled directories that publish
an official Truth of the Network. Someone can isolate you and thus DOS
you, but they can't put you on a fantasy tor network.  But ...
It does, and we also consider decentralization a core value. But even
the tor project would like to decentralize more.

@_date: 2013-05-06 16:13:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] limits of network hacking/netsplits (was: 
Or you can just let it mine honestly and take the Bitcoins. This is
fast (doesn't require weeks of them somehow not noticing that they're
isolated), and yields the values I listed as 'costs' if you would have
otherwise been able to use it to mine the difficulty down to 1.  Cost
is just as much foregone income from the alternative attack you could
have done instead.
At least for attacks that drive the difficulty down it does.
If you want to talk about abusing a pool or creating a partition in
order to create short reorgs? I agree, those don't have to be long
lived and you can find many messages where I've written on that
It's inconsiderate to propose one attack and when I respond to it
changing the attack out from under me. :(  I would have responded
entirely differently if you'd proposed people segmenting the network
and creating short reorgs instead of mining the difficulty down.
Every 2016 blocks can at most lower the difficulty by a factor of 4,
thats where the log4 (number of 2016 groups needed) and 4^n (factor in
cost reduction for each group) come from in the formulas I gave
The signatures can't be inside the tarball because they sign the tarball.
Seems like the website redesign managed to hide the signatures pretty
good. They're in the release announcements in any case, but that
should be fixed.  Even when they were prominently placed, practically
no one checked them. As a result they are mostly security theater in
practice :(, ? so? unfortunately, is SSL: there are many CA's who will
give anyone a cert with your name on it who can give them a couple
hundred bucks and MITM HTTP (not HTTPS!) between the CA's
authentication server and your webserver. Bitcoin.org is hosted by
github, even if it had SSL and even if the CA infrastructure weren't a
joke, the number of ways to compromise that hosting enviroment would
IMO make SSL mostly a false sense of security.
The gpg signatures and gitian downloader signatures provide good
security if actually used, solving the "getting people to use them"
problem is an open question.
And I agree, this stuff is a bigger issue than many other things like
mining the difficulty down.

@_date: 2013-05-15 18:39:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
Sort of, but in a guy fawkes signature you use the commitment to hide
the preimage that proves you had authority to spend a coin.   Adam
proposes you do this in order to hide _which coin you're spending_.
This has obvious anti-DOS complications, but Adam deftly dodged my
initial attempts to shoot him down on these grounds by pointing out
that you could mix blinded and blinded inputs and have priority and
transaction fees come from only the unblinded ones.
Effectively,  it means that so long as you could convince the network
to let you spend some coins, you could also spend other ones along for
the ride and the network wouldn't know which ones those were until it
was too late for it to pretend it never saw them.
I think there are all kinds of weird economic implications to this? a
blinded payment would seem to have a different utility level to an
unblinded one: you can't use it for fees? except you can unblind it at
any time.  And the discontinuousness  ("two types of inputs") and that
it would enable mining gibberish (though perhaps not data storage, if
you see my preimage solution to that) seems awkward and I think I have
to spend some time internalizing it before I can really think through
the implications.

@_date: 2013-05-15 19:45:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] blind symmetric commitment for stronger 
Zerocoin conceals the connection from everyone forever, assuming the
underlying trapdoor problem is computational infeasible, but at great
Adamcoin, depending on how its done, at most conceals the transactions
from people who aren't a party to them... though as time goes on
eventually everyone becomes a party to a sufficiently old coin, and
avoiding publication creates quadratic costs in evaluating a private
clique's claims.... so instead an implementation would make the
identities public but only once they're burred a bit.
Perhaps an extreme version of the idea is easier to understand. Ignore
DOS attacks for a moment and pretend there is never any address reuse:
Everyone creates txouts paying a P2SH addresses that have a OP_PUSH
nonce in them and tell you recipient the nonce out of band.
When the recipients spend those coins they provide the script but not the nonce.
The recipient knows what coins he's spending, but the public does not.
The public can tell there is no double spend though, because they'd
see the same script twice. The person he's paying may be skeptical
that he actually has any coin and didn't just mine some gibberish, but
our spender tells that their receiver the nonce, and that person can
see the coin available for spending in the chain and also see that
there are no double spends.
This could actually go on forever with no ambiguity over who owns
what, but the out of band proofs that you have to give people when you
spend coins would grow with the history of the coins.
Since there wouldn't be much privacy once a transaction was
sufficiently split up in any case, you instead just publish the
unblindings once transactions are sufficiently buried. Thus bounding
the growth of the proofs.   The reason I say I need to internalize
this more is mostly that I need to think about attacks on the
publication for 'tained' transactions being more or less isomorphic
to just refusing to allow spending of the 'tainted' coins in any case.

@_date: 2013-05-20 20:54:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double Spend Notification 
Indeed. I've argued against it pretty aggressively, but I am starting
to find arguments for and against pure fee-based replacement more
equally persuasive.
Regardless of the eventual outcome, fees today aren't a major
motivator vs subsidy and overall network health? and even if some
protection isn't 100% there are plenty of cases where the risk is
averaged across many small transactions and any reduction of risk is a
reduction in operating cost.   (No one is going to double spend your
soda machine at high speed? so you can like increase your prices by
the amount of successful double spends you experience and call it
On the other hand, it's well established that people underestimate the
costs of unlikely risks. More deterministic behavior can result in
safer interactions more than _better_ behavior. If we believe that in
the long term self-interest will result in pure-fee based replacement
being wide spread then it is also good to not build a dependency on
behavior that will not last.
One point that was only recently exposed to me is that replacement
combined with child-pays-for-parent creates a new kind of double spend
_defense_: If someone double spends a payment to an online key of
yours, you can instantly produce a child transaction that pays 100% of
the double spend to fees... so a double spender can hurt you but not
profit from it.  (and if your side of the transaction is
potentially/partially reversible he will lose)...
But then again, a race to burn more money is kinda ... odd and even if
the benefit of resisting the double spends is only a short term
benefit, a short term benefit can be greatly important in encouraging
Bitcoin adoption. ... and the long term behavior is far from certain.
So at least from my position it's far from clear what should be done here.
I've noticed a number of people who seem to be swayed by replace by
fee? or at least its inevitability if not value. So even ignoring
developers there may evolve a community consensus here regardless of
what I think about it.
My SO pointed that that the transaction burning race described above
sounds like an economists wet dream: it's one of those silly cases
they use in experiments to probe human behavior... except it sounds
like a possible eventual outcome in systems used by people.  Perhaps
it would be useful to point some graduate students at this question
and see what they can come up with about it.

@_date: 2013-05-21 00:04:16
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Double Spend Notification 
(to the list:) Is there anyone who is not?  (assuming that it doesn't
allow arbitrary traffic multiplication, which is easily solved)

@_date: 2013-11-04 07:51:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Yea, I've proposed this too (both in the past and in the context of
this). I don't think, however, that the announcements need to be the
miners themselves? but instead just need to be nodes that the miners
think are good (and, for their own sake? ones they're well connected
Miner's could keep a list of address messages nodes they
like/are-connected to, perhaps prioritizing their own nodes, than
exclude ones which are already in the most recent blocks, and include
the best remaining. Of course, if it's using address messages (or
perhaps a new address message syntax) it would automatically support
hidden services.
They should probably be included as OP_RETURN outputs in coinbase
transactions, maybe only limited (by what other clients pay attention
to) to one or two per block.
This should make it harder to get partitioned from the majority
hashrate (or partition the majority hashrate from itself), though
these hosts would be DOS targets, so it isn't a silver bullet.
Making the majority hashrate self-unpartitionabilty stronger is
possible? have miners add an encryption key to their coinbase
transactions, then have subsequent miners mine encrypted addr messages
to single other block sources to automatically weave a miner darknet
with access controlled by successful block creation. But I doubt it's
worth the complexity of bandwidth.

@_date: 2013-11-04 07:58:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
This isn't so.  Their solution creates a weaker form of the
vulnerability at all times, not just when the attacker has a
informational/positional advantage.
Normally delaying your blocks is negative expectation because you will
get orphaned by blocks that are announced before you most of the time
because miners extend the first seen. However, if you can position
yourself all over the network you can condition your announcements on
other blocks being announced and still win the race even if you
Eliminating the first seen rule means that a miner with enough
hashpower (including the largest pools existing today) could execute
this attack without positioning themselves all over the network, the
improvement is that a low hashrate attacker couldn't do as well, even
with positioning themselves all over the network.  I don't think this
can be described as "simply corrects the error".  The largest pool
would gain an advantage in delaying their blocks and would receive a
superliner share of mining income from doing so, something they can't
simply do today without attacking the network.
At the moment I believe we can improve the situation with propagation
advantage without the other changes, so we should do that first while
thinking carefully about this.
Simply relaying late blocks might be fine, if anything it would at
least make it easier to keep reliable orphan stats... though I'm
concerned with the bandwidth overhead and risk of flooding if its not
implemented carefully.

@_date: 2013-11-04 22:37:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Likewise, I did too and am also not very tolerant with "trusted" or
"centeralized" things in general.
An authenticated miner announced set of nodes is _far_ from a cure
all, as any attack they stop can be recovered by adding "and dos
attacks the public miner announced nodes" to the attack's
requirements... but we build security with layers.
Bitcoin's security is only improved when we can weave the network
tighter and make partitioning it more difficult.

@_date: 2013-11-05 15:06:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Possible Solution To SM Attack 
uh. and so when my solution is, by chance, unusually low... I am
incentivized to hurry up and release my block because?
I've simulated non-first-block-heard strategies in the past (in the
two nearly tied miner with network latency model) and they result in
significant increase in large (e.g. >>6 block) reorgs). It's easy to
make convergence worse or to create additional perverse incentives.

@_date: 2013-11-08 12:33:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] we can all relax now 
On Fri, Nov 8, 2013 at 11:49 AM, Andreas M. Antonopoulos
The BC.i timestamps have historically been inaccurate relative to my
local GPS clock measurements on my own nodes... but not just that, it
sounds like he's comparing block timestamps and bc.i numbers.
Thats insane, because it tells you the delay between when the miner
_started_ a work unit and when BC.i claims to have found it. Even
assuming bc.i's times were accurate and assuming miner clocks are
accurate (they are often not) you expect there to be be a gap because
it takes time to compute work, send it to the miner, search for a
valid nonce (an average of 2^31 hash operations, often executed
sequentially on a single core taking ten seconds or so on a lot of
hardware) and then return a result.
Evidence of selfish miners wouldn't be block timestamps (which are
inaccurate and controlled by miners anyways), or data on
blockchain.info (which is inaccurate and controlled by bc.i) ... but
the existence of an unusual amount of orphan blocks. High levels of
blocks are _necessary_ evidence of this sort of things, there can be
other explanations of high orphaning levels, but they're required here
and couldn't be faked.

@_date: 2013-11-15 01:59:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
LaTeX moon language to PDF moon language conversion:

@_date: 2013-11-15 18:47:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] Proposal: Base58 encoded HD Wallet 
On Mon, Jul 22, 2013 at 2:37 PM, Jean-Paul Kogelman
Greetings.  Any recent progress on this?
Do we believe this proposal can replace BIP38?  If not, what are the
limitations that would prevent it from doing so?

@_date: 2013-11-19 09:01:26
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
The IETF makes a clear distinction between individual proposals and
documents which have been accepted by a working group. The former are
named after their authors.  Work is not assigned a number until it is
I believe it is important to distinguish complete work that people
should be implementing from things which are incomplete,  and even
more important to distinguish the work of single parties.
Otherwise you're going to get crap like BIP90: "Increase the supply of
Bitcoins to 210 million" being confused as an earnest proposal
supported by many that has traction.

@_date: 2013-11-19 09:54:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
Maybe!  The subject matter its assigned for is already _widely_
deployed, for better or worse.
(by comparison in the IETF, informational RFCs for already widely
deployed things are issued pretty liberally)
I'm not sure how we should be distinguish BIPs which are documenting
things which are already defacto standards vs ones which are proposing
that people do something new.
Mostly I think we don't want the BIP itself being a lever to force
something down people's throats, but rather the process should help
build consensus and review about how to do something? and then
document that consensus.

@_date: 2013-11-22 12:56:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [PATCH, 
Is there a reason not to have a parallel get rpc to get the current list?

@_date: 2013-11-24 08:26:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Network propagation speeds 
Could you publish the block ids and timestamp sets for each block?
It would be useful in correlating propagation information against
block characteristics.

@_date: 2013-10-18 16:58:27
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP39 word list 
some fairly old wordlist solver code of mine:
it has a 52x52 letter visual similarity matrix in it (along with a citation)

@_date: 2013-10-19 13:40:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A critique of bitcoin open source 
Hopefully Nick will show up someplace and offer some specific pointers
to where we failed him.
The only interaction I can find from him on IRC is in  rather
than --- Day changed Mon Sep 16 2013
11:45 < csmpls> Hi, I'm interested in contributing to the official
bitcoin project. Is there a mailing list I can join?
11:46 < neo2> csmpls, contributing how?
11:47 < csmpls> neo2 - probably start by approaching a low priority
issue like this one 11:48 < michagogo> csmpls: There *is* a mailing list
11:48 < michagogo> ;;google bitcoin-dev mailing list
11:48 < SourceForge.net: Bitcoin: bitcoin-development:
Bitcoin-development Info
11:48 < csmpls> Great, thanks.
11:48 < michagogo> I don't know how active it is, though
11:49 < michagogo> There's also the  channel
I got involved with Bitcoin without previously interacting with other
contributors (AFAIK) and maybe things have changed in ways invisibly
to me. But I don't think so. Michagogo, who was answering there, is a
newer participant and I don't think anyone knows him from anywhere.
Certainly if things have become less welcome to new participants that
would be bad.
I can point out a number of other recent contributors who, as far as I
can tell, just showed up and stared contributing.  But I don't think
that the existence of exceptions is sufficiently strong evidence that
there isn't a problem.
The specific complaints I can extract from that article are:
"I wasn't even allowed to edit the wiki"
I'm confused about this, if he's referring to en.bitcoin.it.  Editing
it is open to anyone who is willing to pay the 0.01
( anti-spam fee. This isn't
a policy set by the bitcoin development community, though I'm not sure
that its a terrible one. I've both paid it on behalf of other users
and made edits on behalf of people who didn't want to go to it.  At
least relative to some policy which requires actual approval the
payment antispam is at least open to anyone with Bitcoin.
"My IRC questions about issues on the github page were never answered"
Without a nick I'm unable to find more than the above, unfortunately.
So I don't yet know what we need to improve there.
" would rather talk about conspiracies, or about
destroying other cryptocurrencies"
I've been pretty aggressive about punting out offtopic conversation
from  lately. Enough that I worried that my actions would
be the inspiration for this complaint. Much of the time discussion
like that is brought in and primarily continued by people who are not
active in the development community at all, but deflecting it to other
challenge without creating a hostile environment (or one that merely
feels hostile to new people) is hard.  Nicks comments themselves may
be a useful thing for me to show to people in the future on that
"Bitcoiners are a bunch of paranoid, anti-authoritarian nutjobs"
I actually don't think that this stereotype accurately reflects the
development community. (In fact, I personally enjoy the great sport of
being called a statist by some of these aformentioned jutjobs, but
none of them are developers). On his other article Nick also asserts
"Most contributors hide their identities", but this is factually
untrue as far as I can tell. (In that same article he writes,
"Bitcoin's core code is written in Typescript, which is compiled into
"I looked at the many items sitting in pull request purgatory"
Many of the long standing pull requests are actually created by people
with direct commit access.  We use a model which has a relatively long
pipeline, a fact which I think is justified by the safety
criticialness of the software and our current shortages of active
review. Hopefully long term motion towards increased codebase
modularity will allow faster merging of "safe" changes.
But I suspect there will always be a backlog, at least of "unsafe" changes.
Which brings me to,
"I didn't even know what I had to do"
Above all, I think the most important takeaway from this is that we
need to have better introductory materials.
One obvious place to put them would be
  but the IRC question makes me
believe that Nick hadn't actually found that page, it's a little

@_date: 2013-10-19 16:13:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Root key encoding / BIP process Was: A 
I responded to you in PM on July 19, 2013, 05:57:15 PM.
Then I followed up with a technical review after I didn't see much
other technical review happening:
Which you responded to, correcting a few of my misunderstandings and
offering to make changes to the specification to make it more clear
and to correct a few of the limitations I pointed out.
At that point I put aside further action on your proposal waiting for
you to make those updates.
The reason to go through a serialization point for BIP numbers is to
avoid assigning them to things to people's pet ideas that haven't been
reviewed by or represent any identifiable part of the Bitcoin
community. (After all: You're free to publish any specs at all on your
own without a BIP. BIPs are not "official" but they should be stronger
than "some guy says this" if they are to mean anything).  I don't
generally see my role in this process as acting as an approver, but
rather just someone recognizing approval that already exists.
Generally I try not to assign numbers to things before I see evidence
of discussion which I can reasonably expect to result in an "community
outcome".  In some cases this means that I'll take up the role of
going through and being a second set of eyes on the document myself
(directly contributing to creating some community approval), as I did
in this case.
On October 2nd, you followed up with
indicating that you'd made the changes addressing my points.
My apologies, I missed this completely as I not working on Bitcoin
things pretty much at all during 09/26 to 10/13 due to other
obligations. Thanks for your patience. Following up here was
absolutely the right thing to do if I'm dropping the ball.
Pieter, do you have any opinions to offer on this?  (Also, generally
to the list. I'm singling out Pieter only because just asking "anyone"
to comment tends to be less effective.)

@_date: 2013-10-19 16:20:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A critique of bitcoin open source 
FWIW, he did post to the mailing list and he got an underwhelming response:
When I responded to him on BCT I said "I was about to suggest you hit
the mailing list for some initial comments first? but I see you've
done that. I'll issue a number in a couple days once there has been a
little chance for some discussion.".
Since much discussion didn't materialize I went and gave it a
technical once over, posting to the forum.  In hindsight, I probably
should have also posted my review to the mailing list, which might
have served to restart some additional discussion.

@_date: 2013-10-22 00:03:53
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
Take care, the information in the wiki is woefully incomplete.

@_date: 2013-10-22 00:56:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
Which would say something interesting like "If the bitcoin network
implements inconsistent behavior in the consensus critical parts of
the protocol the world ends. As such, conformance or _non_-conformance
with this specification (in particular, sections 4. 5. and 6.) may be
required for security."
A Bitcoin protocol RFC would be a great place to exercise RFC 6919
keywords.  (  )

@_date: 2013-10-25 13:05:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 38 
Greetings, (repeating from our discussion on IRC)
No prior messages about your proposal have made it to the list, and no
mention of the assignment had been made in the wiki.
The first I ever heard of this scheme was long after you'd written the
document when I attempted to assign the number to something else then
noticed something existed at that name.
Since you had previously created BIP documents without public
discussion (e.g. "BIP 22"
 [...] Or, I wonder
did your emails just get eaten that time too?), I'd just assumed
something similar had happened here.
I didn't take any action at the time I first noticed it, but after
someone complained about bitcoin-qt "not confirming with BIP38" to me
today it was clear to me that people were confusing this with
something that was "officially" (as much as anything is) supported, so
I moved the document out.  (I've since moved it back, having heard
from you that you thought that it had actually been
With respect to moving it forward: Having a wallet which can only a
single address is poor form. Jean-Paul Kogelman has a draft proposal
which is based on your BIP38 work though the encoding scheme is
different, having been revised in response to public discussion.
Perhaps efforts here can be combined?

@_date: 2013-10-25 20:31:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
One limitation of the payment protocol as speced is that there is no
way for a hidden service site to make use of its full authentication
capability because they are unable to get SSL certificates issued to
A tor hidden service (onion site) is controlled by an RSA key.
It would be trivial to pack a tor HS pubkey into a self-signed x509
certificate with the cn set to foooo.onion.
If we specified in the payment protocol an additional validation
procedure for [base32].onion hosts that just has it hash and base32
encode the pubkey (as tor does) then the payment protocol could work
seamlessly with tor hosts. (Displaying that the payment request came
from "foooo.onion").  I believe that the additional code for this
would be trivial (and I'll write it if there is support for making
this a standard feature).
This would give us an fully supported option which is completely CA
free... it would only work for tor sites, but the people concerned
about CA trechery are likely to want to use tor in any case.

@_date: 2013-10-25 21:06:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
The x.509 in the payment protocol itself is for authentication and
non-repudiation, not confidentiality.
It's used to sign the payment request so that later there is
cryptographic evidence in the event of a dispute:
"He didn't send me my alpaca socks!" "Thats not the address I told you to pay!"
"He told me he'd send my 99 red-balloons, not just one!"  "No way,
that was the price for 1 red-balloon!"
Just using SSL or .onion (or whatever else) gets you confidentiality
and authentication.  Neither of these things get you non-repudiation.
The payment protocol is extensible, so I hope that someday someone
will support namecoin authenticated messages (but note: this requires
namecoin to support trust-free SPV resolvers, otherwise there is no
way to extract a compact proof that can be stuck into a payment
request) and GPG authenticated messages.
But those things will require a fair amount of code (even fixing the
namecoin protocol in the nmc case), and GPG could be done just by
externally signing the actual payment request like you'd sign any
file... and considering the sorry state of their _practical_
usability, I don't think they're worth doing at this time.
By contrast, I _think_ the tor onion support would require only a
relatively few lines of code since it could just be the existing x.509
mechanism with just a simple special validation rule for .onion, plus
a little tool to repack the keys.  I think it would easily be more
widely used than namecoin (though probably both would not really be
used, as gavin notes).
w/ Gavin's comments I'll go check in with the tor folks and see if
anyone has ever though of doing this before and if there is already a
canonical structure for the x.509 certs used in this way.

@_date: 2013-10-28 02:32:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
On Mon, Oct 28, 2013 at 2:26 AM, Andreas Schildbach
I do not believe we should do that:  It would be a non-trivial
increase the protocol bandwidth requirements.

@_date: 2013-10-30 10:13:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
Actually, we'll probably need to explicitly document that a failure to
reject is by no means a promise to forward.
If a node is using priority queued rate limiting for its relaying then
it might "accept" a transaction from you, but have it fall out of its
memory pool (due to higher priority txn arriving, or getting
restarted, etc.) before it ever gets a chance to send it on to any
other peers.
Finding out that it rejected is still useful information, but even
assuming all nodes are honest and well behaved I don't think you could
count on its absence to be sure of forwarding.

@_date: 2013-09-09 01:53:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Some current turbulence on the Bitcoin 
Please return your seats and fasten your seat-belts.
All Bitcoin-qt / Bitcoind nodes will currently fail to come back up
after a restart, reporting
": *** coin database inconsistencies found"
"Do you want to rebuild the block database now?"
Reindexing _will not_ correct the problem.  In Bitcoin-qt you should
say no to this reindex prompt as it will not help for this problem and
will only waste your time.
To workaround:
Please specify the command-line or configuration file argument
-checklevel=2  to Bitcoind or Bitcoin-qt.
The issue appears to have been introduced by 0.8.0 and is only a local
issue, beyond the annoyance restarting nodes it appears to be harmless
and carries no forking risk but will take a software update to fix
This problem will persist until no more than 288 blocks after 256818,
unless another trigger transaction is added to the blockchain (which
may well happen).
More information will be forthcoming once a patch is available.

@_date: 2013-09-09 02:25:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Some current turbulence on the Bitcoin 
I now have a patch up for review.
(You should wait until other developers have had a chance to review
before rushing out and applying it. The checklevel=2 workaround is
adequate for now.)

@_date: 2013-09-09 16:25:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Some current turbulence on the Bitcoin 
There is a good writeup of how to perform the workaround in Windows at

@_date: 2013-09-10 15:08:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP0039 Mnemonic code for generating 
On Tue, Sep 10, 2013 at 2:03 PM, Matthew Mitchell
This sounds like something that $20 of mechanical turk time could help
out with a lot.  Put up the 2048 words and ask people to rate them for
potential offensiveness and threatening. :)
Nouns often make for fairly neutral words, though careful for place
names which have had political complications. E.g. gdansk vs danzig.

@_date: 2013-09-10 15:43:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP0039 Mnemonic code for generating 
On the subject of unexpected results, for the longest time wikipedia
had problems with images randomly not displaying for some users.
Images were stored by their cryptographic hash. If the hash was
deadbeef the URL would be:
Turns out that a lot of people are running addons that block /a/ad/  ...
Not running afoul of various censorware filters should be a design
target too, as insane as it seems. Simply because "The key is
'Tienanmen Square people monkey'"  "People monkey isn't working!" is a
hard situation to trouble shoot!

@_date: 2013-09-17 06:20:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Faster databases than LevelDB 
I'd looked at the hyperleveldb, but their performance graphs made it
seem like it would be slower for the actual database sizes we're using
Is there a competitor that specializes in being more robust to corruption? :(

@_date: 2013-09-30 15:08:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] bitcoind stops responding 
============================== START ==============================
On Mon, Sep 30, 2013 at 3:00 PM, Fatima Castiglione Maldonado ?
This is unrelated to Slush's question.
The complexity of IsConfirmed/Ismine is exponential and starts taking
tens of seconds at a chain of a dozen unconfirmed transactions (and
growing from there)
There are some patches that change this, but since the whole network
will only average about 7tx per second, you're probably doing
something wrong if you're building great big chains of unconfirmed
transaction.  Are you aware of sendmany?

@_date: 2014-04-01 12:07:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
A minor nitpick:  It is well known that the Bitcoin core developers
are some of the most active TypeScript coders around,
E.g.   and But I think this is an important step forward: Seminal alternative
crypto-currencies such as SolidCoin showed us that economic parameters
can be freely changed at any time, for any (or no) reason at all; and
so we should take this opportunity to demonstrate our commitment to
adopting innovative features like non-inflation regardless of their
origins in other crypto-currencies.

@_date: 2014-04-01 14:47:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
With ten people commenting on this proposal there are quite a few ways
in which you could partition their views. Only one possible integer
partitioning has everyone in the same partition, so consensus seems
But owing to a rather large bribe (or at least not less large than any
other offered by competing parties) I hereby assign BIP 42 for this

@_date: 2014-04-04 07:14:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
On Fri, Apr 4, 2014 at 6:51 AM, Nikita Schmidt
Operation mod the group order is how secret keys must be combined in
type-2 private derivation for BIP-32. It's also absolutely essential
if you want to build a secret sharing scheme in which the shares are
usable for threshold ECDSA.
I still repeat my concern that any private key secret sharing scheme
really ought to be compatible with threshold ECDSA, otherwise we're
just going to have another redundant specification.

@_date: 2014-04-04 09:25:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Well, if you're not doing anything homorphic with the result the
computation should probably be over a small field (for computational
efficiency and implementation simplicity reasons) and the data split
up, this also makes it easier to deal with many different data sizes,
since the various sizes will more efficiently divide into the small
field.   The field only needs to be large enough to handle the number
of distinct shares you wish to issue, so even an 8 bit field would
probably be adequate (and yields some very simple table based
If that route is taken, rather than encoding BIP32 master keys, it
would probably be prudent to encode the encryption optional version
 ... and if we're
talking about a new armored private key format then perhaps we should
be talking about Mark Friedenbach's error correcting capable scheme:
(though it would be nicer if we could find a decoding scheme that
supported list decoding without increasing the complexity of a basic
implementation, since an advanced recovery tool could make good use of
a list decode)
I'd think that changing to a small field with a simple implementation,
and encoding the form with encryption, etc. probably makes it distinct
enough from an implementation of ECDSA thresholding that redundancy
isn't a problem.

@_date: 2014-04-04 10:08:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Yes, I'm proposing using the binary extension field of GF(2^8).  There
are many secret sharing and data integrity applications available
already operating over GF(2^8) so you can go compare implementation
approaches without having to try them our yourself. Obviously anything
efficiently encoded as bytes will efficiently encode over GF(2^8).
I do think there is a material difference in complexity that comes in
layers rather than at a single point. It's much easier to implement a
complex thing that has many individually testable parts then a single
complex part. (Implementing arithmetic mod some huge P is quite a bit
of work unless you're using some very high level language with
integrated bignums? and are comfortable hoping that their bignums are
sufficiently consistent with the spec).

@_date: 2014-04-04 10:51:56
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
I suggest you go look at some of the other secret sharing
implementations that use GF(2^8), they end up just being a couple of
dozen lines of code. Pretty simple stuff, and they work efficiently
for all sizes of data, there are implementations in a multitude of
languages. There are a whole bunch of these.
With respect for the awesome work that GMP is?  It's 250,000 lines of
LGPLed code.  It's not just "pic microcontrollers" that would find
that scale of a dependency unwelcome.
It lets you efficiently scale to any size data being encoded without
extra overhead or having additional primes. It can be compactly
implemented in Javascript (there are several implementations you can
find if you google), it shouldn't be burdensome to implement on a
device like a trezor (much less a real microcontroller).
And yea, sure, it's distinct from the implementation you'd use for
threshold signing. A threshold singing one would lack the size agility
or the easy of implementation on limited devices.  So I do think that
if there is to be two it would be good to gain the advantages that
can't be achieved in an threshold ECDSA compatible approach.

@_date: 2014-04-05 10:29:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
In my opinion you can have whatever style you want on the BIPs, so
long as you pledge to slay all who come and complain about the new

@_date: 2014-04-06 02:21:32
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Standardizing OP_RETURN message format 
For just having a dummy output for an all fee transaction you do not
need to have a PUSH at all.

@_date: 2014-04-07 06:50:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
FWIW, A few months before that we had even less than 8500 by the bitnodes count.
Bitcoin.org recommends people away from running Bitcoin-QT now, so I'm
not sure that we should generally find that trend surprising.

@_date: 2014-04-07 06:53:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
Gah, accidentally send.... I wanted to continue here that it was less
than 8500 and had been falling pretty consistently for months,
basically since the bitcoin.org change.  Unfortunately it looks like
the old bitnodes.io data isn't available anymore, so I'm going off my
memory here.
The Bitnodes counts have always been somewhat higher than my or sipa's
node counts too, fwiw.

@_date: 2014-04-07 07:04:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
Nah.  It reported multiple metrics. The "100,000" number was an mostly
useless metric that just counted the number of distinct addr messages
floating around the network, which contains a lot of junk.  They also
reported an actual connectable node count previously and while they
may have tweaked things here and there as far as I can tell it has
been consistent with the numbers they are using in the headlines now.

@_date: 2014-04-07 08:53:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
Right now running a full node consumes about $1 in disk space
non-reoccurring and costs a couple cents in power per month.
This isn't to say things are all ducky. But if you're going to say the
resource requirements are beyond the capabilities of casual users I'm
afraid I'm going to have to say: citation needed.

@_date: 2014-04-07 09:57:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
That is an implementation issue? mostly one that arises as an indirect
consequence of not having headers first and the parallel fetch, not a
requirements issue.
Under the current bitcoin validity rules it should be completely
reasonable to run a full contributing node with no more than 30 kb/s
inbound (reviving two copies of everything, blocks + tansactions ) and
60 kbit/sec outbound (sending out four copies of everything). (So long
as you're sending out >= what you're taking in you're contributing to
the network's capacity). Throw in a factor of two for bursting, though
not every node needs to be contributing super low latency capacity.
This is absolutely not the case with the current implementation, but
it's not a requirements thing.

@_date: 2014-04-07 10:16:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
The distinction is very important because it's a matter of things we
can and should fix vs things that cannot be fixed except by changing
goals/incentives!  Opposite approaches to handling them.
When I read "resource requirements of a full node are moving beyond" I
didn't extract from that that "there are implementation issues that
need to be improved to make it work better for low resource users" due
to the word "requirements".

@_date: 2014-04-07 10:44:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
The actual validation isn't really the problem today. The slowness of
the IBD is almost exclusively the lack of parallel fetching and the
existence of slow peers.  And making the signature gate adaptive (and
deploying the 6x faster ECDSA code) would improve that future.
Go grab sipa's headers first branch, it has no problem saturating a
20mbit/sec pipe syncing up.

@_date: 2014-04-07 11:49:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
I'm still very concerned that a binary archive bit will cause extreme
load hot-spotting and the kind of binary "Use lots of resources YES or
NO" I think we're currently suffering some from, but at that point
enshrined in the protocol.
It would be much better to extend the addr messages so that nodes can
indicate a range or two of blocks that they're serving, so that all
nodes can contribute fractionally according to their means. E.g. if
you want to offer up 8 GB of distributed storage and contribute to the
availability of the blockchain,  without having to swollow the whole
20, 30, 40 ... gigabyte pill.
Already we need that kind of distributed storage for the most recent
blocks to prevent extreme bandwidth load on archives, so extending it
to arbitrary ranges is only more complicated because there is
currently no room to signal it.

@_date: 2014-04-07 12:02:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
This isn't at all how pruning works in Bitcoin-QT  (nor is it how I
expect pruning to work for any mature implementation). Pruning can
work happily on a whole block at a time basis regardless if all the
transactions in it are spent or not.

@_date: 2014-04-07 12:03:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
A bitmap also means high overhead and? if it's used to advertise
non-contiguous blocks? poor locality, since blocks are fetched

@_date: 2014-04-07 14:56:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
The comment was specifically in the context of an out of order fetch.
Verification must be in order. If you have fetched blocks out of order
you must preserve them at least long enough to reorder them. Thats

@_date: 2014-04-07 17:38:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
On Mon, Apr 7, 2014 at 5:33 PM, Nikita Schmidt
Nah, it doesn't. E.g.
I think this is really one of the bigger selling points.

@_date: 2014-04-07 19:07:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Yes thats only a decode but the same process (long division with
manual carries) works just fine the other way. There is absolutely no
need to use big integers for this.

@_date: 2014-04-08 09:44:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] have there been complains about network 
We don't form or need to form a great many connections.
We don't use UDP.
Yes, TCP has a congestion window too, still sometimes some poorly
designed or configured routers suffer from buffer bloat.
Adding our own UDP network stack involving a ton of exposed code
sounds like a great way to gain inadvertent bugs or backdoors.
But there doesn't have to be and shouldn't just be one network
transport for Bitcoin. You can gateway to other protocols and run them
in parallel.  I think it would be great for someone to go build an
alternative transport protocol to gateway to and see what useful
things they can accomplish.

@_date: 2014-04-09 08:54:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Sadly today Electrum requires more than a full node, it requires a
number of large additional indexes over what a full node has and
pruning is precluded. I don't think that increasing the resource
utilization of the node is a good way to go there for the purposes
expressed here. (not that electrum couldn't be used here, but not
unmodified without the resource usage increasing route)
Bitcoin's own P2P protocol is already the API for a ordinary SPV
client. So I don't believe any new API would be require, except
perhaps for some process management stuff (which also isn't provided
for Electrum).

@_date: 2014-04-09 08:57:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
If Bitcoin core activates pruning a full node can be supported in?
say? 4GBytes or so. (That gives enough space to store the utxo about
350MB now, and a couple gigs for blocks to serve out).
I'd imagine getting information from SPV wallet developers how much
disk usage agility they think is required is part of what Wladimir is
looking for.

@_date: 2014-04-09 11:50:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Who says anything about a broken incentive model. You've made past
claims about resource requirements that I think made no sense and then
failed to defend them when they were challenge.
With suitable software improvements running a full node could be done
in as little as a few gigabytes in disk space (e.g. cost 25-50 cents),
and as 50-100kbit/sec bandwidth in and out ongoing, and a moderate
amount of ram. Power costs are already just a few cents per month.  By
far the greatest cost is the figuring out and setting up part, which
bundling could fix. The exact resources could be tunable to what the
users are willing and able to contribute.
If improved marginal security and privacy in addition to supporting
the network is not enough incentive to overcome costs like these then
Bitcoin is already doomed.  I think that fundamental costs aren't an
issue at all, just implementation warts and education are.
Part of asking the question is feeling out which improvements need to
happen first, and what the prospects of getting the bundling going
once those improvements are made.

@_date: 2014-04-09 12:33:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
The list has open membership, there is no particular qualification or
background required to post here. Optimal use of an information source
requires critical reading and understanding the limitations of the
medium. Counting comments is usually not a great way to assess
technical considerations on an open public forum.  Doubly so because
those comments were not actually talking about the same thing I am
talking about.
Existing implementations are inefficient in many known ways (and, no
doubt, some unknown ones). This list is about developing protocol and
implementations including improving their efficiency.  When talking
about incentives the costs you need to consider are the costs of the
best realistic option.  As far as I know there is no doubt from anyone
technically experienced that under the current network rules full
nodes can be operated with vastly less resources than current
implementations use, it's just a question of the relatively modest
implementation improvements.
When you argue that Bitcoin doesn't have the right incentives (and
thus something??) I retort that the actual resource _requirements_ are
for the protocol very low. I gave specific example numbers to enable
correction or clarification if I've said something wrong or
controversial. Pointing out that existing implementations are not that
currently as efficient as the underlying requirements and that some
large number of users do not like the efficiency of existing
implementations doesn't tell me anything I disagree with or didn't
already know. Whats being discussed around here contributes to
prioritizing improvements over the existing implementations.
I hope this clarifies something.

@_date: 2014-04-09 14:04:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Not just a speculative pathway. Pieter implemented headers first:
  and it was
everything we hoped it would be? it easily can saturate residential
broadband, produces less load hot-spotting, copes with unreliable
peers, etc. It's now just slowly making its way into Bitcoin core.

@_date: 2014-04-10 04:50:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
On this list.
I think you actually do need the kept ranges to be circulated,
otherwise you might need to hunt for a very long time to find the
right nodes with the blocks you need.  Alternatively, you give up and
don't hunt and pick some node that has them all and we get poor load
distribution. I'd rather be in a case where the nodes that have the
full history are only hit as a last resort.
WRT the changing values, I think that is pretty uniquely related to
the most recent blocks, and so instead I think that should be handled
symbolically (e.g. the hybrid approach... a flag for the "I keep the
most recent 2000 blocks", I say 2000 because thats about where the
request offset historgrams flattened out) or as a single offset range
"I keep the last N=200",  and the flag or the offset would be in
addition to whatever additional range was signaled. The latter could
be infrequently changing.
Signaling _more_ and more current range data on connect seems fine to
me, I just don't think it replaces something that gets relayed.
Based on the safety against reorgs and the block request access
patterns we observed I'm pretty sure we'd want any node serving blocks
at all to be at least the last N (for some number between 144 and 2000
or so). Based on the request patterns if just the recent blocks use up
all the space you're willing to spend, then I think thats probably
still the optimal contribution...
(Just be glad I'm not suggesting coding the entire blockchain with an
error correcting code so that it doesn't matter which subset you're

@_date: 2014-04-10 05:10:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain pruning 
The last time we discussed this sipa collected data based on how often
blocks were feteched as a function of their depth and there was a huge
increase for recent blocks that didn't really level out until 2000
blocks or so? presumably its not uncommon for nodes to be offline for
a week or two at a time.
But sure I could see a fixed range as also being a useful contribution
though I'm struggling to figure out what set of constraints would
leave a node without following the consensus?   Obviously it has
bandwidth if you're expecting to contribute much in serving those
historic blocks... and verifying is reasonably cpu cheap with fast
ecdsa code.   Maybe it has a lot of read only storage?
I think it should be possible to express and use such a thing in the
protocol even if I'm currently unsure as to why you wouldn't do 100000
- 200000  _plus_ the most recent 144 that you were already keeping
around for reorgs.
In terms of peer selection, if the blocks you need aren't covered by
the nodes you're currently connected to I think you'd prefer to seek
node nodes which have the least rare-ness in the ranges they offer.
E.g. if you're looking for a block 50 from the tip,  you're should
probably not prefer to fetch it from someone with blocks 100000-150000
if its one of only 100 nodes that has that range.

@_date: 2014-04-10 15:33:36
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Chain pruning 
Because with headers first synchronization the major problems that
they solve? e.g. block flooding DOS attacks, weak chain isolation, and
checking shortcutting can be addressed in other more efficient ways
that don't result in putting trust in third parties.
They also cause really severe confusion about the security model.
Instead you can embed in software knoweldge that the longest chain is
"at least this long" to prevent isolation attacks, which is a lot
simpler and less trusting.  You can also do randomized validation of
the deeply burried old history for performance, instead of constantly
depending on 'trusted parties' to update software or it gets slower
over time, and locally save your own validation fingerprints so if you
need to reinitilize data you can remember what you've check so far by

@_date: 2014-04-11 09:54:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Though I mentioned it, it was in jest? I think right now it would be
an over-design at least for the basic protocol.  Also, storing
'random' blocks has some locality problems, when verifying blocks you
need to obtain them contiguously, and so we can take advantage of the
locality of reference.  For the non-error-coded case I believe nodes
with random spans of blocks works out asymptotically to the same
failure rates as random.
One thing that I like to point out is that there is absolutely no need
for the entire network to use the same p2p protocol. Diversity here
would be very good.  I think it would be really good for someone to
have an alternative p2p protocol using these techniques even though I
think they aren't yet compelling enough to be table stakes in the
basic protocol.
There are some very helpful things you can do with forward error
correction for faster and more efficient block relaying too:
(The conversation Peter Todd was referring to was one where I was
pointing out that with suitable error coding you also get an
anti-censorship effect where its very difficult to provide part of the
data without potentially providing all of it)
I think in the network we have today and for the foreseeable future we
can reasonably count on there being a reasonable number of nodes that
store all the blocks... quite likely not enough to satisfy the
historical block demand from the network alone, but easily enough to
supply blocks that have otherwise gone missing.

@_date: 2014-04-16 15:06:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Warning message when running wallet in 
Bringing the thread back on-topic:
I think eventually multi-wallet support will make it so that a wallet
won't be created by default. Instead users would create-wallet, which
would also give them options like using a HSM (e.g. trezor) or
multisig secured wallet.  That would be a great point where, if they
elect to run and ordinary unsecured wallet, and the software detects
that the host is known-to-not-likely-be-secure it could whine at them
and direct them to a security best practices page.
Then you also avoid whining at people who never run a wallet or use a
hsm making the host security somewhat moot.

@_date: 2014-04-22 01:15:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
I haven't seen commentary from you on the Kogelman draft, it would be
helpful there:

@_date: 2014-04-22 11:46:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Satoshi didn't create testnet.  Testnet exists so you can do public
tests involving multiple people and services, so you can have shadow
versions of sites and services interacting with each other and trading
worthless tokens. Importantly, testnet also creates a public live fire
environment where grey hats can try out their attacks while minimizing
damage (and it's been very successful at this).  Testnet is not an
alternative to the unit and integration tests that exist in Bitcoin
core but exists for more or less completely different reasons.
Testnet is not normally addressed in BIPs at all, except for soft fork
bips that had compressed deployment schedules on testnet.  For address
like specification we have always used a version byte and there is a
common encoding for version bytes that flags the network ID in the

@_date: 2014-04-22 23:16:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
I think the convention we have so far is that addresses and address
relate thing we share normally contain an opaque 'version' identifier
which we use to identify the purpose for the data (E.g. network
meaning, etc.) and I think its a generally reasonable custom.

@_date: 2014-04-23 11:57:01
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Hm? I didn't think this is at all what they did.  What they claim to
do is to prioritize transactions in their mempool from people who pay
them, potentially over and above other transactions which they may or
may not have received first.
This may still be bad news for someone taking an irreversible action
in response to an unconfirmed payment and it may or may not be really
ill advised in general, but I think it's less sinister than it sounds
in your posts.  Is there some reason to believe it isn't what it
claims to be?
I think we have very clear evidence that the Bitcoin community doesn't
care if miners reorder transactions in their mempool to profitable
ends: In  it's
demonstrated that GHash.IO, currently the largest publicly identified
pool was used to rip off Betcoin dice via double-spends.
Accepting zero-conf inherently has some risk (well, so does all
business, but there is substantially more in a zero-conf payment).
Even in a spherical-cow Bitcoin absent anything like Bitundo someone
can just give a double spend to a large miner and currently give the
whole rest of the network the one paying the merchant.  They will,
with high success rate be successful.   Worse, it may _appear_ to the
network that the miner was a "bitundo" but they really were not.   The
blockchain exists to establish consensus ordering, prior to the
blockchain there is no order, and so it is not easy to really say
which transaction came first in any meaningful sense.
But in business we balance risks and the risk that sometimes a
transaction will be reversed exists in every electronic payment system
available today, in most of them the risk persists for _months_ rather
than minutes.  Businesses can still operate in the face of these
More importantly, it's possible to deploy technological approaches to
make zero-conf very secure against reversal: Things like performing
multi-sig with a anti-double-spending system, or using an external
federated payment network... but this stuff requires substantial
development work? though it's not work thats likely to happen if
people are still confused about the level of security that zero-conf
I think miners 'voting' to reallocate coins, even if they're
thoroughly convinced that the owner of the coins is a nasty party, is
a much greater violation of the Bitcoin social contract than some
twiddling with the unspecified unconfirmed transaction ordering.
Doubly so because a 'nasty' party with non-trivial hash-power can
doublespend their own transactions with a pretty good success rate (as
was the case for the GHash.io betcoin spends) including not-just
zero-conf (though with obviously reduced effectiveness), and all of
your reliable detection depends on it being a public service.
A much better defense is having the control of hash power very well
distributed and so there isn't any central point that excerts enough
influence to change the risk statistics much.  Giving miners the
ability to steal each others payments is, if anything, a force away
from that decentralization.

@_date: 2014-04-23 12:47:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
A finney attack is where you attempt to mine a block with a
transaction paying you, and as soon as you are successful you quickly
make a transaction spending that coin to someone else, then release
the block after they've taken an irreversible action. If everything is
automated it should have something like a 99% success rate, though it
has a cost of some small increase in the number of orphan blocks you
You might have coined the term, but I don't think the attack you're
describing is the attack Hal described:
What you're talking about is just disagreement about the content of
the memory pool, but we have no consensus mechanism there (the
blockchain _is_ the consensus mechanism).  Mempools are sometimes
inconsistent all on their own, without any attacker being involved.
I think thats an unsophisticated view.
Consider this protocol.
I take some of my funds and assign them to a 2 of 2 multisig with
myself and Oscar. I do not announce this transaction until I get Oscar
to sign a timelocked anyonecanpay refund to send the coin back to me
(say in 3 months).  Oscar gives me my refund and I announce the
Later I can make instant payments with oscar signing up until the
refund time comes clue to anyone who trusts Oscar to never double
spend.  For the receiver this is purely additive with regular
blockchain security: in that even with Oscar's help I cannot double
spend except where I would have been successful absent Oscar. On the
sender side, Oscar cannot up and steal my funds and he can't try to
extort me (except by creating a delay up to the refund time).
Oscar himself can be implemented as a majority M parties to further
increase confidence, though if you're talking about using this for low
value retail transactions? the fact that any cheating by oscar is
cryptographically provable (just show them the double signatures)
maybe be strong enough alone. (Though there is a multitude of other
proposals to provide more evidence of Oscar's honesty). There are also
ways to blind Oscar so he can't reliably identify which transactions
are ones he signed for.
I don't think this is at all a "return to trusted third parties"? that
it's a shrug and an admission of defeat. Its a very narrowly scoped
trust, filling in precisely where large scale decentralized consensus
is fundamentally weak... the result is something which combines
advantages from both classes and is stronger than either trust or
blockchains alone.  (I'm also not trying to say that an implementation
of this is _simple_ by any means, working out all the details is
By contrast, I think proposals which overly depend on colluding miners
to behave in very specific ways are themselves just a way of saying
block chains suck unless we turn the miners themselves into a trusted
third party. I'm much more in favor of adding a little bit of
mastercard to transactions where mastercard is really what people
want, than turning mining? and thus bitcoin itself? into mastercard,
especially since miners? self selecting as they are? are a pretty poor
set of parties to act as trusted agents. :)
We have an existence proof that it isn't so? you can say that it
wasn't consistent enough, but what is? There wasn't any major doubt
that they were actually doing it. They're the largest identifiable
pool as we speak.
I think, instead, that strong zero-conf security isn't a part of what
many people think of when they think of Bitcoin's characteristics.
Zero conf is risky, and I think for a lot of people thats okay.  If it
isn't there are ways to improve it that don't involve asking miners to
participate in a majority vote to take away funds from people.

@_date: 2014-04-23 13:24:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
The difference is when you transact.  In the attack Hal described you
transact with your victim only after finding a block but before
announcing it.
Right, this works in the Bitcoin network today absent any collusion by
the miners. You give one miner a transaction and you give every other
node you can reach another transaction.  You then hope your selected
miner finds the next block and 'undoes' the transaction you gave the
rest of the network.
But it isn't at all the same thing.  Miners select themselves based on
controlling hash-power. You can distrust a miner all you like but all
your distrust does not prevent him from participating in the
consensus, potentially to your detriment.  Moreover, the set of miners
has to be the same for everyone or otherwise the network doesn't
converge. There are miners I _know_ to be scoundrels, but there is
nothing I can do about it.
Someone you ask to not double spend is an entirely separate matter.
They aren't self-selecting: you select who you trust to not make
double spends and there is no need for this trust to be globally
consistent. If they behave in an untrustworthy way you can instantly
stop honoring them because the bad action is provable beyond any doubt
and never trust them again (unlike mempool consistency)... and you can
do this even if everyone else is too foolish to do so for some reason.
The trustworthness of oscars needs only be limited and is different in
kind from the kind of 'trust' we need over the history? they
arbitrating over the ordering of some subset of transactions right at
the tip of the chain, and only those transaction of people who have
specifically chosen to use them, of lower value transactions where you
need instant settlement.  Why pay twice? Because you're actually
getting a different part of your security from each, and the result is
There is no such thing as an uncorruptable party, invoking that is a
useless strawman. Instead we can consider how difficult the corruption
is and what can happen if they're corrupted and hope to balance the
risks and the controls for those risks.  Any self-selectingness as
anonymity (in the not-previously-enumerated sense) of mining is
important for censorship security but it's terrible for other things
like getting reliable mempool behavior.
Because you can choose to stop trusting an oscar while you?
individually? can't choose anything about ghash.io.  To stop GHash.io
we would have to take away their hardware or change the Bitcoin
protocol to make their hardware useless, and in the latter case we'd
_all_ have to agree to do this not just some (perhaps quite large)
subset of us who doesn't want to trust them, and even though it is
quite apparent what they did there is still some room to claim doubt.
Mining is universal? everyone must use the same miners, trust seldom
is seldom universal and shouldn't be. The trust we have in mining is
exceptionally limited, I think any effort to increase it is doomed to
fail? both because trust heavy systems stink, because mining is a bad
fit for trust, and because increasing the requirements create other
exposures and vulnerabilities.

@_date: 2014-04-23 13:53:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
Large scale consensus can't generally provide instantly irreversible
transactions directly: Increasing the block speed can't help past the
point where the time starts getting close to the network diameter...
you simply can't tell what a consensus of a group of nodes is until
several times the light cone that includes all of them.  And if you
start getting close to the limit you dilute the power working on the
consensus and potentially make life easier for a large attacker.
Maybe other chains with different parameters could achieve a different
tradeoff which was better suited to low value retail transactions
(e.g. where you want a soft confirmation fast). A choice of tradeoffs
could be very useful, and maybe you can practically get close enough
(e.g. would knowing you lost a zero-conf double spend within 30
seconds 90% of the time be good enough?)... but I'm not aware of any
silver bullet there which gives you something identical to what a
centralized service can give you without invoking at least a little
bit of centralization.

@_date: 2014-04-23 14:22:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New BIP32 structure 
Hopefully it would be clarified as only a MUST NOT do so silently...
"I have funds split across two wallets and it WONT LET ME SPEND THEM"
sounds like a terrible user experience. :)

@_date: 2014-04-23 14:39:26
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
You can see me proposing this kind of thing in a number of places (e.g.
 "p2pool
only forces the subsidy today, but the same mechnism could instead
force transactions.. e.g. to get you fast confirmation.", or
previously on BCT for the last couple years) but there are still
limits here:  If you don't follow the fast-confirmation share chain
you cannot mine third party transactions because you'll be at risk of
mining a double spend that gets you orphaned, or building on a prior
block that other miners have decided is bad.  This means that if the
latency or data rate requirements of the share chain are too large
relative to ordinary mining it may create some centralization
That said, I think using a fast confirmation share-chain is much
better than decreasing block times and could be a very useful tool if
we believe that there are many applications which could be improved
with e.g. a 30 second or 1 minute interblock time.  Mostly my thinking
has been that these retail applications really want sub-second
confirmation, which can't reasonably be provided in this manner so I
didn't mention it in this thread.
One of the neat things about this is that you can introduce it totally
separately from Bitcoin without any consensus or approval from anyone
else? E.g. P2pool builds such a chain today though it doesn't enforce
transactions.  It would immediately provide the useful service of
demonstrating that some chunk of hashpower was currently working on
including a particular set of transactions.  Once the details were
worked out it could be added as a soft-forking requirement to the
commonly run system.

@_date: 2014-04-23 14:48:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New BIP32 structure 
Or implement them but in a form where the different wallets can have
different security policies and thus wouldn't share a common piece of
private key material.  I can see it being pretty confusing to have
multiple wallets which are both sharing a private key and not sharing
a private key.

@_date: 2014-04-23 15:28:02
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Development Roadmap of Bitcoin Core 0.9.2 
Tails users usually can't really build it from source? talks is a live
boot mostly stateless linux distribution for privacy applications.
It's really good in general.
I agree that we shouldn't be statically linking QT on linux generally
(due to things like theming), though maybe we could just have the
build process dump out a seperate extra static QT binary just for
these other cases? I feel like the work maintaining it would be less
than what we've had in answering questions/complaints about it.

@_date: 2014-04-24 00:21:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New BIP32 structure 
It doesn't appear to me that reoccurring payments, receive accounts,
multisig addresses, etc can be used with this proposal, but instead
you must use a different purpose code and another BIP and are not
compatible with the draft here.
Am I misunderstanding it?   Will Electrum be limiting itself in this
way?  I'd consider it a unfortunate loss of functionality if wallets
couldn't implement reoccurring payment chains without making users
generate entirely different wallets (which they couldn't share funds
across) since addresses for recurring payments was one of the main
motivations in BIP32.

@_date: 2014-04-24 01:19:20
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
We use coinbase sigs to gauge the safety of enforcing a soft fork
several times and not just for P2SH, to determine when enforcement of
it will be decisive and not result in risking a partition in the
network that might permit transaction reversals. This is not voting.
As a feature decision mechanism his is a rather coercive thing because
it gives the highest hash-power bidders control even when their
interests may be rather thoroughly unaligned with population that owns
and uses bitcoin in general, but as a plain indicator of when its safe
to enforce a new rule (same mechanism, different motivation? though a
point is that safe usage means using much more than 50% as the
threshold) it works pretty well.
Yes, making really distributed systems that work in a complex world is
hard. It certantly would be /easier/ to just declare miners "trusted
parties" and require them to always collude to produce a single
consensus view of the world that is always honest and never
contradictory, except that it doesn't work. Because they aren't
individually trusted or even trustworthy.
Temporarily censoring transactions by orphaning otherwise valid blocks
that contain them for as long as you retain a majority is possible and
impossible to prevent in this architecture. This isn't the same as
deleting.  Deleting suggests the common misconception that a majority
of miners can do anything they want.

@_date: 2014-04-24 02:25:02
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
May I direct your attention to the third post in that thread?
Luke attempting to ret-con the enforcement flag into a vote didn't
make it one, and certantly wouldn't make it a fair, just, or sane
method of one. And so much for the effectiveness? you didn't implement
it for years even after it was deployed.
And yes, you can take any decision system and draw comparisons to
voting and call it a vote but that doesn't mean is serves the same
role or was intended for that purpose.
Yes, you can reorg out the blocks and actually remove them, but I
understood that you were _not_ proposing that quite specifically. But
instead proposed without reorging taking txouts that were previously
assigned to one party and simply assigning them to others.
I don't think thats the root of the the disagreement at all. I think
all sorts of changes are interesting, especially ones that increase
flexibility or fix bugs but less so ones that would impose significant
changes on parties without their consent especially things that look
like taking someone's coins and assigning them to someone else.
I think the root is that you believe that the miners are, should be,
or even could be trustworthy in ways that I do not,  and as a result
you expect to be able to extract the performance of a trusted
centralized system out of them that I do not. Bitcoin is a system
where the incentives are well enough aligned that you appear to only
need a small amount of altruism to make it reliable. ... and even
summoning that altruism is a challenge? as miners hand over control of
their hash-power to centralized pools (some known to have behaved
poorly in the past), etc.
I would like that performance if it came at no cost: But proposals
that miners conspiring to blacklist transactions/blocks produced by
other people is something with a risk of a worse violation of the
system's promises than some disagreement of the ordering of
unconfirmed transactions.  Pretty much immediately after your post
Peter Todd? in his trouble making manner? went and posted on reddit
proposing the mechanism be used to claw back mining income from a
hardware vendor accused of violating its agreements on the amount of
self mining / mining on customers hardware.  While Peter's suggestion
was no doubt intentionally trouble making? I'm not clear on where the
line is here: Harm from reordering pretty much non-existent currently
and is highly speculative, while the harm to miners by hardware
vendors who've promised to not compete with their own customers or use
their equipment is not at all speculative and very salient to miners.
This especially in light of the fact that the system already has an
equitable method to decide what order transactions should be in... but
instead you propose an additional complex heuristic system where based
on some unspecified collusion some majority of miners take a
minorities coins and assign them to themselves.
Unlike reorginization this form of wealth transferal has no collateral
damage meaning that a majority cabal can use it to deprive a minority
outsider of some or all income without risking disrupting the network,
it would also lay the groundwork for additional forms of censorship
which I believe would be at odds with the purpose and architecture of
the system... and, as I noted above, it wouldn't actually prevent
theft, it would just mean that no single block could make its theft
services available to all comers (or even any of the public at all).
The simple mechenism of allowing only only a small number of paid
reordering transactions per block would prevent forming a quorum on
the decision to revoke the coinbase, and you'd even get additional
income from the probe transactions without even helping any real
double spends at all. The incentives seem very hard to analyze.

@_date: 2014-04-24 21:59:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 
On Thu, Apr 24, 2014 at 9:52 PM, Sergio Lerner
If you are freely specifying things, and you control the headers than
you can can already make SPV evaluations of work have log(n) scaling.
See:  at lists.sourceforge.net/msg04318.html
(wrt headers in reverse, perhaps you might also want to mine
for ideas).

@_date: 2014-04-25 08:46:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP32 "wallet structure" in use? Remove 
I don't believe that wallet interoperability at this level is possible
in general except as an explicit compatibility feature. I also don't
believe that it is a huge loss that it is so.
The structure of the derivation defines and constrains functionality.
You cannot be structure compatible unless you have the same features
and behavior with respect to key management.  To that extent that
wallets have the same features, I agree its better if they are
compatible? but unless they are dead software they likely won't keep
the same features for long.
Even if their key management were compatible there are many other
things that go into making a wallet portable between systems; the
handling of private keys is just one part:  a complete wallet will
have other (again, functionality specific) metadata.
I agree that it would be it would be possible to support a
compatibility mode where a wallet has just a subset of features which
works when loaded into different systems, but I'm somewhat doubtful
that it would be widely used. The decision to use that mode comes at
the wrong time? when you start, not when you need the features you
chose to disable or when you want to switch programs. But the obvious
thing to do there is to just specify that a linear chain with no
further branching is that mode: then that will be the same mode you
use when someone gives you a master public key and asks you to use it
for reoccurring changes? so at least the software will get used.
Compatibility for something like a recovery tool is another matter,
and BIP32 probably defines enough there that with a bit of extra data
about how the real wallet worked that recovery can be successful.
Calling it "vendor lock in" sounds overblown to me.  If someone wants
to change wallets they can transfer the funds? manual handling of
private keys is seldom advisable, and as is they're going to lose
their metadata in any case.  No one expects to switch banks and to
keep their account records at the new bank. And while less than
perfect, the price of heavily constraining functionality in order to
get another result is just too high.

@_date: 2014-04-25 13:13:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP - Selector Script 
You're reading that response the wrong way. It isn't in any way
opposed to the specification, it's pointing out that the specification
is _unclear_ about the applications, it mentions one but doesn't
explain it and it wouldn't be apparent to all readers. Thats all.
It could be clarified by saying something like "allows spending to be
controlled by the publication of information, for example in another
transaction so that they can only be completed atomically [citation to
a revision of the contracts page]".

@_date: 2014-04-25 13:19:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP - Hash Locked Transaction 
I think the hash-locked transactions are very useful, and I think
Peter agrees (no?)
But I agree with him that that for the oracle case the EC public
points are superior. (Also: Reality keys works like this.)

@_date: 2014-04-29 07:21:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
These parties wouldn't generally consider themselves attackers? nor
would many users (presumably everyone who mines on ghash.io, for
example)? rather they'd they may consider someone using hashpower
voting to reassign coins to be an attacker, and reassigning their
coins instead to be a morally justified and pragmatic response.
I think we're capable here of discussing the specifics without needing
to use generalizations which invite definitional arguments... I don't
think that bombastic language like doomed helps the dialog.

@_date: 2014-07-31 19:28:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] deterministic transaction expiration 
Transactions that become invalid later are have pretty severe
consequences because they might mean that completely in an absence of
fraud transactions are forever precluded due to a otherwise harmless
While there may be uses for that, the resulting outputs should be
considered differently fungible? like coinbases which are immature?
and should probably be only used with great caution... not as a
mechanism for ordinary transactions.

@_date: 2014-07-31 20:31:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] deterministic transaction expiration 
A distinction there is that they can only become invalid via a
conflict? replaced by another transaction authored by the prior
signers. If no other transaction could be created (e.g. you're a
multisigner and won't sign it again) then there is no such risk.  It
now introduces chance events ("act of god") into the mix where they
they didn't exist before.  Basically it takes was what is a very loose
one way coupling and makes it much tighter. I'm sure if you spend a
bit thinking you can come up with some more corner cases that it would
expose? e.g. flooding the network with unrelated high fee transactions
in order to push a transaction out to where it can never be mined at

@_date: 2014-08-18 10:21:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Outbound connections rotation 
Connection rotation would be fine for improving a node's knoweldge
about available peers and making the network stronger against
I haven't implemented this because I think your motivation is
_precisely_ opposite the behavior. If you keep a constant set of
outbound peers only those peers learn the origin of your transactions,
and so it is unlikely that any particular attacker will gain strong
evidence. If you rotate where you send out your transactions then with
very high probability a sybil pretending to be many nodes will observe
you transmitting directly.
Ultimately, since the traffic is clear text, if you expect to have any
privacy at all in your broadcasts you should be broadcasting over tor
or i2p.

@_date: 2014-08-18 11:13:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd:  Outbound connections rotation 
Previously when you and I had discussed this I'd proposed that some
number (say) four of the most long lived connections which had proven
themselves useful (e.g. by relaying blocks) be pinned up and not be
eligible for dropping. By protecting some subset of peers you
guarantee that an attacker which simply introduces a lot of nodes
cannot partition the network which existed prior to when the attack
I may not be understanding you. Might be a definitions thing, I'm
using the definition: A sybil attack is when a party takes on many
identities (nodes) in the network.
What ivan highlights is a bit of a tradeoff between concealing
identities and linkages.  Relaying transactions through only a single
peer ever (until that one is no longer on the network) is the best
strategy for concealing your identity (ignoring tor and what not), as
only that peer learns anything about your identity.  But it may reveal
a lot about how different transactions are linked, since people
observing that peer will observe that your transactions are
The optimal strategy for avoiding linkages (ignoring tor, again), is
to randomly pick a different peer for each transaction and relay the
transaction only to that peer.  This can (and probably should) be
distinct from your normal network connectivity.
Probably for anti-linkage I'd suggest that a facility for that kind of
announcement should be done. If used over tor it would also protect
your identity.   Then the regular topology of the network can be
optimized for learning and partition resistance.

@_date: 2014-08-18 12:37:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Outbound connections rotation 
On Mon, Aug 18, 2014 at 11:37 AM, Ivan Pustogarov
I don't understand what you're talking about here; if you have no peer
at all you will learn nothing about the Bitcoin network.
Can you clarify?
What mechanism are you referring to specifically?
On our initial connections we do have a preference for nodes we knew
were up recently. This could be made further. That the current
behavior isn't great isn't an argument for making it worse on that

@_date: 2014-08-18 13:43:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Outbound connections rotation 
I'm afraid I'm losing you here.  The node advertises himself to
everyone he is connected to and in/or out, those nodes pass along
those advertisements.  When I receive an advertisement from a node I
do not know how far away the advertised peers is, presumably I can
accurately exclude it from being 0-hops? itself?) 1 or more should be
indistinguishable. Is there a reason that they're distinguishable that
I'm missing?
Can you explain to me how you propose to produce this mapping?

@_date: 2014-08-18 16:20:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Outbound connections rotation 
Okay, sorry, I thought you were saying something else. I understand.

@_date: 2014-08-19 09:38:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
On Tue, Aug 19, 2014 at 9:07 AM, Justus Ranvier
TLS is a huge complex attack surface, any use of it requires an
additional dependency with a large amount of difficult to audit code.
TLS is trivially DOS attacked and every major/widely used TLS
implementation has had multiple memory disclosure or remote execution
vulnerabilities even in just the last several years.
We've dodged several emergency scale vulnerabilities by not having TLS.

@_date: 2014-08-19 11:54:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Reconsidering github 
The obvious thing to do is setup the second repository and get it
going. Git doesn't really care all that much whats "primary".  If we
have a working workflow elsewhere then making a change won't be a leap
of faith.

@_date: 2014-08-19 16:54:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: Encrypt bitcoin messages 
On Tue, Aug 19, 2014 at 4:39 PM, Justus Ranvier
I think it's desirable (and you can go look in  logs for
me talking about it in the past)? but all of engineering is
tradeoffs... and the ones involved here don't make it a high priority
in my book, esp when people should be using Bitcoin over tor in any
case, which provides better privacy and also covers encrypt + auth.
In general I think authentication is more important than encryption,
since authentication is table stakes required for a number of
anti-partitioning-attack measures.  My past thinking on opportunistic
encryption is that once you're authenticating also encrypting would be
fairly little work, but it should be auth that drives that kind of

@_date: 2014-08-19 18:34:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Reconsidering github 
Please take the hyperbole elsewhere. Good dialog it's going to happen
with the insults and adhomenem.
Regardless of where the repositories live their integrity is protected
by digital signatures and cryptographic hashes. Running them elsewhere
can be virtuous for other reasons, but it doesn't play much into this
since the same tools must be used to guarantee their security.

@_date: 2014-08-23 13:54:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Reconsidering github 
Nothing suggested here would ever remove the ability to go and explore
and read the changes just as you're doing so.
Already the way it works is that our local repositories are
authoritative for each of us. (Git itself is a decentralized system
regardless of github's efforts to make it look otherwise).

@_date: 2014-08-26 18:08:02
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] standardize on bitcoin signed ecies ? 
Using the same keys for signing and encryption is generally considered
a bad practice, for a number of reasons.
If the keys aren't the same, there is much less reason to use
something specific to Bitcoin.
Getting all the details right in an encryption implementation is very
difficulty, previously published efforts in Bitcoin software have been
_severely_ flawed and insecure. I am not confident that an effort
right now would receive adequate review.

@_date: 2014-12-15 17:38:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Recent EvalScript() changes mean 
This change was authored more than three months ago and merged more
than two months ago.
[And also, AFAICT, prior to you authoring BIP65]
I didn't participate in that pull-req, though I saw it... it had five
other contributors working on it and I try to have minimal opinions on
code organization and formatting.
But the idea sounded (and still sounds) reasonable to me.  Of course,
anything could still be backed out if it turned out to be ill-advised
(even post 0.10, as I think now we've had months of testing with this
code in place and removing it may be more risky)... but your comments
here are really not timely.
Everyone has limited resources, which is understandable, but the
concerns you are here are ones that didn't involve looking at the code
to raise, and would have been better process wise raised earlier.
I don't see why you conclude this. Rather than violating the layering
by re-parsing the transaction as the lower level, just make this data
additional information that is needed available.
Yes, does mean that rebasing an altcoin that made modifications here
will take more effort and understanding of the code than a purely
mechanical change.
I don't agree. The character of this change is fairly narrow. We have
moderately good test coverage here, and there were five participants
on the PR.
This is all true stuff, but the fact of it doesn't follow that any
particular change was especially risky.
Beyond the general 'things were changed in a way that made rebasing
an-altcoin take more work' do you have a specific concern here?
Other than travling back in time three months and doing something
differently, do you have any suggestions to ameliorate that concern?
E.g. are their additional tests we don't already have that you think
would increase your confidence with respect to specific safety
There won't be any public users of the library until there can
actually _be_ a library.
PR4890's primary objective was disentangling the script validation
from the node state introduced by the the signature caching changes a
couple years ago, making it possible to build the consensus components
without application specific threading logic... and makes it possible
to have a plain script evaluator call without having to replicate all
of bitcoind's threading, signature cache, etc. logic.  Without a
change like this you can't invoke the script engine without having a
much larger chunk of bitcoind running.
0.10 is a major release, not a maintenance release. It's specifically
in major releases that we make changes which are not purely code
motion and narrow bugfixes (Though many of the changes in 0.10 were
nicely factored into verify pure code motion changes from behavioral
changes). There are many very important, even critical, behavioural
changes in 0.10.  That these changes have their own risks are part of
why they aren't in 0.9.x.

@_date: 2014-12-20 21:30:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Area of Focus 
Pretty much appears to be the case. In every instance it appears to be
automated. This predates the msft no-ip.com stuff.
We also had similar problems with the IRC based method that the
software originally used.
It's the same story for mail relay spam blacklisting.  There is a
whole industry out there selling people semi-snake-oil blocking
solutions to make the baddness of the internet go away. The low margin
business demands a cheap and highly automated approach... lots of
inappropriate things get blocked. Nagging people to fix things is time
consuming, better to move out of their sights a bit, so that they at
least have to specifically target Bitcoin. If they do, it'll at least
be worth the time spent fixing it.
I believe opendns is blocking all of sipa.be still as we speak, so if
you'd like to see it for yourself try to load while using opendns.

@_date: 2014-12-24 19:44:37
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Reading List for Getting Up to Speed 
You're counting 170kloc of machine generated code with the translation
strings there.
The top level (/src) and libconsensus are only about 36kloc. This is
small enough that I would strongly recommend at least skimming all of

@_date: 2014-12-30 04:51:51
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP: Voluntary deposit bonds 
If the motivation is purely enabling different rules in a soft-fork
than I think nothing needs to be done.
Instead of providing inputs to a coinbase: you provide an unusual
anyone can spend transaction in the block which pays to fees; and
simultaneously add a soft-forking rule that makes that anyone can
spend rule no longer anyone can spend.
To make that more concrete.  E.g. You make your anyone can spend
output   "PUSH OP_NOP3".  Now
this anyone can pay transaction is really just a coinbase input.
The construction is reasonably efficient, and also more flexible-- in
that it could control the data under the hash in more flexible ways
than available in the existing sighash flags.
As an aside, I'm not sure that I agree with the claim that making
coinbases have inputs is a simple modification... as we use one of the
inputs already as the special coinbase field and at least that must be
special cased.

@_date: 2014-12-30 18:28:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP: Voluntary deposit bonds 
On Tue, Dec 30, 2014 at 4:25 PM, Sergio Lerner
Sergio there is no "abuse" there,  OP_NOP3 in that case would be
redefined to OP_COINBASE_FOO_CONSISTENCY.
(I say FOO because it's not clear what rule you actually hope to apply there.)
What you suggested has no purpose by itself: it would need an
additional change which overlays functionality in order to actually do
something. Such a change would likely be "ugly"-- it's easy to be
elegant when you do nothing.

@_date: 2014-02-10 04:25:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] MtGox blames bitcoin 
In the real world fault seldom falls in a single place. Bitcoin is at
fault? in many places? for making it harder for implementers to get
things right.   MtGox is at fault for not implementing in a way that
copes with behaviors in the Bitcoin protocol which have been known
since at least 2011.
Not that Bitcoin-QT handles Malleability fantastically? but because it
tracks inputs it will still detect the mutant transactions.
An interesting point which I haven't pointed out elsewhere is that for
the question of basic funds safety in re-issuing a transaction
mallablity is basically irrelevant.
Say you pay someone and it doesn't go through (or it does and you
don't see it because its been mutated and your software can't detect
that), and they ask you to reissue.... if you reissue without
double-spending any of the original inputs you are at risk of getting
robbed. This is true with or without malleability.  Without the
double-spend of at least one input the original transaction could just
go through in addition to your reissue.
Say that you do make sure to double spend at least one input?  then
the result is funds safe safe, regardless of if a mutation happened.
Say you want to support _canceling_ a payment (send me the goat
instead!) rather than reissue you still must double-spend the
attempted payment to cancel it, since it still might go through if you
don't.  And the double spend works to protect this case regardless of
if the transaction was mutated.
For support and accounting purposes you absolutely do need tools to
identify mutated transactions, so long as mutation exists... so we
ought to provide some better tools there.  But I can't think a case
where mutation handling is necessary or sufficient for cancellation
security, but? rather? input tracking appears to be both necessary and
sufficient in all cancellation cases.
This helps explain why Bitcoin-QT? whos mutation handling kinda
stinks? doesn't ever end up in a really bad situation with mutants: it
tracks inputs pretty well.
In any case, I've always been happy to help out Mtgox with technical
issues. Having some specs for a stable transaction ID would probably
be helpful to many applications, even if it isn't the critical key you
need for cancellation security.  Removing mallability entirely has
been a soft long term goal, and there were recently (as in today) some
posts about it? look at the list archives... though it won't happen
fast since all signers/wallets will need to be updated.

@_date: 2014-02-10 08:45:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] MtGox blames bitcoin 
I am, unless you count the fact that I own some Bitcoin and some
mining hardware as "'investment' connections" (and that case your
comments are worthless).
(By not naming anyone else I don't mean to imply there are no others,
but I don't want to speak for anyone else. Nor would I necessarily
expect the other part(ies|y) to step forward, since this mostly
appears to be an invitation to step up and be attacked.)

@_date: 2014-02-10 12:40:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Malleability and MtGox's announcement 
At this point the attack should fail. Before crediting the funds back Gox
should form a new transaction moving at least one of the coins the prior
transaction was spending and wait for that transaction to confirm.
Without performing this step? even if there were no malleability at all
you'd have the risk that someone would go resurrect the old transaction
and get a miner to mine it. Then it goes through.
With performing it, even if they missed the mutated transaction in the chain
their cancellation transaction could not confirm (because its a double spend).
Back in September a lot of people were having stuck transactions and
when I looked it was because Mtgox was spending immature coins: newly
generated coins which cannot be spent for 100 blocks since their creation.
(A rule since Bitcoin's started)
Then later it was noticed that they were producing transactions with invalid
DER encodings (excessively padded signatures). The Bitcoin network used
to accept these transactions, but in order to more towards eliminating
Bitcoin 0.8 and later will not relay and mine them.
Then after people started using mutation to fix those excessively padded
transactions and/or someone was exploiting Gox's behavior? it seems that
Gox's wallet may have been in a state where it thought a lot of coins weren't
spent that were and was reusing them in new transansactions... this one
is harder to tell externally? I saw it appeared to be producing a LOT of
double spends with different destinations, but I'm guessing as to the exact

@_date: 2014-02-10 12:55:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Malleability and MtGox's announcement 
MtGox had a php script that returned base64 data for all their stalled
Not just attackers used that, some people trying to unstick their
transactions tried manually fixing them with honest intent and no idea
it would potentially confuse mtgox's software.

@_date: 2014-02-11 12:49:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] MtGox blames bitcoin 
Try paying a consultant if your ego demands that you have a technical
expert to entertain your musing with immediate response.
My response was absolutely relevant.
If you reissue a transaction without respending the prior transactions
coins, you will end up double paying. Only spending the inputs in
question can prevent the prior transaction (itself or in other form)
from going through.
Once you respend the inputs there is no risk of actually losing funds
due to an issue regardless of how you track coins in your higher level

@_date: 2014-02-12 10:03:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
This is fine and good. But it only scratches the surface of the
problems created by malleability, especially for fancier transaction
Mutation allows you to invalidate a chain of unconfirmed transaction
by mutating the parent. This breaks any protocol which depends on
creating a precomputed nlocked time refund transaction.
So a canonical ID can be used to prevent some buggy behavior it
doesn't actually fix the problem. Fortunately the non-fixed parts
aren't too critical today.
This is incorrect.  MtGox was automatically issuing replacement
transactions resulting in double payments.
When you attempt to replace/reissue/cancel a transaction you __MUST__
double-spend the original transaction. If the original transaction has
not been conflicted then it is possible someone will pull the original
transaction out of a hat and both your replacement and the original
will be confirmed.  It is not safe at any time to look to see if the
original has been confirmed yet, and if not reissue? not because
mutation may mean you're looking in the wrong place? but because the
state of the world could change nano-seconds after you looked.
If you do double-spend the original then there is no chance that both
will go through, you'll have atomic exclusion and only one transaction
or the other will be confirmed.

@_date: 2014-02-12 16:47:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
It has been, but there are probably many people like you who have not
bothered researching who may also be curious.
Because doing so would be both unnecessary and ineffective.
Unnecessary because we can very likely eliminate malleability without
changing what is signed. It will take time, but we have been
incrementally moving towards that, e.g. v0.8 made many kinds of
non-canonical encoding non-standard.
Ineffective? at least as you describe it? because the signatures
_themselves_ are malleable.

@_date: 2014-02-19 12:39:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
You did. See the other sighash flags.
In exchange you make the behavior basically impossible do deploy
without first blocking all ongoing transactions. This seems foolish.
All signers need to be updated to change their behavior to be
anti-malleability compatible, they can change their version at the
same time... and leave things actually working for the things which
can't be easily updated.

@_date: 2014-02-19 13:05:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
For some reason it took me a couple reads to get this so I thought I'd
restate it in a more blunt form.
There may exist people today who have send funds to addresses,
authored nlocktime releases, and destroyed the key the funds are at
now in order to achieve a timelock.  This might be a foolish thing to
do, but it's the kind of thing that you have to worry about when
potentially breaking existing transactions.
(This kind of us is, fwiw, another example of why ANYONE_CAN_PAY is useful).

@_date: 2014-02-20 06:15:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
You mean P2SH... which your implementation has only picked up support
for in the last month or so?

@_date: 2014-02-20 06:36:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
Sure I'm all for doing what Pieter suggested? it's basically the plan
we've been executing for some time already but with the version check
to make it sane to complete.
My reserved sounding comments were relative to the proposals to do
things with nversion=1 transactions, frankly I think thats completely
insane. Though while we're on the subject of reservations, I am far
from confident that we've uncovered all the possible malleability
routes? that list gained a new, never before discussed entry, when
Pieter was writing it a couple weeks ago.  We also have no proof of
the absence of further algebraic malleability in DSA (though I think
its somewhat unlikely, a solid proof of it has been somewhat elusive).

@_date: 2014-02-20 22:30:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
We're in agreement.  I had mistakenly believed you were supporting the
discussion about trying to force these constraints on current version
transactions, in which case "wallets will pick up at different times"
is an absolute deal breaker.  :)

@_date: 2014-02-24 15:13:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] On OP_RETURN in upcoming 0.9 release 
At least there is no ambiguity that such usage is abusive. Adoption of
the practices matters too. Right now I've seen a lot of people
promoting data storage as a virtuous use, and gearing up to directly
store data when a commitment would work.
If it turns out that encouraging people to use hashes is a lost cause
it can always be further relaxed in the future, going the other way is
much harder.

@_date: 2014-01-03 10:30:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] An idea for alternative payment scheme 
Allow me to introduce an even more wild idea.
The payee publishes two public keys PP  PP2.
The payer picks the first k value he intends to use in his signatures.
The payer multiplies PP2*k = n.
The payer pays to pubkey PP+n  with r in his first signature, or if
none of the txins are ECDSA signed, in an OP_RETURN additional output.
The payer advises the payee that PP+(pp2_secret*r) is something he can
redeem. But this is technically optional because the payee can simply
inspect every transaction to check for this condition.
This is a (subset) of a scheme by Bytecoin published a long time ago
on Bitcoin talk.
It has the advantage that if payer drops his computer down a well
after making the payment the funds are not lost, and yet it is still
completely confidential.
(The downside is particular way I've specified this breaks using
deterministic DSA unless you use the OP_RETURN, ... it could instead
be done by using one of the signature pubkeys, but the pubkeys may
only exist in the prior txin, which kinda stinks. Also the private
keys for the pubkeys may only exist in some external hardware, where a
OP_RETURN nonce could be produced when signing).
These schemes have an advantage over the plain payment protocol
intended use (where you can just give them their receipt number, or
just the whole key) in that they allow the first round of
communication to be broadcast (e.g. payee announced to EVERYONE that
he's accepting payments) while preserving privacy.

@_date: 2014-01-13 01:52:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
However, if you're able to use the payment protocol then you probably
don't need stealth addresses to prevent reuse.
E.g. What can I put for a donation address on a totally static
webpage? or on a billboard?
At least thats what I understood these things were trying to solve?
fix privacy where realtime two way communication just isn't available.

@_date: 2014-01-13 12:10:56
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
The stealth address stuff is the ECDH to create multiple payment
addresses without querying the payee.
Uh while I'm responding again, what I'd discussed with Peter Todd in
IRC used two EC points in the stealth address. One for the payment and
one for the ECDH.  The reason to use two is that it makes delegating
detection possible and so you don't have to have you spending keys
online to even detect these payments.  Why'd that get dropped?
I don't think this is a good idea if you have to constantly keep your
spending key(s) online even to detect payments, even with the limited
use-cases envisioned.

@_date: 2014-01-13 12:47:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
If you have enough of a communications channel to do that you can just
give the person the finished address (authentication aside).
The idea here is to cover the cases where the communication is one way
or nearly so.  Consider a donation address on a fully static webpage,
in a forum signature or email address, or on a billboard. ... or where
users-being-users mean that the user isn't going to start up their
wallet software to compute a new multiplier every time they give out
an address no matter how much we whine at them.

@_date: 2014-01-15 12:38:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
ACK.  Regardless of the 'political' overtones, I think stealth is a
little cringe-worthy.
"Private address" would be fine if not for confusion with private-keys.
"Static address" is perhaps the best in my view. (also helps improve
awareness that normal addresses are intended to be more one-use-ness)

@_date: 2014-01-15 17:23:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bait for reusable addresses 
One challenge with reusable addresses is that while they result in a
small constant overhead for full nodes in searching for their own
transactions they create large overheads for SPV nodes.
One way to address this is for the SPV nodes to hand their servers
their blinding private key so that the server may test addresses on
their behalf. The primary problem with this is that it is
non-reputable:  If I show you a blinding private key and say a set of
transactions are related you will be utterly convinced of it, the
transactions really are related. This makes the privacy brittle.
It also has a downside of not being indexable for the server, the
server must do O(clients * reusable-address-txn) work and the work
includes an ECC multiply.
An idea that Adam Back had originally proposed was including optional
"bloom bait", a small token? say 8 bits? that distinguished
transactions which allowed an anonymity set vs filtering trade off.
Such a bait would be indexable, enabling faster lookup too.
But bloom bait has privacy problems more severe than the current SPV
bloom filtering. While you leak information to your SPV servers today
if you use bloom filtering the leak usually goes no further. So a
compromise requires both a statistical attack _and_ using SPV servers
that log data against your interest.  With bloom bait the whole
network can see the relation. That is unfortunate.
I suggest instead that with optional bait is included in an address
that the sender compute H(nonce-pubkey) and then pick one byte at
random out of the first 16 and xor it with the specified bait and
store the result in the transaction.  An SPV server can now index the
bait as it comes in by extracting 16 8-bit keys from each transaction
(the 16 bytes xored with the bait in the transaction).  When the
client wants to search for transactions it can give the server a list
of keys its interested in? including their real key and number of
random number of cover keys.
ObTechnicalWank:  This is a specific simple instance of a general
class of solutions which are related to locally decodable error
correcting codes: E.g. the transaction data represents a codeword in a
vector-space and the degree of freedom provided by the adjustable
prefix is used to ensure that codeword is never more than a certain
distance from a specified point.  The point isn't made public in the
transaction and it's hidden from the server by providing several
points.   There is still an information leak here? as if someone
believes a set of transactions are related they can intersect their
radiuses and test if the intersection is empty, and if it's not assume
that they found the secret bait? but it is substantially lower an
information leak than the prefix case.
I didn't give any though into the parameters 8-bits and 16 dimensions.
Some reasoning should be done to fix the parameters in order to make
them the most useful: e.g.
Systems derived from more complex linear codes might give better
performance, e.g. two secret bloom baits, two prefixes in the
transaction bait0^random_char[0-8], bait1^random_char[0-8],  server
extracts 16 keys.. and returns to the client transactions which have
at least two key matches with their list.
Obviously whatever is used needs to be easy to implement, but schemes
loosely based on fountain codes should only require picking some
things and xoring... so they should be simple enough.

@_date: 2014-01-15 17:32:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] unlinakble static address? & spv-privacy 
Ignoring prefixes the cost for each reusable address is only a small
percentage of the full node cost (rational: each transaction has one
or more ECDSA signatures, and the derivation is no more expensive), so
I would only expect computation to be an issue for large centralized
services. (non-full nodes suffer more from just the bandwidth impact).
I'd point out that regardless of how long the desired prefix is, the
encoded prefix should probably always be constant length in all
reusable addresses. If you don't want a particular prefix then the
sender should just pick random data for the rest of the space. There
is no need to publish any additional distinguishing data in the form
of how long the prefix is.

@_date: 2014-01-17 21:09:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
Doing ECDH with our curve is within a factor of ~2 of the fastest
encryption available at this security level, AFAIK.  And separate
encryption would ~double the amount of data vs using the ephemeral key
for derivation.
Using another cryptosystem would mandate carry around additional code
for a fast implementation of that cryptosystem, which wouldn't be
So I'm not sure much can be improved there.

@_date: 2014-01-18 15:50:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Stealth Addresses 
Well super-fast hand optimized code for (your choice of) 160 bit curve
may not ever exist, making it slower in practice. Plus the extra code
to carry around even if it does exist?

@_date: 2014-07-04 04:37:26
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] ASIC-proof mining 
Thanks for sharing. Ideas similar to what you're describing have come
up a number of times before.
I believe the particular formulation you're suggesting is not workable
for a number of reasons.
If I understand what you're proposing correctly, it that it has very
high (nearly symmetrical) verification costs, all the verifiers have
to also hash all of that information to check the result. It is
imperative for the system that the proof of work be cheap to verify,
since every system needs to verify it and have no incentive to skip
verifying it, needs to use it to block DOS attacks, etc.
I believe this design would also completely preclude lite nodes (SPV
nodes, section 8 of  which are the
most popular Bitcoin wallets. SPV wallets do not need to store the
blockchain, in fact they technically need no storage at all? and are
secure, given some assumptions about the decentralization and honesty
of mining. It would make Bitcoin more or less infeasible to use on
mobile devices and force many people using wallets onto centralized
services providers which they'd have to trust to process their
Another longer term side effect of making verification costly is that
it makes it much less reasonable to provide zero knoweldge proofs for
data in Bitcoin? closing off a whole set of useful tools like strongly
private proofs of solvency, and strongly private bitcoin-backed
pseudonymous identities.
I also believe this would also break pruning (section 7 of
bitcoin.pdf): Right now a fully validating node can be created that
uses only on the order of 1GB of disk space, without pruning the
number is 25 GB and the gap is just going to grow over time. The
elimiating of pruning would be a major scalability hit.
A smaller, but potentially still important issue is that the proposed
proof of work function would be expensive to run even once. This may
result in it not being effectively progress free? if a miner would
typically only make a small number of tries before success then it
would make mining like a race where faster miners would have a
super-linear advantage over others instead of statistically rewarding
miners fairly.
There are ways to make what I think you're trying to accomplish work
with fewer tradeoffs that have been suggested before (see
 "POW which involves
queries against the UTXO set")... the general idea there is that the
candidate block header is used to randomly select one or a few random
entries in the set of spendable coins (UTXO set), which are then
included in the hashing. If the UTXO set is also committed in every
block via a hash tree when the miner finds a solution he can also
extract a compact membership proof that shows the UTXO he included in
his hashing were the right ones.  This way the work can still be
verified by systems that don't have the blockchain (though they may
use 10x more bandwidth? unfortunate on its own and perhaps enough to
still make zero knoweldge proofs less practical), and because the
queries are against the UTXO set instead of the whole blockchain it's
not incompatible with pruning.
Though even with those fixes, I am far from sure that this would be
helpful: It would not preclude specialized high efficiency hardware
for mining (see for set of general arguments in this space), and the hardware that
existed may not be actually useful for validation in much the same way
that you cannot use existing mining hardware as a general sha256
This specialized hardware might look more like an massively parallel
flash or dram array with integrated computation (e.g.
 )? and these differences may
not all be good: by shifting costs from operating energy to gate-count
it moves the total costs into hardware which is one-time and amortized
over use (generally for modern process, compute bound equipment costs
more in energy than the marginal costs in fabrication after a month or
two of operation), potentially creating an advantage for
earlier/larger participants. Plus a CRAM like design might also have
massive throughput advantages compared to commodity hardware operating
in a bus limited mode its hard to say until millions have been sunk in
trying to optimize it, but even if it does not? one of the arguments
made in asic-faq.pdf is because mining should be, in theory, nearly
perfect competition even the small advantage in costs from eliminating
unneeded peripherals can basically drive everyone without that
advantage out.
As an aside, there is an altcoin "boolberry" that implements something
where 2MB of data is extracted from the blockchain and then mined one.
But because the extraction is not in the inner-loop mining pools just
send it out to miners... and of course it could be uploaded to a
dedicated mining coprocessor (or FPGA, or GPU) if anyone ever got
around to doing the optimizations... it also has most of the other
issues I raised above relative to your proposal. It's still too new to
see what failure modes it suffers the most from first, and the
altcoins that it is mostly competing with suffer from their own ill
advised (_very slow_) POW.
No need to be sorry? talking about these things is how people learn.
While I don't think this idea is good, and I'm even skeptical about
fixed versions? I promise you many other people were thinking similar
or even less useful things and will find the discussion interesting.

@_date: 2014-07-16 07:57:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Draft BIP for geutxos message 
In IETF documents there is a required security considerations section,
see In many of our documents the whole thing is a security consideration
but for ones like these we should probably always document the
weaknesses as set out from the rest of the document.  See how BIP32
enumerates the one-private-key-breaks the chain.
On this point the getutxos document is doing well.  Perhaps breaking
some things out of the auth section into a security /
security-limitations section.  In particular, can this document
specifically call out that a local network attacker can MITM all the
(If Mike would prefer, I can send a diff with proposed changes)

@_date: 2014-07-17 13:08:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Pay to MultiScript hash: 
This seems strictly less flexible and efficient than the Merkelized
Abstract Syntax Tree construction, though perhaps slightly easier to
implement it wouldn't be any easier to deploy.
Something like this was very recently proposed on this list (by Tier
Nolan), you might want to see the "Selector Script" thread.

@_date: 2014-07-17 16:34:21
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
Another option would be to just guarantee to keep at least the last N
sent in each direction to bound memory usage. N could be negotiated.
Going more complex than that may not have wins enough to justify it...
would be good to measure it.
(If you're not aware of it, check out?
 for a
more complex idea)

@_date: 2014-07-18 18:25:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
Most things I've seen working in this space are attempting to minimize
the data transfered. At least for the miner-interested case the round
complexity is much more important because a single RTT is enough to
basically send the whole block on a lot of very relevant paths.
I know much better is possible (see up-thread where I linked to an old
proposal to use forward error correction to transfer with low data
transfer (but not optimal) and negligible probability of needing a
round-trip, with a tradeoff for more overhead for lower roundtrip

@_date: 2014-07-18 23:48:18
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
Oh that does sound interesting? its the property I was trying to
approximate with the FEC..  It achieves the one-shot, but there is
overhead. One plus we have is that we can do some tricks to make some
computational soundness arguments that we'd actually get average
performance on average (e.g. that someone can't author transactions in
such a way as to jam the process).
Thank you, I've certantly queued the paper for reading.

@_date: 2014-07-18 23:56:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Small update to BIP 62 
No, though thats true too. I was talking about the properties of the DSA nonce:
An attacker is not obligated to follow your protocol unless you can
prevent him. You can _say_ use derandomized DSA all you like, but he
can just not do so, there is no (reasonable) way to prove you're using
a particular nonce generation scheme without revealing the private key
in the process. The verifier cannot know the nonce or he can trivially
recover your private key thus he can't just repeat the computation
(well, plus if you're using RFC6979 the computation includes the
private key), so short of a very fancy ZKP (stuff at the forefront of
cryptographic/computer science) or precommiting to a nonce per public
key (e.g. single use public keys), you cannot control how a DSA nonce
was generated in the verifier in a way that would prevent equivalent
but not identical signatures.
(I believe there was some P.O.S. altcoin that was vulnerable because
of precisely the above too? thinking specifying a deterministic signer
would prevent someone from grinding signatures to improve their mining
odds... there are signature systems which are naturally
randomness-free: most hash based signatures and pairing short
signatures are two examples that come to mind... but not DSA, schnorr,
or any of their derivatives).

@_date: 2014-07-19 00:03:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Signature with negative integer? 
A rather timely post.  See the other thread on BIP0062. What you're
looking at is an example of a well-known-to-implementers-here where
invisible and undocumented "over permissiveness" in interpreting
invalid encoding in a cryptographic library (OpenSSL in our case)
which would have been probably-not-unwelcome in many other protocol
uses results in an unexpected consensus critical normative rule in
Modern releases of Bitcoin core will no longer relay or mine them but
they're still valid in blocks should they show up.
BIP62 proposes, among other things, soft-forking (backwards
compatible) changes that will strictly limit the DER encoding to avoid
ambiguity. If adopted by the network implementations would still need
to grandfather in existing weird transactions but could do so on a
txid by txid basis since there would be no more broken encoding
permitted in blocks, and use different DER decoding code without risk
of consensus inconsistency (so long as it uses der decoding which is
functionally identical to what BIP62 requires? of course).

@_date: 2014-07-21 13:19:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Policy for DNS seeds 
Agreed.  I've seen some amount of use of dnsseeds which I would
consider inadvisable considering their weakness.
Also agreed, we ought to have a separate onionseed process for hosts
which can reach hidden services which would be inherently
authenticated and somewhat more anonymous. The existing introduction
method already doesn't work well for onlynet=onion hosts, so that
would be a good place to start.
I was deliberately vague here in that I'm trying to avoid foreclosing
reasonable activities like omitting nodes which are uselessly slow,
diverged from the network, or running very old software.  The test I'm
suggesting is that if "why am I doing this" is "to connect users to
functioning nodes" then it's probably okay, but if its to achieve some
other end? probably not.
Yes, this is one of the reasons we use DNS (and also one of the
reasons the document suggests a non-zero minimum ttl)... but belt and
suspenders, even though technical measures are protective here it's
good to make it clear that this isn't acceptable.
Yep. That was the intent. (well not testnet, but the notion that if
there really were a good reason to do something else a discussion
should cover it)

@_date: 2014-07-24 19:39:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Time 
Is breadwallet tamper resistant & zero on tamper hardware? otherwise
this sounds like security theater.... I attach a debugger to the
process (or modify the program) and ignore the block sourced time.

@_date: 2014-07-27 19:29:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
How do you know what traffic it's actually doing.
I'm confused as to how its doing anything at all, as it doesn't have
the exit flag. (IIRC, Tor directories won't give you the exit flag
unless you exit 80/443 to a pretty substantial chunk of IPv4 space).
Because of this no normal tor node should be selecting it as an exit.
Could this just be lying about its traffic levels?

@_date: 2014-07-27 19:45:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
Or its a complete snipe hunt, I'm unable to find any nodes with it
connected to them. Does anyone here have any?
Last discussion on the measures for anti-global-resource-consumption
was at   but it hasn't
seemed to be a huge issue such that adding more protocol surface area
was justified.

@_date: 2014-07-27 20:07:56
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
[unimportant update] Turns out that my IPv4 nodes already have
iptables blocking of that subnet, presumably due to other misconduct
there, which might be why I'm not seeing it.
Several other people appear to be observing it, and all it seems to be
doing is listening without sending transactions? e.g. surveillance
node... not the first time thats happened, but the weird tor
non-exit-flagged-exit adds a fun level of intrigue to it.

@_date: 2014-07-27 20:44:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
As I pointed out above, ? it isn't really.  Without the exit flag, I
believe no tor node will select it to exit 8333 unless manually
configured. (someone following tor more closely than I could correct
if I'm wrong here)
dsnrk and mr_burdell on freenode show that the bitnodes crawler showed
it accepting _inbound_ bitcoin connections 2-3 weeks ago, though it
doesn't now.
Fits a pattern of someone running a bitcoin node widely connecting to
everyone it can on IPv4 in order to try to deanonymize people, and
also running a tor exit (and locally intercepting 8333 there),  but I
suspect the tor exit part is not actually working? though they're
trying to get it working by accepting huge amounts of relay bandwidth.
I'm trying to manually exit through it so I can see if its
intercepting the connections, but I seem to not be able.
Some other data from the hosts its connecting out to proves that its
lying about what software its running (I'm hesitant to just say how I
can be sure of that, since doing so just tells someone how to do a
more faithful emulation; so that that for whatever its worth).

@_date: 2014-07-28 07:08:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Abnormally Large Tor node accepting only 
The bitcoin protocol is more or less identityless. It's using up lots
of network capacity, "number of sockets" is as pretty close as you
It isn't relaying transactions or blocks as far as anyone with a
connection to it can tell.
and sure, probably not much to worry about? people have been running
spy nodes for a long time, at least that much is not new.

@_date: 2014-07-31 14:29:40
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
I explain how to do this on the network block coding page.
Given that you know the sizes and orders of the transactions (e.g.
from a reconciliation step first), the sender sends non-syndromic
forward error correcting code data somewhat larger than their estimate
of how much data the user is missing.  Then you drop the data you know
into place and then recover the missing blocks using the fec.
There is no overhead in this approach except for FEC blocks that are
incompletely missing (and so must be completely discarded), and the
need to have the transmitted the transaction list and sizes first.
(note, that just more bandwidth, not an additional round trip).

@_date: 2014-07-31 14:51:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
"(e.g. from a reconciliation step first)" the list can be communicated
in the space roughly equal to the size of the difference in sets plus
coding the permutation from the permissible orderings.   If you did
have some "simple approach" that guaranteed that some transactions
would be present, then you could code those with indexes... the FEC
still lets you fill in the missing transactions without knowing in
advance all that will be missing.   (Also, the suggestion on the
network block coding page of using part of a cryptographic permutation
as the key means that for unknown transactions the transmission of the
new unknown keys is always goodput? doesn't add overhead)
It's "only a bound" but you can pick whatever bound you want,
including? if you send data equal to the missing amount, then it'll be
always successful, but no bandwidth savings.   Though if the transport
is unordered (e.g. UDP or non-blocking SCTP) even sending 100% of the
missing amount can save time by eliminating a round trip that might
otherwise be needed for a retransmission.

@_date: 2014-07-31 16:18:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Squashing redundant tx data in blocks on 
============================== START ==============================
False positives, and if you have more than one peer? false negatives.
(or a rule for what you must keep which is conservative in order to
avoid creating huge storage requirements? but then also has false
Yes, minimizing latency in the face of multiple peers.
Otherwise no. And certantly no reason to to implement something simple first.

@_date: 2014-06-02 00:54:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [QT] Feature proposal: Displaying current 
If thats done it should be done in a way in which it's impossible that
a stray keypress could switch it or someone may eventually have a very
very bad day.

@_date: 2014-06-04 03:15:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] # error "Bitcoin cannot be compiled 
As a matter of procedure we do not use assertions with side effects? the
codebase did at one point, but have cleaned them up.  In an abundance of
caution we also made it refuse to compile without assertions enabled: A
decision who's wisdom was clearly demonstrated when not long after, some
additional side-effect having assert was contributed. In the real world
errors happen here and there, and making robust software involves defense
in depth.
Considering the normal criticality of the software it should always be with
the assertions. Without them is an untested configuration.  It would
probably be superior to use our own assertion macros (for one, they can
give some better reporting?) that don't have the baggage ordinary
assertions have, but as a the codebase is a production thing, making larger
changes all at once to satisfy aesthetics would be unwise... simply
refusing to compile in that untested, unsupported configuration is prudent,
for the time being.

@_date: 2014-06-04 03:31:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] # error "Bitcoin cannot be compiled 
Yes... it takes only about 10 lines of code to have a nicer assert
than the posix one, all my own software does... and with the noreturn
attribute on the failure path it behaves the same for most static
analysis tools as a regular assert does. I would have just dropped one
in, but an IFDEF seemed more prudent at the time.

@_date: 2014-06-06 02:03:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] NODE_BLOOM service bit 
The performance Bytecoin/Monero/Fantom/etc. systems that use ECDH
addresses for all transactions seem to be suggesting that the prefixes
aren't really needed.
At least with current system rules doing the ECDH for each transaction
seems pretty reasonable.

@_date: 2014-06-06 09:58:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bloom bait 
Huh? How are you thinking that something that gets put in transactions
and burned forever into the blockchain that lets you (statically) link
txout ownership is "no different" from something which is shared
directly with a couple peers, potentially peers you trust and which
are run by yourself or your organization?

@_date: 2014-06-06 10:10:51
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bloom bait 
I'm not trying to pick nits about all the options,  I just found it
surprising that you were saying that data published in a super public
manner is no different than something used between nodes.
Communication is a two way street, Adam and I (and others) are
earnestly trying? that we're not following your arguments may be a
suggestion that they need to be communicated somewhat differently.
I'm still failing to see the usefulness of having any prefix filtering
for DH-private outputs. It really complicates the security story? in
particular you don't know _now_ what activities will turn your prior
information leaks into compromising ones retrospectivelly, and doesn't
seem at very necessary for scanning performance.

@_date: 2014-06-09 11:23:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP38 Encrypted Address Discussion 
See the not yet finished proposal at
It's generally a lot more sound and well thought out than BIP38.
Though right now I believe it's being revised to support secret

@_date: 2014-06-17 15:46:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] CoinJoin bounty fund question 
The correct place for more information is the Bitcointalk forum thread
where it was announced:

@_date: 2014-03-05 11:51:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Both of these things have long been promoted as virtuous in part
because they increase robustness against this sort of thing.
But while I don't disagree with these things the reality is that many
people do not follow either of these piece of advice and following
them requires behavioral changes that will not be adopted quickly...
so I don't think that advice is especially useful.
And even if it were?, good security involves defense in depth, so
adding on top of them things like side-channel resistant signing is
I haven't had a chance to sit down and think through it completely but
I believe oleganza's recent blind signature scheme for ECDSA may be
helpful (
The idea is that instead of (or in addition to? belt and suspenders)
making the signing constant time, you use the blinding scheme to first
locally blind the private key and point being signed, then sign, then
unblind.  This way even if you are reusing a key every signing
operation is handling different private data... and the only point
where unblinded private data is handled is a simple scalar addition.

@_date: 2014-03-05 12:54:04
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New side channel attack that can recover 
But it's still incomplete.
Say you have an address? used only once!? with a txout with a lot of value.
Someone starts paying you small amounts to that address over and over
again. You haven't asked them to, they're just doing it.
Do you ignore the funds?? maybe tell some customer that was ignorantly
paying you over and over again to a single address "sorry, those are
my rules: I only acknowledge the first payment, those funds are
No, of course not.  You spend the darn coins and if you're on a shared
host perhaps you disclose a private key.
The probability of an attack actually going on is low enough compared
to the cost of spending the coins in that case that even someone with
good knoweldge of the risks will choose to do so.
So absolutely, not reusing addresses massively increases your safety
and limits losses when there is theft. But it isn't enough alone. (Nor
is smarter signing, considering complex software like this has bugs
and its hard to be confident that something is side channel free? esp
when you allow attacker interference).

@_date: 2014-03-05 13:44:30
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Do take care that branchless doesn't mean side-channel free: On
non-trivial hardware you must have uniform memory accesses too.
(and that itself isn't enough for sidechannel freeness against an
attacker that can do power analysis... then you star worrying about
the internal structure your primitive adders and the hamming weight of
your numbers, and needing to build hardware that uses differential
logic, and yuck yuck yuck:  This is why you still shouldn't reuse
addresses, and why a blinding approach may still be sensible, even if
you believe your implementation is hardened against side-channels)

@_date: 2014-03-05 14:25:02
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Absolutely. I believe these things are worth doing.
My comment on it being insufficient was only that "my signer is
branchless" doesn't make other defense measures (avoiding reuse,
multsig with multiple devices, not sharing hardware, etc.)
No. At a minimum to hide a memory timing side-channel you must perform
no data dependent loads (e.g. no operation where an offset into memory
is calculated). A strategy for this is to always load the same values,
but then mask out the ones you didn't intend to read... even that I'd
worry about on sufficiently advanced hardware, since I would very much
not be surprised if the processor was able to determine that the load
had no effect and eliminate it! :) )
Maybe in practice if your data dependencies end up only picking around
in the same cache-line it doesn't actually matter... but it's hard to
be sure, and unclear when a future optimization in the rest of the
system might leave it exposed again.
(In particular, you can't generally write timing sign-channel immune
code in C (or other high level language) because the compiler is
freely permitted to optimize things in a way that break the property.
... It may be _unlikely_ for it to do this, but its permitted? and
will actually do so in some cases?, so you cannot be completely sure
unless you check and freeze the toolchain)
I wouldn't be surprised.

@_date: 2014-03-05 14:27:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Process for getting a patch aproved? 
To the reference software?  Normally you'd open a github account and
submit there.
Though if for some reason you can't? though its strongly preferred?
sending a git-format-patch via email might be an acceptable fallback.

@_date: 2014-03-08 15:13:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin wiki is down 
This works.  The wiki is in the process of changing control/operation.
Nothing to fear.

@_date: 2014-03-08 17:57:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Security shouldn't depend on perfect use.  There are many things that
result in multiple key use: Bitcoin address authentication (something
which the pool you created uses!), someone spamming you with multiple
payments to a common address which you didn't solicit (what, are you
just going to ignore the extra coins?), ... or just practical
considerations? I note the mining pool you founded continually pays a
single address for 'fall back' payments when it can't pay in the
coinbase transact, I know you consider that a bug, but its the reality
Most security issues aren't the result of one problem but several
problems combined, so it's important to make each layer strong even if
the strength shouldn't be important due to proper use in other layers.
Fortunately, libsecp256k1 has a nearly constant time/constant memory
access multiply for signing which should reduce exposure substantially
(and is generally built in a way that reduces vulnerabilities).

@_date: 2014-03-16 06:54:49
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum 1.9.8 release 
The cryptosystem in that repository appears to be insecure in several
ways and is not actually implementing ECIES.
The most important of which is that instead of using a
cryptographically strong mac tied to the ephemeral secret it uses a
trivial 16 bit check value.  This means that that I can decode an
arbitrary message encrypted to a third person if they allow me to make
no more than 65536 queries to a decryption oracle to decrypt some
other message.
Also, in the event that a random query to a decryption oracle yields a
result (1:2^16 times) the result directly reveals the ECDH value
because it is only additively combined with the message value. If the
implementation does not check if the nonce point is on the curve (an
easy implementation mistake) the result can yield a point on the twist
instead of the curve which is far more vulnerable to recovery of the
private key.  ECIES uses a KDF instead of using the ECDH result
directly to avoid this.
There may be other problems (or mitigating factors) as it was very
hard for me to follow what it was actually doing.
(The particular implementation has a number of other issues, like
apparently not using a cryptographically strong RNG for its EC nonce..
but I assume you didn't copy that particular flaw)

@_date: 2014-03-16 07:39:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum 1.9.8 release 
It also leaks on the order of 7 bits of data about the message per
message chunk.  I'm also think it's likely that there are some
messages which are just incorrectly decrypted.   ... it's really
screwy and suspect.

@_date: 2014-03-17 08:55:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 2-way pegging (Re: is there a way to do 
One point to note here is that the if the whole move and quieting
period stuff sounds
cumbersome? thats because it is. Even with the best efficiency optimizations the
security requirements result in somewhat large and slow transactions?
and thats totally fine!
A key point here is that normally someone who needs to use coins on one chain or
the other can use fast atomic cross-chain transactions[1][2] and not
bother with the
slow direct movement across. The cross chain swapping, however, requires an
(untrusted) counterparty on the other chain, while the 2-way peg migrations can
be performed alone in order to provide liquidity and balance demand.
[1] (Hm the citation there is weird, that predates TierNolan's post)
[2] CoinSwap: Transaction graph disjoint trustless trading
(private version)

@_date: 2014-03-25 15:34:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
I don't think it would be entirely unfair to describe one of the
possible ways a secondary coin becoming unbacked can play out as
inflation? after all, people have described altcoins as inflation. In
the worst case its no _worse_ inflation, I think, than an altcoin is?
I think that chain geometries which improve the scale/decentralization
trade-off are complementary. If PT's ideas here do amount to something
that gives better scaling without ugly compromise I believe it would
still be useful no matter how well the 2-way peg stuff works simply
because scaling and decenteralization are both good things which we
would pretty much always want more of...

@_date: 2014-03-29 07:36:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Practically speaking you cannot unless the technique used is one
carefully selected to make it possible. This proposal isn't such a
scheme I beleieve, however,  and I think I'd strongly prefer that we
BIP standardize a formulation which also has this property.
The paper you want is
There will soon be a paper coming out from some princeton folks about
refining that and applying it to Bitcoin.
You can use the secret sharing from threshold ecdsa in the
not-super-useful way where you just recombine the private key and
sign... but you can also use it to compute a secret shared signature
and then interpolate back the signature... avoiding the need for any
trusted device in holding the signature.

@_date: 2014-03-29 10:46:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
But see the first question.  Basically you can do an interactive step
to generate a master pubkey and then use BIP32 non-hardened derivation
to build thresholded children.
BIP38 is a bad example (because it was created without public
discussion due to a technical snafu).
In this case I don't see anything wrong with specifying secret
sharing, but I think? if possible? it should be carefully constructed
so that the same polynomials and interpolation code can be used for
threshold signatures (when encoding compatible data).
If it requires entirely different code than the code for threshold
signing it might as well be a file generic tool like SSSS.

@_date: 2014-03-29 13:05:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dust recycling 
Hm. maybe it could be called a "return operator" or something like that? :)
Use dust-b-gone and make it someone elses problem to get it relayed. :)

@_date: 2014-03-29 13:33:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Dust recycling 
Then start the server yourself. There is no replacement for
aggregating multiple bits of dust in single transactions. Other ways
are less efficient.
You can already OP_RETURN to include your dust, but miners aren't
going to automatically take that over transactions paying more in
terms of fee/byte.

@_date: 2014-05-03 12:08:33
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bug with handing of OP_RETURN? 
In this case it's not a question extra txouts, its a question of
additional pushes. Assuming you didn't get the push overhead for free,
the only issue I see with that off the cuff is extra complexity in the
IsStandard rule... but really, why?  Your application already needs to
define the meaning of the data? what point is there in making your
hash commitment less uniform with the rest of the network?

@_date: 2014-05-07 17:47:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Announcing the Statoshi fork 
The tor network works based on a centralized (well, in theory,
federated) trusted directory service. (More info in
Much of data that is not related to relay related is just generated by
probes, e.g. semi-trusted bandwidth authorities that measure node
performance back to the directory authorities.
More info on their monitoring work is available here:

@_date: 2014-05-12 09:53:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Prenumbered BIP naming 
I've noticed some folks struggling to attach labels to their yet to be
numbered BIPs.
I'd recommend people call them "draft--" like draft-maxwell-coinburning in the style of pre-WG IETF

@_date: 2014-05-12 10:11:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Prenumbered BIP naming 
It isn't a big deal, but according to the process numbers shouldn't be
assigned for things that haven't even been publically discussed. If
someone wants to create specifications that are purely the product of
they own work and not a public discussion? they should feel free to do
that, but BIP isn't the process for that.  So, since things need to be
discussed, it can be useful to have something to call a proposal
before other things happen? thats all. The same kind of issue arises

@_date: 2014-05-15 10:48:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] DNS seeds unstable 
If software is using the DNS seeds in a way where one or two being
unavailable is problematic, then the software may be using them
Generally DNS seeds should only be used as fast connectivity hints,
primarily for initial connectivity. Relying on them exclusively
increases isolation vulnerabilities (e.g. because the dns seed
operators or any ISP or network attacker on the path between you and
the seeds can replace the results with ones that isolate you on a
bogus network).

@_date: 2014-05-17 09:40:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Paper Currency 
Not just scan it, but they actually must successfully sweep it?
otherwise they can be trivially double spent. This is especially bad
since any prior bearer can perform such an attack. E.g. record the
private key of everyone that passes through your hands and then
doublespend race any redemption that happens >24 hours after you spend
them. The wrong person would likely be blamed and even if you were
blamed you could plausibly deny it ("Must have been the guy that gave
it to me!").
Othercoin seems to have much better properties in the space of offline
transactions: Separately, Cassius also ran into some regulatory issues selling
physical bitcoin artifacts. Especially printing things that seem to be
redeemable for a named USD value sounds especially problematic.
Some random comments? The base58 encoding is fairly human unfriendly.
It's fine for something being copy and pasted, but I've found typing
or reading it works poorly due to mixed case.  I expect the A/B side
to be difficult to educate users about. "This side is private" is more
easily understood, you could just pick one of your sides and call it
private.  I find it kind of odd that this design seems to have no
facility for checking its txouts without recovering the private key,
though considering no one should rely on such a measurement without
sweeping perhaps thats for the best.
(As far as the numbering goes, I think you should be calling these
draft-felix-paper-currency  etc. As a matter of hygienic practice I
will not assign a matching bip number for something that went public
with a number outside of the assignment.)

@_date: 2014-05-19 11:43:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] patents... 
You have received outdated advice on this point. In Re Seagate
( this
precident was over-turned (and has subsequently been upheld in other
cases). Avoiding willfull infringement no longer requires paying off a
patent attorney to get a freedom to operate review.  This isn't to say
that reading patents is always productive now:  They're often nearly
inscrutable (especially to people without substantial patent reading
experience), and you may discover potential infringement that creates
more work for you to sort out (especially since people without patent
experience tend to read patents much more broadly than they actually
There are other defensive approaches which are interesting than hoping
to use patents as a counter attack: For one? filing a patent gets the
work entered in the only database that USPTO examiners are
_guaranteed_ to consult when doing a prior art search, so it may have
a fighting chance of precluding someone else patenting the same
material later (they may also search the internet and use other
resources, but they're guaranteed to consult the existing patents and
applications). Patents can also be used defensively as leverage in a
licensing negotiation: Without your own patents you don't get invited
to the negotiating table at all with someone else who may hold patents
in a space that you're working on.  These are somewhat thin advantages
so great care is required to make sure that things are setup so that
badness cannot happen later when inevitable changes of ownership

@_date: 2014-05-19 14:07:38
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Working on social contracts (was: Paper 
I promise that if bad people show up with a sufficient pointy gun that
I'll do whatever they tell me to do. I'll make bad proposals, submit
backdoors, and argue with querulous folks on mailing lists, diverting
them from real development and review work, all as commanded. Maybe
I'll try to sneak out a warning of some kind, maybe... but with my
life or my families or friends lives on the line? probably not.
... and I think that anyone who tells you otherwise probably just
hasn't really thought it through.  So what is the point of commitments
like that?  People change, people go crazy, people are coerced. Crap
happens, justifications are made, life goes on? or so we hope.
What matters is building infrastructure? both social and technical?
that is robust against those sorts of failures. If you're depending on
individual developers (including anonymous parties and volunteers) to
be somehow made more trustworthy by some promises on a mailing list
you've already lost.
If you care about this you could instead tell us about how much time
you promise to spend reviewing technical work to make sure such
attacks cannot be successful, regardless of their origins. Where are
your gitian signatures? I think thats a lot more meaningful, and it
also improves security for everyone involved since knowing that such
attacks can not succeeded removes the motivation for ever trying.
A lot of what Bitcoin is about, for me at least, is building systems
which are as trustless as possible? ruled by unbreakable rules
embodied in the software people chose to use out of their own free
will and understanding. Or at least thats the ideal we should try to
approximate. If we're successful the adhomenim you've thrown on this
list will be completely pointless? not because people are trusted to
not do evil but because Bitcoin users won't accept technology that
makes it possible.
So please go ahead and assume I'm constantly being evil and trying to
sneak something in... the technology and security can only be better
for it, but please leave the overt attacks at the door. Think
gentleman spies, not a street fighting death match. The rude attacks
and characterizations just turn people off and don't uncover actual
attacks.  Maybe the informal guideline should be one flame-out
personal attack per cryptosystem you break, serious bug you uncover,
or impossible problem you solve. :)

@_date: 2014-05-21 14:02:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
Just makes it easier to sort out things like your git account (or the
git site) being compromised and used to submit commits.

@_date: 2014-05-23 10:32:34
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
PGP in general is not very thoughtful about security. There are a lot
of things it does poorly. This is easily excusable considering the
historical context it came from? it was the first real cryptographic
tool I used, at the time its distribution had concerns about legality,
just getting things into people's hands was an achievement enough.
now, but there is a long way to go in figuring out how to many any
cryptographic tool usable to people.
PGP is a general purpose tool? which is the hardest kind to write? its
also used in a lot of irreversible contexts: If your key deploys a bad
software release and it steals everyone's data or wipes their disks?
thats not an irreversible action by any means.
If you want threshold pgp though? it's possible. The RSA cryptosystem
is directly compatible with threshold cryptography. It's just that no
one has written the tools. There are implementations of the bare
cryptosystem however.
One of my longer term would-be-nice goals for a upgrade bitcoin script
2.0 would be being thoughtful enough in the design that it could be
adopted as a signing cryptosystem in other applications (e.g. tools
similar to GPG)? allowing for things like creating a public key which
can only issue trust level 0 certifications, only certifications for
certain organizations (e.g. *.debian.org) unless thresholded with an
offline key, or only signing for messages meeting a certain
programmatic predicate generally.

@_date: 2014-05-24 16:16:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Cut-through propagation of blocks 
FWIW, there are a lot of improvements which can be made before more
complex changes like cut-through-forwarding that change the protocol.
For example, the reference software has a 100ms sleep in p2p message
processing which could be replaced with a semaphore, this would
dramatically lower latency for block relaying.
Likewise nodes which are becoming bandwidth overloaded could adapt
their concurrent connection counts down (and ones that are underloaded
could accept more connections).
Relaying to multiple peers could be done in parallel instead of
serialized, and the order in which peers are relayed to could be
adapted to place more apparently useful and faster peers first, e.g.
every time a peer is the first to tell you about a block or
transaction you accept they move up the list, every time their socket
send queue fills they move down.
Luke-Jr had implemented cut through behavior previously and had posted
a patch, but absent those other network processing improvements it
didn't appear to help.
If you want to go full out crazy in optimizing in this space, there
are fancier things that can be done to further reduce latency and
increase efficiency:
  ... but
some of this stuff really should be done as a seperate protocol. There
is no need to have Bitcoin transport all using a single protocol, and
we can get better robustness and feature velocity if there are a
couple protocols in use (you could just run a block-transport-protocol
daemon that connects to your local node via the classic protocol).

@_date: 2014-05-24 17:14:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Cut-through propagation of blocks 
This directly opens an attack where as soon as you find a block you
announce the header to the world and then you delay announcing the
block content.  You can continue to mine on the block but no one else
can (or alternatively they break their rule and risk extending an
invalid block? bad news for SPV wallets)? then when you find a
successor block or someone else finds a competing block you
immediately announce the content.
It basically means that you can always delay announcing a block and be
sure that doing so doesn't deprive you of your winning position.
With an alternative transport protocol, assuming the content has
already been relayed a block could be sent in a couple back to back
UDP packets.  (e.g. a few bytes per transaction to disambiguate the
transaction order out of the already sent transactions).  So I think
very similar latency could be achieved without doing any thing which
might increase the motivations for miners to misbehave.

@_date: 2014-05-25 02:51:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Cut-through propagation of blocks 
If someone wanted to implement swanky UDP non-blocking transports or
complex network coding schemes I'd probably want to see the proven in
actual use before sticking them in the reference code, so yes.
It's also the case that the ~last addition we made to the P2P code
added a remotely exploitable crash bug.
There are some pretty distinct use cases out there? fast block
relaying, supporting thin clients, minimizing bandwidth (e.g. via
compression and tx/block redundancy elimination), etc. Some of them
may not be well handled by an external gateway, some of them (e.g.
block relaying) very much could be.
The nice thing with alternative protocols and gatewaying is that it
can proceed completely asynchronously with implementation development,
e.g. revving versions as fast as the users of the protocol care, and
could potentially be used immediately with other bitcoin
implementations... and if its buggy it doesn't break the nodes using
it: I'd be much more likely to run an experimental gateway in another
process on a node than experimental p2p code inside my production
bitcoinds themselves.

@_date: 2014-05-30 09:03:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is up 
On Fri, May 30, 2014 at 8:40 AM, Andreas Schildbach
We'd used an approach like that previously and I believe it produced
fairly bad load imbalances, especially since some resolvers only pass
on a single result. And that was before there was a wide deployment of
broken client software that trusted the dnsseeds exclusively.

@_date: 2014-11-07 02:04:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] The difficulty of writing consensus 
Yes, I agree for most contract purposes CTLV is what you want to be
using, instead of refund transactions beyond being more clearly
correct, it shrinks the protocol state machine by one step.
Though BIP62 also achieves the secondary goal of making required
implementation behaviour more explicit (e.g. the parts enforced in all
transactions), and that shouldn't be discounted.
They're somewhat orthogonal, somwhat complementary things.

@_date: 2014-11-10 00:52:05
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP draft - Auxiliary Header Format 
Some initial comments...
Tying in the protocol changes is really confusing and the fact that
they seem to be required out the gates would seemingly make this much
harder to deploy.   Is there a need to do that? Why can't the p2p part
be entirely separate from the comitted data?

@_date: 2014-11-27 02:22:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Deanonymisation of clients in Bitcoin P2P 
I'm confused by this, I run quite a few nodes exclusively on tor and
chart their connectivity and have seen no such connection dropping
Can you tell me more about how you measured this?
[As an aside I agree that there are lots of things to improve here,
but the fact that users can in theory be forced off of tor via DOS
attacks is not immediately concerning to me because its a conscious
choice users would make to abandon their privacy (and the behaviour of
the system here is known and intentional). There are other mechanisms
available for people to relay their transactions than connecting
directly to the bitcoin network; so their choice isn't just abandon
privacy or don't use bitcoin at all.]

@_date: 2014-11-27 20:30:16
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Deanonymisation of clients in Bitcoin P2P 
You're mistaken. :)
If a node is used exclusively via tor it effectively doesn't have a IP address.
(short of bugs of a class that aren't discussed here)
The paper is about fingerprinting approaches that probabilistically
connect transactions to hosts that you can already identify their IPs.

@_date: 2014-11-27 23:46:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 65 and OP_CHECKLOCKTIMEVERIFY 
Updating the stack is not soft-fork compatible and any use would
immediately fork the network.
A invertible test is also not soft-fork compatible e.g. someone writes
a script that does {) OP_NOT,  in other words "the test
must fail", then the network would fork because older nodes would see
it as passing (which was the required criteria for non-forking the
network in the non-inverted caes).
You can happily get non-nullable true/false behaviour without these
risks by having the VERIFY test inside a branch and having the signer
provide its falseness as an input to the branch. This is explained in
the BIP.
E.g. OP_IF  OP_CHECKLOCKTIMEVERIFY OP_ELSE  OP_END
A useful an powerful mental model is that SCRIPT is not running a
program, but instead the signer is proving to the network that they
know inputs that make the program return true.
(In practise we verify this by actually doing some execution, though
this isn't technically necessary it's the simplest thing to implement
although it is inefficient... but even in this simple model keeping in
mind that we're VERIFYING not executing in the network opens our eyes
to transformations like the IF bracketing of a VERIFY opcode.)
They can do this, with the above approach.
Then the scripts validity isn't a pure function of the the
transaction, and once valid transactions could become invalid while in
the mempool. This breaks existing invariants and would make the coins
potentially less fungible because they wouldn't be reorg safe. That
locktime validity is basically monotonic is a useful intentional
property. :)
The things you're suggesting were all carefully designed out of the
proposal, perhaps the BIP text needs some more clarification to make
this more clear.

@_date: 2014-11-28 05:30:32
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Deanonymisation of clients in Bitcoin P2P 
Because if the user does not use tor or an analogous infrastructure
(e.g. something else reimplementing tor's functionality) the user can
be deanonymized in many different ways.
At the end of the day, if I'm listening widely to the network, and
your host is regularly the first to hand me your transactions then I
can draw reasonably reliable conclusions... and this is true even if
there is a complete absence of identifiable characteristics otherwise.
And, on the flip side if the host is persistently behind tor, even
with some watermarkable behaviour, their privacy is protected.  So
making sure that hosts can continually use tor (or similar systems)
should be the higher priority.  (And, of course, not reimplementing
tor  leverages the millions of dollars of investment and dozens of
subject matter experts working on that system).

@_date: 2014-11-28 12:03:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP 65 and OP_CHECKLOCKTIMEVERIFY 
On Fri, Nov 28, 2014 at 11:45 AM, Flavien Charlon
I used the word 'less' intentionally.   A double spend requires an
active action. Roughly 1% of blocks are lost to reorganizations by
chance, longer otherwise harmless reorgs as we've had in the past
could forever destroy large chunks of coins if descendants had the
unwelcome properties of having additional constraints on them. Past
instances where the network had a dozen block reorganization which
were harmless and simply confirmed the same transactions likely would
have caused substantial losses if it reorganizations precluded the
recovery of many transactions which were valid when placed earlier in
the chain.
Additionally your '6 confirmations' is a uniform rule. The
recommendation is just a count, it's tidy.  It's not a "traverse the
recent history of each coin you receive to determine if its script
conditions make it unusually fragile and subject to irrecoverable
loss", which is the space you can get into with layering violations
and transaction validity depending on arbitrary block data.

@_date: 2014-10-03 09:17:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
This would not be soft-forking compatible.
It also would be unsafe in that it would result in transactions which
once mined could not be restored in a reorg through no fault of the
participants, which makes the coins less fungible and differently safe
to accept. It risks creating weird pressures around immediate block
admission since a one additional block delay could forever censor such
a transaction (E.g. increases the power of single miners to censor or
steal). Avoiding this is a conscious decision in Bitcoin and also part
of the justification for the 100 block maturity of newly generated
It also would require violating the script/transaction/block layering
more substantially, complicating implementations, and making the
validity of a script no longer a deterministic pure function of the
Avoiding these issues is a conscious design in OP_CHECKLOCKTIMEVERIFY.
I would strenuously oppose a proposal which failed in any of these
You can already achieve the not spendable after logic with a
cancellation spend that moves the coin in the usual way. (Which
doesn't even require the participant be online, with the help of some
network service to queue unlocked transactions).
It is intentionally so, and yet it covers the intended use cases;
including ones with alternative key groups, they are just not

@_date: 2014-10-05 16:40:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] The Bitcoin Freeze on Transaction Attack 
I should point you to some of the tools that have been discussed in
the past which are potentially helpful here:
The first is the use of locktime on normal transactions.  This
behavior is already in Bitcoin core git:   The idea is that users of
the system should locktime their transaction at a point as high as
they expect it to get included.  If used well this means that there
should always be a base of fees which can only be collected by future
blocks, creating an incentive to move forward.  This may be
particularly effective if the limitations on blocksize mean that there
is always a healthy standing load.
The second is having block commitments in transactions
( The idea is that
the data under signature in a transaction could commit to some recent
block which _must_ be in the chain or the transaction's fee cannot be
collected (or, at least, not all of the fee).  This would allow
transacting users to 'vote with their fees' on the honest chain.
Arguably this could also be used to pay for doublespending forks, but
you can already do that by paying fees via a chain that stems from the
doublespend.  This greatly complicates strategy for forking miners,
since future transactions which you haven't even seen yet may have
fees conditional on the honest chain.
I think both of the above are obviously useful, should be done, but
don't completely address the concern, they may be adequate.
The third is fee forwarding.  An example form would be that the miner
gets half the fees, the rest are added to a pool which pays out half
in every successive block.  This can prevent unusually high fees from
making as much reorg pressure and more correctly models what people
would like to pay for: getting their txn buried.   The huge problem
with this class is that miners can demand users pay fees "out of
band", e.g. with additional txouts (just make a different version of
the tx for each miner you wish to pay) and escape the process.  I have
had some notions about fees that come in the form of adjusting the
difficulty of creating a block slightly (which is something that can't
be paid out of band), but such schemes becomes very complicated very
fast.  I am unsure if any form of fee forwarding is workable.
Something you might want to try to formalize in your analysis is the
proportion of the network which is "rational" vs
"honest"/"altruistic".  Intuitively, if there is a significant amount
of honest hashrate which is refusing to aid the greedy behavior even
at a potential loss to themselves this strategy becomes a loser even
for the purely greedy participants. It would be interesting to
characterize the income tradeoffs for different amounts of altruism,
or whatever convergence problems an attempt by altruistic
participaints to punish the forkers might create.

@_date: 2014-10-05 16:50:56
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] The Bitcoin Freeze on Transaction Attack 
Ah, I should also mention a somewhat more far out approach which helps
here as a side effect:
If transactions were using the BLS short signature scheme (a very
compact EC signature based on pairing cryptography) there is a scheme
so that you securely can aggregate the signatures from multiple
messages into a single signature (also has nice bandwidth properties)
and still verify it. It also works recursively, so aggregates can be
further aggregated.
A consequence of this is that you cannot remove a (set of)
signature(s) from the aggregate without knowing the (set of)
signature(s) by itself.
If the coinbase transaction also contains a signature and if some
non-trivial portion of fee paying users relayed their transaction
privately to miners it,  then other miners would only learn of the
transaction in aggregated form.  Without knowing the transaction by
itself they could not pull it out of a block separately from the
coinbase payment and add it to their own block in a fork.
(In general this provides several anti-censorship properties, since if
someone passed you an aggregate of several transactions you could only
accept or reject them as a group unless you knew the members
The use in aggregation can be done in a way which is purely additive
(e.g. in addition to regular DSA signatures), so even if the
cryptosystem is broken the only harm would be allowing
disaggregation... but unfortunately the pairing crypto is pretty slow
(verification takes a 0.5ms-ish pairing operation per transaction).

@_date: 2014-10-05 17:01:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] The Bitcoin Freeze on Transaction Attack 
You can imagine that instead of subsidy Bitcoin came with a initial
set of nlocktimed transactions that pay fees, one block at a time, for
each block from the start until the subsidy goes away.
Perhaps that mental model might make it clear why some people think
that the nlocked transactions and the block size being lower than the
instant offered demand (there is always a backlog) are both things
which address the concern of this thread. :)

@_date: 2014-10-07 19:16:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] The Bitcoin Freeze on Transaction Attack 
I'm not following this. Perhaps I'm getting lost in terminology here.
It's already to provide double spending bounties to greedy-rational
miners, via a simple approach that has been known since at least early
in 2011.    I pay someone then create a later fraudulent doublespend
which is nlocked at the height the original payment occurred, paying
large fees. Then I spend the output of the fraudulent spend nlocked
one block higher, and spend the output of that one again, nlocked one
block higher, and so on... each step paying fees.
A hypothetical purely greedy miner which considers all sequences of
acceptable forks and transactions would see that they have higher
expected returns assisting the theft (assuming the honest party
doesn't fight back by also adopting a similar strategy), at least if
the population of greedy miners is large relative to altruistic ones.
I don't see how miners being able to roll forward fees makes anything
worse, since the transactions themselves can also roll forward fees.

@_date: 2014-10-09 06:28:19
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
Right, ... moreover, even with all the malleability fixes, they only
work if you refrain from using certain features (e.g. cannot do an
anyone-can-pay) and we cannot be completely sure all accidental
vectors for malleability are gone (we've been unable to construct a
proof that our strengthening of ECDSA turns it into a strong
signature, though it seems likely).
Having the locktime control in a scriptPubKey sidesteps all those
limitations and simplifies protocols (e.g. not requiring some three
step state machine and a bunch of risky validation code to be sure a
refund you receive is actually workable).

@_date: 2014-10-09 06:40:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [BIP draft] CHECKLOCKTIMEVERIFY - Prevent 
Yea, no problem since we lack covenants.
Or a least no problem making an example, maybe you'll find it too
contrived since I'm not sure what would motivate it:
You and I put 5 btc each into a kickstarter-escrow to pay Alice+some
oracle that decides if alice did her job.  But if a timeout expires
before alice manages to get the sign off the funds must be returned
completely to their original payers.
Returning them to in two outputs, one to me, one to you is trivial
with a pre-signed refund.
You could make there be multiple alice outputs or refund, but then you
can't guarantee an atomic reversal (e.g. maybe Alice gets half if we

@_date: 2014-10-12 07:14:32
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Request for review/testing: headers-first 
"Much faster" is an understatement. Benchmarking here shows one hour
five minutes syncing to 295000.   Old code isn't even at 250000 after
7 hours.
(I'm using 295k as the target here because after that point ecdsa
dominates, and then your 6+x faster libsecp256k1 makes more of a

@_date: 2014-10-14 02:45:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Malleable booleans 
An argument against is that you can currently do something like this:
OP_DUP OP_IF OP_HASH160 PUSH OP_EQUALVERIFY OP_ELSE  OP_CHECKSIGVERIFY OP_ENDIF
E.g. if your input is non-zero you're giving a hash, if it's zero
you're skipping that and running another branch.
Of course you could just encode your script another way... but by that
same logic you can 1 OP_QUALVERIFY to bool-ize any input in the true
path.  The inconsistency in handling makes it more likely that script
authors will screw up with bad (for them) consequences, however.
[I just asked pieter out of band to clarify if he means "minimal
encoded size", or must be 0 or 1 minimally encoded... as the former
doesn't fix the malleability, but the later is more disruptive]

@_date: 2014-10-15 09:22:46
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP process 
This all makes a lot of sense to me, and would help a lot with the
workflow.  Unfortunately github pulls and issues really have nothing
to faciltate a multistage workflow... e.g. where something can go
through several steps.
We're also having problems with people failing to comment on things,
not even "I looked at this and have no opinion", which is really
obstructing things.

@_date: 2014-10-28 02:36:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] DS Deprecation Window 
Even with that, the miner cannot tell, his only safe option is to
include no transactions at all.
Consider a malicious miner can concurrently flood all other miners
with orthogonal double spends (which he doesn't mine himself). These
other miners will all be spending some amount of their time mining on
these transactions before realizing others consider them

@_date: 2014-10-28 20:36:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd:  death by halving 
Or very old, indeed, if you are using unsigned arithmetic. [...]
It isn't, but many people have performed planning around the current
behaviour. The current behaviour has also not shown itself to be
problematic (and we've actually experienced its largest effect already
without incident), and there are arguable benefits like encouraging
investment in mining infrastructure.
This thread is, in my opinion, a waste of time.  It's yet again
another perennial bikeshedding proposal brought up many times since at
least 2011, suggesting random changes for
non-existing(/not-yet-existing) issues.
There is a lot more complexity to the system than the subsidy schedule.

@_date: 2014-10-28 21:43:42
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Fwd: death by halving 
I am not saying that it is "not relevant", I'm saying the discussion
is pointless:
No new information has arrived since the very first times that this
has been discussed except
that the first halving passed without incident.
If people were not sufficiently convinced that this was a serious
concern before there was concrete evidence (however small) that it was
okay, then discussion is not likely going to turn out differently the
50th or 100th time it is repeated...
except, perhaps, by wearing out all the most experienced and
knowledgeable among us as we become tired of rehashing the same
discussions over and over again.
On Tue, Oct 28, 2014 at 9:23 PM, Ferdinando M. Ametrano
This is wildly at odds with reality. I don't mean to insult, but
please understand that every post you make here consumes the time of
dozens (or, hopefully, hundreds) of people. Every minute you spend
refining your post has a potential return of many minutes for the rest
of the users of the list.
At current difficulty, with a SP30 (one of the
leading-in-power-efficiency) marginal break-even is ~1144.8852 * $/kwh
== $/btc.
At $0.10/kwh each block has an expected cost right now, discounting
all one time hardware costs, close to $3000.

@_date: 2014-10-30 23:34:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Increasing regularity of block times? 
Irregularity is a required property for convergence. Imagine what
would happen in a network where a blocks were produced at an exact
interval: Almost everyone would produce one the exact same time, and
the network would fragment and because the process would continue it
would not converge. It is precisely the variance  being some huge
multiple of the network radius which allows the network to converge at
When lower variance is tolerable for convergence it can be achieved by
reducing the expectation. Maybe some other distribution can be proven
to be convergent to, it's difficult to reason about.
Bitcoin testnet implements a rule that allows lower difficulty blocks
after a delay (20 minutes, in fact), but it's a testing-toy... not
secure or intended to be so. At least one altcoin has copied that
behavior and been exploited on account of it.
If you're simply looking for faster evidence that the network is
working on a particular transaction set, at some lower timescale:,
then thats already possible.  e.g. look into how the p2pool sharechain
builds a consensus around mining work used for pooling. The same
mechanism can be used to give faster transaction selection evidence.
I'll dig up some citations for you later. Cheers.

@_date: 2014-09-01 13:48:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Small update to BIP 62 
Not related to this change but the definition of rule 4 may not be
sufficiently specific? without a definition someone could reasonably
reach a different conclusion about OP_1NEGATE being a "push
operation", or might even decide any operation which added to the
stack was a "push operation".
Any particular reason to enforce 2 and 4 but not also 5?  Violation of
5 is already non-standard and like 2,4 should be safely enforceable.
Perhaps the rules should be reordered so that the applicable to all
transactions ones are contiguous and first?
This should clarify that the scriptPubkey can still specify rules that
are inherently malleable? e.g. require the input stack contain two
pushes which OP_ADD to 11.  Or a more elaborate one? a 1 of 2 check
multisig where the pubkey not selected for signing is selected by a
push in the signature. The current text seems to ignore isomorphisms
of this type. ... they're not important for what the BIP is trying to
achieve, but the document shouldn't cause people to not think that
sort of thing exists.

@_date: 2014-09-15 16:10:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Does anyone have anything at all signed 
If the server could replace the public key, it could replace the
signature in all the same places.
Please, can this stuff move to another list? It's offtopic.

@_date: 2014-09-25 18:53:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP43 Purpose code for voting pool HD 
I've pinged some people privately but also pinging the list? no
commentary on this proposal?

@_date: 2015-04-15 03:34:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Build your own nHashType 
I'm not sure if I'm super fond of that particular non-programmatic but
many options approach; It sort of has the problem that there are
relatively few useful options that don't rapidly extend into a choose
your own adventure with too many options to count; so you take a
complexity penalty perhaps without a matching functionality payoff.
but thats not why I'm commenting...
I wonder if anyone noticed that any sighash masking that eliminates
the txin txid enables covenants?
Covenants are payments which constrain their future payments (like
deed covenants), I've written about them in the past
  ... they can
sometimes be pretty useful but are also potentially a irritating hit
to fungibility, at least if used stupidly.
the approach here is that you make the scriptpubkey contain "[push:
0x30, 0x06, 0x02, 0x01, 0x04, 0x02, 0x01, 0x04, flags] [push pubkey
resulting from pubkey recovery] OP_CHECKSIG"  and set the flags to
match only the things you want to enforce in the spending transaction
hash them up and recover the EC public point.   You can think of that
construct as giving a you a OP_MASKED_TRANSACTION_HASH_EQUALS  ... the
recovered pubkey is just a kind of message hash, though a weird and
expensive to compute one.
I don't currently see how to get a perpetual covenant out of it-- e.g.
a coin that anyone can spend, but only to its same scriptpubkey, (the
obvious way requires the ability to be able to checksig stuff on the
stack) though I wouldn't be shocked if it were possible with a
sufficiently complex sighash flag and nothing else.

@_date: 2015-04-24 20:16:57
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
This is not a free choice. There are several concerns, from mild to
severe, that arise when you do not sign enough.
In particular not covering the ID allows for transaction replay which
can result in monetary losses far more severe than any possible
mishandling of malleability could result in. Byzantine attackers can
costlessly replay your old transactions any time anyone reuses an
address, even accidentally (which cannot be easily prevented since
they can race).
Other fun effects also show up like being able to backwards compute
signatures to result in a kind of limited covenant- coins which can
only be spent a particular way which has some implications for
fungibility. (See here for a discussion in general of covenants:
There are no free lunches;  the proposal linked to there is itself a
game of wack-a-mole with assorted masking flags; many of which we have
no notion of if they're useful for any particular application(s); and
it doesn't provide tools to address the replay issue; and in order to
'improve' malleability via that mechanism you must always mask out the
inputs completely; meaning you'd always be exposed to replay and not
just in specialized 'contract' applications where "there won't be
address reuse" could be a strong assumption enforced by the

@_date: 2015-04-24 20:58:39
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Reusable payment codes 
So this requires making dust payments to a constantly reused address
in order to (ab)use the blockchain as a messaging layer.
Indeed, this is more SPV compatible; but it seems likely to me that
_in practice_ it would almost completely undermine the privacy the
idea hoped to provide; because you'd have an observable linkage to a
highly reused address.
It would also more than double the data sent for the application where
'stealth addresses' are most important: one-time anonymous donations
(in other contexts; you have two way communication between the
participants, and so you can just given them a one off address without
singling in the public network.)
So this creates strong "binding" that we would really strongly like to
avoid in the network; basically what this says is that "You can only
pay to person X if you use scheme Y for your own Bitcoins"-- who says
any of your inputs have a ECDH pubkey at all? Of if they do, who says
its one that you have access to the private key for for use in this
scheme (e.g. it could be in a HSM that only signs according to a
policy).   We should avoid creating txout features that restrict what
kind of scriptPubkey the sender can use, or otherwise we'll never be
able to deploy new signature features. (We can see how long P2SH took
to gain adoption because some wallets refused to implement sending to
it, even though doing so was trivial).
This kind of binding was intentionally designed out of the stealth
address proposal;  I think this scheme can be made to work without any
increase in size by reusing the payment code as the ephemeral public
key (or actually being substantially smaller e.g. use the shared
secret as the chain code, but I should give it more thought)
Also, SPV wallets do not need to have access to the public keys being
spent by a particular transaction they learn about; providing that
access is fundamentally expensive and pushes things back towards
This is fundamentally more expensive to compute; please don't specify
This appears incompatible with multisignature; which is unfortunate.
I do very much like the fact that this scheme establishes a shared
chain once and then doesn't need to reestablish; this was one of the
improvements I wanted for the stealth address.
I'm disappointed that there isn't any thought given to solving the
scanning privacy without forcing non-private pure-overhead messaging
transactions on heavily reused addresses. Are you aware of the IBE
approach that allows someone to request a third party scan for them
with block by block control over their delegation of scanning?

@_date: 2015-08-01 00:17:25
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Block size hard fork 
Transactions can be recieved or accepted in different orders by
different nodes. The purpose of the blockchain is to resolve any
potential conflicting transactions by providing a globally agreed
total ordering.
As soon as one of the forks accepts a different transaction in a
conflicting set then there will be transactions which exist on one
chain which cannot exist on the other.
One can quite easily transact in a way to intentionally produce such a
split to seperate the existance of your coins onto the seperate forks;
just as anyone would need to do to perform a reorg-and-respend attack
on a single blockchain.
Additionally, new coins will be issued, along with fees, on both
chains. These new outputs become spendable after 100 blocks, and any
transaction spending them can exist exclusively on one chain.
Also any transaction whos casual history extends from one of the above
cases can exist only on one chain. This also means that someone who
has single-chain coins (via a conflict or from coinbase outputs) can
pay small amount to many users to get their wallets to consume them
and make more of the transactions single chain only-- if they wanted
the process to happen faster.
The migration remark is a considerable oversimplification. Imagine if
I released a version of the software programmed to reassign ownership
of a million of the earliest created unmoved coins to me at block
400k, and then after that I made transaction to pay 5 coin/block in
fees. Would miners move to this chain?  It pays more in fees!

@_date: 2015-08-05 20:16:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
Your understanding is outdated.
The relay network includes an optimized transmission protocol which
enables sending the "entire" block typically in just a smal number of
bytes (much smaller than the summaries you suggest, which still leave
the participants needing to send the block).
E.g. block 000ce90846 was 999950 bytes and the relay network protocol
sent it using at most 4906 bytes.
No trust is required in this scheme because the entire block is
communicated using only a couple packets.
The current scheme is highly simplified and its efficiency could be
increased greatly with small improvements, or if miners created blocks
in an aware manner.... but with a maximum size blocks turning into 5kb
with the current setup, there hardly appears to be a reason to do so
right now.
Ultimately there is no need for information communicated with a block
at discovery time proportional to the size of the block; with the
right affordances it can be accomplished with a small constant amount
of data.
If not for this already being deployed I personally believe the
network would have already fallen into complete centeralization as a
response to larger blocks: this was constructed and deployed in order
to pull the network back from having a single pool with more than half
the hashrate.

@_date: 2015-08-05 22:14:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
Correct. Bitcoin Core has cached validation for many years now... if
not for that and other optimizations, things would be really broken
right now. :)
I'm also mystified by a lot of the large block discussion, much of it
is completely divorced from the technology as deployed; much less what
we-- in industry-- know to be possible. I don't blame you or anyone in
particular on this; it's a new area and we don't yet know what we need
to know to know what we need to know; or to the extent that we do it
hasn't had time to get effectively communicated.
The technical/security implications of larger blocks are related to
other things than propagation time, if you assume people are using the
available efficient relay protocol (or better).
SPV mining is a bit of a misnomer (If I coined the term, I'm sorry).
What these parties are actually doing is blinding mining on top of
other pools' stratum work. You can think of it as sub-pooling with
hopping onto whatever pool has the highest block (I'll call it VFSSP
in this post-- validation free stratum subpooling).  It's very easy to
implement, and there are other considerations.
It was initially deployed at a time when a single pool in Europe has
amassed more than half of the hashrate. This pool had propagation
problems and a very high orphan rate, it may have (perhaps
unintentionally) been performing a selfish mining attack; mining off
their stratum work was an easy fix which massively cut down the orphan
rates for anyone who did it.  This was before the relay network
protocol existed (the fact that all the hashpower was consolidating on
a single pool was a major motivation for creating it).
VFSSP also cuts through a number of practical issues miners have had:
Miners that run their own bitcoin nodes in far away colocation
(>100ms) due to local bandwidth or connectivity issues (censored
internet); relay network hubs not being anywhere near by due to
strange internet routing (e.g. japan to china going via the US for ...
reasons...); the CreateNewBlock() function being very slow and
unoptimized, etc.   There are many other things like this-- and VFSSP
avoids them causing delays even when you don't understand them or know
about them. So even when they're easily fixed the VFSSP is a more
general workaround.
Mining operations are also usually operated in a largely fire and
forget manner. There is a long history in (esp pooled) mining where
someone sets up an operation and then hardly maintains it after the
fact... so some of the use of VFSSP appears to just be inertia-- we
have better solutions now, but they they work to deploy and changing
things involves risk (which is heightened by a lack of good
monitoring-- participants learn they are too latent by observing
orphaned blocks at a cost of 25 BTC each).
One of the frustrating things about incentives in this space is that
bad outcomes are possible even when they're not necessary. E.g. if a
miner can lower their orphan rate by deploying a new protocol (or
simply fixing some faulty hardware in their infrastructure, like
Bitcoin nodes running on cheap VPSes with remote storage)  OR they can
lower their orphan rate by pointing their hashpower at a free
centeralized pool, they're likely to do the latter because it takes
less effort.

@_date: 2015-08-06 18:42:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
On Thu, Aug 6, 2015 at 6:17 PM, Tom Harding via bitcoin-dev
It already validates block version numbers.
It only relays valid transactions.
Although, the block relaying itself is explicitly "unvalidated" and
the software client can only usefully be used with a mempool
maintaining full node (otherwise it doesn't provide much value,
because the node must wait to validate the things). ... but that
doesn't actually mean no validation at all is performed, many
stateless checks are performed.
On Thu, Aug 6, 2015 at 5:16 PM, Sergio Demian Lerner via bitcoin-dev
I don't know if Matt has an extensive writeup. But the basic
optimization it performs is trivial.  I wouldn't call it compression,
though it does have some analog to RTP "header compression".
All it does is relay transactions verified by a local node and keeps a
FIFO of the relayed transactions in both directions, which is
synchronous on each side.
When a block is recieved on either side, it replaces transactions with
their indexes in the FIFO and relays it along. Transactions not in the
fifo are escaped and sent whole. On the other side the block is
reconstructed using the stored data and handed to the node (where the
preforwarded transactions would have also been pre-validated).
There is some more than basic elaboration for resource management
(e.g. multiple queues for different transaction sizes)-- and more
recently using block templates to learn transaction priority be a bit
more immune to spam attacks, but its fairly simple.
Much better could be done about intelligently managing the queues or
efficiently transmitting the membership sets, etc.  It's just
basically the simplest thing that isn't completely stupid.

@_date: 2015-08-10 21:45:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] trust 
I think you are drawing a disction that does not exist; rather it's a
disagreement about terminology.
If a system exists that provides high confidence of faithful behavior
even from malicious parties, then it's one this community would call
"trustless" (assuming the security properties are strong enough).
The result is a system where it's possible to trust everyone.
Trust in this case has multiple meanings.   In one meaning trust is an
(well founded) expectation of faithful behavior.  In another it is a
blind reliance.   When "trust everyone" is used, it's speaking to the
first definition, when the trustless definition is used it's referring
to the second defintion-- without blind faith.
A trustless (second def) system allows its users to trust (first def)
everyone, even the inherently untrustworthy (second def).  In doing
so, the considerable cost and inequality created by the maintaince of
trust (second definition) relations is mitigated, and the availablity
of faithful performance increased.  Doing so is a prerequsite to
having a strongly decenteralized system, because otherwise trust
requiremets drive the enviroment towards natural monopolies (as it's
cheaper and more effective for more people to trust (second def) a
smaller number of parties.
Less philosophically, if you're willing to have systems defined by
trust (first definition) (e.g. you do not believe that what I descibed
above coveys value, or hope that witl a small number of very trusted
parties external factors will transform blind faith into a rational
expectation of faithful performance) then there are _much_ more
technically superior ways to structure a system than Bitcoin does
today-- ones that acheive much greater performance, flexibility,
reliablity, and better security (ignoring any insecurity that arises
as a result of the trusted parties strong assumptions).
If one wants to layer trust based things on top of a trust mitigating
framework, they can do so-- and enjoy efficiencies from doing so.
Doing the converse doesn't appear really possible.

@_date: 2015-08-13 19:20:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I think it's important to allow some time for discussion with the
actual proposed text up; as understandings can shift significantly. :)
Btcdrak already asked me for numbers prior to posting text at all and
I asked him to post text...

@_date: 2015-08-17 17:18:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
On Mon, Aug 17, 2015 at 11:51 AM, Oliver Egginger via bitcoin-dev
You seem to be assuming that there is specific reason to believe the
message is unauthentic.  This is not the case.
Contrary to other poster's claims, if the message had been PGP signed
that might, in fact, have arguably been weak evidence that it was
unauthentic: no message from the system's creator that I (or
apparently anyone) was aware of was ever signed with that key.
The headers on the message check out.  The mail server in question is
also not an open relay.  At the moment the only reason I have to doubt
the authenticity of it is merely the fact that it exists after so much
air silence, but that isn't especially strong.
In the presence of doubt, it's better to take it just for its content.
And on that front it is more on-topic, civil, and productively
directed than a substantial fraction of new messages on the list.  I
certainly do not see a reason to hide it.
A focus on the content is especially relevant because one of the core
messages in the content is a request to eschew arguments from
authority; which is perhaps the greatest challenge here: How can the
founder of a system speak up to ask people to reject that kind of
argument without implicitly endorsing that approach through their own
This whole tangest is a waste of time.  If you believe the message is
unauthentic or not the best response is the same as if it is
authentic. Focus on the content. If its worth responding to, do. If
it's not don't. Then move on with life.

@_date: 2015-08-17 19:28:01
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
One can mine without running a node, unfortunately, thats where the
comments about pooled mining come from.
Also, this distionction between full nodes that "Validate" and
(presumably) SPV wallets that don't validate isn't consistent with the
design of Bitcoin.
Thats been suggested, though scalablity reasons make this hard: in the
P2Pool design there is a substantial tradeoff in variance reduction vs
communicatoin costs.

@_date: 2015-08-18 05:16:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
On Mon, Aug 17, 2015 at 8:37 PM, Oliver Egginger via bitcoin-dev
I actually learned something important and infulential in my thinking
from the post. So I am happy it happened regardless of the other
things around it.  Because of the _very_ poor SNR on the list right
now I'm not sure if I would have seen it if it were sent by JoeBob.
(This is a greater issue, and I'm not suggesting that people start
posting with fake identities to get over the noise floor... but I'm
just presenting the facts of it as I see them here).
The rest of the traffic, not so useful, thank heavens for threaded
mail user agents.

@_date: 2015-08-19 01:48:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin XTs Tor IP blacklist downloading system 
On Wed, Aug 19, 2015 at 1:36 AM, Peter Todd via bitcoin-dev
It's not a bug, it's a feature: These concerns and others were
specifically called out when we rejected this submission to Bitcoin
Core in favor of a more generic approach that lacks the privacy
problems and avoids being explicitly punitave to the use of Tor.
At least it's not a full on block as soon a the node fills for the
first time like the first implementation.

@_date: 2015-08-21 04:45:23
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Core Devs : can you share your thoughts about all 
I think this is a bit well, sad, at the moment--  a basic principle in
sound decision making is that one should try to withhold judgement
until after the analysis and options are laid out to avoid prematurely
laying down "battle lines" which then they're socially and politically
committed to a particular answer.
There are several other BIPs in the works right now that aren't out
there yet, as well (as presumably) new insight from the workshop. It
would be a shame if these things would be for naught because of being
decided prematurely.

@_date: 2015-08-24 01:01:26
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
On Mon, Aug 24, 2015 at 12:25 AM, Tom Harding via bitcoin-dev
I can't follow this logic. Can you help?  The existing semantics, to
the extent that they exist at all is that the earliest version starts
with the lowest sequence number then counts up (and if it makes its
way to the highest number, the result is final-- because it could go
no higher).
Thats the semantics 'the inversion' accomplishes for CSV: the that the
first version of a transaction begins with a smaller number which
successful versions increase, and the highest possible number is final
(no delay, because no delay is the shortest delay).
Seperately, to Mark and Btcdrank: Adding an extra wrinkel to the
discussion has any thought been given to represent one block with more
than one increment?  This would leave additional space for future
signaling, or allow, for example, higher resolution numbers for a
sharechain commitement.

@_date: 2015-08-30 03:25:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
Unfortunately, your work extensive as it was made at least two
non-disclosed or poorly-disclosed simplifying assumptions and a significant
system understanding error which, I believe, undermined it completely.
In short these were:
* You assume miners do not have the ability to change their level
 -- In fact they do, not just in theory but in pratice have responded
    to orphaning this way in the past; and it is one of the major
    concerns in this space.
* You assume the supply of bitcoin is infinite (that subsidy never
 -- It declines geometrically, and must if the 21m limit is to be upheld.
    (Though I think this is not equally important as the other concerns)
* You argue, incorrectly, that amount of information which must be
transmitted at the moment a block is discovered is proportional to the
block's size.
 -- Instead the same information can be transmitted _in advance_, as
 has been previously proposed, and various techniques can make doing
 so arbitrarily efficient.
[I would encourage anyone who is interested to read the posted off-list
I contacted you in private as a courtesy in the hope that it would be
a more productive pathway to improving our collective understanding; as well
as a courtesy to the readers of the list in consideration of traffic levels.
In one sense, this was a success: Our conversation concluded with you
enumerating a series of corrective actions that you would take:
To the best of my knowledge, you've taken none of these corrective
actions in the nearly month that has passed.  I certainly understand being
backlogged, but you've also continued to make public comments about your
work seemingly (to me) in contradiction with the above corrective actions.
Today I discovered that another author spent their time extending your
work and wasn't made aware of these points.  A result was that the other
author's work may require significant revisions.
I complained about this to you, again privately, and your (apparent)
response was to post to that thread stating that there was a
details-unspecified academic disagreement with me and "I look forward
to a white paper demonstrating otherwise!", in direct contradiction to
your remarks to me three weeks ago in private correspondence: In private
you said that your model may only hold in an asymptotic sense and that
"the phenomena in the paper (may) be negligible compared to the dynamics
from some other effect"; but in public you said /my/ position was
At this point I thought continuing to withhold this information from
the other author was unreasonable and no longer justified by courtesy
to you, and I forwarded our complete discussion to the other author
with the comment "I'll forward you a set of private correspondence
that I've had with Peter R.  I would recommend you take the time to
read it and consider it.".
I apologize if in doing so I've breached your confidence, none of the
material we discussed was a sort that I would have ordinarily considered
private, and you had already talked about making the product of our
communication published as part of your corrective actions.
I do not think it would be reasonable to demand that I spent time
repeating the same discussion again with the other author, or deprive
them of your side of it and/or the corrective actions which you had
said you would take but have not yet taken.
(In fact, I would argue that you were already obligated to share at least the
substance of the discussion  with the other author, both as part of your
commitment to me as well it being necessiary for intellectual progress.)
As you say, 'we are all here trying to learn about this new amazing
thing called Bitcoin'; and I'm not embarrassed to error towards doing
that and aiding others in doing so, but at the same time I am sorry
to have done so in a way which caused you some injury.
In any case, your prior proposed corrective actions seemed sufficient to me.
It surprises me, greatly, that you are failing to see the extreme
practicality of what I described to you, and that it does not meaningfully
diminish miner control of transaction selection-- but even without it your
remark that the proportionality could be arbitrarily small (Rather than
0, as I argue) is an adequate correction to your work, in my view.
I believe my time would be better spent actually _implementing_ improved
relaying described (and describe what was implemented) than continue
a purely academic debate with you over the (IMO) inconsequential difference
between epsilon and zero.

@_date: 2015-08-30 04:57:36
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
It has tremdous significance to the real-world impact of your results.
If not for the other errors in your work, this point would make the
take away of your work not "a healthy transaction fee market exists
without a block size limit" but rather "a decenteralized bitcoin
cannot exist"-- as, accepting the other errors as fact your model
shows that centralizing mining is always strictly more profitable at
any level of fee demand; because your model equivilent shows that for
any level of fee demand and gamma miners could increase their income
by centeralizing further.
I absolutely agree that simplifications are useful and essential, but
it is critically important to call them out before someone mistakes
theoretical work as useful motivation for policy in the non-simplified
This assumption is unreasonable, and does not-- in fact-- accurately
reflect the situation today.
For example it does not reflect how hashers return work to pools
_today_ (and since 2011) as they so to only by referencing the merkel
root... the pool already knows the  transaction set. In that
particular case it knows it because it selected it to begin with, but
the same behavior holds if the hasher selects the transaction set and
sends it first.
It only _very_ weakly reflects how the relay protocol works (only the
selection and permutation is communicated; not the transaction data
itself; for already known transactions). Even if you assume nothing
more than that (in spite of the existing reality) you have not shown
that the compressed data must be linear in the size of the block.
It does not reflect how P2Pool works (which also sends the
transactions in advance).
There is a simple, and intuitive understanding that does not require
any complex supposition:  You argue that the information must be
transfered when a block is found, thus delaying it.  I point out, no,
any required information (to the extent that there is any at all after
efficient encoding) can be sent in advance of the block.
I believe that you've allowed the fact that the specifc example block
relay protocol doesn't bother sending _all_ information in advance to
confuse it for mere compression.
I believe my reponses are firmly grounded in the physical reality of
actually deployed systems and constructable protocols.
By comparison, even if I were to agree that the bound is not actually
exactly 0 proporionality  you have already agreed that with
"compression" the amount sent could be arbritarily low. The result
being that the behavior you're describing would only be asymptoic and
have no relationship to the actual Bitcoin system that exists in a
finite universe.
But you continue to demand debate over this meaningless point.
Pratically every block today is mined under a protocol which does not
need to communicate anything but constant data when a block is found.
I am getting a little tired of people suggesting things which are
widely deployed are not physically possible.
Yes, that particular example is not the most powerful form of that
idea-- but it has the benefit of _universal_ use.
I find this a little amusing. Even in this messsage you defend
ignoring of centeralization considerations in your paper. But here ask
that I address concerns which you refused to suggest. Why do you
demand my correction use weaker assumptions than your work?
That said-- I already gave you a fairly concrete description of a
pratical protocol which would accomplish your requirement ( (a) reduce
the information transmitted in a latency critical way at block
discovery time to O(1) network wide, (b) without creating
centeralization in chain or transaction selection). I believe my
description in the messages was adequate that anyone working on
Bitcoin Core could go implement a version of it, at least.  You appear
to have simply ignrored it.
If the actual technical details made your eyes glaze over I also
offered a simple intutive understanding:  The no proportional
information need be transfered at the latency critical time, because
it can be transfered in _advance_. Everything beyond that is just
efficiency optimizations to reduce the cost of doing the advanced
That it already uses advanced information techniques in every widely
used mining protocol, that the relay protocol was rapidly adopted, and
that your own model would suggest significant profits from any such
improvement would seem to remove any doubt related to (c)... and again
here you hold me to a higher bar than your own work: as nowhere do you
show that the "limits" your model erroniously extracts are plausable
as limits, and in private you seemed to admit to me that they may well
not be.

@_date: 2015-02-01 20:41:35
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin Core 0.9.4 not on bitcoin.org? 
Because Binaries on Bitcoin.org have always been unaffected by the
issue 0.9.4 was released to address.
The changes were tried first by many people as part of git master.
This was narrow bug fix backport required to prevent the software from
no longer working at all for people who compiled it themselves and
received software updates for their system.

@_date: 2015-02-06 18:21:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] determining change addresses using the 
Sending someone too much can really play hell with their accounting.
Unless you know they're okay with it, I wouldn't suggest it.
I often randomly round up the output when I'm paying to a depository
account... and I've thought that would be a useful feature to have,
but I dunno how to usefully present a UI for "pay at least X but
you're allowed to round-up up to 0.01 BTC more".
Another strategy is this: create two change outputs, with uniform
probablity make one equal to your payment amount (which is also nice
because if your payment amount models future payment amount the change
will be usefully sized) or split your change evenly.

@_date: 2015-02-12 19:49:29
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
How many thousands of BTC must be stolen by miners before you'd agree
that it has, in fact, happened?
I just wanted to pull this out and say that I agree with this
completely; to the point where I'm continually surprised to see people
expressing other views (but they do).
I don't have much opinion about replace-by-fee; It has pluses and
minuses. In the past I've considered it a "oh perhaps best to not talk
about that" idea. I think making zero conf actively less secure would
be generally regrettable, though it might make building alternatives
for fast and acceptably safe transactions more attractive sooner. I do
favor a version of replace by fee that adds the extra constraint that
all prior outputs must be paid equal or more; which would capture many
of the 'opps paid too little' without opening up the malicious double
spends quite as much (so soon).
One challenge is that without rather smart child-pays-for-parent logic
the positive argument for replace by fee doesn't really work.
As a point for historical accuracy: PPC was actively attacked with
stake grinding and had to use developer signed blocks to prevent the
attacker from mining all the blocks and then later made a hard fork to
make it harder, and retains the developer block signing to stop it.
This doesn't contradict your point, which I agree with: an absence of
attacks doesn't mean an absence of vulnerability, and people counting
on things that they wouldn't if they understood them better is
something to avoid. And the prior point about game theory is one I
think some people have a hard time with: partipants are looking out
for their own interests, not some global optimum.  It may not be the
case that everyone (or even anyone) is maximally short sighted; but
it's even more unreasonable to assume that no one will ever break rank
and do something selfish.
I don't know that RBF even needs to be debated on these terms, since
there is an argument for RBF as good even if we assume miners are all
fully protocol conforming.

@_date: 2015-02-20 17:50:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] bloom filtering, privacy 
This approach needs a filter set with a lower FP rate. It doesn't
depend on having a high FP rate for privacy (which is good, since
counting on filter false positives seems to more or less fail to
deliver actual privacy in any case.)
Larger filters mean a somewhat higher baseline bandwidth, though when
users do not reuse addresses and have more addresses than there are
txouts in the block the gap is narrower.
This is talking about a committed bloom filter. Not a committed UTXO set.

@_date: 2015-02-20 18:20:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] bloom filtering, privacy 
Use of the words "UTXO set" here is probably confusing people as it's
likely to make people think of the complete verification state. In
this case it's simply referring to block-local data. (and thus avoids
the large IO overheads of an actual UTXO set).
It's a straight forward idea: there is a scriptpubkey bitmap per block
which is committed. Users can request the map, and be SPV confident
that they received a faithful one. If there are hits, they can request
the block and be confident there was no censoring.
It's possible to tree structure additional layers to the bitmap, so
one can incrementally query to trade0off map size for false request
overhead, it's not clear to me how much of a win this would be for
normal parameters.. It's also possible to extract the txout list for
the whole block and commit to that too so it's possible to request
just the outputs and get a faithful copy of them, which is _much_
smaller than the block overall.

@_date: 2015-02-25 18:47:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BCF 2012 Miner vote 
It would appear that the Bitcoin Foundation has decided that their
next two seats would be decided by miners.   (More information
available at: Unfortunately, they seem to have not provided the software needed to
I've taken Luke DashJr's somewhat notorious IsNotorious patch, which
he's used previously to block things like the Horse Stapler Battery
dust-spam attacks and re-purposed it so miners can avoid casting votes
in the election that they don't intend to cast.
Not really an ideal fit, but the code has the benefit of having been
run in production for some time; a necessity for deployment on short
A patch (against git master, but should apply to 0.10 cleanly at least
and probably other versions) is available at:
Let me know if you have any trouble applying it, I'll be glad to do my
civic duty and do what I can to help people participate with the
system was clearly intended by the design.
[Please note that I am relying on some claims from reddit for some of
the candidate addresses (
) because the official voting software is more or less completely
busted for me and I can only see some of the candidates. If any are
wrong, please let me know.]

@_date: 2015-01-04 17:04:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Re-enabling simple tx replacement 
Unless there is a serious bug that I am not aware of this is not the
case. The unlocked transaction is not relayable and will not be
mempooled (well, until right before it locks).
This appears to have absolutely no protection against denial of
service, it seems to me that a single user can rapidly update their
transaction and exhaust the relay bandwidth of the entire network.

@_date: 2015-01-04 17:35:37
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Re-enabling simple tx replacement 
Can you send me the actual raw transaction (that site doesn't appear
have a way to get it, only some cooked json output; which doesn't
include the sequence number).
As I said, it's a severe bug if unlocked transactions are being
relayed or mempooled far in advance.
Ah I missed that the replacement had to be final. Thats indeed a much
more sane thing to do than I was thinking (sorry for some reason I saw
the +1 and thought it was just checking the sequence number was
If they can relay the first one to begin with its an an issue, the
replacement just makes it twice an issue. :)

@_date: 2015-01-04 17:44:59
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Re-enabling simple tx replacement 
Nevermind, I guess. I think I figured out your problem: The behaviour
on testnet is busted because the non-mempooling is enforced by
IsStandardTx which is bypassed in testnet. We should enforce that
This isn't the case on the real network.

@_date: 2015-01-04 18:31:12
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Re-enabling simple tx replacement 
Thanks for presenting your solution as code in any case. In spite of
the fact that I gave it a crappy read this time, it really is a useful
way to communicate and I wish more people did that.

@_date: 2015-01-09 13:26:14
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bi-directional micropayment channels with 
I don't agree with your understanding.  Expecting replacement to work
and be enforced is completely unsafe. People (sanely) refuse to use
protocols which are broken by refund malleability, which is a much
narrower expectation for miners than expecting the sequence ratchet to
go one way.

@_date: 2015-01-09 14:50:09
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bi-directional micropayment channels with 
This being unsafe doesn't require "a lot" though, if 1% of the
hashpower is naughty, an attacker will have a 1% success rate. Naughty
can also just mean broken in various ways, like mining while somewhat
partitioned (didn't hear the update) potentially due to a DOS attack,
or because of some garbage collection policy made it forget the
transaction to conserve resources.  An unkind user can simply run
software that automatically attempts (by sending naughty miners an
earlier conflict right before the locktime expires).  "Use Blue
Rewards wallet for 2% cash back for all the Bitcoin purchases you make
online!" :P
Of course, all the miners who don't play along will very much see how
much income they're missing.
Sadly, they cannot.  This is why I specifically mentioned child pays for parent.
In any case,  sometimes a 1% fault rate is acceptable. But generally
for cases that they are, even weaker constructs (e.g. no payment
channel at all, just accept an IOU) are also often acceptable, and
cannot be modulated in their success by resource starvation attacks on
the network.
We have objective proof of substantial miners behaving maliciously,
that much isn't a speculative concern.
The school of thought view is a bit too black and white. My
perspective is that absolute soundness is best (rules which cannot be
broken at all), followed by cryptographic soundness (rules that
breaking requires P=NP, theft of a secret, or insane luck), followed
by economic soundness (rules that cannot be profitably broken),
followed by honesty soundness (rules that hold when the participants
follow the rules and aren't faulty).  We should try to move up that
stack as far towards absolutely soundness as possible; and be
increasingly cautious about compromises as we move down it espeically
because the last two are unstable and difficult to reason about
because they strongly import the vulgarities of humanity into the
security model.   If we could make the whole system absolutely sound
or cryptographically sound, I would think we should (and would) even
if it implied other compromises. But we can't and so users of Bitcoin
must navigate this risk stack.
One thing that I think you miss in this argument is that one man's
integrity is another man's malice.  The history of security and
privacy is filled with instances where someone's trust was violated
because there someone was, rightly or wrongly, convinced that Some
Reason was Good Enough to justify it. Because of this a risk analysis
has to import the clarity of judgement, morality, coerceability,
personal values, etc. of everyone in the trust chain; and many of
these things are unknowable; this greatly increases the costs of
transacting, and the efforts to mitigate those costs (and the failures
to remove the harms) result in an unequitable enviroment where some
people get unjust rewards and unequal access to justice. The gain from
cryptographic tools is being able to make some level of stronger
assurances which cut out most of that trust, they're predictable,
'cheap' on a marginal basis, and fair in a fundamental sense (in
theory everyone has equal access to math).  So, while I could even buy
the argument that miners will never believe themselves to be "actually
malicious", history shows that people's ability to convince themselves
of the justification of something is basically unbounded, even
outright thieves often believe they're owed their spoils-- and there
are a lot of ways to misbehave in Bitcoin that stop short of theft.
And so, where we cannot have cryptographic security enforce the rules,
we-- those who use and depend on Bitcoin-- _generally_ ought to behave
in ways that cannot be harmed by a failure to follow the rules so that
we don't _invite_ failures to follow the rules and thereby create an
institution of it.
Of course, all things equal I don't want to choose for other people
what tools they can use and what risks they take. But in the case of
relaying locked transactions this isn't an otherwise neutral choice: A
straight forward "relay and store any locked spend" policy has
unbounded space and communications complexity.  It's not clear to me
that if any real degree of "you can take your risks, it'll probably
work, but maybe not" can be supported without a very large resource
cost to the network, and without creating incentives to DOS attack the
network (e.g. to make it forget previous spends).  It may be that
there is some set of constraints that actually do make it workable and
don't create the incentives though... meaning that it may _merely_ be
unsafe for people who choose to use it. If so, then it might be
reasonable but we also cannot ignore the incentives it creates in a
wider ecosystem and what their ultimate conclusion might be. E.g. If
you put a bounty for miners to behave 'wrong' in a way the system
cannot prevent, some will. Is the next step to try to say that only
"good" miners can mine?   If so, how many more steps until every
transaction is being tested against a set of system external goodness
criteria?  In that state, is Bitcoin any better than a very
computationally and bandwidth inefficient version of Paypal?
Slipper slope arguments can be a bit slippery. I don't have any clear
answers. I do know that ignoring the risks we know about isn't a good

@_date: 2015-01-10 04:26:23
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] OpenSSL 1.0.0p / 1.0.1k incompatible, 
OpenSSL 1.0.0p / 1.0.1k was recently released and is being
pushed out by various operating system maintainers.  My review
determined that this update is incompatible with the Bitcoin
system and could lead to consensus forks.
Bitcoin Core released binaries from Bitcoin.org are unaffected,
as are any built with the gitian deterministic build system.
If you are running third-party or self-compiled Bitcoin Core
or an alternative implementation using OpenSSL you must not
update OpenSSL or must run a Bitcoin software containing a
(versions of this will be backported to other stable branches soon)
The tests included with Bitcoin Core in the test_bitcoin
utility already detect this condition and fail.  (_Do not ignore or
disable the tests in order to run or distribute software
which fails_)
The incompatibility is due to the OpenSSL update changing the
behavior of ECDSA validation to reject any signature which is
not encoded in a very rigid manner.  This was a result of
OpenSSL's change for CVE-2014-8275 "Certificate fingerprints
can be modified".
While for most applications it is generally acceptable to eagerly
reject some signatures, Bitcoin is a consensus system where all
participants must generally agree on the exact validity or
invalidity of the input data.  In a sense, consistency is more
important than "correctness".
As a result, an uncontrolled 'fix' can constitute a security
vulnerability for the Bitcoin system.  The Bitcoin Core developers
have been aware of this class of risk for a long time and have
taken measures to mitigate it generally; e.g., shipping static
binaries, internalizing the Leveldb library... etc.
It was somewhat surprising, however, to see this kind of change show
up as a "low" priority fix in a security update and pushed out live
onto large numbers of systems within hours.
We were specifically aware of potential hard-forks due to signature
encoding handling and had been hoping to close them via BIP62 in 0.10.
BIP62's purpose is to improve transaction malleability handling and
as a side effect rigidly defines the encoding for signatures, but the
overall scope of BIP62 has made it take longer than we'd like to
(Coincidentally, I wrote about this concern and our unique demands on
 cryptographic software as part of a comment on Reddit shortly before
 discovering that part of this OpenSSL update was actually
 incompatible with Bitcoin:
 The patches above, however, only fix one symptom of the general
problem: relying on software not designed or distributed for
consensus use (in particular OpenSSL) for consensus-normative
behavior.  Therefore, as an incremental improvement, I propose
a targeted soft-fork to enforce strict DER compliance soon,
utilizing a subset of BIP62.
Adding a blockchain rule for strict DER will reduce the risk of
consensus inconsistencies from alternative implementations of
signature parsing or signature verification, simplify BIP62,
and better isolate the cryptographic validation code from the
consensus algorithm. A failure to do so will likely leave us
in this situation, or possibly worse, again in the future.
The relevant incompatible transactions are already non-standard on
the network since 0.8.0's release in February 2013, although there
was seemingly a single miner still mining incompatible transactions.
That miner has been contacted and has fixed their software, so a
soft-fork with no chain forking should be possible.

@_date: 2015-01-23 16:05:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
I think this is unreasonable. There is a straight-forward soft-fork
approach which is safe (e.g. no risk of invalidating existing
transactions). Yes, it means that you need to use newly created
addresses to get coins that use the new signature type... but thats
only the case for people who want the new capability. This is
massively preferable to expecting _every_ _other_ user of the system
(including miners, full nodes, etc.) to replace their software with an
incompatible new version just to accommodate your transactions, for
which they may care nothing about and which would otherwise not have
any urgent need to change.
I've expected this need to be addressed simply as a side effect of a
new, more efficient, checksig operator which some people have been
working on and off on but which has taken a backseat to other more
urgent issues.
Can you help me understand whats taking 40 minutes here? Thats a
surprisingly high number, and so I'm wondering if I'm not missing
something there.

@_date: 2015-01-23 16:52:28
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
Sure; will aggregate up the citations when I'm not travling later today.
I'm not sure where the ^2 is coming from.  So what I'd understand that
you'd do is stream in the input txid:vouts which you spend, then you'd
stream the actual inputs which would just be hashed and value
extracted (but no other verification), and you'd build a table of
txid:vout->value, then the actual transaction to be signed.
This should have O(inputs) hashing and communications overhead. Is
there a step I'm missing?

@_date: 2015-01-23 18:51:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] SIGHASH_WITHINPUTVALUE 
I'm quite familiar with embedded development :), and indeed trezor MCU
is what I would generally consider (over-)powered which is why I was
somewhat surprised by the numbers; I'm certainly not expecting you to
perform dynamic allocation... but wasn't clear on how 40 minutes and
was I just trying to understand. Using a table to avoid retransmitting
reused transactions is just an optimization and can be done in
constant memory (e.g. falling back to retransmission if filled).
So what I'm understanding now is that you stream the transaction along
with its inputs interleaved in order to reduce the memory requirement
to two midstates and a value accumulator; requiring resending the
transaction... so in the worst case transaction (since you can't get
in more than about 800 inputs at the maximum transaction size) each
input spending from (one or more, since even one would be repeated)
100kb input transactions you might send about 800MBytes of data, which
could take a half an hour if hashing runs at 45KB/s or slower?
(If so, okay then there isn't another thing that I was missing).

@_date: 2015-01-25 14:48:10
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Initially I was of the opinion that we couldn't do that, because
soft-forks which hit transactions many nodes would relay+mine creates
a forking risk... but with the realization that imbalanced R/S plus
checksig-not would only be work with 0.10rc/git changed my mind.
Unlike two years ago miners no longer appear to be racing the bleeding
edge, and it's never show up in a release. Obviously the next RC would
also make those non-standard. And then we'll have some non-trivial
amount of time before the soft-fork activates for whatever stragglers
there are on 0.10 prerelease code to update. The deployment of the
soft-fork rules themselves will already drive people to update.
In terms of being robust to implementation differences, not permitting
overlarge R/S is obviously prudent.
So I think we should just go ahead with R/S length upper bounds as
both IsStandard and in STRICTDER.

@_date: 2015-01-26 18:35:47
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Sure. BIP0066. There was also some feedback on Bitcointalk, which I
think you've addressed:
 I also had off-list
positive feedback from Amir Taak, so we have positive feedback from
several implementers.
One of the points that was raised which we'd discussed pre-proposal
that was brought up there that I thought I should summarize here was
the possibility that someone had previously authored an nlocked spend
with an invalidly encoded signature. In those cases the signature can
just be mutated to get it mined, and would need to be already to pass
IsStandard rules. A case that isn't covered if if they have a chain of
transactions after that nlocked transaction, but those cases would
already be at extreme risk of malleability (esp since their unchanged
form is non-standard), and that coupled with the fact that avoiding
this would undermine the intent of the BIP (independence from  a
specific encoding scheme) seems to have been convincing as much.

@_date: 2015-07-04 03:17:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fork of invalid blocks due to BIP66 violations 
A near majority of the hashrate on the network appears to be SPV mining.
Btcnuggets was a non-upgraded miner that produced an invalid block
after the lock in and f2pool and antpool have been extending it.
Fortunately their extension contains no transactions (an artifact of
SPV mining).  Obviously a complete understanding is going to take some
time;  though I would note that this general failure mode was one we
were aware of and is part of the reason the treshold is so high.

@_date: 2015-07-04 06:15:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] July 4th 2015 invalid block fork postmortem BIP 
Unless there are objections I intend to assign myself a BIP number for
a postmortem for this event.
I've already been reaching out to parties involved in or impacted by
the fork to gather information, but I do not intend to begin drafting
for a few days (past expirence has shown that it takes time to gain
more complete understanding after an event).
If anyone is aware of services or infrastructure which were impacted
by this which I should contact to gain insight for the analysis,
please contact me off-list.
If anyone is interested in contributing to an analysis, let me know
and I'll link you to my repository when I begin drafting.  If you have
begun your own write up, please do not send it to me yet-- I'd rather
collect more data before drawing any analysis from it myself.

@_date: 2015-07-07 23:14:18
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Upcoming DOS vulnerability announcements for 
Just an update here-- I'm delaying this somewhat due to recent network
turbulance and unusual attempted DOS attack activity on relayed
I've also had some requests from other cryptocurrency implementors to
use a somewhat longer horizon here.

@_date: 2015-07-20 20:55:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] For discussion: limit transaction size to 
This seems like a fairly indirect approach. The resource being watched
for is not the size (otherwise two transactions for 200k would be
strictly worse than one 200k transactions) but the potential of N^2
costs related to repeated hashing in checksig; which this ignores.
The cost of the indirection is forclosing future applications which
involve larger signatures but have no quadratic component and are thus
fast to verify-- or requring yet another hard fork to remove the
limit, or a kludgy soft fork that splits the same data across two
"transactions" which get processed as a unit... all would be
Alternative 1 sounds more attractive to be for this reason as it's more direct.

@_date: 2015-07-29 16:53:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I doubt the rest of us really enjoy hearing these "lessons" from from
you where you wildly distort history to reflect your views.
As others have pointed out-- even if this were true, --- so what?
Many errors were made early on in Bitcoin.
But in this case it's not actually true and I'm really getting fed up
with this continued self-appointment of all that the creator of the
system thought. Your position and knoweldge is not special or
priveleged compared to many of the people that you are arguing with.
It was _well_ understood while the creator of the system was around
that putting every consensus decision into the world into one system
would not scale; and also understood that the users of Bitcoin would
wish to protect its decenteralization by limiting the size of the
chain to keep it verifyable on small devices.
Don't think you can claim otherwise, because doing so is flat out wrong.
In the above statement you're outright backwards-- there was a clear
expectation that all who ran nodes would mine. The delegation of
consensus to third parties was unforseen. Presumably Bitcoin core
making mining inaccessable to users in software was also unforseen.
You wrote a long list of activities that are actually irrelevant to
many node users with the result of burrying the main reason any party
should be running a node (emphasis mine).
The incentives of the system demand as it exist today that many other
economically significant parties run nodes in order to keep the half
dozen miners from having a blank check to do whatever they want
(including supporting their operations through inflation)-- do not
think they wouldn't, as we've seen their happy to skip verification
(Which, incidentially, is insanely toxic to any security argument for
SPV; ---- and now we see the market failure that results from your and
Gavin years long campaign to ignore problems in the mining ecosystem:
The SPV model which you've fixated on as the true nature of bitcoin
has been demonstrated in practice to have a potentially empty security
The information I have currently is that the parties engaging in that
activity found it to be tremendously profitable, even including losses
from issues.

@_date: 2015-07-29 19:53:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Mike, my first use of Bitcoin was in 2009.  I wasn't vigorously active
in the Bitcoin community until the beginning of 2011, indeed. But this
is just a couple months after you (E.g. first code available for
BitcoinJ was March 2011-- if you go by forums.bitcoin.org account
times my account was created May 5th 2011 vs yours Dec 14th 2010; less
than five months after yours). I was also working with related systems
long before (E.g. RPOW in 2004). So give me a break, there is no rank
to pull here.
Yet again you've managed to call me a bullshitter and guilty of
"invention" when in fact I'm actually quoting the system's creator
(although without the explicit fallacious argument from authority
style you seem prefer). For someone who seems to base all his
arguments on interpretations of someone's words you sure seem to call
their words lies awfully often:
"Piling every proof-of-work quorum system in the world into one
dataset doesn't scale."
"Bitcoin users might get increasingly tyrannical about limiting the
size of the chain so it's easy for lots of users and small devices."
----  If you'll note,, the post was Dec 10th 2010 and, presumably, made with
an improved understanding of the implications of the system then
comments made in 2008 before the system was even operational.
(The same message also mentions that smart contracts can be used to
create trustless trade with off-chain systems;  As well, later in that
thread: "it will be much easier if you can freely use all the space
you need without worrying about paying fees for expensive space in
Bitcoin's chain.")
I haven't bothered arguing from old posts in the past because I find
the practice of argument from authority on this subject abhorrent. It
undermines the unique value of Bitcoin to argue based on a single
personal opinion, to do so is to miss the point of Bitcoin in a deep
and fundamental way. And in my opinion what you're doing is actually
much worse: arguing from distortions of random quotations.  But it's
hard to tolerate the continue revision of history from you in silence.
Moreover, I find those arguments with respect Moore's law especially
unconvincing because while I cannot read the mind of people who are
not a part of this discussion and haven't chosen to comment, I've used
the same argument myself and I know what I was thinking when I used it
(and can establish as much, since I'm more verbose I elaborated on
it):  When someone pointed at Bitcoins _global_ broadcast medium and
loudly said that it cannot work because its absurd; and it's very easy
to point out broad scaling behavior about what Bitcoin could achieve
with complete centralization. Once this has been accepted the argument
is _over_ in Bitcoin's favor: Bitcoin's competition has highly
centralized administration and so once someone has accepted Bitcoin
can (in some way) accommodate the worlds transactions, even if that
comes at the cost of 99% of the decentralization, it's clear that
Bitcoin offers something interesting. (And for example, I elaborated
on this in a Wiki edit in Aug 2011,
 -- though I shouldn't need to point this out to you, since it was you
who subsequently erased these words from the page.)
For example, you fought vigorously to get Bitcoin Core off
Bitcoin.org, which would ensure that users were not previously
equipped with a node suitable for operating mining (which then
contributed substantially to the poor usability of solutions like
P2Pool; with 98% of it's install time spent waiting for Bitcoin Core
to sync).
You've (in my view) aggressively advocated increasing the resource
utilization of Bitcoin-- increasing the cost to participate in mining
without delegation, with no consideration (or at least disclosure) of
the ramifications on the system overall:
Gavin, for example, has advocated removing mining support from Bitcoin
core on several occasions; and constantly professes ignorance on
anything mining.  His own interests are up to him, but to not be
concerned about a central part of the system for anyone working on
changing it at such a deep level is-- I think-- a bit problematic.
But I didn't intend to lay blame here, if anything I blame myself for
not being more proactive in arguing against things things in the past.
The trend towards mining centralization is a result of various forces,
many of which are modulated by the very things we're discussing here
(or could be modulated by things we haven't discussed).  You're the
principle advocate of increasing the cost of a decentralized ecosystem
around verification and driving the system towards a state where it is
only viable in a more centralized mode.  Bitcoin is an artificial
construction, not a force of nature, and when someone seeks to change
it they ought to take responsibility for what happens--- it's not
acceptable to say "oh well, it's not eh fault of anyone" when the
incentives drive it in a bad direction.
Is that your strategy on the systems resource consumption in general?
Full throttle, no action when it goes off the rails,  when the easily
foreseeable negative outcomes happen it won't be the "fault" of
anyone? If so, I don't think that is acceptable. We need to face the
areas in which the system is failing, now and in the future... and not
just pump for growth at all cost and shrug and say "oh well, we tried"
when the predictable failure happens. It's far from clear to me that
the world will get a second shot at this in the next several decades
if Bitcoin lapses into the same-old, same-old.
The fixation comment was a specific reply to your long list of the
"only reasons" to run a full node, which seemed to be basically said
that the only reason to run one was to act as a server for SPV
clients; as it listed several points on that-- all three of the
numbered points were "serving SPV wallets"-- and buried the rest.  I'm
sorry if I read too much into it, though it's also consistent with
your prior responses that the non-scalability of Bitcoin as a whole is
irrelevant due to SPV.
I don't think there is anything fundamentally bad with SPV, it is what
it is; it's a tool and an important one. But at the moment it is far
more limited than you give it credit for both because it is only
secure under certain assumptions which have been provably violated not
just at risk of violation, and because the more complete vision of it
(e.g. with fraud proofs) has never been implemented.
Now that I've established the "small device" text you're railing on
here actually came from the system's creator prior to your
involvement, can I expect an admission that your own "personal liking"
doesn't have special authority over the system?  But I hope you don't
create an altcoin: I think it's possible to find ways to accommodate
people with very different preferences under one tent, and if we are
to build and support a worldwide system we _must_ find those ways
rather than fragmenting the marketplace.

@_date: 2015-07-29 20:09:05
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Wed, Jul 29, 2015 at 7:56 PM, Owen via bitcoin-dev
Precisely.  And as "just a payment system" Bitcoin is not an
especially great one: The design requirements for decenteralization
impose considerable costs.  To the extent that the technology in
Bitcoin is useful at all for building "just another payment system"
this technology in in the process of being agressively copied by
parties with deep fiat relationships (including in partnership with
centeral banks).  If the focus for Bitcoin's competative advantage
becomes exclusively "better" payments then it will almost certinatly
fail in the market-place against competing systems which avoid the
Bitcoin currency adoption related obsticles (but also gain none of
Bitcoin's important social/political promise).
Also, critically, if Bitcoin's security properties are manintained and
enhanced then Bitcoin can be used to build secure systems which _also_
accomidate those applications and we can have both. But if Bitcoin's
security properties are not strong then then advanced tools cannot be
built for it.  E.g. atomic swaps make trustless trades with external
systems possible; but they are especially sensitive to long
reorginizations by miners... so they can only be securely used where
those reorgs are infeasable.  So while I agree that we must be willing
to tolerate not catching every conceivable use case; most of the time
all that means is addressing them via a less direct but more focused
solution rather than ignoring them completely.

@_date: 2015-06-14 22:38:51
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposal: Move Bitcoin Dev List to a 
I support this proposal.
But for clarity sake, we should recognize that Linux Foundation isn't
a charity chartered to act in the public good, is a trade organization
which acts in the commercial interest of it's membership.
I do not think this presents a problem: LF's membership's interests
are not at odds with ours currently, and aren't likely to become so
(doubly so with sourceforge as the comparison point). We are, after
all, just talking about a development mailing list; in the unlikely
case that there were issues in the future it could be changed, and
they've demonstrated considerable competence at this kind of operation
as well, and I would be grateful to have their support.  I mention it
only because the 'foundation' name sometimes carries the charity
confusion, and to be clear that I think the stakes on this matter are
small enough that it doesn't require a careful weighing of interests.
These concerns may matter for other initiatives but as you note, LF
has zero stake beyond the general support of the open source
I do not believe it would be wise to delete the SF account, at least
while there are many active links to it. As it might well be recreated
to 'mirror' things as a 'service' to those following the old links.
I also agree with Jeff's comments wrt, bitcoin-security.

@_date: 2015-06-14 23:02:51
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Sorry I wasn't a part of the IRC conversation where this was first
discussed, though I'm very happy to see a concrete implementation
along with the proposal.
I'm not a great fan of this proposal for two reasons: The first is
that the strict ordering requirements is incompatible with future
soft-forks that may expose additional ordering constraints. Today we
have _SINGLE, which as noted this interacts with poorly, but there
have been other constraints proposed that this would also interact
with poorly.
The second is that even absent consensus rules there may be invisible
constraints in systems-- e.g. hardware wallets that sign top down, or
future transaction covenants that have constraints about ordering,  or
proof systems that use (yuck) midstate compression for efficiency.
Right now, with random ordering these applications are fairly
indistinguishable from other random uses (since their imposed order
could come about by chance) but if everyone else was ordered, even if
wasn't enforced.. these would be highly distinguishable. Which would
be unfortunate.  Worse, if most transactions are ordered the few that
aren't could be mishandled (which is, I assume, part of why you
propose requiring the ordering-- but I think the soft fork constraints
there hurt it more).
[Sorry for the delay in stating my views here; I wanted to talk them
over with a few other people to see if I was just being stupid and
misunderstanding the proposal]
I think perhaps the motivations here are understated. We have not seen
any massive deployments of accidentally broken ordering that I'm aware
of-- and an implementation that got this wrong in a harmful way would
likely make far more fatal mistakes (e.g. non random private keys).
As an alternative to this proposal the ordering can be privately
derandomized in the same way DSA is, to avoid the need for an actual
number source.  If getting the randomness right were really the only
motivation, I'd suggest we propose a simple derandomized randomization
algorithm--- e.g. take the order from (H(input ids||client secret)).
I think there is actually an unstated motivation also driving this
(and the other) proposal related to collaborative transaction systems
like coinjoins or micropayment channels; where multiple clients need
to agree on the same ordering. Is this the case? If so we should
probably talk through some of the requirements there and see if there
isn't a better way to address it.

@_date: 2015-06-14 23:04:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Is it your opinion that its fine if the result is that it makes the
usage trivially distinguishable e.g. where it might be subjected to
higher tx fees, or might break some software which incorrectly expects
all transactions to be ordered since most are?

@_date: 2015-06-15 02:33:03
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
It is the case that the generalized sighash flag design I was thinking
about was actually completely neutral about ordering, and yet still
replaced SINGLE.
I need to think a bit on that.

@_date: 2015-06-16 23:32:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Lost proposals from FellowTraveler/Monetas 
Is there any discussion thats been had relating to the BIP-drafts at:
Fellow Traveler has apparently been waiting 9 months for progress on
these proposals and I'm trying to find out where the breakdown
happened. While do not doubt that I am somehow at fault, I can't
figure out how.
Searching my email and this list archive for "Base58 Serialization for
Transaction-Based Color Descriptors" or "Order-based Smart Property
Coloring" or the draft names bip-ccids, etc. turn up no hits at all.
I've asked several other people and there list archives show nothing.
Has anyone else taken part in discussions related to these proposals
(or seen them all before)? If so, please point me to the discussion.
Otherwise I'll just go ahead and create threads for each under the
assumption that there is yet another mailing list screwup. :(

@_date: 2015-06-18 15:58:31
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
This is the only reddit comment I've made using the word revert in
recent memory, so I know you couldn't be referring to another.
And, I probably should have continued "and resulted with an immediate
revocation of commit rights on the assumption that his account had
been compromised."
There is nothing controversial about that.

@_date: 2015-06-18 19:32:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
It's probably due to me: When I point to trends and broadband
_distribution_ in the US (much less the less developed parts of the
world), I'm being "hypothetical", and when I point to my _own_
connectivity as a concrete example it's "personal". It's no joke that
communication is _hard_ but it's a shared responsibility however, and
no need to assume anyone isn't reading.,

@_date: 2015-06-27 06:21:03
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Upcoming DOS vulnerability announcements for Bitcoin 
On July 7th I will be making public details of several serious denial of
service vulnerabilities which have fixed in recent versions of Bitcoin Core,
including CVE-2015-3641.
I strongly recommend anyone running production nodes exposed to inbound
connections from the internet upgrade to 0.10.2 as soon as possible.
Upgrading older systems, especially miners, is also important due to the
BIP66 soft-fork which is about to reach enforcing status, see also:

@_date: 2015-06-29 05:43:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
Even accepting the premise that policy is pure local fiat, the
conclusion doesn't follow for me. BIPs about best practices or
especially anything where interop or coordination are, I think,
reasonable uses of the process.
E.g. you might want to know what other kinds of policy are in use if
you're to have any hope of authoring transactions that work at all!

@_date: 2015-03-11 19:46:22
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum 2.0 has been tagged 
On Wed, Mar 11, 2015 at 7:24 PM, Ricardo Filipe
There are genuine principled disagreements on how some things should
be done. There are genuine differences in functionality.
We cannot expect and should not expect complete compatibility. If you
must have complete compatibility: use the same software (or maybe not
even then, considering how poor the forward compatibility of some
things has been..).
What we can hope to do, and I think the best we can hope to do, is to
minimize the amount of gratuitous incompatibility and reduce the
amount of outright flawed constructions (so if there are choices which
must be made, they're at least choices among relatively good options).

@_date: 2015-03-11 20:16:13
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP for standard multi-signature P2SH 
We have had repeated problems in the past with people working on and
circulating prior draft proposals squatting on each others numbers,
and each demanding access to the same numbers. As a matter of
procedure I will not assign squatted numbers, but also discussion
should come in advance of number assignment; general subject here
seems reasonable but many proposals are withdrawn by the party
tendering them after further discussion shows the effort to be without
public interest or actually subsumed by other functionality. :)
Proposals should not be called "BIP[nn]" until they're actually a BIP.
  Feel free to call it bip-kerin-multisignature or any other
placeholder name that won't be confused with a finished BIP for
If there is any public documentation on the process which caused you
specific confusion, please feel free to point me at it and I'll be
sure to fix it! Sorry for any confusion there.

@_date: 2015-03-11 23:34:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP for standard multi-signature P2SH 
Thats more or less what posting to the list is supposed to be.
Creating a draft document requires no approval, beyond filling out the
right form.
Perhaps calling out that as a distinct step would be better, indeed.

@_date: 2015-03-12 00:11:24
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum 2.0 has been tagged 
The fact remains that there are several apparently unresolvable
well-principled perspectives on this subject.
(And I can speak to this personally: There are several BIPs in this
space that I'd rather not see in product with my name on it.)
Unless two wallets have exactly the same feature set, cross importing
keys is going to confuse or break something. Even if you're trying to
be fairly generic the testing overhead for all possible strategies and
structures is large. Expecting compatibility here would be like
expecting two large commercial accounting packages to support the same
internal file formats. Compatibility is only straight forward when the
feature set is as limited as possible.
The space for weird behavior to harm users is pretty large... e.g. you
could load a key into two wallets, such that one can see all the funds
by the other, but not vice versa and and up losing funds by
incorrectly assuming you had no coins; or inadvertently rip of your
business partners by accounting for things incorrectly.
Even ignoring compatibility, most demanded use cases here are ones
that create concurrent read/write use of single wallet without some
coordinating service is inherently somewhat broken because you can
double spend yourself, and end up with stalled and stuck transactions
and causing people to think you tried ripping them off.
I certainly recognize the desirable aspects of just being able to load
a common wallet, and that inexperienced users expect it to just work.
But I don't think that expectation is currently very realistic except
within limited domains. It may be more realistic in the future when
the role of wallets is better established. I don't see any _harm_ in
trying to standardize what can be, I just don't expect to see a lot of
Ultimately, the most fundamental compatibility is guaranteed:  you can
always send your funds to another wallet. This always works and
guarantees that you are never locked in to a single wallet. It is well
tested and cannot drive any software in to weird or confused states.

@_date: 2015-03-12 04:09:44
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Electrum 2.0 has been tagged 
Just loading a key doesn't keep transaction history however, if the
loading wallet can't understand or infer metadata about the
transactions. You get some mass of data but to tell actually what the
transactions are, or what they were for, forensic accounting is
required and some data will be potentially unrecoverable.
The best way to preserve historical information is to use reporting
from the wallet in question; which will accurately record the best
available output for this. (E.g. Bitcoin-qt has a CSV export or you
can take a json list-transactions out of it).
This cuts both ways, we've seen significant losses for users in
Bitcoin Core where they've used the console to import keys that they
also used in other insecure clients.
For an emergency transition the user is probably better off with an
explicit unstructured mass private key export, and a sweep function;
and guaranteeing compatibility with that is much easier; and because
it moves funds in one direction there is much less chance of going
from secure to insecure.
I suppose it would be too much to ask that these web wallets actually
not be totally centrally controlled and have the potential of just
having someone else stand up a server. I guess not. :(
Emergencies being what the are you do with what you can... indeed, I
agree thats a reason that better compatibility is better. (But perhaps
best is that its insane to use software to handle your money that can
just be taken away from you like that...)
Careful with this line of thinking: We have no mechanism in the BIP
process to exclude weak cryptography.
A BIP is not a measure of cryptographic integrity. There are existing
BIPs which I consider flawed and would not use or recommend.
It result in some level of review, maybe, and so it can be productive
to at least have more eyes on fewer things; which is a reason I
wouldn't say don't bother trying.
And indeed, I do think that what can be standardized should be, my
words weren't intended to dismiss anyone's efforts, only to encourage
realistic (I think) expectations around what will come of it.
And while I hope for no gratuitous incompatibility, I also hope that
no one working on a wallet hesitates for a minute to offer a new and
interesting functionality just because it doesn't fit into a prefab

@_date: 2015-03-13 04:01:45
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] BIP32 Index Randomisation 
This seems overly complicated to me, unless I'm missing something.
Instead, I think you should just give the server the master pubkey P
only without the chaincode.
Then when you transact you generate the address in whatever manner you
like and tell the server the scalar value iL which the user computes
iL = HMAC-SHA512(Key = cpar, Data = serP(Kpar) || ser32(i))[first 32
byes],  (per BIP 32).
and the server computes P + iL*G  and checks agreement with the address.
It would be inaccurate to call this private, as the server still
learns this particular relation. (and really users should _not_ be
using the same chaincode with different parties... as it exacerbates
the private key leak risk), but its certainly more private than giving
people the chain code.
The approach I suggest is also not gratuitously incompatible with
hardened derivation, which is what parties should be doing when they
don't actually need a third party to generate future addresses for
them without their cooperation (as appears to be the case here).

@_date: 2015-03-25 16:34:36
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
Assuming the sender is not an uncooperative idiot, you can simply
include expiration information and the sender can refuse to send after
that time.
If the sender is an uncooperative idiot, they can always change your
target and send anyways.
This would seem to work nearly as well as the non-reorg safe network
impacting version, and yet has no cost beyond the extra size is
communicating the limit.
Requires a unprunable verification state.

@_date: 2015-03-25 19:22:58
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
Not at all.
The sender is always able to intentionally hide their payment under a
rock-- There is no encoding that can prevent that.
The defense against that is to not accept payments not made according
to the payees specification.
To reject reused scriptPubKeys you must remember past scriptPubkeys in
order to test against them.
For illustration purposes imagine a bitcoin system where there is only
a single base unit available for trade.
Verification of that chain requires O(1) storage (the identity of the
current chain tip, and the identity of the spendable coin.).
Verification with duplicate elimination requires O(N) storage (with N
being the length of the history), since you need to track all the
duplicates to reject.
(The same is true for actual Bitcoin as well, though the constant
factors make the difference somewhat less stark.)

@_date: 2015-03-26 20:42:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
If its to be heuristic in any case why not make it a client feature
instead of a consensus rule?
If someone wants to bypass anything they always can, thats what I mean
by "hide their payment under a rock"
E.g. I can take your pubkey, add G to it (adding 1 to the private
key), strip off the time limits, and send the funds.
"What do you mean I didn't pay you? I wrote a check. locked it in a
safe, and burred it in your back garden."
The answer to this can only be that payment is only tendered when its
made _exactly_ to the payee's specifications.
If someone violates the specifications all they're doing is destroying
coins. Nothing can stop people from destroying coins...
Which is why a simpler, safer, client enforced behavior is probably
preferable. Someone who wants to go hack their client to make a
payment that isn't according to the payee will have to live with the
results, esp. as we can't prevent that in a strong sense.

@_date: 2015-03-26 21:44:11
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
Great, that can be accomplished by simply encoding an expiration into
the address people are using and specifying that clients enforce it.

@_date: 2015-03-26 23:00:50
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
No one suggested _anything_ like that. Please save the concern for
someplace its actually applicable.
For a point of pedantry, the protocol actually was designed that way
and in the initial versions of the software there was actually no user
exposed mechanism to reuse a scriptPubkey no matter how hard you

@_date: 2015-03-27 03:13:06
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Address Expiration to Prevent Reuse 
I hope you're mistaken, because that would be a serious attack on the
design of bitcoin, which obtains privacy and fungibility, both
essential properties of any money like good, almost exclusively
through avoiding reuse.
[What business would use a money where all their competition can see
their sales and identify their customers, where their customers can
track their margins and suppliers? What individuals would use a system
where their inlaws could criticize their spending? Where their
landlord knows they got a raise, or where thieves know their net
Though no one here is currently suggesting blocking reuse as a network
rule, the reasonable and expected response to what you're suggesting
would be to do so.
If some community wishes to choose not to use Bitcoin, great, but they
don't get to simply choose to screw up its utility for all the other
You should advise this "country specific bitcoin group" that they
shouldn't speak for the users of a system which they clearly do not

@_date: 2015-05-07 00:37:54
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Block Size Increase 
> Recently there has been a flurry of posts by Gavin at >
 which advocate strongly for increasing >
the maximum block size. However, there hasnt been any discussion on this >
mailing list in several years as far as I can tell.
Thanks Matt; I was actually really confused by this sudden push with
not a word here or on Github--so much so that I responded on Reddit to
people pointing to commits in Gavin's personal repository saying they
were reading too much into it.
So please forgive me for the more than typical disorganization in this
message; I've been caught a bit flatfooted on this and I'm trying to
catch up. I'm juggling a fair amount of sudden pressure in my mailbox,
and trying to navigate complex discussions in about eight different
forums concurrently.
There have been about a kazillion pages of discussion elsewhere
(e.g. public IRC and Bitcointalk; private discussions in the past),
not all of which is well known, and I can't hope to summarize even a
tiny fraction of it in a single message-- but that's no reason to not
start on it.
certainly has a LOT of technical tradeoffs to consider.
There are several orthogonal angles from which block size is a concern
(both increases and non-increases). Most of them have subtle implications
and each are worth its own research paper or six, so it can be difficult
to only touch them slightly without creating a gish gallop that is hard
to respond to.
We're talking about tuning one of the fundamental scarcities of the
Bitcoin Economy and cryptosystem--leaving the comfort of "rule by
math" and venturing into the space of political decisions; elsewhere
you'd expect to see really in-depth neutral analysis of the risks and
tradeoffs, technically and economically.  And make no mistake: there
are real tradeoffs here, though we don't know their exact contours.
Fundamentally this question exposes ideological differences between people
interested in Bitcoin.  Is Bitcoin more of a digital gold or is it more
of a competitor to Square?  Is Bitcoin something that should improve
personal and commercial autonomy from central banks?  From commercial
banks? Or from just the existing status-quo commercial banks?   What are
people's fundamental rights with Bitcoin?  Do participants have a
right to mine? How much control should third parties have over their
transactions?  How much security must be provided? Is there a deadline
for world domination or bust?  Is Bitcoin only for the developed world?
Must it be totally limited by the most impoverished parts of the world?
Bitcoin exists at the intersection of many somewhat overlapping belief
systems--and people of many views can find that Bitcoin meets their
needs even when they don't completely agree politically.  When Bitcoin
is changed fundamentally, via a hard fork, to have different properties,
the change can create winners or losers (if nothing else, then in terms
of the kind of ideology supported by it).
There are non-trivial number of people who hold extremes on any of
these general belief patterns; Even among the core developers there is
not a consensus on Bitcoin's optimal role in society and the commercial
To make it clear how broad the views go, even without getting into
monetary policy... some people even argue that Bitcoin should act
as censor-resistant storage system for outlawed content; -- I think
this view is unsound, not achievable with the technology, and largely
incompatible with Bitcoin's use as a money (because it potentially
creates an externalized legal/harassment liability for node operators);
but these are my personal value judgments; the view is earnestly held
by more than a few; and that's a group that certainly wants the largest
possible blocksizes (though even then that won't be enough).
The subject is complicated even more purely on the technical side
by the fact that Bitcoin has a layered security model which is not
completely defined or understood: Bitcoin is secure if a majority of
hashrate is "honest" (where "honesty" is a technical term which means
"follows the right rules" without fail, even at a loss), but why might
it be honest? That sends us into complex economic and social arguments,
and the security thresholds start becoming worse when we assume some
miners are economically rational instead of "honest".
consistently full or very nearly full. What we see today are
To elaborate, in my view there is a at least a two fold concern on this
particular ("Long term Mining incentives") front:
One is that the long-held argument is that security of the Bitcoin system
in the long term depends on fee income funding autonomous, anonymous,
decentralized miners profitably applying enough hash-power to make
reorganizations infeasible.
For fees to achieve this purpose, there seemingly must be an effective
scarcity of capacity.  The fact that verifying and transmitting
transactions has a cost isn't enough, because all the funds go to pay
that cost and none to the POW "artificial" cost; e.g., if verification
costs 1 then the market price for fees should converge to 1, and POW
cost will converge towards zero because they adapt to whatever is
being applied. Moreover, the transmission and verification costs can
be perfectly amortized by using large centralized pools (and efficient
differential block transmission like the "O(1)" idea) as you can verify
one time instead of N times, so to the extent that verification/bandwidth
is a non-negligible cost to miners at all, it's a strong pressure to
centralize.  You can understand this intuitively: think for example of
carbon credit cap-and-trade: the trade part doesn't work without an
actual cap; if everyone was born with a 1000 petaton carbon balance,
the market price for credits would be zero and the program couldn't hope
to share behavior. In the case of mining, we're trying to optimize the
social good of POW security. (But the analogy applies in other ways too:
increases to the chain side are largely an externality; miners enjoy the
benefits, everyone else takes the costs--either in reduced security or
higher node operating else.)
This area has been subject to a small amount of academic research
(e.g.  But
there is still much that is unclear.
The second is that when subsidy has fallen well below fees, the incentive
to move the blockchain forward goes away.  An optimal rational miner
would be best off forking off the current best block in order to capture
its fees, rather than moving the blockchain forward, until they hit
the maximum. That's where the "backlog" comment comes from, since when
there is a sufficient backlog it's better to go forward.  I'm not aware
of specific research into this subquestion; it's somewhat fuzzy because
of uncertainty about the security model. If we try to say that Bitcoin
should work even in the face of most miners being profit-maximizing
instead of altruistically-honest, we must assume the chain will not
more forward so long as a block isn't full.  In reality there is more
altruism than zero; there are public pressures; there is laziness, etc.
One potential argument is that maybe miners would be _regulated_ to
behave correctly. But this would require undermining the openness of the
system--where anyone can mine anonymously--in order to enforce behavior,
and that same enforcement mechanism would leave a political level to
impose additional rules that violate the extra properties of the system.
So far the mining ecosystem has become incredibly centralized over time.
I believe I am the only remaining committer who mines, and only a few
of the regular contributors to Bitcoin Core do. Many participants
have never mined or only did back in 2010/2011... we've basically
ignored the mining ecosystem, and this has had devastating effects,
causing a latent undermining of the security model: hacking a dozen or
so computers--operated under totally unknown and probably not strong
security policies--could compromise the network at least at the tip...
Rightfully we should be regarding this an an emergency, and probably
should have been have since 2011.  This doesn't bode well for our ability
to respond if a larger blocksize goes poorly. In kicking the can with
the trivial change to just bump the size, are we making an implicit
decision to go down a path that has a conclusion we don't want?
(There are also shorter term mining incentives concerns; which Peter
Todd has written more about, that I'll omit for now)
I made a few relevant points back in 2011
after Dan Kaminsky argued that Bitcoin's decentralization was pretext:
that it was patently centralized since scaling directly in the network
would undermine decentralization, that the Bitcoin network necessarily
makes particular tradeoffs which prevent it from concurrently being all
things to all people.  But tools like the Lightning network proposal could
well allow us to hit a greater spectrum of demands at once--including
secure zero-confirmation (something that larger blocksizes reduce if
anything), which is important for many applications.  With the right
technology I believe we can have our cake and eat it too, but there needs
to be a reason to build it; the security and decentralization level of
Bitcoin imposes a _hard_ upper limit on anything that can be based on it.
Another key point here is that the small bumps in blocksize which
wouldn't clearly knock the system into a largely centralized mode--small
constants--are small enough that they don't quantitatively change the
operation of the system; they don't open up new applications that aren't
possible today. Deathandtaxes on the forum argued that Bitcoin needs
a several hundred megabyte blocksize to directly meet the worldwide
transaction needs _without retail_... Why without retail? Retail needs
near instant soft security, which cannot be achieved directly with a
global decentralized blockchain.
I don't think 1MB is magic; it always exists relative to widely-deployed
technology, sociology, and economics. But these factors aren't a simple
function; the procedure I'd prefer would be something like this: if there
is a standing backlog, we-the-community of users look to indicators to
gauge if the network is losing decentralization and then double the
hard limit with proper controls to allow smooth adjustment without
fees going to zero (see the past proposals for automatic block size
controls that let miners increase up to a hard maximum over the median
if they mine at quadratically harder difficulty), and we don't increase
if it appears it would be at a substantial increase in centralization
risk. Hardfork changes should only be made if they're almost completely
uncontroversial--where virtually everyone can look at the available data
and say "yea, that isn't undermining my property rights or future use
of Bitcoin; it's no big deal".  Unfortunately, every indicator I can
think of except fee totals has been going in the wrong direction almost
monotonically along with the blockchain size increase since 2012 when
we started hitting full blocks and responded by increasing the default
soft target.  This is frustrating; from a clean slate analysis of network
health I think my conclusion would be to _decrease_ the limit below the
current 300k/txn/day level.
This is obviously not acceptable, so instead many people--myself
included--have been working feverishly hard behind the scenes on Bitcoin
Core to increase the scalability.  This work isn't small-potatoes
boring software engineering stuff; I mean even my personal contributions
include things like inventing a wholly new generic algebraic optimization
applicable to all EC signature schemes that increases performance by 4%,
and that is before getting into the R&D stuff that hasn't really borne
fruit yet, like fraud proofs.  Today Bitcoin Core is easily >100 times
faster to synchronize and relay than when I first got involved on the
same hardware, but these improvements have been swallowed by the growth.
The ironic thing is that our frantic efforts to keep ahead and not
lose decentralization have both not been enough (by the best measures,
full node usage is the lowest its been since 2011 even though the user
base is huge now) and yet also so much that people could seriously talk
about increasing the block size to something gigantic like 20MB. This
sounds less reasonable when you realize that even at 1MB we'd likely
have a smoking hole in the ground if not for existing enormous efforts
to make scaling not come at a loss of decentralization.
I'm curious as to what discussions people have seen; e.g., are people
even here aware of these concerns? Are you aware of things like the
hashcash mediated dynamic blocksize limiting?  About proposals like
lightning network (instant transactions and massive scale, in exchange
for some short term DOS risk if a counterparty opts out)?   Do people
(other than Mike Hearn; I guess) think a future where everyone depends
on a small number of "Google scale" node operations for the system is
actually okay? (I think not, and if so we're never going to agree--but
it can be helpful to understand when a disagreement is ideological).

@_date: 2015-05-09 00:42:08
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Bitcoin-development Digest, Vol 48, 
[snip code]
Intriguing; and certainly a change of the normal pace around here.
In these posts I am reminded of and sense some qualitative
similarities with a 2012 proposal by Mr. NASDAQEnema of Bitcointalk
with respect to multigenerational token architectures. In particula,r
your AES ModuleK Hashcodes (especially in light of Winternitz
compression) may constitute an L_2 norm attractor similar to the
motherbase birthpoint metric presented in that prior work.  Rethaw and
I provided a number of points for consideration which may be equally
applicable to your work:
Your invocation of emotive packets suggests that you may be a
colleague of Mr. Virtuli Beatnik?  While not (yet) recognized as a
star developer himself; his eloquent language and his mastery of skb
crypto-calculus and differential-kernel number-ontologies demonstrated
in his latest publication ( ) makes me think that he'd be an ideal collaborator for your work in
this area.

@_date: 2015-05-09 03:36:07
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Though I'm a fan of this class of techniques(*) and think using something
in this space is strictly superior to not, and I think it makes larger
sizes safer long term;  I do not think it adequately obviates the need
for a hard upper limit for two reasons:
(1) for software engineering and operational reasons it is very
difficult to develop, test for, or provision for something without
knowing limits. There would in fact be hard limits on real deployments
but they'd be opaque to their operators and you could easily imagine
the network forking by surprise as hosts crossed those limits.
(2)  At best this approach mitigates the collective action problem between
miners around fees;  it does not correct the incentive alignment between
miners and everyone else (miners can afford huge node costs because they
have income; but the full-node-using-users that need to exist in plenty
to keep miners honest do not), or the centralization pressures (N miners
can reduce their storage/bandwidth/cpu costs N fold by centralizing).
A dynamic limit can be combined with a hard upper to at least be no
worse than a hard upper with respect to those two points.
Another related point which has been tendered before but seems to have
been ignored is that changing how the size limit is computed can help
better align incentives and thus reduce risk.  E.g. a major cost to the
network is the UTXO impact of transactions, but since the limit is blind
to UTXO impact a miner would gain less income if substantially factoring
UTXO impact into its fee calculations; and without fee impact users have
little reason to optimize their UTXO behavior.   This can be corrected
by augmenting the "size" used for limit calculations.   An example would
be tx_size = MAX( real_size >> 1,  real_size + 4*utxo_created_size -
3*utxo_consumed_size).   The reason for the MAX is so that a block
which cleaned a bunch of big UTXO could not break software by being
super large, the utxo_consumed basically lets you credit your fees by
cleaning the utxo set; but since you get less credit than you cost the
pressure should be downward but not hugely so. The 1/2, 4, 3 I regard
as parameters which I don't have very strong opinions on which could be
set based on observations in the network today (e.g. adjusted so that a
normal cleaning transaction can hit the minimum size).  One way to think
about this is that it makes it so that every output you create "prepays"
the transaction fees needed to spend it by shifting "space" from the
current block to a future block. The fact that the prepayment is not
perfectly efficient reduces the incentive for miners to create lots of
extra outputs when they have room left in their block in order to store
space to use later [an issue that is potentially less of a concern with a
dynamic size limit].  With the right parameters there would never be such
at thing as a dust output (one which costs more to spend than its worth).
(likewise the sigops limit should be counted correctly and turned into
size augmentation (ones that get run by the txn); which would greatly
simplify selection rules: maximize income within a single scalar limit)
(*) I believe my currently favored formulation of general dynamic control
idea is that each miner expresses in their coinbase a preferred size
between some minimum (e.g. 500k) and the miner's effective-maximum;
the actual block size can be up to the effective maximum even if the
preference is lower (you're not forced to make a lower block because you
stated you wished the limit were lower).  There is a computed maximum
which is the 33-rd percentile of the last 2016 coinbase preferences
minus computed_max/52 (rounding up to 1) bytes-- or 500k if thats
larger. The effective maximum is X bytes more, where X on the range
[0, computed_maximum] e.g. the miner can double the size of their
block at most. If X > 0, then the miners must also reach a target
F(x/computed_maximum) times the bits-difficulty; with F(x) = x^2+1  ---
so the maximum penalty is 2, with a quadratic shape;  for a given mempool
there will be some value that maximizes expected income.  (obviously all
implemented with precise fixed point arithmetic).   The percentile is
intended to give the preferences of the 33% least preferring miners a
veto on increases (unless a majority chooses to soft-fork them out). The
minus-comp_max/52 provides an incentive to slowly shrink the maximum
if its too large-- x/52 would halve the size in one year if miners
were doing the lowest difficulty mining. The parameters 500k/33rd,
-computed_max/52 bytes, and f(x)  I have less strong opinions about;
and would love to hear reasoned arguments for particular parameters.

@_date: 2015-05-10 21:07:43
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] A way to create a fee market even without 
Users can pay fees or a portion of fees out of band to miner(s); this
is undetectable to the network.
It's also behavior that miners have engaged in since at least 2011 (in
two forms;  treating transactions that paid them directly via outputs
as having that much more in fees;  and taking contracts for fast
processing for identified transactions (e.g. address matching or via
an API) e.g. "I'll pay you x at the end of the month for each of my
transactions you process, you can poll this API". I'm aware of at
least two companies having had this arrangement with miners).
I think what you suggested then just further rewards this behavior as
it allows bypassing your controls.-- I suspect generally any scheme
the looks at the fee values has this property.

@_date: 2015-05-10 21:33:15
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Thats not the long term effect or the motivation-- what you're seeing
is that the subsidy gets in the way here.  Consider how the procedure
behaves with subsidy being negligible compared to fees.   What it
accomplishes in that case is that it incentivizes increasing the size
until the marginal "value" to miners of the transaction-data being
left out is not enormously smaller than the "value" of the data in the
block on average.  Value in quotes because it's blind to the "fees"
the transaction claims.
With a large subsidy, the marginal value of the first byte in the
block is HUGE; and so that pushes up the average-- and creates the
"base fee effect" that you're looking at.  It's not that anyone is
picking a fee there, it's that someone picked the subsidy there.  :)
As the subsidy goes down the only thing fees are relative to is fees.
An earlier version of the proposal took subsidy out of the picture
completely by increasing it linearly with the increased difficulty;
but that creates additional complexity both to implement and to
explain to people (e.g. that the setup doesn't change the supply of
coins); ... I suppose without it that starting disadvantage parameter
(the offset that reduces the size if you're indifferent) needs to be
much smaller, unfortunately.

@_date: 2015-05-12 19:03:55
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
It's a little frustrating to see this just repeated without even
paying attention to the desirable characteristics from the prior
Summarizing from memory:
(0) Block coverage should have locality; historical blocks are
(almost) always needed in contiguous ranges.   Having random peers
with totally random blocks would be horrific for performance; as you'd
have to hunt down a working peer and make a connection for each block
with high probability.
(1) Block storage on nodes with a fraction of the history should not
depend on believing random peers; because listening to peers can
easily create attacks (e.g. someone could break the network; by
convincing nodes to become unbalanced) and not useful-- it's not like
the blockchain is substantially different for anyone; if you're to the
point of needing to know coverage to fill then something is wrong.
Gaps would be handled by archive nodes, so there is no reason to
increase vulnerability by doing anything but behaving uniformly.
(2) The decision to contact a node should need O(1) communications,
not just because of the delay of chasing around just to find who has
someone; but because that chasing process usually makes the process
_highly_ sybil vulnerable.
(3) The expression of what blocks a node has should be compact (e.g.
not a dense list of blocks) so it can be rumored efficiently.
(4) Figuring out what block (ranges) a peer has given should be
computationally efficient.
(5) The communication about what blocks a node has should be compact.
(6) The coverage created by the network should be uniform, and should
remain uniform as the blockchain grows; ideally it you shouldn't need
to update your state to know what blocks a peer will store in the
future, assuming that it doesn't change the amount of data its
planning to use. (What Tier Nolan proposes sounds like it fails this
(7) Growth of the blockchain shouldn't cause much (or any) need to
refetch old blocks.
I've previously proposed schemes which come close but fail one of the above.
(e.g. a scheme based on reservoir sampling that gives uniform
selection of contiguous ranges, communicating only 64 bits of data to
know what blocks a node claims to have, remaining totally uniform as
the chain grows, without any need to refetch -- but needs O(height)
work to figure out what blocks a peer has from the data it
communicated.;   or another scheme based on consistent hashes that has
log(height) computation; but sometimes may result in a node needing to
go refetch an old block range it previously didn't store-- creating
re-balancing traffic.)
So far something that meets all those criteria (and/or whatever ones
I'm not remembering) has not been discovered; but I don't really think
much time has been spent on it. I think its very likely possible.

@_date: 2015-05-12 20:02:36
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
It might be more interesting to think of that attack as a bandwidth
exhaustion DOS attack on the archive nodes... if you can't get a copy
without them, thats where you'll go.
So the question arises: does the option make some nodes that would
have been archive not be? Probably some-- but would it do so much that
it would offset the gain of additional copies of the data when those
attacks are not going no. I suspect not.
It's also useful to give people incremental ways to participate even
when they can't swollow the whole pill; or choose to provide the
resource thats cheap for them to provide.  In particular, if there is
only two kinds of full nodes-- archive and pruned; then the archive
nodes take both a huge disk and bandwidth cost; where as if there are
fractional then archives take low(er) bandwidth unless the fractionals
get DOS attacked.

@_date: 2015-05-12 20:47:41
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Proposed additional options for pruned 
Prior discussion had things like "the definition of pruned means you
have and will serve at least the last 288 from your tip" (which is
what I put in the pruned service bip text); and another flag for "I
have at least the last 2016".  (2016 should be reevaluated-- it was
just a round number near where sipa's old data showed the fetch
probability flatlined.
But that data was old,  but what it showed that the probability of a
block being fetched vs depth looked like a exponential drop-off (I
think with a 50% at 3-ish days); plus a constant low probability.
Which is probably what we should have expected.
I'm not fond of this; it makes the system dependent on centralized
services (e.g. trackers and sources of torrents). A torrent also
cannot very efficiently handle fractional copies; cannot efficiently
grow over time. Bitcoin should be complete-- plus, many nodes already
have the data.

@_date: 2015-05-26 19:10:25
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
I'm having a hard time. Can you help me understand a specific case
where this makes a difference.
It appears to be a gratuitous requirement;  if I have another unused
input that happens to be larger by the required fee-- why not just use
The inherent malleability of signatures makes it unreliable to depend
on the signature content of a transaction until its good and buried,
regardless. And an inability to replace an input means you could not
RBF for additional fees without taking change in more cases; there
ought to be a benefit to that.

@_date: 2015-05-26 23:11:17
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
I'm having a hard time parsing this.  You might as well say that its
part of a weeblix for how informative it is, since you've not defined
The signatures of a transaction can always be changed any any time,
including by the miner, as they're not signed.
They can already do this.
The RBF behavior always moves in the direction of more prefered or
otherwise the node would not switch to the replacement. Petertodd
should perhaps make that more clear.
But your "maybe"s are what I was asking you to clarify. You said it
wasn't hard to imagine; so I was asking for specific clarification.
They can already do that.
Then why do you not argue that changing the input set does not change
the weeblix?
Why is one case of writing out a participant different that the other
case of writing out a participant?
How could a _stronger_ warning be required?

@_date: 2015-05-27 08:18:52
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
You've misunderstood it, I think-- Functionally nlocktime but relative
to each txin's height.
But the construction gives the sequence numbers a rational meaning,
they count down the earliest position a transaction can be included.
(e.g. the highest possible sequence number can be included any time
the inputs are included) the next lower sequence number can only be
included one block later than the input its assigned to is included,
the next lower one block beyond that. All consensus enforced.   A
miner could opt to not include the higher sequence number (which is
the only one of the set which it _can_ include) it the hopes of
collecting more fees later on the next block, similar to how someone
could ignore an eligible locked transaction in the hopes that a future
double spend will be more profitable (and that it'll enjoy that
profit) but in both cases it must take nothing at all this block, and
risk being cut off by someone else (and, of course, nothing requires
users use sequence numbers only one apart...).
It makes sequence numbers work exactly like you'd expect-- within the
bounds of whats possible in a decentralized system.  At the same time,
all it is ... is relative nlocktime.

@_date: 2015-05-27 22:22:48
@_author: Gregory Maxwell 
@_subject: [Bitcoin-development] Long-term mining incentives 
The prior (and seemingly this) assurance contract proposals pay the
miners who mines a chain supportive of your interests and miners whom
mine against your interests identically.
There is already a mechanism built into Bitcoin for paying for
security which doesn't have this problem, and which mitigates the
common action problem of people just sitting around for other people
to pay for security: transaction fees. Fixing the problem with
assurance contracts effectively makes them end up working like
transaction fees in any case.  Considering the near-failure in just
keeping development funded, I'm not sure where the believe this this
model will be workable comes from; in particular unlike a lighthouse
(but like development) security is ongoing and not primarily a fixed
one time cost. I note that many existing crowdfunding platforms
(including your own) do not do ongoing costs with this kind of binary
Also work reminding people that mining per-contract is a long
identified existential risk to Bitcoin which has been seeing more
analysis lately:

@_date: 2015-11-13 21:53:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Contradiction in BIP65 text? 
The first text is explaining nlocktime without BIP65 in order to
explain the reason for having BIP65.

@_date: 2015-11-14 00:29:51
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Contradiction in BIP65 text? 
The most frequent related point of confusion I see is that people have
a dangerously wrong mental model of how scrpitpubkeys work.
It seems people think that wallets will infer whatever they can
possibly spend and display that.  This is not how wallets work, and if
any wallet were ever created like that its users would immediately go
broke (and it's author should be taken out and shot. :) ).
Rather, wallets must only display funds paid to scriptpubkeys (also
addresses) they actually generated or, at least, would have generated.
Otherwise someone can just create a 1 of 2 {them, you}  multisig and
then claw back the coins after you think you've been paid.
As such there is no risk of anyone sneaking in CLTV locked funds for
on you except by virtue of spectacular software bugs that would likely
cause you to destroy funds in a zillion other ways first.

@_date: 2015-11-15 01:08:16
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Sometimes errors are such that you can catch them (if you're super
vigilant and know an error is even possible in that case)-- and
indeed, in that case you can get a "I don't know, something is
wrong.", other times errors are undetectable.
I cited an issue in leveldb (before we used it) where it would
silently fail to return records.
That invariant requires the software to be completely free of errors
that would violate it.

@_date: 2015-11-15 02:10:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
The size of a block is not a type 1 failure. There is a clear, known,
unambiguous, and easily measured rule in the system that a block is
not permitted to have a size over a threshold. A block that violates
this threshold is invalid.   The only way I can see to classify that
as a type 1 failure is to call the violation of any known system rule
a type 1 failure.  "Spends a coin that was already spent" "provides a
signature that doesn't verify with the pubkey" "creates bitcoins out
of thin air" -- these are not logically different than "if size>x
return false".
The only consensus consistency error we've ever that I would have
classified as potentially type 1 had has been the BDB locks issue.
Every other one, including the most recently fixed one (eliminated by
BIP66) was a type 2, by your description. They are _much_ more common;
because if the author understood the possible cases well enough to
create an "I don't know" case, they could have gone one step further
and fully defined it in a consistent way. The fact that an
inconsistency was possible was due to a lack of complete knowledge.
Even in the BDB locks case, I don't know that the type 1 distinction
completely applies, a lower layer piece of software threw an error
that higher layer software didn't know was possible, and so it
interpreted "I don't know" as "no"-- it's not that it chose to treat I
don't know as no, its that it wasn't authored with the possibility of
I don't know in mind, and that exceptions were used to handle general
failures (which should have been treated as no). Once you step back to
the boundary of the calling code (much less the whole application) the
"I don't know" doesn't exist anymore; there.

@_date: 2015-11-15 03:30:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
It is not technically distinct--today; politically-- perhaps, but--
sorry, no element of your prior message indicated that you were
interested in discussing politics rather than technology; on a mailing
list much more strongly scoped for the latter; I hope you can excuse
me for missing your intention prior to your most recent post.
That said, I believe you are privileging your own political
preferences in seeing the one rule of the bitcoin system as
categorically distinct even politically. No law of nature leaves the
other criteria I specified less politically negotiable, and we can see
concrete examples all around us -- the notion that funds can be
confiscated via external authority (spending without the owners
signature) is a more or less universal property of other modern
systems of money, that economic controls out to exist to regulate the
supply of money for the good of an economy is another widely deployed
political perspective. You, yourself, recently published a work on the
stable self regulation of block sizes based on mining incentives that
took as its starting premise a bitcoin that was forever inflationary.
Certainly things differ in degrees, but this is not the mailing list
to debate the details of political inertia.
You're welcome, but I would have preferred that you instead of your
thanks you would have responded in kind and acknowledged my correction
that other consensus inconsistencies discovered in implementations
thus far (none, that I'm aware of) could be classified as "maybe"; and
in doing so retained a semblance of a connection to a the technical
purposes of this mailing list.

@_date: 2015-10-01 01:08:44
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Pedantic note on the use of "eventual consistency" to 
Obligatory pedantic correction: In Bitcoin we don't actually achieve
"eventual consistency" of the kind which is usually described in the
literature. In Bitcoin the probability of reorg to a particular point
diminishes over time but never is guaranteed to be _zero_ (at least
within the framework of bitcoin itself), and at the same time we have
stronger ordering properties than is normally implied by eventual
consistency (so, e.g. an update may never happen if its conflicted
This is completely irrelevant to your point-- soft forks obey the
normal consistency process for bitcoin where a hard fork (especially a
mutual one) does not... but I'm sure there is an academic out there
that cringes when we use the words "eventual consistency" to describe
Bitcoin, and I feel like I'd be remiss to not offer this minor
correction. :)

@_date: 2015-10-02 16:37:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Crossing the line? [Was: Re: Let's deploy BIP65 
(a full
year before your post, as someone else had already responded to you on
Reddit; client mode was just what implementation inside Bitcoin was
But this is silly. The only point I was making was that when you were
referring to the limitations of BitcoinJ which would not generalize to
not state it as a property of SPV I think it is preferable to make
that decision,especially when it would not generalize to ones that
implemented everything described in section 8, or even just more
complete checks on the data they were already receiving. Who coined
the tern is irrelevant to that (although you indisputably did not use
even the abbreviation before others). Jtimon's later post on the
misuse of fallacious arguments should have been enough that I
shouldn't have to spell this out.

@_date: 2015-10-02 16:45:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
They discuss a very old version of the Cuckoo cycle paper, and I
believe none of their analysis is applicable to the most recent
revision. :(
In any case, I commented more about functions of this class here:
I don't believe changing the POW function is impossible in principle,
but I expect it would only happen due to problems with the composition
of current hash-power and not even if it were universally agreed that
some other construction were technically better (though that is a high

@_date: 2015-10-05 18:04:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
On Mon, Oct 5, 2015 at 5:26 PM, Tom Zander via bitcoin-dev
I think rather successfully. That Mike himself continues to misexplain
things is not surprising since he has all but outright said that his
motivation here is to disrupt Bitcoin in order to try to force his
blocksize hardfork on people. Since this motivation is uncorrelated
with any property of soft-forks or CLTV we should not expect his
position to change.
You're repeating Mike's claims there-- not anyone elses. Take your
complaint up with him-- not the list.

@_date: 2015-10-05 18:35:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
I don't agree-- I think you've made the mistake of just accepting the
particular framing that Mike has provide; one that (no shock) only
supports his conclusions.
I am aware of no instance where an active contributor to core has made
the claim that no change to consensus can happen without 100% support
(and doubly so, 100% including people who are expressly trying to
disrupt the project by posing opposition which, as you note, is
largely unrelated to the merits of the proposals). Mike has lead you
to believe people have claimed this, but no one has-- it's a view
which is simple, clear, and completely not reflecting reality. Don't
fall for strawman arguments.
In this situation it is also a particularly strong apples/oranges comparison:
Soft forks can happen at any time at the whim of miners-- no
technology which we are aware of (beyond the technology of
centralization) is able to prevent them-- they are not necessarily
even detectable; on this basis they are categorically different than
hard forks.
Moreover, the space of soft-forks the contributors to Bitcoin Core
would ever consider is a tiny space of all possible soft-forks, and
are ones which cannot be rationally understood to meaningfully
undermine the properties provided by the rules enforced within the
software; again making them different from some other proposals and of
a lesser concern.
Finally, the behavior of the technology arising from the inherent
compatibility, radically lowers (in most of our experience and
opinion) the cost of deployment; again-- making them different. They
prevent a industry wide flag day, and tight release synchronization
which is harmful to decentralization promoting software diversity.
As I think I commented in one of my messages-- I respond to the
technical arguments not because I believe they are earnestly
motivated, but because they provide an avenue for learning for myself
and others. Even someone trying to disrupt the process and nothing
else can help us learn by acting as an adversary that causes us to
extend our minds and understanding. The process for CLTV has been
ongoing for something like a year and a half and has little risk of
being substantially disrupted at this point.

@_date: 2015-10-05 19:41:30
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
On Mon, Oct 5, 2015 at 7:13 PM, Tom Zander via bitcoin-dev
Such a shame you appear to reserve that wisdom for those you disagree
with, biting your tongue when others emit all forms of ad hominem--
such as suggesting we've spent less volunteer time on Bitcoin and thus
our opinion has less merit (or that we haven't written certian kinds
of software (even when, ironically, we have!), and thus our opinion
doesn't have merit, and so on). I think everyone would benefit from
it, especially as that kind of correction is best received from
someone who agrees with you.
In this case, I think, however your correction is also misplaced at
least on this message; though I would otherwise welcome it.  I'm not
complaining about the man; but pointing out the behavior of stating an
opinion no one as held as theirs and attacking it is not a productive
way to hold a discussion. It's an argument or a behavior, not a
person, and beyond calling it bad I attempted to explaining (perhaps
poorly) why its bad.
What Sergio is saying is not the same; Mike argued some established
criteria existed where it didn't-- and I was pointing that out; and
talking about how the situation here is not very similar to the one
that Mike was trying to draw a parallel to. I enumerated a number of
specific reasons why this is the case. If the differences between
Sergio's comments and mine are still unclear after this clarification,
I'd be glad to talk it through with you off-list-- in spite of your
(welcome) compliments, communication is just fundamentally difficult,
and no amount eloquence changes that. If there is continued
misunderstanding, I do not doubt its my fault; but it's probably not a
good use of hundreds/thousands of people's time for you to help me
interactively improve my explanation on list. :)

@_date: 2015-10-05 20:56:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
On Mon, Oct 5, 2015 at 8:35 PM, Tom Zander via bitcoin-dev
What "consensus rule" do you refer to?
Indeed, I suggest you look to actions-- it's not hard to find changes
in Bitcoin Core that one contributor or another disliked. Did you try?
 (In this case, I don't even believe we have any regulator
contributors that disagree).

@_date: 2015-10-05 21:26:01
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
On Mon, Oct 5, 2015 at 9:08 PM, Tom Zander via bitcoin-dev
I'm providing some perspective and scope-- referencing again your
comment about following actions-- what element of the many dozens of
responses suggests to you that _anyone_ is not being listened to?
While I'm sure its not intended; your selective editing ends up
butchering the meaning---- I pointed out that there have been
disputes, even ones involving regular contributors (and, implicitly,
that I'm not lying by omission in not mentioning that the dispute was
a joke or from someone well known to attack Bitcoin) or-- in other
words, evidence that the disagreement was not less meaningful than
what you're talking about here. That's all, sorry I was unclear again.
Did you see in my message that I invited you to take a look for
examples-- I think they're easily found and you would find it
informative. I really recommend spending some time looking.

@_date: 2015-10-05 21:30:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
On Mon, Oct 5, 2015 at 9:27 PM, Peter R via bitcoin-dev
Allow me to state unequivocally-- since we've had problems with people
stating non-factuals as fact without getting adequately clear
correction--, there is no gridlock here and an effort to manufacturer
one for political reasons will not be successful.

@_date: 2015-10-08 20:03:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] merged multisig inputs 
Yes, the signatures for separate inputs are entirely separate.

@_date: 2015-10-21 07:48:39
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
Flagability cannot work recursively which is necessary for any
improvement to be useful for multi-phase protocols. (which, I think,
is the only real application of this class of improvement-- third
party mutation can be prevented by enforced canonical encodings;)
One still wants sighash flags--, but they're going to inherently
result in malleability.
I'm still sad that uniform segregated witeness is so hard to deploy,
adding another id to every utxo set won't be a nice cost. :( But I
have been trying for a long time to come up with anything better and
not being successful.

@_date: 2015-10-21 08:26:47
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
Oh good. Luke solved it.
To deploy SW without a disruptive flag day this encoding could be used:
A new P2SH like scriptPubkey type is defined. In the soft-fork, the
scriptsig for this scriptPubkey is required to be empty.
Signatures are not covered under txid, but carried along side. Then
committed to in blocks in a separate hashtree.
The only disadvantage to the approach used in elements alpha that I
can come up with so far (in the few minutes since luke turned my can't
into a can) is that that the approach in EA did not disrupt the normal
relay handling process, and this would, since relay that transports
the extradata either needs to use a different hash that includes the
witness, or have a separate mechanism for witness transport.

@_date: 2015-10-21 10:14:01
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
"sort of"
Using the sighash normalization doesn't allow creating a utxo set or
scanning the blockchain while only transferring ~1/3rd of the data
(allowing for reduced security fast start, and private lite wallets);
it requires txin ID rewriting when the witness changes on a parent
transaction; it requires hashing each transaction multiple times (for
the normalized ID, and the old ID), it requires storing two IDs for
every transaction in the UTXO set. -- but indeed, it's easier to
deploy (though not infinitely easier as I thought before).

@_date: 2015-10-21 19:27:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
On Wed, Oct 21, 2015 at 6:22 PM, Danny Thorpe via bitcoin-dev
Indeed they are different, but canonical encoding enforcement prevents
the third party malleability completely on ordinary transactions.
It is an an _immediate_ solution which is already deployed as a
standardness rule-- once miners update to 0.11.1 or 0.10.3 (or
equivalent) only miners will be able to malleable ordinary payments,
to the best of our current understanding.
The thing being discussed here does not provide an immediate benefit
to that particular issue.  It addresses multistep contracts and other
But it does not prevent third party mutation until people change their
public keys to new scheme (which based on p2sh we should expect a well
over a year deployment), which they cannot being doing until a soft
fork is made and settled in the network, for which the code is not yet
written. CLTV suggests that the current timeframe for a soft fork is
around a year and though I'd like to see that improved.
So canonical encoding is both sufficient (to the best of our current
understanding) for preventing third party malleability on ordinary
transactions, and the _only_ option for to have an actually immediate
Please don't mix up third party malleability with this work which is
important in its own right.

@_date: 2015-10-22 08:57:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP] Normalized transaction IDs 
On Thu, Oct 22, 2015 at 8:26 AM, Christian Decker via bitcoin-dev
For ordinary transactions which are not performing interesting smart
contracts that particular is better addressed via canonical encoding,
which is immediately available and doesn't have the associated costs
(new pubkey type adoption, 20%-30% UTXO size increase, need for nodes
to fixup txid references, etc.).
Please, as I said up-thread: this is good and importantstuff to work
on, but it shouldn't be oversold.

@_date: 2015-10-29 08:17:27
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
The word "database" is likely confusing people here.  This is not a
database in an ordinary sense.
The bitcoin core consensus engine requires a highly optimized ultra
compact data structure to perform the lookups for coin existence. The
data stored is highly compressed and very specialized, it would not be
useful to other applications.  Right now, on boring laptop hardware,
during network synchronization updates to this database run at over
10,000 records per second, while the system is also busy doing the
other validation chores of a node. This is backended by a high
performance transactional key value store.  The need for performance
here is essential to even keeping up with the network, it's not about
enabling any kind of fancy querying (bitcoin core does not offer fancy
querying), it's about the base load that every node must handle to
usably sync up and keep up with the Bitcoin network.
The backend can be swapped out for something else that provides the
same properties, but doing so does not give you any of the
inspection/analytics that you're looking for.  Systems that do that
exist, and they require databases taking hundreds of gigabytes of
storage and take days to weeks to import the network data.  They're
great for what they're for, but they're not suitable for consensus use
in the system for space efficiency, performance, and consensus
consistency reasons.

@_date: 2015-10-30 03:35:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
On Fri, Oct 30, 2015 at 3:04 AM, Simon Liu via bitcoin-dev
This sounds like a misunderstanding of what consensus criticial means.
It does not mean that it must be right (though obviously that is
preferable) but that it must be _consistent_, between all nodes.
Because it provides no value, the data is opaque and propritarily
encoded with a compression function which we may change from version
to version, and because many of these alternatives are enormously
slow; enough that they present problems with falling behind the
network even on high performance hardware.
Moreover, additional functional which will not be sufficiently used
will not adequately maintained and result in increased maintains costs
and more bugs.

@_date: 2015-10-30 04:28:47
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
If you add to those set of assumptions the handling of write ordering
is the same (e.g. multiple updates in an change end up with the same
entry surviving) and read/write interleave returning the same results
then it wouldn't.
But databases sometimes have errors which cause them to fail to return
records, or to return stale data. And if those exist consistency must
be maintained; and "fixing" the bug can cause a divergence in
consensus state that could open users up to theft.
Case in point, prior to leveldb's use in Bitcoin Core it had a bug
that, under rare conditions, could cause it to consistently return not
found on records that were really there (I'm running from memory so I
don't recall the specific cause).  Leveldb fixed this serious bug in a
minor update.  But deploying a fix like this in an uncontrolled manner
in the bitcoin network would potentially cause a fork in the consensus
state; so any such fix would need to be rolled out in an orderly
It's not unreasonable, but great care is required around the specifics.
Bitcoin consensus implements a mathematical function that defines the
operation of the system and above all else all systems must agree (or
else the state can diverge and permit double-spends);  if you could
prove that a component behaves identically under all inputs to another
function then it can be replaced without concern but this is something
that cannot be done generally for all software, and proving
equivalence even in special cases it is an open area of research.  The
case where the software itself is identical or nearly so is much
easier to gain confidence in the equivalence of a change through
testing and review.
With that cost in mind one must then consider the other side of the
equation-- utxo database is an opaque compressed representation,
several of the posts here have been about desirability of blockchain
analysis interfaces, and I agree they're sometimes desirable but
access to the consensus utxo database is not helpful for that.
Similarly, other things suggested are so phenomenally slow that it's
unlikely that a node would catch up and stay synced even on powerful
hardware.  Regardless, in Bitcoin core the storage engine for this is
fully internally abstracted and so it is relatively straight forward
for someone to drop something else in to experiment with; whatever the
I think people are falling into a trap of thinking "It's a ,
I know a  for that!"; but the application and needs are
very specialized here; no less than, say-- the table of pre-computed
EC points used for signing in the ECDSA application. It just so
happens that on the back of the very bitcoin specific cryptographic
consensus algorithim there was a slot where a pre-existing high
performance key-value store fit; and so we're using one and saving
ourselves some effort.  If, in the future, Bitcoin Core adopts a
merkelized commitment for the UTXO it would probably need to stop
using any off-the-shelf key value store entirely, in order to avoid a
20+ fold write inflation from updating hash tree paths (And Bram Cohen
has been working on just such a thing, in fact).

@_date: 2015-09-01 02:25:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Let's kill Bitcoin Core and allow the green 
On Tue, Sep 1, 2015 at 2:16 AM, Peter R via bitcoin-dev
Blockstream currently has no interest in maintaining a separate
implementation of Bitcoin.
At this time I believe doing so would have significantly negative
value; especially in light of the current climate where people are
conflating a tremendously destructive bifurcation of the Bitcoin
ledger with mere (and far more boring) alternative implementations.

@_date: 2015-09-03 06:57:44
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] block size - pay with difficulty 
I really cannot figure out how you could characterize pay with
difficty has in any way involving idle hashpower.
Can you walk me through this?

@_date: 2015-09-03 17:57:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] block size - pay with difficulty 
Sorry, I'm still not following.  I agree that predictability is important.
I don't follow where unpredictability is coming from here. Most (all?)
of the difficulty based adjustments had small limits on the difficulty
change that wouldn't have substantially changed the interblock times
relative to orphaning.
What the schemes propose is blocksize that increases fast with
difficulty over a narrow window. The result is that your odds of
producing a block are slightly reduced but the block you produce if
you do is more profitable: but only if there is a good supply of
transactions which pay real fees compariable to the ones you're
already taking.  The same trade-off exists at the moment with respect
to orphaning risk and miners still produce large blocks, producing a
larger block means a greater chance you're not successful (due to
orphaning) but you have a greater utility.  The orphing mediated risk
is fragile and can be traded off for centeralization advantage or by
miners bypassing validation, issues which at least so far we have no
reason to believe exist for size mediated schemes.
As you know, mining is not a race (ignoring edge effects with
orphaning/propagation time). Increasing difficulty does not put you at
an expected return disavantage compared to other miners so long as the
income increases at least proportionally (otherwise pooling with low
diff shares would be an astronomically losing proposition :)!).
Pay-for-size schemes have been successfully used in some altcoins
without the effects you're suggesting.

@_date: 2015-09-03 19:17:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] block size - pay with difficulty 
Ah, that would clarify things for me me.
Please everyone try to speak specifically enough to catch details like that.

@_date: 2015-09-03 23:18:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposed minor change to BIP 01 to use a PR for 
The process in BIP01 was written when we used a different solution for
storing and presenting BIPs.
I'm thinking of suggesting that the number request process be changed
to opening a pull req with BIP text with no number (e.g. just using
the authors name and an index as the number) as the mechenism to
request number assignment.
Is there any reason that anyone would find this objectionable?
(Please do not respond to this message with anything but a strictly
directed answer to that question, start a new thread for a different
subject. Thanks!)

@_date: 2015-09-04 00:24:18
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposed minor change to BIP 01 to use a PR for 
We used to have a WIKI page for all the BIP stuff and that worked
better IMO, the use of git(hub) for it was a step forward in a number
of ways but made the number assignment part an odd duck. We should
have fixed it then, but it wasn't obvious (enough) that it needed
fixing at the time. Live and learn.

@_date: 2015-09-18 20:31:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
On Fri, Sep 18, 2015 at 8:27 PM, Matt Corallo via bitcoin-dev
uh. There is fairly little global consistency in DST usage. Lots of
places do dst on different dates.
So if it's in some DST timezone it's likely to move twice each change
for some subset of the people who do it.
E.g. europe and US end DST one week apart.

@_date: 2015-09-23 19:24:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Weak block thoughts... 
See also my response to Peter R's paper that was republished to the
list at (See sections at "For example, imagine if miners only include
transactions that were previously committed" and especially "Miners
volutarily participate in a fast consensus mechenism which commits to
On Wed, Sep 23, 2015 at 3:43 PM, Gavin Andresen via bitcoin-dev
Take care, here-- if a scheme is used where e.g. the full solution had
to be exactly identical to a prior weak block then the result would be
making mining not progress free because bigger miners would have
disproportionately more access to the weak/strong one/two punch. I
think what you're thinking here is okay, but it wasn't clear to me if
you'd caught that particular potential issue.
Avoiding this is why I've always previously described this idea as
merged mined block DAG (with blocks of arbitrary strength) which are
always efficiently deferentially coded against prior state. A new
solution (regardless of who creates it) can still be efficiently
transmitted even if it differs in arbitrary ways (though the
efficiency is less the more different it is).
There is a cost to these schemes-- additional overhead from
communicating the efficiently encoded weak blocks. But participation
in this overhead is optional and doesn't impact the history.
I'm unsure of what approach to take for incentive compatibility
analysis. In the worst case this approach class has no better delays
(and higher bandwidth); but it doesn't seem to me to give rise to any
immediate incrementally strategic behavior (or at least none worse
than you'd get from just privately using the same scheme).
On Wed, Sep 23, 2015 at 4:28 PM, Peter R via bitcoin-dev
The income to miners as a whole doesn't depend on these sorts of
optimizations, competitive advantages do... so better common open
infrastructure helps mostly in the case of putting propagation
disadvantaged miners on an equal playing field. You'll note that none
of them are exactly sharing their SPV mining source code right now....
in any case, there are simple, expedient, and low risk ways to improve
their equality in that respect: centralize (e.g. use bigger pools).

@_date: 2015-09-27 01:39:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Weak block thoughts... 
Unless the weak block transaction list can be a superset of the block
transaction list size proportional propagation costs are not totally
As even if the weak block criteria is MUCH lower than the block
criteria (which would become problematic in its own right at some
point) the network will sometimes find blocks when there hasn't been
any weak block priming at all (e.g. all prior priming has made it into
blocks already).
So if the weak block commitment must be exactly the block commitment
you end up having to add a small number of transactions to your block
above and beyond the latest well propagated weak-blocks... Could still
work, but then creates a pressure to crank up the weak block overhead
which could better be avoided.

@_date: 2015-09-27 19:50:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Weak block thoughts... 
Keep in mind, because of efficient differential transmission the cost
to you is effectively nothing if your transaction acceptance policy is
predictable, it's a hand-full of bytes sent. And by failing to send
yours you do little to nothing to deny others the improvement.
Lets imagine an alternative weak-blockless weak block implementation:
Every N seconds, every miner send to every other miner what they're
working on.  This isn't totally crazy-- efficient differential
transmission will keep the amount transmitted small.
Any block found can be referenced to any of these earlier worklists.
What the effect be of not transmitting yours?
If your block is unlike everyone elses, you would suffer great delays
in the event you found a block.
If your block is mostly like everyone elses, you wouldn't suffer as
much delay-- but the transmission costs would be negligible in that
case. ... the size sent is proportional to the improvement you get
when finding a block.
In either case, no one else is harmed by you not sending yours... they
still send their lists.
A problem with that scheme is that unless you've layered an identity
based access control system on it anyone can DOS attack it, because
anyone can send as much as they want, they don't even have to be
actual miners.
What weak blocks adds to that is using hashcash as a rate limiting
mechanism-- a coordination free lottery weighed by hash-power decides
who can transmit.
What if you don't participate in the lottery and share your solutions?
 No major harm for the other users... the other users will just choose
a somewhat lower weak-block threshold to get the updates at the
desired rate than they would otherwise. To the extent that what you
were working on was different from anyone else, you'll suffer because
you failed to make use of your chance to influence what could be
efficiently transmitted to include your own blocks.
You could also ask a question of why would you transitively relay
someone elses announcement-- well if it helped their blocks too  (by
reflecting things they also want to mine) the answer is obvious. But
what if it was disjoint from the things they wanted to mine and didn't
help compared to the weak blocks they already relayed?  In that case
it's still in likely in their interest to relay it because if a block
similar to it is produced and they extend that block they may end up
orphaned because of propagation delays their parent block suffered.
What if they receive an announcement which is so "ugly" that they
wouldn't extend the chain with the strong block version of it (they'd
intentionally try to fork it off?)-- in that case they wouldn't want
to relay it.  So much the same logic as why you relay other parties
blocks applies, including-- relaying helps the network, but if you
don't it'll still get along fine without you.

@_date: 2015-09-29 18:31:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I'm surprised to see this response-- BIP65 is a year old now which is
plenty of time to mature and for issues to be uncovered. It (and its
predecessors) have had extensive discussion-- with no controversy
exposed during its entire lifetime, but in any case...
I am having a little difficulty making sense of this complaint. For
all any of us know miners are already enforcing the validity of CLTV,
it's indistinguishable on the visible behavior.  At the same time in
BitcoinXT's 101 proposal the change in system rules is similarly
"invisible" to existing "SPV" wallets in the same way that enforcement
of CLTV is "invisible": both are no change from their perspective.
Have I missed a proposal to change BIP101 to be a real hardfork (e.g.
be invalid from the perspective of historical bitcoinj clients too)?
---- I'd think it to be completely reasonable to do so, even while not
thinking that it would be reasonable here:  Softforks and hardforks
are not the same thing, not technically, and not politically. Miners
can collectively, at their whim, impose any kind of soft fork they
want, at any time and you won't even necessarily be able to tell...
that is just how the system works. Hardforks on the other hand, can
only happen with the consent of the participants-- they can directly
violate system properties that the participants believe to be largely
nonvolatile, and they _force__ software upgrades, so I think having a
higher bar makes good sense there.
The particular mechanism used in the proposal as-is has been used many
times before (and has been refined over time) and we have considerable
experience with it. The behavior is not, in fact, truly invisible to
non-upgraded participants: it's is visible by way of the block version
changing.  Bitcoin Core, going back years, responds by issuing a
warning-- "%s: %d of last 100 blocks above version %d\n" which then
becomes "Warning: This version is obsolete; upgrade required!".  Users
of the software (directly or via automation) are free to decide to
take whatever policy action they wish to take, delay accepting
transactions, patching software, etc..  The same could be done by any
client of the system if they cared to do so.
I believe the versionbits mechanism will be superior, but-- among
other things-- its deployment has been complicated by BitcoinXT
deploying an incomplete approximation of it.  Versionbits primary
advantage is related to having multiple concurrent proposals in
flight, which will be good to have but isn't itself a reason not to
pull a proposal up ahead of versionbits.

@_date: 2015-09-29 20:31:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Are 'soft forks' misnamed? [was: Let's deploy BIP65 
As an aside, this list loses utility if people insist on taking
tangential questions to the list in the middle of threads. It's
preferable to either split the thread or take the message off list.
The naming arose from a series of historical naming-by-comparisons:
The bitcoin network has self-arising forks in state when miners
concurrently create blocks. These are natural, unavoidable, and
If a nodes enforce different and incompatible rules-- for example,
some decide to require that the subsidy stay at 25 BTC forever, then a
fork may come into existence which is not self resolvable.
Thus the term hardfork arose to talk about rule changes which were
incompatible with the existing network, would require all users to
upgrade, would exclude all non-consenting users from the resulting
system, and which have the power to arbitrarily rewrite rules.  This
is in contrast to "forks" which are boring, natural, and happen every
Its often possible to make critical fixes and powerful improvements to
the Bitcoin consensus rules by using the unavoidable power of miners
to filter transactions according to their own rules.  New features and
fixes can be carved out of existing "do anything" space in the
protocol: like carving a statue out of a block of marble. Doing so
reduces the incidence of flag days which are costly to coordinate and
actively risky for users and avoids forcing constant software churn,
which is bad for decentralization. Such changes are a strict narrowing
of permissible actions. And as such, so long as they have a
super-majority hashpower behind them any network forking that happens
to result from them is automatically self-resolving.
So by contrast with hardfork the term softfork came into use to
describe these _compatible_ protocol rule changes.
There is explicit support for compatible rule changes the bitcoin
protocol in the form of no-op opcodes and free form, non-enforced,
version fields (for example). Every fix or enhancement you've heard
about to Bitcoin's consensus rules (going back to the system's
original author) was performed via some form of this mechanism.
In the modern form, the behavior to be soft-forked out is first made
non-standard (if it wasn't already-- they almost always are) meaning
that participants will not relay, mine, or display unconfirmed txn in
their wallets transactions which violate the new rule.  But if a
violation shows up in a block, the block is still accepted.  After
that the blockchain itself is used to coordinate a vast super-majority
of hashpower (recently 95% has been used) agreeing to enforce the new
rule which results in confidence confidence of low disruption on
account of the enforcement. Then when the threshold is reached, they
enforce (automatically).  Old software continues to enforce all the
old rules they always enforced, the only difference in behavior
relates to non-standard transactions and contests between otherwise
valid blocks.  Even unupgraded participants can tell that the network
is doing something new on account of the block version changing (and,
for example, Bitcoin Core warns users about this).
The primary disadvantage of this approach is that it only allowed you
to carve functionality of of "do anything" space, which is quite
natural for some features (especially since the Bitcoin protocol
includes tons of do anything space)--- e.g. height in coinbase, DER
strictness, transactions that have integer overflow creating a
kazillion coins-- but less natural for others.
Of course, it's always possible for the majority of hashpower to have
hidden transaction exclusion rules that _no one_ but them knows about
and this cannot be prevented, but at least the mechanism proscribed in
modern soft-forks is transparent (the network tells you that its doing
something you don't understand).

@_date: 2015-09-30 02:57:52
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
Actually getting rid of the immediate bit forcing was something I
considered to be an advantage of versionbits over prior work.
Consider,  where possible we carve soft fork features out from
non-standard behavior.  Why do we do this?  Primarily so that
non-upgraded miners are not mining invalid transactions which
immediately cause short lived forks once the soft-fork activates.
(Secondarily to protect wallets from unconfirmed TX that won't ever
The version forcing, however, guarantees existence of the same forks
that the usage of non-standard prevented!
I can, however, argue it the other way (and probably have in the
past):  The bit is easily checked by thin clients, so thin clients
could use it to reject potentially ill-fated blocks from non-upgraded
miners post switch (which otherwise they couldn't reject without
inspecting the whole thing). This is an improvement over not forcing
the bit, and it's why I was previously in favor of the way the
versions were enforced.  But, experience has played out other ways,
and thin clients have not done anything useful with the version
A middle ground might be to require setting the bit for a period of
time after rule enforcing begins, but don't enforce the bit, just
enforce validity of the block under new rules.  Thus a thin client
could treat these blocks with increased skepticism.

@_date: 2015-09-30 20:15:31
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Yes, your article contained numerous factual and logical inaccuracies
which I corrected (many of which you had been previously corrected on
as well by others). (For example:
I would have hoped that after so many corrections you would have
updated your beliefs.
Yes, because what 101 does is not a hard-fork from the perspective of
BitcoinJ clients. Please do not conflate BitcoinJ with all of SPV; a
SPV client could validate the information received more extensively or
respond to alerts in reject rule violating blocks--  BitcoinJ does
not, but this is BitcoinJ's design decision to lack security in this
respect and not something inherent to SPV).
Directly fixing the time-warp attack, for example, would be a hard
fork from the perspective of BitcoinJ clients.  Recovering the fixed 0
bits in the header for use as extra-nonce would be a hard fork from
the perspective of BitcoinJ clients. Changing the transaction format
to include an explicit nonce for ECDH (e.g. stealth addresses) would
be a hard fork from the perspective of BitcoinJ clients. Increasing
the precision of Bitcoin by 1000 would be a hard fork from the
perspective of BitcoinJ. As would adjusting the hashtree to commit to
fees, including fees under OP_CHECKSIGs hash, or switching to the
segregated witness commitment structure from elements alpha that
allows syncing the chain without fetching signatures... all that would
be hardforks from the perspective of BitcoinJ.
Because of an cheaply avoidable the lack of validation in BitcoinJ no
increase of the blocksize is a hard-fork from its perspective. Nor
would increasing the subsidy to miners, or allowing third parties to
confiscate coins. But other SPV clients could, if they wanted to,
reject blocks the violated most of these criteria.
The argument you are presenting against BIP65 is that it is bad
because it is silently accepted. But this applies no less to 101 for
SPV clients, and in 101's case it's a failure to enforce pre-existing
rules which the users might care a great deal about. Worse, counting
on this kind of behavior can build a dependence on weak security forms
of SPV and inhibits the use of full security SPV.
In truth, both of BIP101 and BIP65 are detectable even by the most
simplistic and pre-change clients due to the voluntarily use of block
version signaling. Any participant in the network is free to take
whatever action they choose to take in response to such an event.
Bitcoin Core's behavior is to issue alerts to the user when unexpected
block versions show up on the network.  Users and implementer are free
to turn changes like BIP65 into hardforks from the perspective of
their own system, necessitating manual intervention, by simply forcing
the block version to be a particular value (or shutting down when
there are many blocks of a new version; until manually authorized to
For many changes, including CLTV the actual soft fork change is by far
the most natural way of implementing the change itself. One simply
takes an existing non-standard placeholder op code sequence and
assigns it the new VERIFY style meaning. It is clean, tidy, and the
result is nearly as if the system has had it all along. The only
complexity is around the activation and can be dropped in future code.
Beyond that, the primary upside is no forced industry wide "flag day"
where everyone is _forced_ to modify their software arises, taking
considerable cost.  People who care about the new rule can use it,
people who don't don't. All the rules that you care about enforcing
remain in force-- you still prevent inflation, you still will not
tolerate the theft of your own coins (or those of most other people),
etc.. No one is necessarily caught by surprise since the block
versions communicate that something is happening, allowing network
participants to choose to act (or not).
For example, for years you stonewalled P2SH and multi-signature.  You
didn't care about it. You didn't think it was valuable. You didn't add
it to your software, even after it was well specified and deployed in
production. Could it have been done as a hard-fork?  Likely not: you
would have prevented it. But as a soft-fork you were free to ignore it
with no ill-effect for a long time existing for those who cared about
it, and not for you, until widespread use resulted in demand enough to
justify accepting a patch that permitted sending to it.
What if we'd needed a hard fork to enable CoinJoin or other privacy
features?  I think would have blocked that too.
The relative ease of handling soft-forks which you are indifferent to
means that there is little reason to object to a compatible change
that gives other people flexibility they care about greatly but which
you are indifferent to; and it forces people who would oppose a
functionality because they don't want others to have some piece of
freedom to try to frame justifications in language other than "I don't
think it's worth the cost" since they have the nearly free option of
ignoring the change-- they're forced to actually argue against other
people having that freedom.
Soft-forks also allows us to deploy fixes to the Bitcoin protocol
which are more important-to-have but not urgently critical (like
height in coinbase),  or sometimes to deploy fixes to critical
vulnerabilities without first handing everyone excruciatingly detailed
instructions on exploiting them, simply by closing off an pattern of
protocol which is obviously bad and risky.
The primary cost of a soft-fork for non-participants is simply some
risk of increased network instability around the change-- but short
lived forks happen every day, and longer lived ones happen from time
to time. Larger amounts of instability occur from time to time due to
network partitioning, misconfiguration, and software bugs-- and client
software must be prepared to cope with it; this is a fact of bitcoin
and decenteralized systems in general. Upgraded, change enforcing,
client software is not exposed to the this instability, and
non-upgraded software could choose to mitigate any exposure by
monitoring the block versions. This is a far better situation that the
natural instability that will happen from time to time in a
decentralized system.
By contrast, the programmed activation point of BIP101 at 75% almost
guarantees activation among considerable controversy, promising
network instability which BitcoinJ clients would experience upgraded
or not, even if the larger block side was ultimately the losing side
in the switch. I find it more than a little strange that you think the
instability of a 75%-version-hashpower cut is acceptable but the a
95%-version-hashpower compatible change is not.
Finally, there is the demonstrated track record: They work; they
deliver new features to people. Our experience in the half dozen or
more soft-forks in the system so far is that in practice do not cause
significant problems, including financial losses for SPV wallet users.
Even with that complete success there has been room for improvement,
which is why the process has evolved over time to feature things like
preemptive non-standardness, high switchover thresholds, etc.  and
these will continue to evolve over time.
I hope that you can put aside your effort to force a blocksize
increase on others for a moment and add functionality, of the kind the
Bitcoin Core has had for years, to BitcoinJ to improve the experience
with soft-forks if you think it isn't good enough as is...

@_date: 2015-09-30 22:59:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
The term comes from the Bitcoin whitepaper.
They run them correctly with respect to the rules that they implement,
nothing about a soft-fork changes that.
The system could have been designed in a way that wasn't full of nice
compatibility features. The history of bitcoin could have been that
past improvements were all performed with hard forks instead of soft
But neither of these things are true. (And I think it's very likely
that there would have been fewer improvements if that were the case).
At the end of the day we need to deliver software to our users that
delivers on their needs and doesn't undermine their privacy or
security;  even if its really hard.  So when someone calls out
something that I'm working on that could use improvement, my response
isn't to tell them how much I'm not going to listen to them because
I've accomplished some long list of things that they haven't; that
software I've written runs on hundreds of millions of devices; ...
rather my response is to hear out their concerns, even when due to
extensive context I'm confident that they are probably confused or
dishonestly motivated; because there is always a potential to learn,
and always a potential to do better.  I have found this to be pretty
productive, as even when both parties walk away with the same
positions they started with, I usually learn something along the way
just because I paid attention.
BIP16 was published on 2012-01-01. Enforcement on the network began on
April 1st 2012.
Support for merely sending to P2SH addresses was merged into BitcoinJ,
Nov 30th _2013_, after it was written by Mike Belshe.
In the interim you spent considerable time arguing against
implementing it, e.g. in one example incident:
--- Day changed Thu Sep 12 2013
10:03 < TD> heck, if a recipient really really wants to receive a p2sh
payment for some reason, they can just put the p2sh output into the
payment request message
10:17 < TD: In any case, P2SH has been deployed for
something like two years now. Your arguement seems to be basically we
should be creating a false tying between payment protocol messages and
things like escrow usage in order to coerce people to adopt the
payment protocol in places where an address would do.
10:17 < I think thats a bit cruddy.
10:19 < TD> no, i'm saying p2sh is a feature that just isn't usable. i
did point this out at the time it was merged - gavin believed that
apps to do complicated multi-device wallets would appear before a
payment protocol did, and people wouldn't like the look of long
addresses. that was pretty much the rationale given. that didn't
happen, obviously
10:20 < TD> developing features that are used only by bitcoind
developers, isn't the right way to go, and p2sh definitely falls into
that category
10:35 < TD> i'm not going to do it myself because anyone who is
capable of producing and running something that uses p2sh is capable
of working with the payment protocol as well, and that gives a better
user experience overall. but if someone else wants to, go for it.
I don't think there is anything wrong about calling this stonewalling.
At least thats how it came across to myself and others. I'm sorry if I
judged too harshly there.
To be clear, by pointing out your past opposition and non-deployment
in this message I am _not_ trying to attacking you for failing to
support P2SH.
I pointing out that right or wrong..... That you actively argued
against it. That you chose not to implement it, and only accepted a
patch for it a year and a half later. From your own words it seems
clear that you didn't implement it due to actual opposition, but even
if the non-implementation was simply engineering priorities, the fact
remains that you didn't implement for a very long time.
And that is _okay_, we still got it anyways, and today tens of
thousands of transactions per day use it and P2SH secures about 10% of
all Bitcoin value.  This is possible because with a soft fork users
using other software can gain functionality which might be critical to
them (As Jgarzik was saying about Bitpay in the discussion I was
quoting from) that you don't have the time or interest to implement in
your own software.
Yes, Matt Corallo added it to code which by your admission no one was
using.  I agree this is not relevant.
I think it's likely that I've spent significant more time unpaid on my
evenings and weekends creating software for others than you have (and
continue to do so; as well as having donated years worth of income
supporting other people's Free Software work), but it's a bit of an
unfair comparison: I'm a fair bit older than you. :)
And yet, I think that is all irrelevant here because I'm not current
criticizing bitcoinj for lacking features!   Quite the opposite, I am
pointing out that the advantage of soft-forks is that its OKAY for
software to lack soft-fork features,  which means that participants
who code only on evenings and weeks are free to continue participating
with the priorities they choose!
Forcing _all_ upgrades to be via hard-fork takes away the freedom to
make that trade-off; and concurrently reduces the collection of fixes
upgrades we could potentially deploy; because will always be
implementations out there like BitcoinJ in 2012 that didn't have the
resources (or interest) to fully implement this feature or that
feature, at least not right away.
And for many things, they simply don't have to, and that should be okay.
We handle this in Bitcoin Core. Our chosen and intentional way to
handle this is setting a notice. This gives users the freedom to do
what they like, while also behaving in a reasonably sane way by
You don't have to like it, you can behave differently in your own
software or on your own hosts-- all the data is available to you.
(I wouldn't object out of principle to a default config option to take
more aggressive action on unexpected versions... but no one has ever
asked for one... and I'm doubtful anyone would ever do so.)
They continue to enforce all the same rules as before. With the soft
fork Bitcoin Core users are informed that unexpected things are going
on, and they are free to look at whats going on and decide how to
handle it, or just accept that the new thing is almost certainly
something they don't care about (after all, the rules they signed up
for before are all still in effect, and at any time miners could be
silently imposing new 'soft fork' like rules without their knowledge--
having a big reaction to ones the network was kind enough to tell them
about doesn't seem that reasonable).
For many users and many soft-forks there is no substantial security
implication, and you cannot say that they were not getting the
security level they were getting before. But regardless, even what it
is different, they're free to decide on the cost tradeoff with
upgrading, and they're not forced onto an upgrade hamsterwheel that
disenfranchises their role in the system.
If you have a specific generalized security implication in mind,
you're failing to state it. In your writings you like to assert that I
 "did not respond" or were "not convincing"-- that is not generally my
style, I don't usually think anyone owes me point by point answers,
but I think on this point it seems clear that there is some
implication which is in your head that is a mystery to at least myself
and Jgarzik.
A 75% measurement doesn't actually mean 75% support, due to variance.
Even ignoring that-- you recognize the acceptability of reorgs. The
situation is no worse for an SPV client for a soft-fork; and it's
better because (1) convergence is still guaranteed with exponential
probability (a hard fork can be mutual and no convergence may be
possible-- as is the case for more conceivable hard forks), and (2)
for BIP65 (and current soft forks generally) a _much_ more
conservative threshold is set (because in Bitcoin Core and the general
community around here considers 75% to be too low to achieve high
stability, based on our past experience).
A couple points:
That same invalid blocks for weeks (actually months) from BIP16 is the
behavior you will get with a hard fork, for at least the same reasons
(miners asleep at the switch). Much more for a controversial hard-fork
as there will be principled objections.
Blocks get produced that get orphaned every day and this is
unavoidable, so users already must deal with occasional cases where
confirmations get undone.
More recent soft-forks have reduced the incidence of invalid blocks by
substantially increasing the threshold, including better notification
in Bitcoin core, communicating directly with miners more, and making
non-conforming transactions non-standard in advance. These mitigations
have been effective in practice; and we have not seen the same
behavior (which, as, noted is not known to have enabled any fraud in
any case -- in part because to non-upgraded wallets it looks just like
the orphaning that normally happens but with somewhat increased
frequency.).  I think it's unfortunate that people proposing hard
forks have not learned the same lessons, even though the stakes are
higher and the self-resolution of the system is greatly diminished.

@_date: 2015-09-30 23:25:03
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
An extra way to look at this is that even absent any rule changes--
users who are asleep at the switch may lose effective security over
time because attackers learn new tricks against existing
vulnerabilities. Security requires a bit of vigilance, inherently.
In many specific cases I think it's hard-to-impossible to articulate a
concrete way that security is lost by users at all, excluding some
small amplification of orphan blocks.
On Wed, Sep 30, 2015 at 9:06 PM, Mike Hearn via bitcoin-dev
This is the outcome guaranteed for absentee miners with a hard fork,
but it is not guaranteed for a soft fork.
Miners who have changed their code in inadvisable ways can produce
invalid blocks as a result. There are many seemingly innocuous ways
one can produce invalid blocks, and miners have stumbled on a few of
them over the years.
Pedantically, modifying IsStandard() will not have this effect:
Unknown NOPs are now handled via a script validation flag--
SCRIPT_VERIFY_DISCOURAGE_UPGRADABLE_NOPS.  Experience (e.g. with
STRICTDER) has show that script validation flags are much more robust
to casual twiddling than IsStandard is.
The only way that script validation flags have been observed getting
bypassed in the field was a miner that had disabled all signature
validation completely (and whom had a not-completely-negligible amount
of hashpower. :( )... as it's a lot more clear that you might be
exposing yourself to trouble if you mess with the validation flags.
IIRC; There is no released version of Bitcoin that has IsStandard
which has failed failed to treat the NOPs as non-standard.
There was a brief time in git master between when IsStandardness was
relaxed and NOPs were addressed via a validation flag but I am
reasonably confident that didn't make it into a release.
Regardless, anyone actually running that code of that vintage would
already be incompatible with the current network already due to prior
soft forks.
And as a matter of fact, invalid CLTVs don't currently appear to get
mined. Checking this again pre-release would be a good checklist item.
For prior soft-forks we've monitored and tested for this (with the
goal of going and yelling at any broken miners to fix their behavior).
