
@_date: 2017-11-03 01:45:40
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Simplicity proposal - Jets? 
Hi everyone,
I agree that the paper could use some more details on the rationale
behind "jets". After a couple of reads, I think I can "ELI5 them":
As far as I understand, jets are a smart optimization that makes complex
Simplicity contracts way cheaper to compute (ideally, comparable to
Script or EVM).
For this purpose, jets leverage the most important element of the
Simplicity Bit Machine: the frames stack.
In a Simplicity program, every expression or sub-expression can be
thought of as a pure function that when applied on a certain initial
read frame, results in the active write frame having a different value.
This happens deterministically and without any side effects.
So, if the Simplicity interpreter finds some expression whose result
when applied upon a certain read frame is already known (because it has
already been executed or it was somehow precomputed), it doesn't need to
execute such expression step-by-step once again. Instead, it just need
to write the known result to the active write frame.
The paper suggests that at all times the interpreter knows the result of
applying many common operations on all possible combinations of inputs
in the range of 8 to 256 bits. In other words, the interpreter won't
need to calculate "123 + 321" or compare "456 > 654 because the results
of those expressions will be already known to it. These are stupid
examples, but the savings are real for hash functions internals,
elliptic curve calculations or even validation of signatures.
As said before, this can help making Simplicity programs lighter on CPU
usage, but it has many other benefits too:
+ Jets can replicate the behavior of complex chunks of Simplicity code
with the guarantee that they can't introduce side effects.
+ Interpreter-bundled jets are formally proven. The more a Simplicity
program relies on jets, the more it benefits from their safety. When
proving the soundness of your program, you can just ignore the jets,
assume they are valid and focus on your own logic.
The paper also suggests that different sets of jets could make up
different single purpose dialects, just like domain-specific languages
bring richer vocabulary and semantics to the bare syntax and grammar of
general-purpose languages.
I hope Russel or Mark can correct me if I got something totally wrong. I
must admit I really like this proposal and hereby declare myself a huge
fan of their work :)

@_date: 2017-11-03 09:46:16
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Simplicity proposal - Jets? 
Oops. That makes much more sense than what I said. Thanks a lot for the

@_date: 2017-11-03 17:42:38
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Simplicity proposal - Jets? 
If I did understand it right, you don't need to publish the Simplicity
code for the "jetable" expression.
That's the whole point of MAST. Each Simplicity expression can be
identified by its MAST root (the Merkle root of all branches in its
Abstract Syntax Tree).
Imagine you want to write a Simplicity script that is roughly equivalent
to P2PKH. Regardless of directly writing such script or using a higher
level smart contract language, you won't likely write for yourself the
part in which you compute the hash of the public key. Instead, you are
expected to include some external library providing hash functions or at
least copy and paste such function into your code.
As everyone is expected to use the same, let's say, RIPEMD160
implementation, it doesn't matter how you included such function in your
program. The point is that once you build the MAST for your program,
such function will be completely replaced by its MAST root---which is
nothing but a hash.
This way, when the Simplicity interpreter (the BitMachine) bumps into
the hash, it can look for it in a predefined jets dictionary and find
the binary for a precompiled, formally proven implementation of a
function that is perfectly equivalent to the original Simplicity code.

@_date: 2017-11-21 14:16:48
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Why SegWit Anyway? 
1. SegWit transactions spend less "weight", which is limited for every
block. Base transaction data weights as much as 4x the witness data.
2. SegWit signatures can be cheaper to verify (linear instead of
quadratic). Prior to this, DoS attacks were possible by using forged
transactions including signatures which could take several minutes to
The immediate result of this is that miners can fit more transactions
into a block and at the same time spend less power building the blocks.

@_date: 2017-10-19 15:41:51
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Improving Scalability via Block Time Decrease 
Blockchains with fast confirmation times are currently believed to
suffer from reduced security due to a high stale rate.
As blocks take a certain time to propagate through the network, if miner
A mines a block and then miner B happens to mine another block before
miner A's block propagates to B, miner B's block will end up wasted and
will not "contribute to network security".
Furthermore, there is a centralization issue: if miner A is a mining
pool with 30% hashpower and B has 10% hashpower, A will have a risk of
producing a stale block 70% of the time (since the other 30% of the time
A produced the last block and so will get mining data immediately)
whereas B will have a risk of producing a stale block 90% of the time.
Thus, if the block interval is short enough for the stale rate
to be high, A will be substantially more efficient simply by virtue of
its size. With these two effects combined, blockchains which produce
blocks quickly are very likely to lead to one mining pool having a large
enough percentage of the network hashpower to have de facto control over
the mining process.
Another possible implication of reducing the average block time is that
block size should be reduced accordingly. In an hypothetical 5 minutes
block size Bitcoin blockchain, there would be twice the block space
available for miners to include transactions, which could lead to 2
immediate consequences: (1) the blockchain could grow up to twice the
rate, which is known to be bad for decentralization; and (2) transaction
fees might go down, making it cheaper for spammers to bloat our beloved
UTXO sets.
There have been numerous proposals that tried to overcome the downsides
of faster blocks, the most noteworthy probably being the "Greedy
Heaviest Observed Subtree" (GHOST) protocol:
Personally, I can't see why Bitcoin would need or how could it even
benefit at all from faster blocks. Nevertheless, I would really love if
someone in the list who has already run the numbers could bring some
valid points on why 10 minutes is the optimal rate (other than "if it
ain't broke, don't fix it").

@_date: 2017-09-11 22:37:55
@_author: =?UTF-8?Q?Ad=c3=a1n_S=c3=a1nchez_de_Pedro_Crespo?= 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
Coincidentally, the kind of Merkle tree that Mark describes in his
proposal is exactly the one that we use at Stampery.
The Stampery BTA whitepaper[1] includes pseudocode for many of the
algorithms outlined by this proposal, including fast-SHA256, the tree
building process and the inclusion proving routine.
The wording is slightly different but the logic is just the same, so I
hope it helps future implementations in case of eventual adoption.
