
@_date: 2015-08-06 18:11:40
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Block size following technological growth 
Whilst 1mb to 8mb might seem irrelevant from a pure computer science
perspective payment demand is not really infinite, at least not if by
"payment" we mean something resembling how current Bitcoin users use the
If we define "payment" to mean the kind of thing that Bitcoin users and
enthusiasts have been doing up until now, then suddenly 1mb to 8mb makes a
ton of sense and doesn't really seem that small: we'd have to increase
usage by nearly an order of magnitude before it becomes an issue again!
If we think of Bitcoin as a business that serves customers, growing our
user base by an order of magnitude would be a great and celebration worthy
achievement! Not at all a small constant factor :)
And keeping the current user base happy and buying things is extremely
interesting, both to me and Gavin. Without users Bitcoin is nothing at all.
Not a settlement network, not anything.
It's actually going to be quite hard to grow that much. As the white paper
says, "the system works well enough for most transactions". And despite a
lot of effort by many people, killer apps that use Bitcoin's unique
features are still hit and miss. Perhaps Streamium, Lighthouse, ChangeTip,
some distributed exchange or something else will stimulate huge new demand
for transactions in future ..... but if so we're not there yet.

@_date: 2015-08-09 16:29:07
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
The reason BIP 70 doesn't do this is the assumption that alt coins are ...
well .... alt. They can vary in arbitrary ways from Bitcoin, and so things
in BIP70 that work for Bitcoin may or may not work for other coins.
If your alt coin is close enough to BIP 70 that you can reuse it "as is"
then IMO we should just define a new network string for your alt. network =
"dogecoin-main" or whatever.
You could also use the genesis hash as the network name. That works too.
But it's less clear and would involve lookups to figure out what the
request is for, if you find such a request in the wild. I don't care much
either way.

@_date: 2015-08-10 14:53:37
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
We're not modifying BIP 70, it's now immutable and can only be extended.
There's really not much point in having a dedicated chain ID for regtest
mode. You shouldn't be finding BIP70 requests for regtest outside of your
own developer machine, where the id doesn't matter.

@_date: 2015-08-11 13:03:15
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Future Of Bitcoin-Cores Wallet 
Hey Jonas,
I think your analysis of what (some) users need is a good one.
We've discussed this before so I know you prefer your current approach, but
I personally would take a slightly different path to reach the same end:
   1. Support serving of SPV wallets from pruned storage. This means some
   protocol upgrades, BIPs, etc. It helps all SPV wallets, including on phones.
   2. Then make a bitcoinj based desktop wallet app, that contains a
   bundled bitcoind.
   3. Make the app sync TWO wallets simultaneously, one from the P2P
   network as today, and another from the local bitcoind via a local socket
   (or even just passing buffers around internally)
   4. The app should then switch from using the wallet synced to P2P to the
   wallet synced to localhost when the latter is fully caught up, and back
   again when the local node is behind.
   5. If there's a discrepancy, alert the user.
There are big advantages of taking this path! They are:
   - The switching back and forth between local full-security mode (which
   may be behind) and remote SPV security (fully synced) is instant and
   transparent to the user. This is important for laptop users who don't run a
   local node all the time. The different audit levels can be reflected in the
   UI in some way.
   - The bitcoinj wallet code already has support for things like
   multi-sig, BIP32, seed words, micropayment channels, etc. You can disable
   Bloom filtering if you like (download full blocks).
   - You can do a local RPC or JNI/JNA call to get fee estimates, if wanted.
   - The modern JVM tools and languages are much, much more productive than
   working with C++.
If you want a thing that runs a home server, then the best way to do that
IMO would be to bundle Tor and make it auto-register a Tor hidden service.
Then you can just define a QR code standard for 'pairing' a wallet to a
.onion address. Any bitcoinj based wallet can sync to it, and as it's your
own node, you can use a Bloom filter sized to give virtually no false
positives. No additional indexing is then required.

@_date: 2015-08-15 19:02:18
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
As promised, we have released Bitcoin XT 0.11A which includes the bigger
blocks patch set. You can get it from
     I feel sad that it's come to this, but there is no other way. The Bitcoin
Core project has drifted so far from the principles myself and many others
feel are important, that a fork is the only way to fix things.
Forking is a natural thing in the open source community, Bitcoin is not the
first and won't be the last project to go through this. Often in forks,
people say there was insufficient communication. So to ensure everything is
crystal clear I've written a blog post and a kind of "manifesto" to
describe why this is happening and how XT plans to be different from Core
(assuming adoption, of course).
The article is here:
    It makes no attempt to be neutral: this explains things from our point of
The manifesto is on the website.
I say to all developers on this list: if you also feel that Core is no
longer serving the interests of Bitcoin users, come join us. We don't bite.

@_date: 2015-08-15 21:21:52
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
It is a reasonable question. Let me try and explain why it's done this way.
*In theory*, you do have a vote. If a majority of miners were to club
together and decide to change the protocol against the wishes of the wider
user base, then we'd get a fight between the hashpower majority and the
so-called economic majority. And because bitcoins only have value because
you can buy things with them, in theory, the wishes of the economic
majority should always win.
*In practice*, the code we have today doesn't let us measure what the
economic majority wants. It's not even really clear how that term is
defined. Intuitively we can understand that people who are trading real
goods and services for bitcoin have the final say, because they can always
just refuse to accept BTC. But defining it precisely enough to put in an
algorithm is tricky.
Then you have the question of how to express the vote? For miners, it's
easy: they express their vote by switching to a different full node
implementation that gives them different blocks.
For users, it'd mean switching to a different wallet. If their wallet is
fully validating and the decision is implemented via hard fork, this is
sufficient. If the wallet is *not* fully validating and cannot detect the
fork point by itself, then it'd need help in the form of checkpointing.
Some months ago I pointed out this possibility and a whole bunch of people
freaked out - then bitcoin.org decided to start censoring any wallet that
said it'd ignore what miners wanted.
So if you want a user vote, that's an issue that'd have to be tackled: the
people who admin the main communication channels Bitcoin users have vowed
to censor any program that doesn't slavishly follow 51%+ hash power. That
attempt to control the conversation is certainly not libertarian or
democratic in nature, but there you go.
We can also imagine voting via proof-of-stake. This might be useful as a
form of opinion poll, but wallet developers would have to actually add
support for such a protocol to their apps, and then we're back to the same
issue as with mining pools. Plus, of course, static wealth does not equal
economic importance.
Luckily the wallet market is a decentralisation success story. There are
wallets everywhere these days. Man+dog make their own wallet, it seems. So
it's not silly to think a coin voting protocol could one day happen.

@_date: 2015-08-16 15:49:15
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Hi Eric,
Sorry you feel that way. I devoted a big part of the article to trying to
fairly represent the top 3 arguments made, but ultimately I can't link to a
clear statement of what Bitcoin Core thinks because there isn't one. Some
people think the block size should increase, but not now, or not by much.
Others think it should stay at 1mb forever, others think everyone should
migrate to Lightning, people who are actually *implementing* Lightning
think it's not a replacement for an increase ..... I think one or two
people even suggested shrinking the block size!
So I've done my best to sum up the top arguments. If you think I've done a
bad job, well, get writing and lay it out how you see it!
I don't think the position of "Bitcoin is open source but touching THESE
parts is completely bogus" is reasonable. Bitcoin is open source or it
isn't. You can't claim to be decentralised and open source, but then only
have 5 people who are allowed to edit the most important parts. That's
actually worse than central banking!
This isn?t a democracy - consensus is all or nothing.
This idea is one of the incorrect beliefs that will hopefully be disproven
in the coming months. Bitcoin cannot possibly be "all or nothing" because
as I pointed out before, that would give people a strong financial
incentive to try and hold the entire community to ransom: "I have 1
terahash/sec of mining power. Pay me 1000 BTC or I'll never agree to the
next upgrade".
Or indeed, me and Gavin could play the same trick.

@_date: 2015-08-19 17:45:06
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin XTs Tor IP blacklist downloading system 
The code was peer reviewed, in the XT project. I didn't bother submitting
other revisions to Core, obviously, as it was already rejected.
The quantity of incorrect statements in this thread is quite ridiculous.

@_date: 2015-08-20 11:00:14
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I keep seeing this notion crop up.
I want to kill this idea right now:
   - There were months of public discussion leading to up the authoring of
   BIP 101, both on this mailing list and elsewhere.
   - BIP 101 was submitted for review via the normal process. Jeff Garzik
   specifically called Gavin out on Twitter and thanked him for following the
   process:
         As you can see, other than a few minor typo fixes and a comment by sipa,
   there was no other review offered.
   - The implementation for BIP 101 was submitted to Bitcoin Core as a pull
   request, to invoke the code review process:
      Some minor code layout suggestions were made by Cory and incorporated.
   Peter popped up to say there was no chance it'd ever be accepted ..... and
   no further review was done.
So the entire Bitcoin Core BIP process was followed to the letter. The net
result was this. There were, in fact, bugs in the implementation of BIP
101. They were found when Gavin submitted the code to the XT community
review process, which resulted in *actual* peer review. Additionally, there
was much discussion of technical details on the XT mailing list that
Bitcoin Core entirely ignored.

@_date: 2015-08-24 17:21:47
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
NACK: stated rationales are invalid: both privacy and DoS (see below for
experimental data).
1 - Bloom filtering doesn't add privacy for node operators, it adds privacy
for lightweight wallets. And in fact, with a high FP rate it does do that.
Most users want both low bandwidth usage *and* query scrambling, which is
harder to do but not impossible. There is a clear roadmap for how to
implement that with smarter clients: no protocol changes are needed.
So the first stated rationale is spurious: disabling Bloom filtering
doesn't improve privacy for anyone. It can only hurt.
2 - SPV usage is rising, not falling.
Peter's data is flawed because he ignored the fact that SPV clients tend to
connect, sync, then disconnect. They don't remain connected all the time.
So merely examining a random snapshot of what's connected at a single point
in time will give wildly varying and almost random results.
A more scientifically valid approach is to check the number of actual
connections over a long span of time. Here's the data from my node:
mike at plan99:~/.bitcoin$ grep -Po 'receive version message: ([^:]*):'
debug.log |sort |uniq -c|sort -n|tac|head -n 10
  11027 receive version message: /getaddr.bitnodes.io:
   6264 receive version message: /bitcoinseeder:
   4944 receive version message: /bitcoinj:
   2531 receive version message: /Snoopy:
   2362 receive version message: /breadwallet:
   1127 receive version message: /Satoshi:
    204 receive version message: /Bitcoin XT:
    128 receive version message: /BitCoinJ:
     97 receive version message: /Bither1.3.8/:
     82 receive version message: /Bitaps:
Once crawlers are removed, SPV wallets (bitcoinj, breadwallet) make up the
bulk of all P2P clients. This is very far from 1% and falling, as Todd
wrongly suggests.
3 - It is said that there is a DoS attack possible. This claim does not
seem to have been researched.
I decided to test it out for real, so I implemented a DoS attack similar to
the one we've seen against XT nodes: it sends getdata for large (1mb)
filtered blocks over and over again as fast as possible.
As was reported and makes sense, CPU usage goes to 100%. However I couldn't
see any other effects. RPCs still react immediately, the Qt GUI is fully
responsive, I was even able to sync another SPV client to that node and it
proceeded at full speed. It's actually pretty nice to see how well it held
Most importantly transactions and blocks continued to be relayed without
delay. I saw my VPS node receive a block only eight seconds after my local
node, which is well within normal propagation delays.
There's another very important point here: I profiled my local node whilst
it was under this attack. It turns out that Bloom filtering is extremely
fast. 90% of the CPU time is spent on loading and deserializing the data
from disk. Only 10% of the CPU time was spent actually filtering.
Thus you can easily trigger exactly the same DoS attack by just using
regular getdata requests on large blocks over and over. You don't need
Bloom filtering. If you don't want to actually download the blocks just
don't TCP ACK the packets and then FIN after a few seconds .... the data
will all have been loaded and be sitting in the send buffers.
So even if I refine the attack and find a way to actually deny service to
someone, the fix would have to apply to regular non-filtered block fetches
too, which cannot be disabled.
In summary: this BIP doesn't solve anything, but does create a big upgrade

@_date: 2015-08-31 21:11:52
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
I think your summary of what people actually want from decentralisation is
pretty good, Justus.
+1 Insightful
It's been quite impressive to see so many Bitcoin users and developers
saying, "Bitcoin is totally decentralised because it's open source and
nobody is in charge...... oh nooooooo we didn't mean you could change *those
lines! *If you want to change *those lines* then *we* must agree first!"
Believing simultaneously that:
1. Bitcoin is decentralised
2. Nobody should modify the code in certain ways without the agreement of
me and my buddies
is just doublethink.

@_date: 2015-07-14 00:31:36
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
Hi Thomas,
FYI there is a company called Netki is also working on a kind of DNSSEC
integration with BIP70, there's a thread here about their efforts:
If you would like to work on this, perhaps it's worth teaming up with them?
Obviously they plan to have an open spec and open source implementation.
Now w.r.t. the other things - I think we have discussed this before, but to
reiterate:  the biggest flaw with doing things the way you suggest is that
in practice, no email provider is going to implement your scheme any time
soon. Most obviously the big web mail providers won't. Therefore hardly
anyone will use it.
Whilst having an extension cannot really hurt, obviously, BIP70 will not be
amended to reduce the certificate types it allows in favour of a system
that has a very low chance of mainstream adoption. Restricting options like
that would just make no sense at all.
I think your primary concern is that if your email account is hacked,
someone could get a cert issued in your name, and you'd be unable to revoke
it? But that's not quite true. Every CA I know of allows you to revoke a
certificate that was issued for your email address if you have access to
that email address. Now, if you don't know that this issuance took place,
you cannot invoke that procedure of course .... but that's what certificate
transparency is already working on solving in a scalable manner:
  That site doesn't currently index email address certs, but it certainly
could with minimal extra effort by the creators as they're almost identical
to domain name certs.
So the existing infrastructure seems to have everything in place to solve
that issue.
Now, if you still want a mechanism that eliminates the CA entirely, I think
there's a better approach which is backwards compatible with existing email
providers. It works like this:
   1. User sends a public key in the subject line to a one-time collector
   address like     (who runs this
   service is arbitrary as they do not need to be trusted). On receiving the
   email, the headers are made available via
    for download by the
   users wallet.
   2. The act of sending the email triggers DKIM signing of the subject
   line and From header, and thus, the public key and email address are bound
   together via the ESP's own signing key.
   3. The textual email headers can be run through the DKIM validation
   algorithm in combination with the domain key retrieved via DNS.
With this scheme, setup is largely automatic and involves the wallet asking
the operating system to open a mailto: URL. The user just has to press
"send" and the wallet can then sit on a long-lived HTTPS connection waiting
for the headers to turn up. Once the headers are downloaded, they can be
saved to disk and this becomes your "DKIM certificate" which can then be
used with a new pki_type in BIP70.
Note the following useful characteristics of this approach:
   1. It does not require the email provider to know/care about Bitcoin.
   DKIM is already widely deployed by major email providers due to its
   benefits for spam and phishing protection: the majority of all email on the
   internet is DKIM signed. So you automatically have a system that works with
   nearly all consumer email accounts.
   2. The enrolment UI is straightforward, assuming the user has a working
   mailto: handler on their system. Even webmail services like Gmail can
   attach themselves to mailto: handling these days.
   3. There are DKIM validation libraries already in existence, so new code
   required is minimal.
And the downsides:
   1. There is no way to revoke such a "certificate" because you have, of
   course, abandoned the PKI which specifies how to handle all these details.
   You could potentially hijack/reuse OCSP to allow such a custom cert to be
   revoked, but then the question is, who actually runs such a revocation
   server. Doing things like this is why we have CAs in the first place.
   2. The UX leaves a bit of binary nonsense in the users sent folder that
   clutters up their account.
   3. Does it even solve the right problem? A lot of users don't actually
   use emails as identifiers anymore. In the modern world people are using
   their social networking profiles (i.e. Facebook) and phone numbers (e.g.
   for WhatsApp) as the personal identifier of choice. Email address support
   might be solving yesterdays problem.

@_date: 2015-07-14 13:45:06
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
Hi Thomas,
Re: NetKi, I think any proposal in this space has to be an open standard,
almost by the definition of what it is. At any rate, it may be worth
talking to them. They have signed up to implement their system at least.
I did understand that your proposal does not rely on email - for instance a
web forum could issue username at reddit.com type aliases, even if those are
not also email accounts. I am just continuing the comparison against email
address certs.
It's also the case that a domain can use the DKIM setup without actually
offering email accounts. They can just have a web form or API that triggers
sending of the signed email (or simply, providing the signed headers
themselves). Thus the same system can be used transparently by both email
providers and other sites that don't give their users email addresses, but
would still like to use the same system.
Hardly anyone uses email certificates today, so I don't think it would
No, but obviously we'd like to change that! The holdup is not the
certificate side of things, really, but rather the lack of a
store-and-forward network for signed payment requests. I keep asking
someone to build one but I fear the problem is almost too simple ......
everyone who looks at this decides to solve 12 other problems
simultaneously, it gets complicated, then they never launch :(
Once there's a simple and robust way to get PaymentRequest's from one end
user to another, even when that first user is offline, then getting an
email cert issued is no big deal.
That does not look so... not until (1) BIP70 wallets integrate with
Any solution that separates identity providers from certificate issuers
would have these requirements, though. And as many identity providers today
do not wish to become CAs too, it seems fundamental.
I don't think it's such a problem, mind you. The crt.sh website is actually
a frontend to the CT protocol, which is a somewhat blockchain like audit
log that's eventually intended to contain all issued certificates. Right
now, of course, they focus on SSL certs because those are the most common
and important. If other kinds of certs became more widely used, support in
the infrastructure would follow.
Don't get me wrong - I would like to see a way for a domain to delegate
BIP70 signing power to a third party. For instance, this would mean payment
processors like BitPay could sign on the behalf of the merchant, and the
merchant identity would then show up in wallets. The "chain a cert off a
domain cert" trick would be a good way to do that, though rather than
hacking the X.509 stack to validate invalid stuff, at this point it may as
well be a custom (better) cert format. There's little reason to use X.509
beyond backwards compatibility.
But the most popular identity providers today either don't care about
Bitcoin at all, or worse, are developing competitors to it. So for real
adoption to occur, we must have solutions that do not require identity
provider cooperation. I realise this is a business reason rather than a
technical reason, but it's a very strong one - so bootstrapping off
existing infrastructure with a split CA/ID provider design still makes much
more sense to me.

@_date: 2015-07-18 13:21:14
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
It is worth noting that DNS lookups can be done via Tor. In effect that
gives you 1000+ proxies instead of 56 or 4. BitcoinJ already has code that
can do this.
I would agree that it makes sense for proxying of DNS requests to be an
optional part of the protocol. Wallet developers can then compete on
privacy vs robustness vs whatever other issues there may be.

@_date: 2015-07-18 13:43:14
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
He's talking about patches I didn't even write (Gavin and Tom did), but
have included in Bitcoin XT:
   See the README section starting with "Relaying of double spends"

@_date: 2015-07-18 13:46:26
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
With warm Tor directory caches it's surprisingly fast - fast enough to be
usable and I'm a notorious stickler for low latency UX. If you want to do
LOTS of lookups so you can cross-check and merge their answers, that's
slower of course.
With cold Tor caches it's indeed too slow. However, reaching "tor by
default" is a part time hobby of the bitcoinj project for a while now and
there are plenty of ideas for how to make things fast enough. For instance,
using a cold cache whilst simultaneously refreshing it in the background,
doing nightly refreshes when charging, lengthening the expiry time, and the
Tor guys I believe want to implement directory differentials too which
would also help.
That seems reasonable for Electrum. I don't strongly care about these
protocols myself (and Justin knows this already), but whatever is decided
should give maximum freedom to wallet developers who disagree with you and
wish to explore other tradeoffs.

@_date: 2015-07-20 15:46:39
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
Hey Thomas,
Was great to hang out with you in Berlin last week!
Absolutely agree! Convenience for the user is an absolute must. I just
don't know how to let users exchange large data packets without some kind
of server acting as a dropbox in the middle. That leaves two solutions:
1) Set up a way for users to exchange large data packets by using other
people's web servers, e.g. with no user visible signup flow (pure p2p/done
automatically in the background by user wallets)
2) Make the data packets small instead.
We can debate better signing methods that work towards (2). The reason I am
unsure about this is that the point of BIP70 is to be extended with useful
features. Even if we find a way to squeeze a human-meaningful
signature/cert into a URI, that's only one of BIP70s features. The next set
we want to add might end up running out of space again.
A lot of the problems come from how limited QR codes are. So perhaps there
is also a third approach: either find a better replacement for QR codes
(maybe something that uses colour like Microsoft Tag
), or drop them as a design
Calling Jeff Garzik, Jeff, are you there? I recall BitPay did some
experiments to find out how much data you can stuff into a QR code before
it hurts scannability too much, do you have a writeup anywhere?
This is the main reason I feel uneasy about anything that isn't "build a
store+forward network". QR codes are so fundamental to our ecosystem, but
sooooooo limited, that I'm not sure how else to move forward. We were told
when designing BIP70 that even putting a URL in the QR code is pushing it!
There was talk of compressing the URL in some way. So adding even an ECC
signature which is much longer seems risky.
We can imagine a parallel universe where our societies technology was more
NFC oriented: laptops had NFC tags in them, phones had better NFC support
etc. Then we would have more bytes to play with and we wouldn't face this
pointer-indirection problem :( But laptops don't have the hardware and
Apple sits on their NFC API because they can't imagine any use case that
isn't credit cards :( :(
To get more specific, DNSSEC uses RSA 1024 bit. This causes two problems:
   1. A DNSSEC proof is large, bytes wise. Even a single RSA signature
   won't fit nicely in a QR code, I think.
   2. 1024 bit is the absolute minimum strength you can get away with,
   really. DNSSEC assumes frequent key rotations to try and help, which
   complicates things.
So I'm not sure using DNSSEC fixes the usability problem we want to fix.
I will do a separate reply to break out some thoughts on replacing QR codes.
Would it be possible to create the same kind of "lightweight payment
Given that the pre-existing value of the PKI is much lower for individuals
than for companies/websites, where they all have certs already, building a
Bitcoin-specific or entirely new/independent PKI for people is not so
unthinkable, I agree.
In theory such a cert could be as minimal as:
thomasv at electrum.org
so literally just a signature + a UTF-8 string, and that's it! You don't
need anything more if you're willing to sacrifice extensibility,
revocability, etc.
The pubkey of the CA would be obtained by running the pubkey recovery
algorithm on the signature, and then checked against a table of trusted

@_date: 2015-07-20 16:40:36
@_author: Mike Hearn 
@_subject: [bitcoin-dev] QR code alternatives (was: Proposal: extend bip70 
Hey Thomas,
Here are some thoughts on a third way we can tackle our BIP 70 usability
problem sans servers: by finding an upgrade to QR codes that give us more
space and then optimising the hell out of BIP70 to make it fit.
*Better QR codes*
Let's start with this paper, High Capacity Colored Two Dimensional Codes
. It develops an upgrade to
standard QR codes that extend them with the use of colour. The resulting
codes have ~4x the capacity but similar levels of scanning robustness.
This paper is also interesting: DualCodes
It works by overlaying one QR code on top of another using shades of grey.
The resulting code is still scannable by older applications (backwards
compatibility!) but an enhanced reader can also extract the second code.
They explicitly mention digital signatures as a possible use case.
In both cases the code does not appear to be available but the same
approach was used: extend libqrcode for creation and ZXing for decoding
(Android). We could ask the authors and see if they're willing to open
source their work.
BIP 70 has the potential to add many features. But most of them, even the
extensions currently proposed only as ideas, can be expressed with
relatively few bytes.
So with a 4x boost in capacity, or a 2x boost with backwards compat, what
could we do?
*Optimised BIP70*
If we define our own certificate formats and by implication our own CAs,
then we can easily make a certificate be 32 bytes for the ECC
signature+length of the asserted textual identity, e.g. email address.
Can we go smaller? Arguably, yes. 32 bytes for a signature is for Really
Strong Security? (a 256 bit curve), which gives 128 bits of security. If we
are willing to accept that a strong adversary could eventually forge a
certificate, we can drop down to a weaker curve, like a 128 bit cure with
64 bits of security. This is well within reach of, say, an academic team
but would still pose a significant hurdle for run of the mill payment
fraudsters. If these short CA keys expired frequently, like once a month,
the system could still be secure enough.
As we are defining our own PKI we can make CA keys expire however
frequently we like, up to the expiry period of the BIP70 request itself.
Thus certificates that expire monthly is not an issue if the wallet has a
way to automatically refresh the certificate by using a longer term
stronger credential that it keeps around on disk.
If we accept a single payment address i.e. no clever tricks around merge
avoidance, such a QR code could look like this:
However this requires text mode and wastes bytes at the front for the URI
If we're willing to accept QR codes that can't be read by a standalone app
and which requires an embedded reader, then we can just scrap the legacy
and serialise a binary BIP70 request directly into the QR code. Andreas'
wallet, for example, can already handle this because it has an embedded QR
reader. I don't know what the situation on iOS is like.
If we were to use the DualCodes system we could define the primary QR code
as being an unsigned payment request, and the second layer as being the
signature/pki data.
*Getting response data back to the recipient*
One reason to have a store/forward network is the "forward" part: we don't
only want to host a static PaymentRequest, but also receive a private
response e.g. for the memo field, or to implement the well known "Stealth
Address" / ECDH in the payment protocol proposals:
Stealth addresses try and (ab)use the block chain as a store/forward layer
and break SPV in the process as well as wasting lots of resources. ECDH in
BIP70 avoids those issues but at the cost of requiring a separate
store-and-forward network with some notion of account privacy.
These ideas come with another steep price: restoring a wallet from seed
words is no longer possible. You must have the extra random data to
calculate the private keys for money sent to you :(  If you lose the extra
data you lose the money. It can be fixed but only by having wallets
regularly sweep the sent money to keys derived from the BIP32 seed, meaning
privacy-hurting merging and extra traffic.
I don't know of any way to solve this except by using some servers,
somewhere, that store the Payment messages for people: potentially for a
long period of time. If we have such servers, then having them host BIP70
requests is not a big extra requirement.
I have imagined this being a p2p-ish network of HTTPS servers that accept
POSTs and GETs. But if we are thinking about alternatives, it could also be
a separate service of the existing Bitcoin P2P network. That's what
OP_RETURN (ab)use effectively does. But as these messages don't really have
to be kept forever, a different system could be used: Payment messages
could be broadcast along with their transactions and stored at every node,
waiting for download. But unlike regular transactions, they are not stored
forever in a block chain. They are just written to disk and eventually
erased, perhaps, ordered in a mempool like way where more fee attached ==
stored for longer, even though the nodes storing the data aren't actually
receiving the fee.
A signature over the Payment metadata using the same output keys as the
transaction would bind them together for the purposes of broadcast, but
doesn't need to be stored after that.
As the data storage is just a helpful service but not fundamentally
required, nodes could shard themselves by announcing in their addr messages
that they only store Payment metadata for e.g. the half which have a hash
starting with a one bit. And when outputs are seen being spent, the
associated Payment metadata can be erased too, as by then it's fair to
assume that the users wallet has downloaded the metadata and no longer
cares about it.
Of course you have then all the regular DoS issues. But any P2P network
that stores data on the behalf of others has these.

@_date: 2015-07-20 16:42:05
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
Yes, but you still need the final signature. Is it possible to use an EC
signature with DNSSEC? I thought it was an all-RSA system. If I'm wrong
about that, and all you need is 32 bytes, then my argument does not hold.

@_date: 2015-07-20 17:14:03
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
Right, got it. I think we've been talking about two related but separate
issues (DNSSEC vs squeezing payment requests into URIs/qrcodes somehow).
So: DNSSEC attests via an RSA chain to some EC key stored in the wallet
which is then used to sign the payment request or URI, which also contains
a domain name.
By "alias" you mean domain name? I'm not sure what DNS key means in this
I'm still not really convinced that a domain name under some new roots is
an identity people will want to use, but yes, I guess your approach would
work for those who do want it.
It still may be worth exploring the compact cert+optimized BIP70 (no
DNSSEC) in a qrcode if making a network that stores small bits of data
really is beyond us :(

@_date: 2015-07-20 18:09:03
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Proposal: extend bip70 with OpenAlias 
I mean, most users will need to sign up for some new identity under a DNS
tree that they don't currently use (whether that's netki.com or whatever).

@_date: 2015-07-22 23:18:51
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Making Electrum more anonymous 
Hey Joseph,
All those ideas are ones we had years ago and they are implemented in the
current Bitcoin protocol.
The trick, as you may know, is this bit:
The client would also need to be fairly clever
It turns out making a sufficiently clever client to fool even advanced
observers is a lot of programming work, assuming you wish for the Ultimate
Solution which lets you allocate a desired quantity of bandwidth and then
use it to maximize privacy.

@_date: 2015-07-22 23:43:20
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Hi Pieter,
I think a core area of disagreement is this:
In fact Bitcoin Core is running the Bitcoin economy, and its developers do
have the authority to set its rules. This is enforced by the reality of
~100% market share and limited github commit access.
You may not like this situation, but it is what it is. By refusing to make
a release with different rules, people who disagree are faced with only two
1. Swallow it even if they hate it
2. Fork the project and fork the block chain with it (XT)
There are no alternatives. People who object to (2) are inherently
suggesting (1) is the only acceptable path, which not surprisingly, makes a
lot of people very angry.

@_date: 2015-07-23 00:01:32
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Block chain forks merge in the same way as git forks all the time, that's
how the reorg algorithm works. Transactions that didn't make it into the
post-reorg chain go back into the mempool and miners attempt to reinclude
them: this is the "merge" process. If they now conflict with other
transactions they are dropped and this is "resolving merge conflicts".
However you have to want to merge with the new chain. If your software is
programmed not to do that out of some bizarre belief that throttling your
own user base is a good idea, then of course, no merge happens. Once you
stop telling your computer to do that, you can then merge (reorg) back onto
the main chain again.

@_date: 2015-07-23 20:57:04
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
He hasn't ignored you, and he wasn't responding to your email specifically
but rather the general attitude displayed in this forum for the last
several months (and I'd argue the last year or so).
Your data is interesting but ultimately tell us what we already know - that
the next bottleneck after the hard coded limit could easily be propagation
speed. The solution is likely to be a better protocol. Matt's custom
network already has optimised things, rolling some of those ideas into the
P2P protocol may be a good place to start, or something fancier like IBLTs.
Regardless, the *next* bottleneck is not the protocol, it's the hard cap.
So the conclusion remains unchanged: Bitcoin must grow, and solutions for
scaling it up will be found as the need arises.

@_date: 2015-07-24 13:38:46
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
This has nothing to do with block sizes, and everything to do with Core not
directly providing the services businesses actually want.
The whole "node count is falling because of block sizes" is nothing more
than conjecture presented as fact. The existence of multiple companies who
could easily afford to do this but don't because they perceive it as
valueless should be a wakeup call there.

@_date: 2015-07-29 11:59:43
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I do love history lessons from people who weren't actually there.
Let me correct your misconceptions.
Initially there was no block size limit - it was thought that the fee
The term "fee market" was never used back then, and Satoshi did not ever
postulate economic constraints on growth. Back then the talk was (quite
sensibly) how to grow faster, not how to slow things down!
No such event happened, and the hypothesis of which you talk never existed.
The one megabyte limit was nothing to do with anti spam. It was a quick
kludge to try and avoid the user experience degrading significantly in the
event of a "DoS block", back when everyone used Bitcoin-Qt. The fear was
that some malicious miner would generate massive blocks and make the wallet
too painful to use, before there were any alternatives.
The plan was to remove it once SPV wallets were widespread. But Satoshi
left before that happened.
Now on to your claims:
1) We never really got to test things out?a fee market never really got
The limit had nothing to do with fees. Satoshi explicitly wanted free
transactions to last as long as possible.
Satoshi explicitly envisioned a future where only miners ran nodes, so it
had nothing to do with this either.
Validators validate for themselves. Calculating a local UTXO set and then
not using it for anything doesn't help anyone. SPV wallets need filtering
and serving capability, but a computer can filter and serve the chain
without validating it.
The only purposes non-mining, non-rpc-serving, non-Qt-wallet-sustaining
full nodes are needed for with today's network are:
   1. Filtering the chain for bandwidth constrained SPV wallets (nb: you
   can run an SPV wallet that downloads all transactions if you want). But
   this could be handled by specialised nodes, just like we always imagined in
   future not every node will serve the entire chain but only special
   "archival nodes"
   2. Relaying validated transactions so SPV wallets can stick a thumb into
   the wind and heuristically guess whether a transaction is valid or not.
   This is useful for a better user interface.
   3. Storing the mempool and filtering/serving it so SPV wallets can find
   transactions that were broadcast before they started, but not yet included
   in a block. This is useful for a better user interface.
Outside of serving lightweight P2P wallets there's no purpose in running a
P2P node if you aren't mining, or using it as a trusted node for your own
And if one day there aren't enough network nodes being run by volunteers to
service all the lightweight wallets, then we can easily create an incentive
scheme to fix that.
3) Miners don?t even properly validate blocks. And the bigger the blocks
Miners who don't validate have a habit of bleeding money:   that's the
system working as designed.
It did. I designed it. The proofs are short and "reasonably secure" in that
it would be a difficult and expensive attack to mount.
But as is so often the case with Bitcoin Core these days, someone who came
along much later has retroactively decided that the work done so far fails
to meet some arbitrary and undefined level of perfection. "Satisfactory"
and "reasonably secure" don't mean anything, especially not coming from
someone who hasn't done the work, so why should anyone care about that
opinion of yours?

@_date: 2015-07-29 13:15:49
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I don't think it's obvious. You may disagree, but don't pretend any of this
stuff is obvious.
Consider this:  the highest Bitcoin tx fees can possibly go is perhaps a
little higher than what our competition charges. Too much higher than that,
and people will just say, you know what .... I'll make a bank transfer.
It's cheaper and not much slower, sometimes no slower at all.
And now consider that in many parts of the world bank transfers are free.
They aren't actually free, of course, but they *appear* to be free because
the infrastructure for doing them is cross subsidised by the fees on other
products and services, or hidden in the prices of goods sold.
So that's a market reality Bitcoin has to handle. It's already more
expensive than the competition sometimes, but luckily not much more, and
anyway Bitcoin has some features those other systems lack (and vice versa).
So it can still be competitive.
But your extremely vague notion of a "fee market" neglects to consider that
it already exists, and it's not a market of "Bitcoin users buying space in
Bitcoin blocks". It's "users paying to move money".
You can argue with this sort of economic logic if you like, but don't claim
this stuff is obvious.
Nobody threatened to start mining huge blocks given how relatively
Not that I recall. It wasn't a response to any actual event, I think, but
rather a growing realisation that the code was full of DoS attacks.
The most popular mobile wallet (measured by installs) on Android is SPV. It
has between 500,000 and 1 million installs, whilst Coinbase has not yet
crossed the 500,000 mark. One of the most popular wallets on iOS is SPV. If
we had SPV wallets with better user interfaces on desktops, they'd be more
popular there too (perhaps MultiBit HD can recapture some lost ground).
So I would argue that they are in fact very widespread.
Likewise, they are not "notoriously terrible" at detecting chain forks.
That's a spurious idea that you and Patrick have been pushing lately, but
they detect them and follow reorgs across them according to the SPV
algorithm, which is based on most work done. This is exactly what they are
designed to do.
Contrast this with other lightweight wallets which either don't examine the
block chain or implement the algorithm incorrectly, and I fail to see how
this can be described as "notoriously terrible".
Fees were added as a way to get money to miners in a fair and decentralised
Attaching fees directly to all transactions is certainly one way to use
that, but it's not the only way. As noted above, our competitors prefer a
combination of price-hiding and cross subsidisation. Both of these can be
implemented with tx fees, but not necessarily by trying to artificially
limit supply, which is economically nonsensical.
Maybe when there is a need? I already discussed this topic of need here:
Right. Turns out the ledger structure is terrible for constructing the
Validators don't require proofs. That's why they are validators.
I think you're trying to say the block chain doesn't provide the kinds of
proofs that are most important to lightweight wallets. But I would
disagree. Even with UTXO commitments, there can still be double spends out
there in the networks memory pools you are unaware of. Merely being
presented with a correctly signed transaction doesn't tell you a whole lot
..... if you wait for a block, you get the same level of proof regardless
of whether there are UTXO commitments or not. If you don't then you still
have to have some trust in your peers that you are seeing an accurate and
full view of network traffic.
So whilst there are ways to make the protocol incrementally better, when
you work through the use cases for these sorts of data structures and ask
"how will this impact the user experience", the primary candidates so far
don't seem to make much difference.
Remote attestation from secure hardware would make a big difference though.
Then you could get rid of the waiting times entirely because you know the
sending wallet won't double spend.
Yes, let?s wait until things are about to break before even beginning to
bitcoinj already has a micropayment channel implementation in it. There's a
bit of work required to glue everything together, but it's not a massive
project to start using this to pay nodes for their services.
But it's not needed right now:  serving these clients is so darn cheap. And
there is plenty of room for optimising things still further!
MultiBit has always supported it. I apologise for implying you have not
built a wallet. I think yours is mSIGNA, right? Did it used to be called
something else? I recognise the website design but must admit, I have not
heard of mSIGNA before.
Regardless, as a fellow implementor, I would appreciate it more if you
designed and implemented upgrades, rather than just trashing the work done
so far as "notoriously terrible", Satoshi as "not a systems architect" and
so on.

@_date: 2015-07-29 15:41:07
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Disclosure: consensus bug indirectly solved by 
Great work!
It also means the remaining usages of OpenSSL can be safely replaced with
something like LibreSSL or (perhaps better) BoringSSL.

@_date: 2015-07-29 15:48:16
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Disclosure: consensus bug indirectly solved by 
Yes, I know. I said "other uses". For example RPC SSL and BIP 70.

@_date: 2015-07-29 20:03:22
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
No it wasn't. That is something you invented yourself much later. "Small
devices" isn't even defined anywhere, so there can't have been any such
The actual understanding was the opposite. Satoshi's words:
"At first, most users would run network nodes, but as the network grows
beyond a certain point, it would be left more and more to specialists with
server farms of specialized hardware."
That is from 2008:
Then he went on to talk about Moore's law and streaming HD videos and the
like. At no point did he ever talk about limiting the system for "small
I have been both working on and using Bitcoin for longer than you have been
around, Gregory. Please don't attempt to bullshit me about what the plan
was. And stop obscuring what this is about. It's not some personality cult
- the reason I keep beating you over the head with Satoshi's words is
because it's that founding vision of the project that brought everyone
together, and gave us all a shared goal.
If Satoshi had said from the start,
   "Bitcoin cannot ever scale. So I intend it to be heavily limited and
used only by a handful of people for rare transactions. I picked 1mb as an
arbitrary limit to ensure it never gets popular."
... then I'd have not bothered getting involved. I'd have said, huh, I
don't really feel like putting effort into a system that is intended to NOT
be popular. And so would many other people.
Don't think you can claim otherwise, because doing so is flat out wrong.
I just did claim otherwise and no, I am not wrong at all.
(Which, incidentially, is insanely toxic to any security argument for
Since when have we "campaigned" to "ignore problems" in the mining
ecosystem? What does that even mean? Was it not I who wrote this blog post?
Gregory, you are getting really crazy now. Stop it. The trend towards
mining centralisation is not the fault of Gavin or myself, or anyone else.
And SPV is exactly what was always intended to be used. It's not something
I "fixated" on, it's right there in the white paper. Satoshi even
encouraged me to keep working on bitcoinj before he left!
Look, it's clear you have decided that the way Bitcoin was meant to evolve
isn't to your personal liking. That's fine. Go make an alt coin where your
founding documents state that it's intended to always run on a 2015
Raspberry Pi, or whatever it is you mean by "small device". Remove SPV
capability from the protocol so everyone has to fully validate. Make sure
that's the understanding that everyone has from day one about what your alt
coin is for. Then when someone says, gee, it'd be nice if we had some more
capacity, you or someone else can go point at the announcement emails and
say "no, GregCoin is meant to always be verifiable on small devices, that's
our social contract and it's written into the consensus rules for that
But your attempt to convert Bitcoin into that altcoin by exploiting a
temporary hack is desperate, and deeply upsetting to many people. Not many
quit their jobs and created companies to build products only for today's
tiny user base.
My list of "things a full node is useful for" wasn't ordered by importance,
by the way.

@_date: 2015-07-31 12:16:46
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Block size following technological growth 
I agree with Gavin - whilst it's great that a Blockstream employee has
finally made a realistic proposal (i.e. not "let's all use Lightning") -
this BIP is virtually the same as keeping the 1mb cap.
Centralization of mining has been a continual gripe since Slush first
invented pooled mining. There has never been a time after that when people
weren't talking about the centralisation of mining, and back then blocks
were ~10kb.
I see constant assertions that node count, mining centralisation,
developers not using Bitcoin Core in their own businesses etc is all to do
with block sizes. But nobody has shown that. Nobody has even laid the
groundwork for that. Verifying blocks takes milliseconds and downloading
them takes seconds everywhere except, apparently, China: this resource
usage is trivial.
Yet developers, miners and users even outside of China routinely delegate
validation to others. Often for quite understandable technical reasons that
have nothing to do with block sizes.
So I see no reason why arbitrarily capping the block size will move the
needle on these metrics. Trying to arrest the growth of Bitcoin for
everyone won't suddenly make Bitcoin-Qt a competitive wallet, or make
service devs migrate away from chain.com, or make merchants stop using
We need to accept that, and all previous proposals I've seen don't seem to
I think that's a bit unfair: BIP 101 keeps a cap. Even with 8mb+growth
you're right, some use cases will be priced out. I initiated the
micropayment channels project (along with Matt, tip of the hat)
specifically to optimise a certain class of transactions. Even with 8mb+
blocks, there will still be a need for micropayment channels, centralised
exchange platforms and other forms of off chain transaction.
If Bitcoin needs to support a large scale, it already failed.
It hasn't even been tried.
The desperately sad thing about all of this is that there's going to be a
fork, and yet I think most of us agree on most things.  But we don't agree
on this.
Bitcoin can support a large scale and it must, for all sorts of reasons.
Amongst others:
   1. Currencies have network effects. A currency that has few users is
   simply not competitive with currencies that have many. There's no such
   thing as a settlement currency for high value transactions only, as
   evidenced by the ever-dropping importance of gold.
   2. A decentralised currency that the vast majority can't use doesn't
   change the amount of centralisation in the world. Most people will still
   end up using banks, with all the normal problems. You cannot solve a
   problem by creating a theoretically pure solution that's out of reach of
   ordinary people: just ask academic cryptographers!
   3. Growth is a part of the social contract. It always has been.
   The best quote Gregory can find to suggest Satoshi wanted small blocks
   is a one sentence hypothetical example about what *might* happen if
   Bitcoin users became "tyrannical" as a result of non-financial transactions
   being stuffed in the block chain. That position makes sense because his
   scaling arguments assuming payment-network-sized traffic and throwing DNS
   systems or whatever into the mix could invalidate those arguments, in the
   absence of merged mining. But Satoshi did invent merged mining, and so
   there's no need for Bitcoin users to get "tyrannical": his original
   arguments still hold.
   4. All the plans for some kind of ultra-throttled Bitcoin network used
   for infrequent transactions neglect to ask where the infrastructure for
   that will come from. The network of exchanges, payment processors and
   startups that are paying people to build infrastructure are all based on
   the assumption that the market will grow significantly. It's a gamble at
   best because Bitcoin's success is not guaranteed, but if the block chain
   cannot grow it's a gamble that is guaranteed to be lost.
   So why should anyone go through the massive hassle of setting up
   exchanges, without the lure of large future profits?
   5. Bitcoin needs users, lots of them, for its political survival. There
   are many people out there who would like to see digital cash disappear, or
   be regulated out of existence. They will argue for that in front of
   governments and courts .... some already are. And if they're going to lose
   those arguments, the political and economic damage of getting rid of
   Bitcoin must be large enough to make people think twice. That means it
   needs supporters, it needs innovative services, it needs companies, and it
   needs legal users making legal payments: as many of them as possible.
   If Bitcoin is a tiny, obscure currency used by drug dealers and a
   handful of crypto-at-any-cost geeks, the cost of simply banning it outright
   will seem trivial and the hammer will drop. There won't be a large scale
   payment network OR a high-value settlement network. And then the world is
   really screwed, because nobody will get a second chance for a very long
   time.

@_date: 2015-07-31 14:15:04
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Block size following technological growth 
Hey Jorge,
He is not saying that. Whatever the reasons for centralization are, it
It's not obvious. Quite possibly bigger blocks == more users == more nodes
and more miners.
To repeat: it's not obvious to me at all that everything wrong with Bitcoin
can be solved by shrinking blocks. I don't think that's going to suddenly
make everything magically more decentralised.
The 8mb cap isn't quite arbitrary. It was picked through negotiation with
different stakeholders, in particular, Chinese miners. But it should be
high enough to ensure organic growth is not constrained, which is good
I think it would be nice to have some sort of simulation to calculate
Centralization is not a single floating point value that is controlled by
block size. It's a multi-faceted and complex problem. You cannot "destroy
Bitcoin through centralization" by adjusting a single constant in the
source code.
To say once more: block size won't make much difference to how many
merchants rely on payment processors because they aren't using them due to
block processing overheads anyway. So trying to calculate such a formula
won't work. Ditto for end users on phones, ditto for developers who want
JSON/REST access to an indexed block chain, or hosted wallet services, or
miners who want to reduce variance.
None of these factors have anything to do with traffic levels.
What people like you are Pieter are doing is making a single number a kind
of proxy for all fears and concerns about the trend towards outsourcing in
the Bitcoin community. Everything gets compressed down to one number you
feel you can control, whether it is relevant or not.
That isn't what I said at all Jorge. Let me try again.
Setting up an exchange is a lot of risky and expensive work. The motivation
is profit, and profits are higher when there are more users to sell to.
This is business 101.
If you remove the potential for future profit, you remove the motivation to
create the services that we now enjoy and take for granted. Because if you
think Bitcoin can be useful without exchanges then let me tell you, I was
around when there were none. Bitcoin was useless.

@_date: 2015-07-31 16:58:59
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Block size following technological growth 
Because the bigger the ecosystem is the more interest there is in taking
I mean, I guess I don't know how to answer your question. When Bitcoin was
new it had almost no users and almost no miners. Now there are millions of
users and factories producing ASICs just for Bitcoin. Surely the
correlation is obvious?
I'm sorry, but until there's a simulation that I can run with different
Gavin did run simulations. 20mb isn't arbitrary, the process behind it was
well documented here:
*I chose 20MB as a reasonable block size to target because 170 gigabytes
per month comfortably fits into the typical 250-300 gigabytes per month
data cap? so you can run a full node from home on a ?pretty good? broadband
Did you think 20mb was picked randomly?
I have a hard time agreeing with this because I've seen Bitcoin go from
blocks that were often empty to blocks that are often full, and in this
time the number of miners and hash power on the network has gone up a huge
amount too.
You can argue that a miner doesn't count if they pool mine. But if a miner
mines on a pool that uses exactly the same software and settings as the
miner would have done anyway, then it makes no difference. Miners can
switch between pools to find one that works the way they like, so whilst
less pooling or more decentralised pools would be nice (e.g.
getblocktemplate), and I've written about how to push it forward before, I
still say there are many more miners than in the past.
If I had to pick between two changes to improve mining decentralisation:
1) Lower block size
2) Finishing, documenting, and making the UX really slick for a
getblocktemplate based decentralised mining pool
then I'd pick (2) in a heartbeat. I think it'd be a lot more effective.
I did toy with that idea a while ago. Of course there can not really be no
limit at all because the code assumes blocks fit into RAM/swap, and nodes
would just end up ignoring blocks they couldn't download in time anyway.
There is obviously a physical limit somewhere.
But it is easier to find common ground with others by compromising. Is 8mb
better than no limit? I don't know and I don't care much:  I think Bitcoin
adoption is a slow, hard process and we'll be lucky to increase average
usage 8x over the next couple of years. So if 8mb+ is better for others,
that's OK by me.
OK. I write these emails for other readers too :) In the past for instance,
developers who run services without running their own nodes has come up.
Re: exchange profit. You can pick some other useful service provider if you
like. Payment processors or cold storage providers or the TREZOR
manufacturers or whoever.
My point is you can't have a tiny high-value-transactions only currency AND
all the useful infrastructure that the Bitcoin community is making. It's a
contradiction. And without the infrastructure bitcoin ceases to be
interesting even to people who are willing to pay huge sums to use it.

@_date: 2015-10-02 14:23:17
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Crossing the line? [Was: Re: Let's deploy BIP65 
FWIW the "coining" I am referring to is here:
OK, with that, here goes. Firstly some terminology. I'm going to call these
things SPV clients for "simplified payment verification". Headers-only is
kind of a mouthful and "lightweight client" is too vague, as there are
several other designs that could be described as lightweight like RPC
frontend and Stefans WebCoin API approach
At that time nobody used the term "SPV wallet" to refer to what apps like
BreadWallet or libraries like bitcoinj do. Satoshi used the term "client
only mode", Jeff was calling them "headers only client" etc. So I said, I'm
going to call them SPV wallets after the section of the whitepaper that
most precisely describes their operation.

@_date: 2015-10-05 12:59:54
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Putting aside stupid arguments about who is older or who starting using the
term SPV wallet first, let me try and make a better suggestion than what's
in the BIP. How about the following:
A new flag is introduced to Core, --scriptchecks=[all,standardonly,none].
The default is all. When set to "standardonly", non-standard scripts are
not checked but others are. This is similar to the behaviour during a soft
fork. In "none" you have something a bit like SPV mode, but still
calculating the UTXO set. This flag is simple and can be implemented in a
few lines of code. Then an unused opcode is used for CLTV, so making it a
hard fork.
This has the following advantages:
   - Nodes that want the pseudo-SPV behaviour of a soft fork can opt in to
   it if they want it. This prioritises availability (in a sense) over
   correctness.
   - But otherwise, nodes will prioritise correctness by default, which is
   how it should be. This isn't PHP where nonsensical code the interpreter
   doesn't understand just does ...... something. This is financial software
   where money is at risk. I feel very strongly about this: undefined
   behaviour is fine *if you opted into getting it. *Otherwise it should be
   avoided whenever possible.
   - SPV wallets do the right thing by default.
   - IsStandard doesn't silently become a part of the consensus rules.
   - All other software gets simpler. It's not just SPV wallets. Block
   explorers, for example, can just add a single line to their opcode map.
   With a soft fork they have to implement the entire soft fork logic just to
   figure out when an opcode transitioned from OP_NOP to CLTV and make sure
   they render old scripts differently to new scripts. And they face tricky
   questions - do they render an opcode as a NOP if the miner who built it was
   un-upgraded, or do they calculate the flag day and change all of them after
   that? It's just an explosion of complexity.
Many people by now have accepted that hard forks are simpler, conceptually
cleaner, and prioritise correctness of results over availability of
results. I think these arguments are strong.
So let me try addressing the counter-arguments one more time:
   - Hard forks require everyone to upgrade and soft forks don't. I still
   feel this one has never actually been explained. There is no difference to
   the level of support required to trigger the change. With the suggestion
   above, if someone can't or won't upgrade their full node but can no longer
   verify the change, they can simply restart with -scriptchecks=standardonly
   and get the soft fork behaviour. Or they can upgrade and get their old
   security level back.
   - Hard forks are somehow bad or immoral or can lead to "schisms". This
   is just saying, if we hold a vote, the people who lose the vote might try
   starting a civil war and refuse to accept the change. That's not a reason
   to not hold votes.
   But at any rate, they can do that with soft forks too: just decide that
   any output that contains OP_CLTV doesn't make it into the UTXO set.
   Eventually coins that trace back to such an output will become unusable in
   the section of the economy that decided to pick a fight.

@_date: 2015-10-05 13:28:13
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Well, let's agree to disagree on these two things:
- I define "working" for a full node as verifying everything; if a node
starts skipping bits then I'd say it's not really "working" according to
its original design goals
- Saying the pre-fork behaviour is defined and deterministic is true, but
only in the sense that reading an uninitialised variable in C is defined
and deterministic. It reads whatever happens to be at that stack position:
easily defined. For many programs, that may be the same value each time:
deterministic. Nonetheless, it's considered undefined behaviour by the C
specification and programmers that rely on it can easily create security
In the same way, I'd consider a node running a script with a NOP and
reaching the opposite conclusion from other nodes to be a case of undefined
behaviour leading to a non-fully-working node.
But these are arguments about the semantics of words. I think we both know
what each other is getting at.

@_date: 2015-10-05 14:10:40
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Hi Jorge,
I'm glad we seem to be reaching agreement that hard forks aren't so bad
really and can even have advantages. It seems the remaining area of
disagreement is this rollout specifically.
Indeed it will, but the point of fully verifying is to *not* converge with
the miner majority, if something goes wrong and they aren't following the
same rules as you. Defining "work" as "converge with miner majority" is
fine for SPV wallets and a correct or at least reasonable definition. But
not for fully verifying nodes, where non-convergence is an explicit design
goal! That's the only thing that stops miners awarding themselves infinite
free money!
No, I'm focused on the block size issue right now. I don't think there's
much point in improving the block chain protocol if most users are going to
be unable to use it. But the modification is simple, right? You just
replace this bit:
  CHECKLOCKTIMEVERIFY redefines the existing NOP2 opcode
with this
  CHECKLOCKTIMEVERIFY defines a new opcode (0xc0)
and that's it. The section *upgrade and testing plan* only says TBD so that
part doesn't even need to change at all, as it's not written yet.

@_date: 2015-10-05 18:46:28
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
It was an example. Adam Back's extension blocks proposal would, in fact,
allow for a soft forking change that creates more subsidy than is valid (or
does anything else) by hiding one block inside another.
Anyway, I think you got my point.
I'm pretty sure Gregory did not use such an example because it's dead
wrong. You cannot verify the size of a coinbase without being a fully
verifying node because you need to know the fees in the block, and
calculating that requires access to the entire UTXO set.
This sort of thing is why I get annoyed when people lecture me about SPV
wallets and the things they "should" do. None of you guys has built one. I
keep seeing wild statements about theoretical unicorn wallets that nobody
has even designed, and how all existing wallets are crappy and insecure
because they don't meet your ever shifting goal posts.
To everyone making such statements I say: go away and build an SPV wallet
of your own from scratch. Then you will understand the engineering
tradeoffs involved much better, and be in a much better position to debate
what they should or should not be doing.
And bear in mind if it weren't for the work myself and a few others did on
SPV wallets, everyone would be using web wallets instead. Then you'd all
just complain about that instead.
Making it a hard fork instead is changing one line of code (ignoring the
code to set up the flag day, which can be based on the code for BIP101). If
it comes down to it, then I'll do the work to change that one line. But
obviously I'd need to see agreement from the maintainers that such a pull
req would be merged first.
The example is this: find someone that accepts 1-block confirmed
transactions in return for something valuable. There are plenty of them out
there. Once the soft fork starts, send a P2SH transaction that defines a
new output controlled by OP_CLTV. It will be incorporated into the UTXO set
by all miners because it's opaque (p2sh).
Now send a transaction that pays the merchant, and make it spend your
OP_CLTV output with an invalid script. New nodes will reject it as a rule
violator. Old nodes won't. So at some point an old miner will create a
block containing your invalid transaction, the merchant will think they got
paid, they'll give you the stuff and the fraud is done.
This is just embarrassing - do any of you guys at Blockstream actually use
Bitcoin in the real world? Virtually all payments that aren't moving money
into/out of exchange wallets are 0-confirm in reality. I described a
1-confirm attack above, but really ... come on.

@_date: 2015-10-05 18:56:38
@_author: Mike Hearn 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Hey Sergio,
To clarify: my *single* objection is that CLTV should be a hard fork. I
haven't been raising never-ending technical objections, there's only one.
I *have* been answering all the various reasons being brought up why I'm
wrong and soft forks are awesome .... and there do seem to be a limitless
number of such emails .... but on my side it's still just a single
objection. If CLTV is a hard fork then I won't be objecting anymore, right?
CLTV deployment is clearly controversial. Many developers other than me
have noted that hard forks are cleaner, and have other desirable
properties. I'm not the only one who sees a big question mark over soft
As everyone in the Bitcoin community has been clearly told that
controversial changes to the consensus rules must not happen, it's clear
that CLTV cannot happen in its current form.
Now I'll be frank - you are quite correct that I fully expect the Core
maintainers to ignore this controversy and do CLTV as a soft fork anyway.
I'm a cynic. I don't think "everyone must agree" is workable and have said
so from the start. Faced with a choice of going back on their public
statements or having to make changes to something they clearly want, I
expect them to redefine what "real consensus" means. I hope I'm wrong, but
if I'm not ..... well, at least everyone will see what Gavin and I have
been talking about for so many months.
But I'd rather the opcode is tweaked. There's real financial risks to a
soft fork.

@_date: 2015-10-14 11:09:40
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Memory leaks? 
Leaks are not the only explanation possible. Caches and fragmentation can
also give this sort of effect. Unfortunately the tools to debug this aren't
great. You could try a build with tcmalloc and use it to investigate heap
Odinn, trolling like a 3 year old will get you swiftly banned. Last warning.

@_date: 2015-10-20 12:12:12
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Memory leaks? 
OK, then running under Valgrind whilst sending gbt RPCs would be the next

@_date: 2015-10-23 13:30:22
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Mailing List Moderation Now Active. 
Are block size discussions considered acceptable, then?

@_date: 2015-10-28 11:26:56
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Mailing List Moderation Now Active. 
This post by Gavin got rejected by the moderators.
Without a doubt this moderation policy is already a disaster. I'm fully
expecting this message to get rejected too, but so you can see it Rusty: so
far in the reject bin there are messages from:
   - Well known uber-troll Gavin Andresen
   - And his partner in crime, Sergio Damian Lerner
   - Someone discussing a bug
   - Someone who wants to discuss CLTV
   - Someone pointing out that censorship of technical discussion is rarely
   a good idea
   - Someone pointing out that censorship of people complaining about
   censorship is also taking place.

@_date: 2015-09-18 17:22:09
@_author: Mike Hearn 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
This is exactly what SPV libraries like bitcoinj do: they know how to build
a block locator, request the blocks forward from the common branch point,
and handle re-orgs onto whatever the current best chain are by downloading
data from a full node.
If your official position is people should all use bitcoinj to do things
like build extra indexes, then great. Send them our way. It already knows
how to calculate a UTXO set indexed by address.

@_date: 2015-09-18 23:33:33
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Any change that results in this happening all over again in a few years
does not have consensus.

@_date: 2015-09-19 21:43:32
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Which part of "in the next few years" was unclear?
This seems to be a persistent problem in the block size debates: the
assumption that there are only two numbers, zero and infinity.
BIP101 tops out at 8 gigabyte blocks, which would represent extremely high
transaction rates compared to today. *If* Bitcoin ever became so popular,
it would be a long way in the future, and many things could have happened:
   1. Bitcoin may have become as irrelevant as the Commodore 64 is.
   2. We may have invented upgrades that make Bitcoin 100x more efficient
   than today.
   3. Hardware may have improved so much that it no longer matters.
   4. The world may have been devastated by nuclear war and nobody gives a
   shit about internet currencies anymore, because there is no internet.
It's silly to ignore the time dimension in these decisions. Bitcoin will
not last forever: even if it becomes very successful it will one day it
will be replaced by something better, so it does not have to handle
infinite usage.
But hey, as you bring it up, I'd have been happy with no upper limit at
all. There's nothing magic about 8 gigabytes. I go along with BIP 101
because it is still the only proposal that is both reasonable and
implemented, and I'm willing to compromise.

@_date: 2015-09-19 21:57:40
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
And so we get to one of the hearts of the debate.
The axiom upon which you and NxtChg disagree is this: he/she believes
governments can crush Bitcoin if they want regardless of how decentralised
it is, and you don't.
If one believes governments have the power to end Bitcoin no matter what,
then the only true protection comes from popularity. Governments find it
hard to ban things that are wildly popular with their voters. This is the
Uber approach: grow fast, annoy governments, but be popular enough that
banning you is politically risky.
If you don't believe that governments can end Bitcoin because of
decentralisation, then the opposite conclusion is logical: growth can be
dangerous because stateless money will be inherently opposed by the state,
therefore if growth == less decentralisation, growth increases the risk of
state shutdown.
I don't think we have to choose between decentralisation and growth
actually - computers are just amazingly fast. But that's irrelevant here.
The point is, your disagreement is summed up by your statement:
I believe this statement is wrong because governments can shut down Bitcoin
at any point regardless of its level of decentralisation. This is true
   - Most governments can easily spend enough money to do a 51% attack,
   especially if they can compel chip fabs to cooperate for free. This attack
   works regardless of how decentralised Bitcoin is.
   - Any government can end Bitcoin usage in its territory by jailing
   anyone who advertises acceptance/trading of bitcoins, or prices in BTC.
   Because merchants *must* advertise in order to alert customers that
   trades in BTC are possible, this is an attack which is unsolvable. If
   ordinary people can find such merchants so can government agents.
It may appear that trade cannot be suppressed because merchants can all
become anonymous too, a la Silk Road. However, if use of Bitcoin is banned
then it becomes impossible to convert coins into local currency as that
requires cooperation of banks ..... making it useless for even anonymous
merchants. An outlaw currency is useless even to outlaws.
Because Bitcoin's existence ultimately relies on government cooperation and
acceptance, the best way to ensure its survival is growth. Lots of it.

@_date: 2015-09-20 10:25:03
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
And yet the proposed legislation goes nowhere, and the USA continues to
stand alone in having the first world's weakest gun control laws.
You are just supporting my point with this example. Obama would like to
restrict guns, but can't, because they are too popular (in the USA).
The comparison to BitTorrent is likewise weak: governments hardly care
about piracy. They care enough to pass laws occasionally, but not enough to
put serious effort into enforcement. Wake me up when the USA establishes a
Copyright Enforcement Administration with the same budget and powers as the
Internet based black markets exist only because governments tolerate them
(for now). A ban on Tor, Bitcoin or both would send them back to the
pre-2011 state where they were virtually non-existent. Governments tolerate
this sort of abuse only because they believe, I think correctly, that
Bitcoin can have great benefits for their ordinary voters and for now are
willing to let the tech industry experiment.
But for that state of affairs to continue, the benefits must actually
appear. That requires growth.
I think there's a difference between natural growth and the kind of growth
What difference? Are you saying the people who come to Bitcoin because of a
startup are somehow less "natural" than other users?

@_date: 2015-09-21 11:30:10
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
I understand, it does seem off topic. But ..... what was the topic again?
All Jeff's mail and the followups seem to say is there was a meeting where
some people (unnamed) agreed to do something (unspecified) if the metric
used is modified (which doesn't change the fundamental issues).
So there isn't really much on-topic to discuss. If/when Wladimir starts a
thread, with a BIP, and says "this is how it's gonna be in Bitcoin Core",
then there will be things to discuss.

@_date: 2015-09-28 12:48:57
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
There is *no* consensus on using a soft fork to deploy this feature. It
will result in the same problems as all the other soft forks - SPV wallets
will become less reliable during the rollout period. I am against that, as
it's entirely avoidable.
Make it a hard fork and my objection will be dropped.
Until then, as there is no consensus, you need to do one of two things:
1) Drop the "everyone must agree to make changes" idea that people here
like to peddle, and do it loudly, so everyone in the community is correctly
2) Do nothing

@_date: 2015-09-28 13:40:35
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
The rationale of "backwards compatibility" is well known, yet wrong. I've
gone over the arguments here and explained why the concept makes no sense:
Eric - no, it's not sophisticated humour. I've been objecting to soft forks
since this idea first appeared.
There is no consensus. Now pick. Lose the requirement that everyone agree
for consensus changes, and tell people you've done it. Change the spec. Or
do nothing.

@_date: 2015-09-28 14:26:17
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I don't intend to do that, and I don't think I am - I know what the
difference between a soft and hard fork is and am not trying to confuse or
blur the two.
To reiterate: this current BIP implements a soft fork. I am not debating
that. I am saying it should use a hard fork instead. This will ensure no
repeat of the P2SH case where invalid blocks were being found for weeks (or
was it months?) after the new rules kicked in, thus exposing SPV wallets
and old nodes to unnecessary risk for no benefit.
Additionally, I am making it clear that there's no consensus for rolling
out the new opcode in this way. As you say, the mechanism has issues. If
you read the comments when I wrote my article, you can see that others
share the same concerns:

@_date: 2015-09-28 14:54:33
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Just use an opcode that isn't currently defined. Done. What about that
mechanism is prone to failure?
Re: coma. No need for insults. Please read my article and address the
points raised there, which, by the way, do not include any mention of SPV
wallets. Although your belief that SPV wallets are "inherently insecure"
seems needlessly trollish - I certainly would disagree, but it's a
different debate.

@_date: 2015-09-28 15:41:56
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
The opcode definition seems OK.
Yes. It might be worth putting the version bit change behind a command line
flag though: the BIP, as written, has problems (with deployment).
   detect advertised soft-forks and correctly handle them?
I'd really hate to do that. It'd be a Rube Goldberg machine:
   There's no really good way to do what you propose, and we already have a
perfectly workable mechanism to tell SPV clients about chain forks: the
block chain itself. This has the advantage of being already implemented,
already deployed, and it works correctly.
Attempting to strap a different mechanism on top to try and make soft forks
more like hard forks would be a large and pointless waste of people's time
and effort, not just mine (bitcoinj is not the only widely used SPV
implementation nowadays). You may as well go straight to the correct
outcome instead of trying to simulate it with ever more complex mechanisms.

@_date: 2015-09-28 16:17:06
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
There is simply no need for any wallets to change. Making the spec a hard
fork instead of a soft fork means all existing software does the right
thing automatically.
To repeat, please bear in mind that bitcoinj is no longer the only SPV
wallet implementation. BreadWallet has its own code in Objective-C and is
the second most popular SPV implementation (and growing). Additionally,
bitcoinj is incorporated into lots of apps that'd have to have new versions
released, some of which don't have any way to force a user to update.
So it's not just my time you'd waste: it's lots of different people's.
One thing I haven't seen yet is the justification for why a soft fork
should be used here. There's no requirement that it be so, and there are
real downsides. As Eric said, the fact that the mechanism has issues is not
under dispute.
The normal justification for this it's that it's forwards compatible. But
that's not a justification, that's a description.
Re: XT, I already addressed this above.

@_date: 2015-09-28 16:33:23
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
They don't have to - they pick the highest work chain. Any miner who hasn't
upgraded makes blocks on the shorter chain that are then ignored (or
rather, stored for future reorgs). After the fork point, there won't be any
blocks in the main chain that violate the rules and end up being doomed to
being orphaned, which is the underlying problem.
And I think you know this already. There is no "flaw" in bitcoinj in this
respect. It works exactly as it was designed to work.

@_date: 2015-09-28 16:51:22
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Please read my article as it's all explained there.
But to reiterate: the risk is that miners will build invalid blocks on top
of the best work chain, instead of an ignored lower work side chain. This
opens users to payment fraud. With a hard fork, all the blocks by miners
that aren't checking all the rules anymore get neatly collected together on
a side chain after the split, and wallets all know how to ignore that chain.
Yes, you made OP_NOPs be non-standard. So out of the box, miners won't
create invalid blocks, as long as they're running Core past that version.
But this makes the IsStandard function very much like a part of the
consensus rules, as bypassing it can result in invalid blocks being
created. Miners have always understood that they can modify this function,
or even bypass it entirely, without affecting the validity of their blocks.
And some miners do exactly that.
So I'll repeat the question that I posed before - given that there are
clear, explicit downsides, what is the purpose of doing things this way?
Where is the gain for ordinary Bitcoin users?

@_date: 2015-09-28 17:38:28
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
I'm confused - I already said this. For a fork to work, hard or soft, there
must be support from a majority of the hash power.
Therefore, the usual SPV technique of following the highest work chain
results in ignoring the minority chain produced by the hard fork.
BIP 101 is SPV friendly because the wallets would simply follow the 75%
chain and never even be aware anything has changed. It's backwards
compatible with them in this respect: they already know how to ignore the
no-bigger-blocks fork that'd be created if some miners didn't upgrade
during the grace period.
My point about IsStandard is that miners can and do bypass it, without
expecting that to carry financial consequences or lower the security of
other users. By making it so a block which includes non-standard
transactions can end up being seen as invalid, you are increasing the risk
of accidents that carry financial consequences.
That's incorrect: Miners bypassing IsStandard() risk creating invalid
Gah. You repeated what I just said. Yes, I know miners face that risk, my
point is that they do NOT face such a risk when there's no soft fork in
action and have historically NOT faced that risk at all, hence the
widespread practice of bypassing or modifying this function.
All this approach does is make changing IsStandard() the same as changing
AcceptBlock(), except without the advantage of telling anyone about it.
Obviously. So please enlighten me.
How do ordinary Bitcoin users benefit from this rollout strategy? Put
simply, what is the point of this whole complex soft fork endeavour?

@_date: 2015-09-28 19:14:15
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
That is not a benefit. That is a description of what the software will do,
but not why you would want it.
In case this seems like a pedantic point, consider the consequences of
following a chain you aren't checking properly. You get SPV level security
and might calculate a corrupted ledger.
In the case of P2SH, I could make a transaction that spends someone elses
money to myself. In the case of CLTV, I could ignore the locktime
Now yes, eventually, the miner majority will correct and uncorrupt your
ledger for you. But by then it might be too late, you may have already
acted upon the incorrect data by e.g. selling me lots of stuff that I paid
for with somebody else's coins. If you don't care about that risk, hey,
switch to an SPV wallet and save yourself a lot of disk space.
There isn't any difference in how long the divergent state exists for. That
depends only on how fast people upgrade, which is unaffected by the rollout
strategy used.

@_date: 2015-09-29 14:02:59
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
All clients do *not* do this. Why would they? What action would they take?
Try and simulate a hard fork in some complicated roundabout manner? Why not
just do the real thing and keep things simple?
They are already non-standard. That change was made last time I brought up
the problems with soft forks. It brought soft forks that use OP_NOPs a bit
closer to the ideal of a hard fork, but didn't go all the way. I pointed
that out above in my reply to Peter's mail.
So to answer your question, no, it wouldn't satisfy my concerns. My logic
is this:
Hard forks - simple, well understood, SPV friendly, old full nodes do not
calculate incorrect ledgers whilst telling their users (via UI, RPC) that
they are fully synced. Emphasis on simple: simple is good.
Soft forks - to get the benefits of a hard fork back requires lots of extra
code, silently makes IsStandard() effectively a part of the consensus rules
when in the past it hasn't been, SPV unfriendly. Benefits? As far as I can
tell, there are none.
If someone could elucidate *what* the benefits actually are, that would be
a good next step. So far everyone who tried to answer this question gave a
circular answer of the form "soft forks are good because they are soft

@_date: 2015-09-29 14:07:24
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Hi Jorge,
Yes, there is a difference. Assuming the hashrate majority upgrades, in the
Yes, I know what the difference between them is at a technical level. You
didn't explain why this would make any difference to how fast miners
upgrade. The amount of money they lose in both cases is identical: they are
equally incentivised to upgrade with both fork types.
Additionally, you say in a hard fork the other chain may "continue
forever". Why do you think this is not true for miners building invalid
blocks on top of the main chain? Why would that not continue forever?
There just isn't any difference between the two fork types in terms of how
fast miners would upgrade. Heck if anything, a hard fork should promote
faster upgrades, because if a miner isn't paying attention to their
debug.log they might miss the warnings. A soft fork would then look
identical to a run of really bad luck, which can legitimately happen from
time to time. A hard fork results in your node having a different height to
everyone else, which is easily detectable by just checking a block explorer.
Isn't that circular? This thread is about deployment of CLTV, but the BIP
assumes a particular mechanism, so pointing out problems with it is off
topic? Why have a thread at all?

@_date: 2015-09-29 19:29:01
@_author: Mike Hearn 
@_subject: [bitcoin-dev] On bitcoin-dev list admin and list noise 
There's a simple way to cut down on "noise" that doesn't involve people
shouting OFFTOPIC at each other: the maintainer needs to resolve
discussions by making decisions and saying, this is how Core does it. If
you disagree, go make/join a fork because there's no point in discussing
this any further here. People would get the picture pretty fast.
Wladimir doesn't do this. That is something he should fix. Clearly ending
debates so they don't run in circles forever is a basic management
Allowing random people to give each other red cards is merely a great way
to piss off users even further, because anyone who objects to a clearly
bone-headed decision will be told they're discussing philosophy and to go
away (this is meaningless).

@_date: 2015-09-29 19:51:34
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Is it possible for there to be two chains after a 
No, you rely on miners honesty even if you run a full node. This is in the
white paper. A dishonest miner majority can commit fraud against you, they
can mine only empty blocks, they can do various other things that render
your money worthless.

@_date: 2015-09-29 20:02:06
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Is it possible for there to be two chains after a 
I didn't say it was, sorry, the comma was separating two list items. By
"fraud" I meant double spending. Mining only empty blocks would be a DoS
attack rather than double spending.

@_date: 2015-09-30 14:30:25
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
If Core ships CLTV as is, then XT will have to adopt it - such is the
nature of a consensus system.
This will not change the fact that the rollout strategy is bad and nobody
has answered my extremely basic question: *why* is it being done in this
way, given the numerous downsides?

@_date: 2015-09-30 19:11:56
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Hi Gregory,
Why? I have objected to the idea of soft forks many times. I wrote an
entire article about it in August. I also objected in April 2014, for
instance, where Pieter agreed with me that soft forks can result in ugly
hacks, and that they are "not nice philosophically because they reduce the
security model of former full nodes to SPV without their knowledge" (he
thought they were worth it anyway).
This is not a new debate. If you're surprised, it means only you weren't
paying attention to all the previous times people raised this issue.
There's no such thing as a "real" hard fork - don't try and move the goal
posts. SPV clients do not need any changes to do the right thing with BIP
101, they will follow the new chain automatically, so it needs no changes.
Several people have asked several times now: given the very real and widely
acknowledged downsides that come with a soft fork, *what* is the specific
benefit to end users of doing them?
Until that question is answered to my satisfaction I continue to object to
this BIP on the grounds that the deployment creates financial risk
unnecessarily. To repeat: *CLTV does not have consensus at the moment*.
BTW, in the April 2014 thread Pieter's argument was that hard forks are
more risky, which is at least an answer to my question. But he didn't
explain why he thought that. I disagree: the risk level seems lower with a
hard fork because it doesn't lower anyone's security level.

@_date: 2015-09-30 21:56:01
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
The global upgrade is required for all full nodes in both types. If a full
node doesn't upgrade then it no longer does what it was designed to do; if
the user is OK with that, they should just run an SPV wallet or use
blockchain.info or some other mechanism that consumes way fewer resources.
But if you want the software you installed to achieve its stated goal, you
*must* upgrade. There is no way around that.
Jorge has said soft forks always lead to network convergence. No, they
don't. You get constant mini divergences until everyone has upgraded, as
opposed to a single divergence with a hard fork (until everyone has
upgraded). The quantity of invalid blocks mined, on the other hand, is
identical in both types.
Adam has said "there is actually consensus", although I just said there
isn't. Feel free to say what you really mean here Adam - there's consensus
if you ignore people who don't agree, i.e. the concept of "developer
consensus" doesn't actually mean anything. This would contradict your prior
statements about how Bitcoin Core makes decisions, but alright ....
Finally John, I fully agree with what you wrote. Debates that never end are
bad news all round. Bitcoin Core has told the world it uses "developer
consensus" to make decisions. I don't agree that's a good way to do things,
but if Core wants to stick with it then there is no choice - as I am a
developer, and I do not agree with the change, there is no consensus and
the debate is over.
Hey, I have an idea. Maybe we should organise a conference about soft vs
hard forks. Let's have it down the road from where I live, a couple of
weeks from now. Please submit your talk titles to me so I can vet them to
ensure nobody does an offtopic talk ;)

@_date: 2015-09-30 23:01:09
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
tl;dr Nothing I have read here has changed my mind. There is still no
consensus to deploy CLTV in this way.
I responded to your response several times. It was not convincing, and I do
not think you corrected factual inaccuracies. I mean, you said yourself you
once used the correct terminology of forwards compatibility but stopped
only because the term "backwards compatibility" is more common. But that's
not a good reason to use a term with the opposite meaning and is certainly
not a factual correction!
I coined the term SPV so I know exactly what it means, and bitcoinj
implements it, as does BreadWallet (the other big SPV implementation).
Yes, SPV wallets will follow the mining hashpower instead of doing a hard
reject for bigger blocks, because they deliberately check a subset of the
rules: block size is not and never has been one of them. Indeed it's not
even included in the protocol messages. Users have no expectation that SPV
wallets would check that, as it's never been claimed they do.
On the other hand, full nodes all claim they run scripts. Users expect that
and may be relying on it. The unstated assumption here is that the nodes
run them correctly. A soft fork breaks this assumption.
I'm going to ignore the rest of the stuff you wrote about "design decisions
to lack security" or "cheaply avoidable lack of validation". When you have
sat down and written an SPV implementation by yourself, then shipped it to
a couple of million users, you might have better insight into basic
engineering costs. Until then, I find your criticisms of code you think was
missing due to "stonewalling" and so on to be seriously lacking real world
Yes, a hypothetical full node could fork on the version bits. I would be
quite happy with the version number in the header being an enforced
consensus rule: it'd make hard forks easier to trigger. But it hasn't been
done that way, and wishing away the behaviour of existing software in the
field is no good. Luckily, for introducing a new opcode, the same effect
can be achieved by using a non-allocated opcode number.
This is subjective. I'd say picking an entirely new opcode number is most
The rest of your argument boils down to "people don't have to upgrade if
they don't want to", which is addressed in the article I wrote already, and
multiple responses on this thread. Yes, they do, otherwise they aren't
getting the security level they were before.
What? This is nonsensical. P2SH was added to the full verification code
quite quickly, but it didn't matter much because nobody uses bitcoinj for
mining. The docs explicitly tell people, in fact, not to mine on top of
So no, bitcoinj+P2SH was irrelevant from a fork type perspective. It just
had no effect at all. This entire section of your message is completely
The code that did take longer was for wallet support. And the reason it
came later was resource prioritisation: there were more important issues to
resolve. Like I said - write the amount of code I've written, unpaid in
your evenings and weekends, and then you can criticise bitcoinj for lacking
75% is a fine activation threshold. By definition if support is at 75% then
bigger blocks is "winning", but if support fell, then the SPV wallets would
just reorg back onto the 1mb-blocks chain.
Re: demonstrated track record. They "work" only if you ignore the actual
problems that have resulted. P2SH-invalid blocks were being mined for weeks
after the flag day. That's not good no matter how you slice it: even if you
didn't hear about any fraud resulting, it is still risk that can be avoided.

@_date: 2015-09-30 23:06:01
@_author: Mike Hearn 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
A miner that has accepted a newly invalid transaction into its memory pool
and is trying to mine it, will keep producing invalid blocks forever until
the owner shuts it down and upgrades. This was happening for weeks after
P2SH triggered.
For instance, any miner that has modified/bypassed IsStandard() can do
this, or any miner that accepts direct transaction submission, or any miner
that runs an old node from before OP_NOPs were made non-standard.
Which they do, because they will eventually notice they are burning money.
Sorry Jorge, but I don't think your argument makes sense.
