
@_date: 2015-08-06 14:16:56
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
Is there any up to date documentation about TheBlueMatt relay network
including what kind of block compression it is currently doing? (apart from
the source code)
Regards, Sergio.
On Wed, Aug 5, 2015 at 7:14 PM, Gregory Maxwell via bitcoin-dev <

@_date: 2015-08-07 18:18:48
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] If you had a single chance to double the 
What would you do?
a. Double the block size
b. Reduce the block rate to a half (average 5 minute blocks)
Suppose this is a one time hard fork. There no drastic technical problems
with any of them: "SPV" mining and the relay network has shown that block
propagation is not an issue for such as small change. Mining centralization
won't radically change for a 2x adjustment.
So what would be best for Bitcoin?
I suspect some (if not most of you) would choose b. Because reducing the
block interval saves us real time. Waiting 30 minutes for a 3-block
confirmation is... such a long time! Time that we value. Time that
sometimes we waste waiting. Time that makes a difference for us. Doubling
the block size does not change the user perception of Bitcoin in any way.
Then why most discussions go around doubling the block size?
Each change require less than 20 lines of code (*) in the reference code,
and minimum change in other wallets.
Currently there is no idle mining hardware for hire, so the security of six
10-minute block confirmation is equivalent to the security of six 5-minute
block confirmations, as described in Satoshi's paper (if there were 51%
spare mining hardware for hire, then obviously hiring that hardware for 30
minutes would cost less than hiring it for 1 hour).
Why we discuss a 2x block size increase and not a 1/2 block interval
reduction? Aren't we Bitcoin users after all?
Best regards,
 Sergio.
(*) b requires increasing the transaction version number, to support the
old nLockTime rate.

@_date: 2015-08-07 18:37:01
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] If you had a single chance to double the 
It took you 3 minutes to respond to my e-mail. And I responded to you 4
minutes later. If you had responded to me in 10 minutes, I would be of out
the office and we wouldn't have this dialogue. So 5 minutes is a lot of
Obviously this is not a technical response to the technical issues you
argue. But "minutes" is a time scale we humans use to measure time very
On Fri, Aug 7, 2015 at 6:27 PM, Mark Friedenbach

@_date: 2015-08-07 20:08:02
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] If you had a single chance to double the 
In some rare occasions in everyday life, what matters is seconds. Like when
paying for parking in the car while some other cars are behind you in the
line. You don't want them to get upset.
I takes me tens of minutes to shop. But once you choose your merchandise
and your payment starts processing, if the payment system allows several
payments to be pending simultaneously and you're not blocking the next
person to pay, then I don't care waiting some minutes for confirmation. But
saving 10 minutes of confirmation is a lot.
I ague that our time is mostly measured in minutes (but I don't have any
sociological, cultural, genetic or anthropological evidence). It takes
minutes to read an e-mail, minutes to correct a bug, minutes to have lunch,
minutes to drive to the office, minutes to talk to your kids. A payment
taking 1 minute is much better than a payment taking 10. If I could choose
a single thing to change to Bitcoin, I would lower the payment time, even
within the minute scale.

@_date: 2015-08-10 19:11:10
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] If you had a single chance to double the 
What I'm saying is that this ratio may have improved 20x since miners began
using the TheBlueMatt relay network, so deteriorating the ratio 2x does not
put miners in a unknown future, but in an future which is far better than
the state they were a year ago.
But SPV mining has improved the ratio another 2x (because headers can be
pushed even faster, fit in a single network packet, and can do without
inv/getdata round-trips because they basically "pay" for the bandwidth
usage by its own proof of work).
With a better wire protocol you can "propagate" a 10 MB block faster that
the time it takes currently to propagate an empty block.
So 10x deterioration of the ratio would be still something acceptable.
All problems that result from propagation delay are literally doubled by
I'm not saying it's a good thing. I'm saying that it's impossible to avoid.
It's a real incentive. It must exists so Bitcoin is incentive compatible.
We can talk for hours and hours and we won't prevent miners from doing it.
I predicted it back in 2013, without even being a miner myself. It's here
to stay. Forever. It's a pity Greg chose that awful name of "SPV" mining
instead some cool name like "Instant" mining that we could market as
Bitcoin latest feature :)
Do you consider the TheBlueMatt relay network a "good thing". NO! It's a
very bad centralization thing, but it is unavoidable. I would like the
relay network to be embedded on the standard network protocol, using local
route optimizations to reduce latency for block propagation (there is one
old paper on this, and it says that with local prioritization you can have
a lower bound to get a propagation latency of at most two times the optimal
value (possibly generated by the minimum spanning tree)).
It requires trust between miners that know eachother, and fundamentally
No is does not. The incentive follows directly from the cheating cost (the
subsidy). Even if I don't know you, I know you wouldn't waste 25 BTC to try
to cheat me for 25 BTC with a probability of 1/100, that's for sure. On
average, you loose 24.75 BTC per cheat attempt.
"SPV" mining is safe as long as it is done for a certain bounded period of
time and bounded number of blocks (e.g: 30 seconds from that last validated
block, and no more than 1 non-validated block). SPV clients that accept a
transaction with 1 confirmation are already in danger of orphaning, and
long invalid "SPV" mining chain forks (as occurred last month) should never
had occurred if limits were in-place.
SPV mining incentive will stay until there is no subsidy, as many other
incentives. SPV mining also must be there to prevent malicious actors from
DoS-ing the relay network. If it's there, then the DoS incentive disappears.
Let's code "instant" mining into Bitcoin Core, and do it right.
Also as Michael Rudy points out, higher block rate means lower variance,
and that's good for miners. Last, as I already said, having a lower average
block interval strengthens Bitcoin value proposition, so miners would be
delighted that their bitcoins are more worthy.

@_date: 2015-08-16 03:19:17
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] How DECOR++ can eradicate selfish mining incentive by 
In these shocking forking times, nothing more relaxing that to immerse
yourself in a pure technical reading about cryptocurrency design, letting
aside Bitcoin politics for a moment. This message is about cryptocurrencies
design in general, so you're free to skip my message if you think it will
never apply to Bitcoin.
[ full article copied from my blog:
A year ago I proposed the DECOR protocol
, a new rule for
cryptocurrencies to reduce significantly the amount of orphan blocks and
then allow block rate to be as high as one block every 5 seconds, and at
the same time it promised to address the problem of selfish mining
. After one
year, I?ve received very little feedback about it. Yet the selfish mining
 problem has
been argued over and over against certain changes in Bitcoin, as if selfish
mining were something inevitable to all POW-based cryptocurrencies. But it
is not.
In a nutshell, DECOR is a protocol that permits miners to share the block
reward if both mine competing blocks. This is done by publishing block
header siblings (sometime called uncles) into child blocks, and modifying
the cryptocurrency protocol to pay some amount to the miners of uncles. If
all miners are honest, this strategy increases slightly the probability of
1-block reversals, but reduces considerably the probability of longer
reversals, as all miners choose the same parent. A few months after my
post, Ethereum adopted a similar strategy of
paying a certain amount of ether to uncles, but the amount paid was created
out of thin ear, and at that time there could be any amount of uncles, so
basically it distorted the money supply function into a uncapped
inflationary one, if all miners decided to collude. After I reported this
issue, they restricted the number of uncles that can be included, but still
it leaves an incentive for all miners to collude to increase miner revenue.
DECOR does reward sharing, so the supply function cap is maintained. But it
does not solve the Selfish mining problem: miners withholding a block get
paid a full reward but the remaining miners are working (without knowing
it) for a half of the block reward. So my original strategy does not work
for rational (but not necessarily honest) miners. A few posts later I
presented DECOR+  to try
to address the problem of unbalanced rewards: what happens if there are two
competing blocks, but one has a 12.5 BTC reward, but the other has a 20 BTC
reward due to additional fees? But again, if miners are dishonest, the
proposed scheme does not solve the underlying problem, as miners can
artificially increase their fees to win the conflict resolving rule, at
least in all cryptocurrencies that do not burn transaction fees. How can we
fix it?
We?ll fix DECOR by doing three changes. The first is by paying full rewards
to all competing blocks, either the parent or the uncles. To prevent
increasing the money supply, first we set a maximum number of uncles U than
can be included over a period of N blocks. For example we can set U=100 and
N=1000 (a maximum orphan rate of 10%). Then we create rule to decrease the
money supply per time interval in case it previously was increased. So to
prevent miners colluding to increase the money supply in U/N, we either
decrease the subsidies of the following N blocks by the excess amount in
the previous period or we make N coincident with block difficulty re-target
interval and we consider uncles in the rate computation, so mining
afterward simply gets more difficult. If all miners collude to try to
increase their revenue by U/N, they will see their revenue decrease by the
same amount in the following re-target interval.
Miners could start switching between two cryptocurrencies to mine only
during the low difficulty interval and avoid the high difficulty interval.
But here are no competing valuable non-merged mined cryptocurrency using
SHA256D, so this is no problem for Bitcoin. Also the cryptocurrency left
without mining power would become insecure and its price will fall to near
zero. So increasing the immaturity lock time for coinbases to at least N
blocks destroys any miner earnings if all decide to switch all at once.
The second change is to choose the parent block in case of conflict based
on a deterministic random selection in case of deciding between several
chains with the same accumulated difficulty but different tip: we order the
competing tip blocks by their hash digest values, we hash the hashes and we
use the resulting hash digest as seed to a PRNG to choose an index in the
sorted list of the block to choose as parent.
The third change is to process the transactions of all competing blocks
(the actual block and its siblings) in case of a conflict. The transactions
on the parent block will be processed first as normal. The others will be
processed in the order they are referenced in following child blocks.
Conflicting transactions (double-spends) present in uncle blocks with
respect to the main block are skipped, while obviously internal conflicts
in the uncle blocks make them invalid, as usual. Now, as long as the
subsidy dominates the fees, miners have no incentive to withhold blocks.
Let?s analyze what can happen in the long term, when fees dominate the
block reward. In the future there may be two kinds of transactions: public
transactions and private transactions. Public transactions are the current
standard transactions: they pay a fee in the standard way and are broadcast
over the public network. Private transactions may appear if miners decide
to negotiate inclusion in blocks directly with web wallets or gateways:
private transactions will pay fees as an output to the miner?s public key.
Blocks with high rewards competing with blocks with low rewards due to
public transactions will be rare, since for the benefit of the miner most
transactions included in blocks should be present in all other miners
memory pools to accelerate propagation, so all miners are exposed to the
same reward pool. If it happens (by the mistake of a user) that a public
transaction pays an extremely high fee, the withholding incentive may
reappear. But in a far future, when subsidy disappears and miners receive
the payment mainly because of fees, they may adopt the more competitive
commercial strategy of rely mainly in private transactions (or maybe using Mike
Hearn?s assurance contracts
). As fees from
private transactions are not shared between competing blocks, they won?t
affect selfish mining. I conclude that DECOR++ is currently incentive
compatible and it is highly probable that remains incentive compatible in
the future.
To summarize, DECOR++ main protocol properties are:
   - Choose a parent by a deterministic pseudo-random coin toss based on
   competing block headers
   - Give standard subsidy to all competing blocks by including uncles in
   following blocks
   - Give small monetary incentive to include uncle blocks in blocks
   (miners including blocks can get a small share of included blocks rewards).
   - Give small monetary incentive to choose deterministically one of the
   competing blocks as the main block (this can be done by burning some reward
   share if other parent is chosen).
   - Process all transactions in uncle blocks, quietly skipping the ones
   that conflict with existing ones.
   - Pay fees to original miners for all non-conflicting transactions in
   uncle blocks
   - Decrease the money supply in blocks following blocks including uncles
   to compensate for the increase in money supply.
   - Limit the amount of uncles that can be included over an interval of
   blocks, and make that interval long enough to capture normal variances in
   orphan rates.
   - Increase the coinbase immaturity period to at least the period of
   money supply compensation.
Best regards, Sergio.

@_date: 2015-08-18 22:03:53
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Just to add some superfluous and unessential spice to this discussion,
there were two Satoshi users originally registered in sourceforge, one
registered very soon after the other. So I say Satoshi were at least two
people, so it may be the case that one Satoshi re-appeared, but the other
did not.
Ore maybe one Satoshi is for Bitcoin XT, and the other Satoshi is against
Satoshi wars!
On Tue, Aug 18, 2015 at 5:59 PM, Anon Moto via bitcoin-dev <

@_date: 2015-08-21 18:12:17
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A solution to increase the incentive of running a 
I can only say 2 things in the brief time I have now:
1. There is a solution that I proposed for proving you own a copy of the
block-chain. It's using aymmetric-time functions:
2. I'm finishing a paper on a transaction system (DAGCoin) which relies on
proof-of-work per transactions, and no per blocks. Actually it has no
This has been explored in the past, but I came up with some basic ideas
that make this work a few months ago. I will forward a draft to you while
at the same time I try to analyze your proposal. Or I may publish the draft
Best regards,
 Sergio
On Fri, Aug 21, 2015 at 5:50 PM, Tamas Blummer via bitcoin-dev <

@_date: 2015-08-27 16:02:37
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] BIPS proposal for implementing AML-KYC in bitcoin 
I strongly think the original prabhat e-mail is a parody.
And I find very funny that important people have responded.
But maybe I'm wrong!
El jue., 27 ago. 2015 a las 11:04, Chris Pacia via bitcoin-dev (<
bitcoin-dev at lists.linuxfoundation.org>) escribi?:

@_date: 2015-10-05 12:56:33
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork technical 
Some of the people on this mailing list are blindly discussing the
technicalities of a soft/hard fork without realizing that is not Mike's
main intention. At least I perceive (and maybe others too) something else
is happening.
Let me try to clarify: the discussion has nothing to do with technical
arguments. I generally like more hard forks than soft forks (but I won't
explain why because this is not a technical thread), but for CLTV this is
quite irrelevant (but I won't explain why..), and I want CLTV to be
deployed asap.
Mike's intention is to criticize the informal governance model of Bitcoin
Core development and he has strategically pushed the discussion to a
dead-end where the group either:
1) ignores him, which is against the established criteria that all
technical objections coming from anyone must be addressed until that person
agrees, so that a change can be uncontroversial. If the group moves forward
with the change, then the "uncontroversial" criteria is violated and then
credibility is lost. So a new governance model would be required for which
the change is within the established rules.
2) respond to his technical objections one after the other, on never ending
threads, bringing the project to a standstill.
As I don't want 2) to happen, then 1) must happen, which is what Mike
wants. I have nothing for or against Mike personally. I just think Mike
Hearn has won this battle. But having a more formal decision making process
may not be too bad for Bitcoin, maybe it can actually be good.
Best regards
 from a non-developer to my dearest developer friends,
  Sergio.

@_date: 2015-10-06 21:04:53
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Hi Venzen,
 I don't know you and I never said "for fuck's sake" to anyone on IRC. I
don't use IRC, and almost never say 4 letter words.
I wonder how technically savvy people trust IRC ids. Could you send me the
link where such an impostor said something to you in my name?
Your e-mail reads like a TV plot. To me, agent Hearn is just Mike. Agent
Lerner is just an Argentinian dude with some spare time to write here. But
maybe you know something I don't....
Best regards!
 Sergio.

@_date: 2015-10-14 15:14:08
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
I'm reading it.
First comment: since a Bitcoin block time is only greater than the median
of the last 11 blocks, a miner could choose the key block time in order to
generate about 400 miniblocks, instead of the average 60 blocks. Not very
bad, but should be taken into account.
On Wed, Oct 14, 2015 at 3:02 PM, Emin G?n Sirer <

@_date: 2015-09-16 01:37:42
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Proof of unique blockchain storage revised 
One possible way to incentivize the existence of more Bitcoin network nodes
is by paying peers when they provide data in the blokcchain. One of the
problems is that it is not easy to tell if the peer is really providing a
useful service by storing the blockchain or it is just relying the request
to some other peers as a proxy.
In this post I review the use of asymmetric-time functions to be able to
prove unique (IP-tied) blockchain storage and propose improvements to make
it fully practical.
Full post here:
Best regards, Sergio.

@_date: 2016-08-17 21:11:16
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
I think that we're not attacking the real source of the problem: that the
witness data size is not signed. It may be the case that a new source of
malleability is detected in witness programs later, or related to new
opcodes we'll soft-fork in the future.
The problem is real, as some systems (such as hardware wallets or other
low-memory IoT embedded systems) may have hard limits in the size of the
witness program they can accept. So we need a solution for all current and
future segwit extension problems.
We could soft-fork to add an opcode OP_PROGSIZE using segwit script
versioning that pushes in the stack the size of the segwit program being
evaluated, and then the script can take any action it wishes based on that.
<0x50> OP_PROGSIZE OP_GREATERTHAN OP_VERIFY ..... OP_CHECKSIG
Then an attacker cannot create a clone of the transaction having a witness
ECDSA signature longer than 0x50 bytes. (many details omitted in this
On Wed, Aug 17, 2016 at 7:15 AM, Johnson Lau via bitcoin-dev <

@_date: 2016-08-17 21:33:24
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
an exact value.
I think that is overly focusing on "someone might change the feerate",
If I send a transaction to an IoT device (say to an OpenDime or to the old
Firmcoin), and the OpenDime must verify that the transaction has been mined
(SPV verification), then it may expect the witness program to be of certain
maximum size (an implementation-imposed  limit). If a Miner modifies the
witness size and makes it too large, then the device may not be able to
accept the transaction and the bitcoins may be lost. Lost because the
private key is in the device, and because the device cannot accept that
cloned transaction, never ever.
The same is true (although less strict) for side-chains and drive-chains:
they may have certain restrictions on the size of transactions they accept
to lock bitcoins.
That's why I'm proposing that a transaction becomes INVALID if the witness
size is higher than the expected size (by the sender).

@_date: 2016-08-24 17:51:47
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Attack by modifying non-segwit transactions after 
In a previous thread ("New BIP: Dealing with OP_IF and OP_NOTIF
malleability in P2WSH") it was briefly discussed what happens if someone
modifies segwit data during transmission. I think the discussion should
What worries me is what happens with non-segwit transactions after segwit
is activated. I've followed the code from transaction arrival to
transaction relay and it seems that a malicious node could receive a
non-segwit tx, and re-format it into a segwit tx having as high as 400
Kbytes of segwit witness program data, and then relay it. Both transaction
would have the same hash.
The MAX_SCRIPT_ELEMENT_SIZE limit is only enforced on segwit execution, not
in old non-segwit execution, so witness program stack elements could be as
large as 400 Kbytes (MAX_STANDARD_TX_WEIGHT prevents increasing more).
Such large modified transaction will probably not be properly relayed by
the network due too low fee/byte, so the honest miner will probably win and
forward the original transaction through the network.
But if the attacker has better connectivity with the network and he
modifies the original transaction adding segwit witness program data only
up to the point where the transaction is relayed but miners are discouraged
to include it in blocks due to low fees/byte, then the attacker has
successfully prevented a transaction from being mined (or at least it will
take much more).
Also an attacker can encode arbitrary data (such as virus signatures or
illegal content) into passing non-segwit transactions.
One solution would be to increase the transaction version to 3 for segwit
transactions, so a non-segwit transaction cannot be converted into a segwit
transaction without changing the transaction hash. But this seems not to be
a good solution, because it does not solve all the problems. Transactions
having a mixture of segwit and non-segwit inputs could suffer the same
attack (even if they are version 3).
I proposed that a rule is added to IsStandardTX() that prevents witness
programs of having a stack elements of length greater than
MAX_SCRIPT_ELEMENT_SIZE. (currently this is not a rule)
That's a simple check that prevents most of the problems.
A long term solution would be to add the maximum size of the witness stack
in bytes (maxWitnessSize) as a field for each input, or as a field of the
whole transaction.

@_date: 2016-08-26 10:16:36
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Attack by modifying non-segwit transactions after 
Because there was a discussion on reddit about this topic, I want to
clarify that Johnson Lau explained how a check in the code prevents this
So there is no real attack.
Also note that the subject of this thread has a question mark, which means
that I'm asking the community for clarification, not asserting the
existence of a vulnerability.
The segwit code is complex, and some key parts of the consensus code are
spread over the source files (such as state.CorruptionPossible() relation
to DoS banning, IsNull() check in witness program serialization, etc.).
Thanks again Johnson for your clarifications.

@_date: 2016-02-26 20:06:40
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] The first successful Zero-Knowledge Contingent 
It a property of the SKCP system that the person who performed the trusted
setup cannot extract any information from a proof?
In other words, is it proven hard to obtain information from a proof by the
On Fri, Feb 26, 2016 at 6:42 PM, Gregory Maxwell via bitcoin-dev <

@_date: 2016-03-20 23:50:55
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] BIP147 minor error 
The BIP147 reads:
*Sigop cost* is defined. The cost of a sigop in traditional script is 4,
while the cost of a sigop in witness program is 1.
The new rule is total *sigop cost* ? 80,000.
But the code implements:
if (nSigOps + (nWitSigOps + 3) / 4 > MAX_BLOCK_SIGOPS)
 ... error....
Which is not the same.
For example:
nSigOps = 1
nWitSigOps =79999
Is not an error by BIP definition but it's an error by the implemented code.
Regards, Sergio.

@_date: 2016-03-23 21:37:25
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] p2p authentication and encryption BIPs 
It seems that every message must be signed (the protocols lacks MACs). This
can be very resource consuming in terms of CPU and bandwidth since most p2p
messages are small.
On Wed, Mar 23, 2016 at 5:36 PM, Tom via bitcoin-dev <

@_date: 2016-05-10 18:43:01
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Your idea of moving the Merkle root to the second chunk does not work.
The AsicBoost can change the version bits and it does not need to find a
(However *Spondoolies patent *only mentions Merkle collisions:
Back in 2014 I designed a ASIC-compatible block header that prevents
AsicBoost in all its forms.
You can find it here:
Basically, the idea is to put in the first 64 bytes a 4 byte hash of the
second 64-byte chunk. That design also allows increased nonce space in the
first 64 bytes.
But it you want to do a simpler change, you can more easily use the first
32 bits of the Parent Block Hash (now currently zero) to store the first 4
bytes of the SHA256 of the last 16 bytes of the header. That way to "tie"
the two header chunks. It's a minimal change (but a hard-fork)
But some ASIC companies already have cores that are better (on power, cost,
rate, temperature, etc.) than competing companies ASICs. Why do you think a
10% improvement from AsicBoost is different from many of other improvements
they already have (secretly) added? Maybe we (?) should only allow ASICs
that have a 100% open source designs?
If we change the protocol then the message to the ecosystem is that ASIC
optimizations should be kept secret. It is fair to change the protocol
because we don't like that certain ASIC manufacturer has better chips, if
the chips are sold in the market and anyone can buy them? And what about
using approximate adders (30% improvement), or dual rail asynchronous
adders (also more than 10% improvement) ? How do we repair those?
Disclaimer: I have stake in AsicBoost, but I'm not sure about this.
On Tue, May 10, 2016 at 5:27 PM, Tier Nolan via bitcoin-dev <

@_date: 2016-05-10 19:17:42
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
to a link that doesn't say anything about ASIC improvements or patents,
which means that you have been planning to change the protocol rules with
some miners (but not all the community).
All changes to the protocol should be discussed in public here. If you want
to make "further similar optimizations useless as well" then maybe you
should propose a switch to EquiHash.

@_date: 2016-05-11 09:20:55
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
include in the second 64-byte chunk a 4-byte hash of the first chunk, not
the opposite.

@_date: 2016-05-11 11:18:34
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
Jorge Tim?n said..
that one miner can prevent the rest from using?
Everyone seems to assume that one ASIC manufacturer will get the advantage
of AsicBoost while others won't. If a patent license is non-exclusive, then
all can.

@_date: 2016-05-24 11:30:30
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] BIP: OP_PRANDOM 
Bitcoin Beacon paper relevant here
Basically is suggest using deciding a random bit on the majority 1s or 0s
of lsb bits taken from last block hashes.
Iddo Bentov? Technion, Ariel Gabizon,  David Zuckerman
We examine a protocol ?beacon that outputs unpredictable and publicly
verifiable randomness, meaning that the output is unknown at the time that
?beacon starts, yet everyone can verify that the output is close to uniform
after ?beacon terminates. We show that ?beacon can be instantiated via
Bitcoin under sensible assumptions; in particular we consider an adversary
with an arbitrarily large initial budget who may not operate at a loss
In case the adversary has an infinite budget, we provide an impossibility
result that stems from the similarity between the Bitcoin model and
Santha-Vazirani sources. We also give a hybrid protocol that combines
trusted parties and a Bitcoin-based beacon.
On Sun, May 22, 2016 at 10:30 AM, Jeremy via bitcoin-dev <

@_date: 2016-05-24 11:36:35
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] BIP: OP_PRANDOM 
Missing link to paper: Another relevant paper:
On Bitcoin as a public randomness source
Joseph Bonneau, Jeremy Clark, and Steven Goldfeder
On Tue, May 24, 2016 at 11:30 AM, Sergio Demian Lerner <

@_date: 2016-11-24 22:39:05
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] The Excessive-Block Gate: How a Bitcoin Unlimited 
Hi Peter,
How would a person or exchange decide to accept a payment in BU if it does
not know the gate policy of 51% of the miners?
Suppose that the exchange receives B1,S2,S3,S4 (a big block at height 1,
and 3 small blocks at height 2, 3 and 4), and an alternate chain A1,A2,A3
(three small blocks). The first is the longest, but the second may be the
one 51% of the miners will extend.
Without knowing  the policy of at least 51% of the miners (the maximum
acceptance depth) it's unclear if the exchange has to obey the longest
chain or the chain with higher probability of being extended.
If the maximum acceptance depth of the majority of miners is higher than 6
blocks, accepting a transaction with 6 confirmations is risky.
So BU would set a lower bound on the number of confirmations equal to the
maximum acceptance depth of the majority of miners.But miners do not
publish their acceptance depth, so basically users are clue-less. I think
miners should at least advertise their gate block size and acceptance depth
in their coinbase field.
Is there a game-theoretic analysis of confirmation blocks and their
probabilities in BU ?
Without a detailed analysis, unlimited block size seems a risky change to
Bitcoin, to me.
Regards, Sergio.
On Tue, Nov 22, 2016 at 1:31 PM, Peter R via bitcoin-dev <

@_date: 2016-11-25 19:31:22
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] The Excessive-Block Gate: How a Bitcoin Unlimited 
On Fri, Nov 25, 2016 at 12:25 PM, Tom Zander via bitcoin-dev <
Bitcoin Core implementation of the consensus protocol. Sadly (or happily,
for some) there is no "abstract" definition of Bitcoin.
dictates is not in the best interest of miners or full-nodes? then they
will simply choose a rule that maximizes their revenue (or any other
measure of performance, such as lower latency, or less transaction reversal
As far as I understand the document from Peter, there is no change there at
I haven't gone to the code to check, but the video Peter sent does not say
that. It says that miners will mine on top of a block ONLY if the "gate"
has been opened for that block (e.g. there is additional blocks to push a
big block). So a miner having a preferring low block sizes will choose to
mine on top of the A1,A2,A3 chain (3 units of work), while miners
supporting bigger sizes will mine on top of the chain B1,S2,S3,S4 (4 units
of work).
Saying that the chain starting with B1 is not considered by a node X does
not mean that the node X is blind to the information that can be extracted
from the fact that there is a chain of 4 blocks starting from B1.
If there is more information, there may be a better local choice. If there
are better local choices, there is probably a better global equilibrium (or
not equilibrium at all).
Clearly this is not universal: some miners will, and some other miners
won't, because some miners have postponed adding some blocks.
Suppose that I provide a service that accepts payments with 2
confirmations, and in certain time I have the information that the network
is at the same time considering the forks B1 S2 and A1 A2. Then the best I
can do is NOT to accept the 2-confirmation and wait for a resolution of the
fork. Choosing either fork may put me at the risk of immediate reversal.
The existence of fork information changes equilibrium decision to choose
the longest-chain.  This is the same that happens with the GHOST protocol:
the information on the existence of uncles changes the local incentives to
choose the longest chain to some different strategy, and when all nodes
change their strategy, then the supposedly last equilibrium state is that
all follow the GHOST strategy for choosing the heaviest chain.
automatically. The amount of confirmations that a node accepts is not
affected by the miner's policies or the size of the blocks mined, but it
Probably a simple wise addition would be to estimate the accepted block
size for the majority of the miners (S), and only count block confirmations
for wallet transactions taking into account only blocks whose size is lower
or equal than S. So for example, if Alice receives a transaction T in block
B1 and it is confirmed by block B2, but size(B1)>S and size(B2)>S, then the
wallet should tell Alice that transaction T has 0 confirmations. This local
strategy reduces the chances that Alice accept T but is then easily
reversed for the opposite fork growing one block ahead.
 Sergio

@_date: 2016-11-25 20:45:20
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] The Excessive-Block Gate: How a Bitcoin Unlimited 
I now think my reasoning and conclusions are based on a false premise: that
BU block size policies for miners can be heterogeneous.
There can't be short forks because forks are not in the best interest of
the honest miner majority. All miners need to announce and follow the same
block size policy to prevent short forks.
The incentives are established so that all block size negotiations will be
carried between miners in a off-chain manner, not by modifying the policy
nor by announcing anything in the coinbase,
If block size negotiations are meant to be open and carried on on-chain,
then it's much better to let miners increase or decrease the block size
limit by 1% per block (such as what Ethereum does with the gas limit).
On Fri, Nov 25, 2016 at 7:31 PM, Sergio Demian Lerner <

@_date: 2016-10-02 12:49:08
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
Since ScalingBitcoin is close, I think this is a good moment to publish our
proposal on drivechains. This BIP proposed the drivechain we'd like to use
in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it implemented
in Bitcoin. Until that happens, we're using a federated approach.
I'm sure that adding risk-less Bitcoin extensibility through
sidechains/drivechains is what we all want, but it's of maximum importance
to decide which technology will leads us there.
We hope this work can also be the base of all other new 2-way-pegged
blockchains that can take Bitcoin the currency to new niches and test new
use cases, but cannot yet be realized because of current
The full BIP plus a reference implementation can be found here:
BIP (draft):
Code & Test cases:
(Note: Code is still unaudited)
As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked opcode
that counts acks and nacks tags in coinbase fields, and push the resulting
totals in the script stack.
The system was designed with the following properties in mind:
1. Interoperability with scripting system
2. Zero risk of invalidating a block
3. No additional computation during blockchain management and
4. No change in Bitcoin security model
5. Bounded computation of poll results
6. Strong protection from DoS attacks
7. Minimum block space consumption
8. Zero risk of cross-secondary chain invalidation
Please see the BIP draft for a more-detailed explanation on how we achieve
these goals.
I'll be in ScalingBitcoin in less than a week and I'll be available to
discuss the design rationale, improvements, changes and ideas any of you
may have.
Truly yours,
Sergio Demian Lerner
Bitcoiner and RSK co-founder

@_date: 2016-10-02 14:00:01
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
Peter, are you really going to try to down vote a decent free and
open-source proposal that benefits all the Bitcoin community including
you and your future children because a personal attack to me without any
logic or basis?
Is that the way you collaborate to improving Bitcoin?
I just can't believe it.
Let's open another thread elsewhere to discuss hardware and software
patents, and that particular one, if you wish, this is NOT the place.

@_date: 2016-10-02 14:13:13
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] About ASICBoost 
Please Peter Todd explain here all what you want to say about a patent of a
hardware design for an ASIC.
Remember that ASICBoost is not the only patent out there, there are at
least three similar patents, filed by major Bitcoin ASIC manufacturers in
three different countries, on similar technologies.
That suggest that the problem is not ASICBoot's: you cannot blame any
company from doing lawful commerce in a FREE MARKET.
It is a flaw in Bitcoin design that could be corrected if the guidelines I
posted in [1] had been followed.

@_date: 2016-10-02 14:26:18
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
I'm not a lawyer, and my knowledge on patents is limited. I guess RSK WILL
endorse DPL or will make the required actions to make sure the things
developed by RSK remain free and open. This was not a priority until now,
but coding was. For me, coding always is the priority.
I will discuss prioritizing this with the team. Remember it took several
years to BlockStream to decide for DPL and not just publish everything as
they were doing. I suppose the decision it was not a simple one, involving
lawyers advise and all. I guess DPL needs to actually patent the things in
order to open them later, and patenting has a very high cost.
Give us time to decide which open source strategy is the best and cheaper
for RSK. At this point I can assert that RSK has not filed any patent not
is planing to.  This proposal is not encumbered by any patents, and
drivechains is actually not RSK's idea, but Paul Sztorc's.

@_date: 2016-10-02 19:25:52
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] About ASICBoost 
It's good you bring that point, and it's very interesting to analyze what
happened then.
We shared our findings with some core developers much earlier than the BIP
proposal. Wether they kept it secret or they shared it with some ASIC
manufacturers is something I don't know. I even mentioned my wishes to try
to give the patent to public domain.
I remember the reason we proposed the BIP is because ASICBoost actually
does NOT require that BIP at all. And that BIP was not a consensus change,
but just a semantic re-interpretation.
ASICBoost can roll the nVersion field or the Merkle root hash. Doing the
former currently generates a strange warning message on nodes and can be
confusing, but doing the later makes ASICBoost completely stealthy. That
BIP could help the community to monitor its use in non-confusing way to the
users. What is worse? I think forcing it to be stealthy is worse.
I never opposed changing Bitcoin to be more decentralized, but hard-forking
a change to the PoW function may be contentious and that path of thought
must be walked very carefully.

@_date: 2016-10-02 19:36:29
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
On Sun, Oct 2, 2016 at 6:46 PM, Russell O'Connor via bitcoin-dev <
Transactions that redeem an output containing (or referencing by means of
P2WSH) an OP_COUNT_ACKS are not broadcast by the network. That means that
the network cannot be DoS attacked by flooding with a transaction that will
not verify due to being too late.
The only parties that can include the redeem transaction are the miners
Therefore I see no problem that an OP_COUNT_ACKS scriptSig transaction is
invalidated after the liveness times expires.
If there is no expiration, then polls can last forever and the system fails
to provide DoS protection for block validation since active polls can
accumulate forever.

@_date: 2016-10-02 19:57:31
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] The use OP_COUNT_ACKS for paying for a common good 
One side benefit of OP_COUNT_ACKS is that it enables a completely different
use case:
It allow users to pay for any service miners can provide as group for the
common good (e.g. fee payment smoothing over many blocks). For instance,
users could pay miners to jointly buy better Internet service to improve
bandwidth or reduce latency between them.
By sending bitcoins to a script containing OP_COUNT_ACKS requiring 51% of
miners approval and adding a special text tag to such outputs such as
"FOR-MINERS-TO-BUY-X", users can send bitcoins to miners and ask the
majority of them to vote on the proposal, if accepted create a transaction
to redeem those funds. This could help to address the so-called tragedy of
the commons problem that Bitcoin may face in in long-term, by users
crowdfunding mining of the following n blocks.

@_date: 2016-10-14 07:38:07
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] DPL is not only not enough, 
I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
users may be confident they are protected but in fact they are not, as the
future generations of users can be attacked, making Bitcoin technology
fully proprietary and less valuable.
If you read the DPL v1.1 you will see that companies that join DPL can
enforce their patents against anyone who has chosen not to join the DPL.
So basically most users of Bitcoin could be currently under threat of being
sued by Bitcoin companies and individuals that joined DPL in the same way
they might be under threat by the remaining companies. And even if they
joined DPL, they may be asked to pay royalties for the use of the
inventions prior joining DPL.
DPL changes nothing for most individuals that cannot and will not hire
patent attorneys to advise them on what the DPL benefits are and what
rights they are resigning. Remember that patten attorneys fees may be
prohibitive for individuals in under-developed countries.
Also DPL is revocable by the signers (with only a 180-day notice), so if
Bitcoin Core ends up using ANY DPL covered patent, the company owning the
patent can later force all new Bitcoin users to pay royalties.
Because Bitcoin user base grows all the time with new individuals, the sole
existence of DPL licensed patents in Bitcoin represents a danger to Bitcoin
future almost the same as the existence of non-DPL license patents.
If you're publishing all your ideas and code (public disclosure), you
cannot later go and file a patent in most of the world except the US, where
you have a 1 year grace period. So we need to do something specific to
prevent the publishers filing a US patent.
What we need much more than DPL, we need that every BIP and proposal to the
Bitcoin mailing list contains a note that grants all Bitcoin users a
worldwide, royalty-free, no-charge, non-exclusive, irrevocable license for
the content of the e-mail or BIP.
I'm not a lawyer and this is not an advise of any kind. Please check
yourself the DPL v1.1 and get your own idea. I'm speaking on behalf of
myself, and not any company.
Best regards,
 Sergio.

@_date: 2016-10-14 15:10:45
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] DPL is not only not enough, 
Oh God... here we go again..
for miners to find cheap, non-standard ways to generate new work which are
not in the best interest of the protocol".
The BIP actually PROTECTS the network from stealth Shared-Nonce mining and
the fact you rejected it made the Bitcoin network LESS secure because now
we just don't know at what extent it is in use.
Shared-nonce mining can be done with or without that BIP/pull-req.
We didn't disclose more in the BIP because it was not clear if shared-nonce
mining (the fact that Bitcoin had a design flaw) would have a negative
affect on Bitcoin price.
ASICBoost patent may be a patent that protects Bitcoiners from mining
centralization: ASICBoost is the only company that at this point showed
interest in licensing the technology. But I do not control ASICBoost nor
the patent so I cannot do anything about it.
I propose we as a community do a crowdfund to try to license it from that
company (or any other that wants to put theirs in the deal) and put all in
public domain.

@_date: 2016-10-25 13:38:59
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
Responding between lines...
You're right.I will change the naming asap.
Correct, that's inherently a technical limitation. However, there can be
many deterrents from miners stealing money (legal contracts,
mutual-assured-destruction states). Aslo as you mention, you can combine
OP_COUNT_ACK with notary sigantures as AND/OR or a more complex acknowledge
weight distribution.
Ok. I'm not sure, but if everyone agrees to it, I will. Also Segwit
versioning allows to create new opcode multiplexing opcodes, so I was
thinking about adding an "opcode index" to a more generic OP_OPERATE. But
that prevents using all NOP space, but prevents easily counting
OP_ACK_COUNT for checksig block limit.
use of it. I can use (witversion == 1) but add the BIP114 version field so
that the next BIP can make use of it.
proposal is short has been seen by other developers a benefit and not a
drawback. It prevent hundreds of sidechains to be offered, which might hurt
solo miners. 70 bytes allows for approximately 10 active polls.
8. Question: is an ack-poll valid only for 1 transaction? When the
which periodically cleans itself by using a FIFO approach.
OP_COUNT_ACKS  would be non-standard, so the the problem you point out
would only happen if the block including the transaction would be mined
specifically for the purpose to defeat subsequent miners. A honest pre-fork
miner would never include a redeemScript that that verifies an
OP_COUNT_ACKS, since that transaction would never be received by the p2p
protocol (it could if the miner accepts non-standard txs by a different
But I must check this in the BIP source code if OP_COUNT_ACKS is really
non-standard as I designed it to be.
(Thanks Jonhson Lau for taking the time to analyze the BIP.)

@_date: 2017-04-01 00:35:11
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Even if the proposal involves a political compromise, any change to the
code must be technically evaluated.
The patch was made to require the least possible time for auditing. I'm
talking about reviewing 120 lines of code (not counting comments or
space) which 30 of them are changes to constants. A core programmer audited
it in less than one hour.
Also you're risking the unique opportunity to see segwit activated for
Maybe we can reach a similar agreement for segwit activation in two years.
That's will be too late. The remaining cryptocurrency ecosystem do move

@_date: 2017-04-01 08:44:11
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Some people have asked me for the current implementation of this patch to
review. I remind you that the current patch does not implement the
hard-fork signaling, as requested by Matt.
The Segwit2Mb patch can be found here:
For now, the segwit2mb repo has a single test file using the old internal
blockchain building method (test/block_size_tests.cpp). This must be
replaced soon with a better external test using the bitcoin/qa/rpc-tests
tests, which I will begin to work on now after I collect all comments from
the community.
On Sat, Apr 1, 2017 at 3:55 AM, Jared Lee Richardson

@_date: 2017-04-06 17:42:19
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
The 95% miner signaling is important to prevent two Bitcoin forks, such as
what happened with Ethereum HF and Ethereum Classic.
Bitcoin has a very slow difficulty re-targeting algorithm. A fork that has
just 95% miner support will initially (for 2016 blocks) be 5% slower (an
average block every 10 minutes and 30 seconds). The transaction capacity of
the new Bitcoin protocol is reduced only 5%.
However the chain with 5% if the hashing power not only has a 20x capacity
reduction, but confirms transactions in 20x more time. So the mempool will
grow 400 times. It must be noted that fees increased 10x from the moment
blocks were half full, to the moment blocks became saturated. I'm sure no
Bitcoin (pre-fork) user will be willing to pay 100x times the transaction
fees to use such a slow and insecure network.
So a 6-block confirmation will take 20 hours in the original chain and the
original chain will be in this almost useless slow state for an average of
2016 blocks, or 280 days.
If the original blockchain hard-forks to re-adjust the difficulty, then it
will just represent an alt-coin having 5% of Bitcoin community, and it
can't affect Bitcoin (the segwit2mb fork).

@_date: 2017-04-06 17:58:56
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Responding between lines...
Segwit2mb is a last resort option. It does not need to be released quickly.
Not at all. It just needs to be there in case no other option is chosen. I
put tentative dates. We can move them.
The name does not matter much. The name means joining segwit with a 2Mb
hard-fork. It's a grammatical name.
I also disagree with the idea that segwit2mb is a 8mb increase. As stated
by in the Bitcon.org website [1] and backed up by scientific research, ?a
block filled with standard single-signature P2PKH transactions would be
about 1.6MB and a block filled with 2-of-2 multisignature transactions
would be about 2.0MB.?. As standard blocks are a combination between P2PKH,
and 2-of-3 multisignatures, the actual average segwit block size will be
close to 2.0MB.
Because Segwit2Mb doubles the maximum size of a block, the average block
size for a block filed with average transactions is 4.0Mb.
[1] I can explain in a following e-mail why creating 8Mb blocks on purpose is
generally is an irrational choice. And in the case where it could provide
an economic benefit, adding parallel block validation to Core nullifies any
adversary advantage.
fork, however there is nothing to indicate that segwit as it stands isnt
already very high consensus except for a handful of pool operators
protecting fee income.
You and me may never know the reasons why these operators (or many many of
other users) prefer to increase the block-size. I suppose it has to do high
the high current transaction fees as compared to less than a year ago.
Anyway consensus can only be achieved if one understands the others may
have reasons that do not match ours.
Last, if this proposal is rejected by any side, then we'll definitively
learn that side is not looking for any consensual resolution of the
If they do, we'll get a lot of public, verifiable information from that
fact. The cost to include this patch is low compared with the benefit it
can bring and the information we can gather in case one of the sides
rejects it.
However, I've received positive feedback from them until now.
Then we'll learn a lot about hard-forks, the limits of miner "voting" and
the quorums we can expect.

@_date: 2017-04-06 18:03:12
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Ups. My mistake:  the mempool will not grow 400 times, the is no square
I will initially grow 20 times. Multiplied by the number of times a
transaction may need to be replaced with one with higher fees. Maybe 50
times, but not 400.
On Thu, Apr 6, 2017 at 5:42 PM, Sergio Demian Lerner <

@_date: 2017-04-07 17:52:17
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] BIP Proposal: Inhibiting a covert optimization on the 
BIP: TBD
  Layer: Consensus
  Title: Inhibiting a covert optimization on the Bitcoin POW function
  Author: Sergio Demian Lerner   Status: Draft
  Type: Standards Track
  Created: 2016-04-07
  License: PD
This proposal inhibits the covert use of a known optimization in Bitcoin
Proof of Work function.
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
Due to a design oversight the Bitcoin proof of work function has a potential
optimization which can allow a rational miner to save up-to 30% of their
costs (though closer to 20% is more likely due to implementation overheads).
Timo Hanke and Sergio Demian Lerner applied for a patent on this
optimization. The company "Sunrise Tech Group, Llc" has offered to license
it to any interested party in the past. Sunrise Tech Group has been
marketing their patent licenses under the trade-name ASICBOOST.  The
document takes no position on the validity or enforceability of the patent.
There are two major ways of taking advantage of this optimization, as
by the patent:
One way which is highly detectable and is not in use on the network
today and a covert way which has significant interaction and potential
interference with the Bitcoin protocol.  The covert mechanism is not
easily detected except through its interference with the protocol.
In particular, the protocol interactions of the covert method can block the
implementation of virtuous improvements such as segregated witness.
The use of this optimization could result in a big payoff, but the actual
sum depends on the degree of research, investment and effort put into
the improved cores.
On the above basis the potential for covert use of this optimization
in the covert form and interference with useful improvements presents a
danger to the Bitcoin system.
The general idea of this optimization is that SHA2-256 is a merkle damgard
function which consumes 64 bytes of data at a time.
The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while
incriminating a 32-bit nonce which is at the end of this header data. This
means that the processing of the header involves two runs of the compression
function run-- one that consumes the first 64 bytes of the header and a
second which processes the remaining 16 bytes and padding.
The initial 'message expansion' operations in each step of the SHA2-256
function operate exclusively on that step's 64-bytes of input with no
influence from prior data that entered the hash.
Because of this if a miner is able to prepare a block header with
multiple distinct first 64-byte chunks but identical 16-byte
second chunks they can reuse the computation of the initial
expansion for multiple trials. This reduces power consumption.
There are two broad ways of making use of this optimization. The obvious
way is to try candidates with different version numbers.  Beyond
upsetting the soft-fork detection logic in Bitcoin nodes this has
little negative effect but it is highly conspicuous and easily
The other method is based on the fact that the merkle root
committing to the transactions is contained in the first 64-bytes
except for the last 4 bytes of it.  If the miner finds multiple
candidate root values which have the same final 32-bit then they
can use the optimization.
To find multiple roots with the same trailing 32-bits the miner can
use efficient collision finding mechanism which will find a match
with as little as 2^16 candidate roots expected, 2^24 operations to
find a 4-way hit, though low memory approaches require more
An obvious way to generate different candidates is to grind the
coinbase extra-nonce but for non-empty blocks each attempt will
require 13 or so additional sha2 runs which is very inefficient.
This inefficiency can be avoided by computing a sqrt number of
candidates of the left side of the hash tree (e.g. using extra
nonce grinding) then an additional sqrt number of candidates of
the right  side of the tree using transaction permutation or
substitution of a small number of transactions.  All combinations
of the left and right side are then combined with only a single
hashing operation virtually eliminating all tree related
With this final optimization finding a 4-way collision with a
moderate amount of memory requires ~2^24 hashing operations
instead of the >2^28 operations that would be require for
extra-nonce  grinding which would substantially erode the
benefit of the optimization.
It is this final optimization which this proposal blocks.
==New consensus rule==
Beginning block X and until block Y the coinbase transaction of
each block MUST either contain a BIP-141 segwit commitment or a
correct WTXID commitment with ID 0xaa21a9ef.
(See BIP-141 "Commitment structure" for details)
Existing segwit using miners are automatically compatible with
this proposal. Non-segwit miners can become compatible by simply
including an additional output matching a default commitment
value returned as part of getblocktemplate.
Miners SHOULD NOT automatically discontinue the commitment
at the expiration height.
The commitment in the left side of the tree to all transactions
in the right side completely prevents the final sqrt speedup.
A stronger inhibition of the covert optimization in the form of
requiring the least significant bits of the block timestamp
to be equal to a hash of the first 64-bytes of the header. This
would increase the collision space from 32 to 40 or more bits.
The root value could be required to meet a specific hash prefix
requirement in order to increase the computational work required
to try candidate roots. These change would be more disruptive and
there is no reason to believe that it is currently necessary.
The proposed rule automatically sunsets. If it is no longer needed
due to the introduction of stronger rules or the acceptance of the
version-grinding form then there would be no reason to continue
with this requirement.  If it is still useful at the expiration
time the rule can simply be extended with a new softfork that
sets longer date ranges.
This sun-setting avoids the accumulation of technical debt due
to retaining enforcement of this rule when it is no longer needed
without requiring a hard fork to remove it.
== Overt optimization ==
A BIP for avoiding erroneous warning messages when miners use the overt
of the optimization was proposed several years ago, in order to deter the
use of the optimization. But that BIP was rejected.
However, in light of the current discoveries, that BIP could be
The over optimization does not generally interfere with improvements in the
==Backward compatibility==
Greg Maxwell  for the original report, which contained
several errors that were corrected in the present proposal.
This document is placed in the public domain.

@_date: 2017-04-17 10:25:14
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Suggestions to improve opcodes with O(N) complexity 
I came across O(N) behavior of two scripting opcodes, OP_IF and OP_ROLL. By
exploiting edge cases for each of these two sub-optimal algorithms, I
manage to simulate a Segwit block that takes up to 5.6 seconds to verify on
a Ubuntu VM running on a single Core i5 processor. The simulation is based
on a single thread executing EvalScript(), the Bitcoin script execution
function. The tests were not performed processing actual blocks. These
results should not make anyone worry, because there are worse problems in
Bitcoin block verification, and because Bitcoin employs several worker
threads for verifying scripts in parallel. For example, a Segwit block can
request 80000 signature verifications when all transactions are P2WSH. It
is said that Bitcoin Core (in a modern multi-core machine, using its
multi-threading verification capabilities) can verify 8000 ECDSA signatures
per second. Therefore a malicious miner can create a Segwit block that
requires approximately 10 seconds to be verified. Since the examples
presented in this post consume less than 10 seconds, I don?t consider my
findings as vulnerabilities. However, if the block size is to be increased
in the future, these problems should be considered prior increasing the
block size. The scripts presented here as examples do not leave the value
stack empty, but the Bitcoin protocol does not require it. Bitcoin only
requires the top value to be true to accept the script.
OP_IF abuse
Every time a Bitcoin script executes the OP_IF opcode, a boolean value
indicating if the condition was true, false or the conditional was skipped
(also represented as false) is pushed into the vfExec stack.  Every time an
opcode is executed, the number of  false values in the vfExec stack is
counted using the following line:
bool fExec = !count(vfExec.begin(), vfExec.end(), false);
If the count is non-zero, all subsequent instructions except OP_ELSE and
OP_ENDIF are skipped. It is clear that the longer the conditional stack is,
the more it takes to count the false elements.
The following scriptPub or ScriptSig exploits this problem:
OP_IF { 100 times }
0 { 9798 times }
OP_ENDIF { 100 times }
The vfExec vector is filled with 100 elements, and then each element is
scanned 9799 times, totaling more than 979K items scanned. This took 2.5
seconds in my test VM (for a block filled with these scriptSigs).
To re-write this logic with a O(1) algorithm, one simply has to count the
number of true conditions in one variable (trueCount), and the number of
false or skipped conditions following all true conditions in another
(ignoreCount). Detecting if code needs to be executed or not requires just
testing if ignoreCount is zero.
The handling of OP_IF / OP_NOTIF / OP_ELSE should be like the following
fExec = (ignoreCount==0);
case OP_IF:
case OP_NOTIF:
 {
   if (fExec)
    {
     ....compute fValue...
     if (fValue) trueCount++; else ignoreCount++;
    } else
    ignoreCount++;
 } break;
 case OP_ELSE:
 {
 if ((trueCount==0) && (ignoreCount==0))
     return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);
 if (ignoreCount==0) { trueCount--; ignoreCount++; } else
 if (ignoreCount==1) { trueCount++; ignoreCount--; }
 } break;
case OP_ENDIF:
 {
    if ((trueCount==0) && (ignoreCount==0))
        return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);
    if (ignoreCount>0) ignoreCount--; else trueCount--;
 }
 break;
You may have noticed the strange behavior of Bitcoin?s ELSE statement.
Bitcoin allows one to switch between true and false conditions several
times. For example, the following script is valid and leaves the value 2 on
the stack:
1 OP_IF OP_ELSE OP_ELSE 2 OP_ENDIF
The second problem lies in the OP_ROLL opcode. This opcode removes a value
at a given index from the value stack, and pushes it on top. As the Bitcoin
Core stack stores a list of char std::vector by value (not by reference),
and because the stack is itself a std::vector (not a linked list), then
removing the first elements requires moving all elements one position in
memory. The value stack can store a maximum of 1000 elements. The following
script fills the stack and then moves each stack element 200 times, so the
number of moved elements is 200K. This took almost 5.6 seconds in my test
VM (for a block filled with these scriptSigs).
1  {999 times}
998 OP_ROLL { 200 times }
I tried other scripts, such as filling the stack with values of size 520
using DUP3, and then performing rolls, but all of them led to a block that
took less time, if the block is to be filled with the scripts.
One solution to this problem is use a linked list data structure instead of
a std::vector, to allow O(1) removal of items, but it still requires O(N)
for element lookup. A balanced tree where each internal node is augmented
with the number of children underneath can be used to provide efficient
indexed access and efficient element removal. However, the overhead of such
data structure may kill its benefits.
So it may be the case that optimizing OP_ROLL will never really be
But these minor issues have to be taken into account if the scripting
system is modified in any way. There are many subtle interactions. For
instance, it may seem that Segwit allows a transaction having a stack
containing 2 million items to verify correctly, by having the witness stack
filled with 2M zero values, and by executing an empty witness script.
However this is prevented by the cleanstack check.

@_date: 2017-02-12 17:22:33
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Proof of Nodework (PoNW) - a method to 
Hi John,
 RSK platform (a Bitcoin sidechain) is already prepared to do something
similar to this, although very efficiently. We set apart 1% of the block
reward to automatically reward full nodes.
We have two systems being evaluated: the first is based on PoUBS (Proof of
Unique Blockchain Storage) which uses asymmetric-time operations to encode
the blockchain based on each user public key such that decoding is fast,
but encoding is slow. The second is more traditional proof of
retrievability, but it requires some ASIC-resistance assumptions.
In both cases, a special smart contract is being called at every block that
creates periodic challenges. Every full node that wants to participate can
submits a commitment to the Merkle hash root of a pseudo-random sequence of
encoded blocks. Then the smart contract chooses random elements from the
committed dataset, and each full node has a period to submit Merkle-proofs
that such random elements belong to the commitment.
To prevent blockchain bloat we designed a very cool new type of transaction
payload: Ephemeral Payload. Ephemeral payload is a payload in a transaction
that gets discarded after N blocks if no smart contract does reference it.
If is does, it's solidified forever in the blockchain.
Then there is a challenge phase where other full nodes can inform the smart
contract if they find an error in the submitted responses. Then the smart
contract ONLY evaluates the responses which have been questioned by users.
This way the smart contract does very little computation (only when a user
misbehaves) and the blockchain normally does not store any proof forever
(only the ones created by misbehaving users).
Because RSK/Rootstock has a very short block interval (10 seconds), all
this happens very quickly and does not require much computation.
Best regards,
 Sergio Lerner
 Chief Scientist RSK (aka Roostock)
On Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev <

@_date: 2017-02-13 11:48:24
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Proof of Nodework (PoNW) - a method to 
The reward is split between all full nodes. Therefore each full node has an
incentive to check at least some other full nodes responses because there
is a competition for the full node reward. At the end, each full node
response will be checked by more than other node with high probability.
Also each full node does a small pre-deposit, that is consumed if the node
Is any validation of responses mandatory, or does policing the system rely
There is not many defenses against censorship but try to hide your identity
until the end of the protocol. But if somebody knows that your transactions
belong to you, then there is little defense. We just wait more than a
single block for the commitments, so several miners must collude in order
to censor a transaction.
I'm keeping it private against all my desire because I want it to be
reviewed before I publish it. Credibility is very easily lost.
The same idea (Ephemeral Data) has been used to design the Lumino Network.

@_date: 2017-07-07 19:25:12
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A Segwit2x BIP 
Here is a BIP that matches the reference code that the Segwit2x group has
built and published a week ago.
This BIP and code satisfies the requests of a large part of the Bitcoin
community for a moderate increase in the Bitcoin non-witness block space
coupled with the activation of Segwit.
You can find the BIP draft in the following link:
Reference source was kindly provided by the Segwit2x group.
Best regards,
 Sergio.

@_date: 2017-07-10 08:50:33
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A Segwit2x BIP 
Thank you for all your comments. I will improve the BIP based on the
technical suggestions received.
On the subjective/political side that has slipped into this discussion.
Skip this part if not interested in politics.
Regarding the timeline, its certainly rather short, but also is the UASF
BIP 148 ultimatum.
If Bitcoin were a democracy and we had somehow a way to securely perform a
referendum, then this will solve easily. But neither is true. At least now.
More than 80% of the miners and many users are willing to go in the
Segwit2x direction. With the support and great talent of the Bitcoin Core
developers, Segwit2x activation will not cause any major disruptions.
Without Core, there will be a temporary split. Both sides will have to
I want a Bitcoin united. But maybe a split of Bitcoin, each side with its
own vision, is not so bad.

@_date: 2017-07-13 00:10:55
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A Segwit2x BIP 
Some responses..
I think that limiting the maximum transaction size may not be the best
possible solution to the N^2 hashing problem, yet it is not a bad start.
There are several viable soft-forking solutions to it:
1- Soft-fork to perform periodic reductions in the maximum non-segwit
checksigs per input (down to 20)
2- Soft-fork to perform periodic reductions in the number of non-segwit
checksigs per transaction. (down to 5K)
3- Soft-fork to perform periodic reductions in the amount of data hashed by
non-segwit checksigs.
Regardless which one one picks, the soft-fork can be deployed with enough
time in advance to reduce the exposure. The risk is still low. Four years
have passed since I reported this vulnerability and yet nobody has
exploited it. The attack is highly anti-economical, yet every discussion
about the block size ends up citing this vulnerability.
I will mention this worst case in the BIP.
Even if artificially filling the witness space up to 8 MB is
anti-economical, Segwit exacerbates this problem because each witness byte
costs 1/4th of a non-witness byte, so the block bloat attack gets cheaper
than before. I think the guilt lies more in Segwit discount factor than in
the plain block size increase.
I would remove the discount factor altogether, and add a fixed (40 bytes)
discount for each input with respect to outputs (not for certain input
types), to incentivize the cleaning of the UTXO set. A discount for inputs
cannot be used to bloat an unlimited number of blocks, because for each
input the attacker needs to first create an output (without discount).
There is no need to incentivize removing the signatures from blocks,
because there is already an incentive to do so to save disk space.
be redefined, even if it's not a consensus change.

@_date: 2017-07-13 00:19:28
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A Segwit2x BIP 
Well, 40 bytes reduction per input is excessive too :)
But 30 bytes reduction will do fine.
On Thu, Jul 13, 2017 at 12:10 AM, Sergio Demian Lerner <

@_date: 2017-07-13 16:19:35
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] A Segwit2x BIP 
The BIP has been updated.
- The technical spec has been improved: now the block size increase is
specified in terms of weight and not in terms of bytes.
- The increase in the maximum block sigops after HF has been documented.
- Comments added about the worst case block size.
Happy weekend! And don't forget to start signaling something before block
475776 !  It's just 90 blocks away.
Bit 1 or 4,1 or whatever you wish, but please signal something.
To the moon!
On Wed, Jul 12, 2017 at 2:38 PM, Jorge Tim?n via bitcoin-dev <

@_date: 2017-06-02 17:57:12
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Compatibility-Oriented Omnibus Proposal 
I don't see LukeJr 2MB limit to be compatible with the NY agreement. For
the rest, seems fine for me.
On Fri, Jun 2, 2017 at 4:13 PM, Jared Lee Richardson via bitcoin-dev <

@_date: 2017-06-02 17:51:45
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
By "upgrade"  the HF you mean activate 2X and then spoonet 18 months later
or do not activate the 2x HF at all?
On Fri, Jun 2, 2017 at 4:04 PM, Erik Aronesty via bitcoin-dev <

@_date: 2017-06-09 18:54:00
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Drivechain -- Request for Discussion 
I'm a bit late to this party. I just want to add that there exists an
hybrid model where both a federation and the miners provide acknowledges
(sometimes known as "votes" in drivechain terms and "multi-signatures" in a
federation) of the sidechain state.
My Drivechain proposal (Feb 2016) was tailored to enable this particular
use-case, which I'm not sure Paul's proposal enable.
The proposal is on this list under the following subject: Drivechain
proposal using OP_COUNT_ACKS
BIP (draft):
Code & Test cases:
In this proposal, the "poll" time is sidechain-configurable, and I proposed
a few days delay was enough.
Some have said that a longer times are needed, such as 2 months.
So if you want to have a 2 month dalay for sidechain->mainchain transfers
in this code, you still can. However a better dynamic cache of acks/nacks
would be needed. However, for the hybrid use-case, one day is more than
On Tue, May 30, 2017 at 2:11 AM, Paul Sztorc via bitcoin-dev <

@_date: 2017-06-27 12:42:53
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
Currently the only implementation that fulfills the requirements of the NYA
agreement is the segwit2x/btc1 implementation, which is being finalized
this week.
Segwit2mb does not fulfill the NYA agreement.
I'm asking now the segwit2x development team when a BIP will be ready so
that Core has the opportunity to evaluate the technical proposal.
On Wed, Jun 21, 2017 at 1:05 AM, Jacob Eliosoff via bitcoin-dev <

@_date: 2017-03-31 18:09:18
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Hi everyone,
Segwit2Mb is the project to merge into Bitcoin a minimal patch that aims to
untangle the current conflict between different political positions
regarding segwit activation vs. an increase of the on-chain blockchain
space through a standard block size increase. It is not a new solution, but
it should be seen more as a least common denominator.
Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB block
size hard-fork activated ONLY if segwit activates (95% of miners
signaling), but at a fixed future date.
The sole objective of this proposal is to re-unite the Bitcoin community
and avoid a cryptocurrency split. Segwit2Mb does not aim to be best
possible technical solution to solve Bitcoin technical limitations.
However, this proposal does not imply a compromise to the future
scalability or decentralization of Bitcoin, as a small increase in block
size has been proven by several core and non-core developers not to affect
Bitcoin value propositions.
In the worst case, a 2X block size increase has much lower economic impact
than the last bitcoin halving (<10%), which succeeded without problem.
On the other side, Segwit2Mb primary goal is to be minimalistic: in this
patch some choices have been made to reduce the number of lines modified in
the current Bitcoin Core state (master branch), instead of implementing the
most elegant solution. This is because I want to reduce the time it takes
for core programmers and reviewers to check the correctness of the code,
and to report and correct bugs.
The patch was built by forking the master branch of Bitcoin Core, mixing a
few lines of code from Jeff Garzik's BIP102,  and defining a second
versionbits activation bit (bit 2) for the combined activation.
The combined activation of segwit and 2Mb hard-fork nVersion bit is 2
This means that segwit can still be activated without the 2MB hard-fork by
signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).
The tentative lock-in and hard-fork dates are the following:
Bit 2 signaling StartTime = 1493424000; // April 29th, 2017
Bit 2 signaling Timeout = 1503964800; // August 29th, 2017
HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT
The hard-fork is conditional to 95% of the hashing power has approved the
segwit2mb soft-fork and the segwit soft-fork has been activated (which
should occur 2016 blocks after its lock-in time)
For more information on how soft-forks are signaled and activated, see
This means that segwit would be activated before 2Mb: this is inevitable,
as versionbits have been designed to have fixed activation periods and
thresholds for all bits. Making segwit and 2Mb fork activate together at a
delayed date would have required a major re-write of this code, which would
contradict the premise of creating a minimalistic patch. However, once
segwit is activated, the hard-fork is unavoidable.
Although I have coded a first version of the segwit2mb patch (which
modifies 120 lines of code, and adds 220 lines of testing code), I would
prefer to wait to publish the source code until more comments have been
received from the community.
To prevent worsening block verification time because of the O(N^2) hashing
problem, the simple restriction that transactions cannot be larger than 1Mb
has been kept. Therefore the worse-case of block verification time has only
Regarding the hard-fork activation date, I want to give enough time to all
active economic nodes to upgrade. As of Fri Mar 31 2017,
 reports that 6332 out of 6955 nodes (91%)
have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can be
used to identify economic active nodes, because in the 0.12 release dynamic
fees were introduced, and currently no Bitcoin automatic payment system can
operate without automatic discovery of the current fee rate. A pre-0.12
would require constant manual intervention.
Therefore I conclude that no more than 91% of the network nodes reported by
bitnodes are active economic nodes.
As Bitcoin Core 0.12 was released on February 2016, the time for this 91%
to upgrade has been around one year (under a moderate pressure of
operational problems with unconfirmed transactions).
Therefore we can expect a similar or lower time to upgrade for a hard-fork,
after developers have discussed and approved the patch, and it has been
reviewed and merged and 95% of the hashing power has signaled for it (the
pressure not to upgrade being a complete halt of the operations). However I
suggest that we discuss the hard-fork date and delay it if there is a real
need to.
Currently time works against the Bitcoin community, and so is delaying a
compromise solution. Most of the community agree that halting the
innovation for several years is a very bad option.
After the comments collected by the community, a BIP will be written
describing the resulting proposal details.
If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should be
updated to a Segwit2Mb enabled node to prevent them to be forked-away in a
chain with almost no hashing-power.
The proof of concept patch was made for Bitcoin Core but should be easily
ported to other Bitcoin protocol implementations that already support
versionbits. Lightweight (SPV) wallets should not be affected as they
generally do not check the block size.
I personally want to see the Lightning Network in action this year, use the
non-malleability features in segwit, see the community discussing other
exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains and
I want to see miners, developers and industry side-by-side pushing Bitcoin
forward, to increase the value of Bitcoin and prevent high transaction fees
to put out of business use-cases that could have high positive social
I believe in the strength of a unified Bitcoin community. If you're a
developer, please give your opinion, suggest changes, audit it, and take a
stand with me to unlock the current Bitcoin deadlock.
Contributions to the segwit2mb project are welcomed and awaited. The only
limitation is to stick to the principle that the patch should be as simple
to audit as possible. As an example, I wouldn't feel confident if the patch
modified more than ~150 lines of code.
Improvements unrelated to a 2 Mb increase or segwit, as beneficial as it
may be to Bitcoin, should not be part of segwit2Mb.
This proposal should not prevent other consensus proposals to be
simultaneously merged: segwit2mb is a last resort solution in case we can
not reach consensus on anything better.
Again, the proposal is only a starting point: community feedback is
expected and welcomed.
Sergio Demian Lerner

@_date: 2017-03-31 18:50:05
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Yes I understand that segwit2mb represents a "potential" 4Mb block size
But Segwit does not immediately lead to 2 Mb blocks, but can only achieve
close to a 2Mb increase if all Bitcoin wallets switch to segwit, which will
take a couple of years.
Therefore I don't expect transactions per block to quadruple from one day
to another.
On Fri, Mar 31, 2017 at 6:22 PM, praxeology_guy <

@_date: 2017-03-31 19:13:35
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
On Fri, Mar 31, 2017 at 6:22 PM, Matt Corallo I've read every proposal that was published in the last two years and the
choice for NOT implementing any of the super cool research you cite is
We're in a deadlock and it seems we can't go forward adding more
functionality to segwit without the community approval (which include
miners). This is obvious to me.Then we have to go back.
If this last resort solution is merged, we could go back to discuss
improvements with the
Your goal of "avoid
couldn't have expressed it more clearly. However the only "riskier" element
is the hard forking date. We can move the date forward.
a) Utilizing the "hard fork signaling bit" in the nVersion of the block.
This I could consider, as it requires probably a single line of code. Which
BIP specifies this?
The Seghash problem has already been addressed by limiting the maximum size
of a transaction to 1 Mb.
The FindAndDelete problem has already been solved by the Core Developers,
so we don't have to worry about it anymore.
We could add a simple protection, although if we reach community consensus
and 95% of hashing power, does we really need to? Can the old chain still
be alive?
If more people ask for replay protection, I will merge Spoonet scheme or
develop the minimum possible replay protection (a simple signaling bit in
transaction version)
That is an interesting economic change and would be out of the scope of
We can keep discussing spoonet while we merge segwit2mb, as spoonnet
includes most of technical innovations.
company CTOs have expressed that one year for a Bitcoin hard-fork was
period they could schedule a secure upgrade.
proposal has good parameters (17.7% per year).
c) You should likely consider the effect of the many technological
splits, as RSK platform cannot be pegged to two different cryptocurrencies.
We could launch two platforms, but RSK value proposition is "supporting the
advance of Bitcoin, the cryptocurrecy with highest network effect". You
understand that if Bitcoin splits Bitcoin BTC/BTU separately may cease to
be the cryptocurrencies with higher volume/market cap/network effect.
Therefore all RSK people that I talked too would prefer to avoid a split at
all cost, reather that to be the winners of the scaling war.

@_date: 2017-05-08 18:00:10
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Full node "tip" function 
A full node provides several services to the network:
1?Broadcasts blocks (public service)
2?Broadcasts transactions (public/private service)
3?Increases privacy by hiding other node?s IPs
4?Increases network security by protecting it from global DoS.
5?Provides information filtering services to SPV nodes.
6?Provides historic blockchain and state information to new nodes.
With your tip idea you only encourages 6, and by increasing the number of
nodes, also 3 and 4.
The services 1 and 2 cannot be encouraged by tips.
However, it's a good way to start.
There was a way to encourage 2 I described in 2013. (
I'll soon present a solution to encourage full nodes to store the
blockchain based on Proof-of-Unique-Blockchain-Storage (PoUBS), a feature
that RSK will add to incentivize Bitcoin and RSK full nodes. This solution
encourages 6.
On Thu, May 4, 2017 at 4:28 PM, Erik Aronesty via bitcoin-dev <

@_date: 2017-05-08 19:15:48
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Full node "tip" function 
Yes you practically can. No proxy can defeat the protocol investing less
money than buying storage space to store the blockchain.
Even with challenge-response delays of minutes.  That's why it will be
fully controlled by a RSK smart-contract, with no user intervention.
I'm will post about this soon.

@_date: 2017-05-08 19:42:23
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I have processed 1000 blocks starting from Block I computed several metrics, including the supposed size of witness data and
non-witness data (onchain), assuming all P2SH inputs/outputs are converted
to P2PWSH and all P2PKH inputs/outputs are converted to P2WPKH.
This takes into account that other types of transactions will not be
modified by Segwit (e.g. OP_RETURN outputs, or P2PK). This analysis doesn't
take into account that LN transactions may affect the current state,
 increasing the segwit/nosegwit ratio.
Among a lot of information, I've got the following real world results...
acMainChainSpace =352608924
acSegwitSpace =599400403
Ratio segwit/nosegwit=1.6999
This implies that the 75% that discount is not the best option to prevent
witness spam in a block of 4 MB, as stated in
The non-witness data weight factor should not be 4 but 2.35. The closest
integer value is 2, which leads to a 50% witness discount.
The Bitcoinj source code is available for anyone to review. I encourage
anyone to re-compute this with another utility to cross-check. Maybe
Antoine Le Calvez (p2sh.info) would like to double-check.

@_date: 2017-05-09 10:49:05
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
This [1] article says the current discount prevents witness spam. Witness
spam is free space in the witness part of the block that can be filled by
miners to create bigger blocks with almost no cost for the benefit a
cluster of miners with low latency, increasing centralization.
The 75% discount does not prevent it, but on the contrary leaves a lot of
extra witness space for spam.
If the maximum block weight is set to 2.7M, each byte of non-witness block
costs 1.7, and each byte of witness costs 1, then a normal filled block
would be 2.7M bytes (1.7+1), and there will be no need to create ever a 4
Mbyte block. The worst case would be the average case, and the transaction
rate would be the maximum possible.
The current 75% discount can only achieve more transactions per second if
the type of transactions change. Therefore the current 75% discount only
makes the block size worst case worse (4 Mbytes when it should be 2.7
80% of all inputs/outputs are P2PKH. The only way to make use of the extra
space If most P2PKH transactions are replaced by multisigs (typically for
So it seems the 75% discount has been chosen with the idea that in the
future the current transaction pattern will shift towards multisigs. This
is not a bad idea, as it's the only direction Bitcoin can scale without a
But it's a bad idea if we end up doing, for example, a 2X blocksize
increase HF in the future. In that case it's much better to use a 50%
witness discount, and do not make scaling risky by making the worse case
block size 8 Mbytes, when it could have been 2*2.7=5.4 Mbytes.
I've uploaded the code here:
 [1] On Mon, May 8, 2017 at 8:47 PM, Alphonse Pace via bitcoin-dev <

@_date: 2017-05-09 13:19:13
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
Thanks Johnson and Hampus for the clarifications.
However, I would rather do the opposite: soft-fork to 50% now, and
soft-fork again to 75% discount later if needed, because it doesn't affect
the max transactions/second.
Segwit as it is today should be activated. However if it is not before
November, then for the next Segwit attempt I would choose a more
conservative 50% discount.

@_date: 2017-05-09 15:58:25
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
Yes. In a soft-fork is true.
I was thinking about what a HF could do to optimize the balance, and I
forgot I was in the context of a SF.

@_date: 2017-05-09 16:15:32
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
Let n be the non-segwit bytes. Let the seg/noseg ratio be 1.7.
Segwit with 75% discount: (let WITNESS_SCALE_FACTOR=4)
n*WITNESS_SCALE_FACTOR+n*1.7 = 4,000,000
Then n=4,000,000 / 5.7 = 701K
Average block size = 701K*(1+1.7)=1.8 Mbytes
Maximum block size = 4 MBytes
Segwit with 50% discount + 2MB HF: (let WITNESS_SCALE_FACTOR=2)
n*2+n*1.7 = 4,000,000
n = 4,000,000/ 3.7 = 1.08M
Average block size = 1.08M*(1+1.7)=2.9 Mbytes
Maximum block size = 4 MBytes
The capacity of Segwit(50%)+2MbHF is 50% more than Segwit, and the maximum
block size is the same.
On Tue, May 9, 2017 at 3:58 PM, Sergio Demian Lerner <

@_date: 2017-05-09 17:58:35
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I agree with you Matt.
I'm artificially limiting myself to changing the parameters of Segwit as it
This is motivated by the idea that a consensual HF in the current state
would have greater chance of acceptance if it changes the minimum number of
lines of code.

@_date: 2017-05-10 12:25:27
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
Jaja. But no shit. Not perfect maybe, but Bitcoin was never perfect. It has
always been good enough. And at the beginning it was quite simple. Simple
enough it allowed gradual improvements that anyone with some technical
background could understand. Now we need a full website to explain an
But this is becoming more and more out of topic.
On Wed, May 10, 2017 at 11:05 AM, Matt Corallo

@_date: 2017-05-10 16:40:12
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
I'm not advocating. I'm mediating.
This is out of
On Wed, May 10, 2017 at 1:39 PM, Matt Corallo

@_date: 2017-09-12 01:49:34
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
Historically people have published vulnerabilities in Bitcoin only after
publicly stated) policy. If you're a core developer and you know better,
please correct me.
This means that:
- a critical vulnerability, like a remote code execution, will be patched
immediately (without disclosing the actual problem) and all participants
will be notified asap. This is no different from any other open source
project. An example of this case was the OpenSSL Heartbleed vulnerability
that affected Bitcoin.
- a non-critical vulnerability, either because miners only can exploit it
or because it requires vast resources to pull, may require a wait of years
before publication, after a vulnerability was found and reported. This is
because the "natural" node upgrade rate is slow.
It also implies that some times a researcher works hard to investigate a
vulnerability and later he finds out it was previously reported. It also
means that the researcher cannot report to alt-coins which have a different
This policy has nothing to do with a loyalty to Bitcoin Core (or in fact,
the two or so developers that actually receive the e-mails to
security at bitcoincore.org).
This is a policy that has simply proven to work to protect Bitcoiners. It
began long long ago.
On Tue, Sep 12, 2017 at 12:37 AM, Anthony Towns via bitcoin-dev <

@_date: 2017-09-22 16:53:46
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
The policy seems good with the exception of this paragraph:
* Bitcoin devs will disclose vulnerabilities publically after 99% of the
   bitcoin network has upgraded [7], and fixes have been released for
   at least 12 months.
99% upgrade may never be reached. Some nodes cannot even be categorized. I
suggest a number close to 95%.
If the 95% of network has upgraded, it means we're pretty secure from the
point of view of consensus. It is supposed that from the time the fix has
been released, all other alt-coins will also have released their fixes.
Remember we must also incentivize security researchers to do the hard and
silent research work. Most of them do not hold Bitcoins. They do research
because of other interests, including getting public acknowledgment for
their findings. They'll be frustrated if they have to wait 2 years.
I propose this paragraph to replace the previous one:
* Bitcoin devs will disclose vulnerabilities publically after 95% of the
   bitcoin network has upgraded [7], and fixes have been released for
   at least 6 months.
Also I suggest we track vulnerabilities with standard CVE codes. IS there
any drawback of this?
On Thu, Sep 21, 2017 at 11:00 PM, Nathan Wilcox via bitcoin-dev <

@_date: 2017-09-22 17:32:56
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
The lack of signed maximum segwit stack size was one of the objections to
segwit I presented last year. This together with the unlimited segwit stack
However, committing to the maximum stack size (in bytes) for an input is
tricky. The only place where this could be packed is in sequence_no, with a
soft-fork. E.g. when transaction version is 2 and and only when lock_time
is zero.
For transactions with locktime >0, we could soft-fork so transactions add a
last zero-satoshi output whose scriptPub contains OP_RETURN and followed by
N VarInts, containing the maximum stack size of each input.
Normally, for a 400 byte, 2-input transaction, this will add 11 bytes, or a
2.5% overhead.

@_date: 2017-09-22 18:32:00
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
But generally before one signs a transaction one does not know the
signature size (which may be variable). One can only estimate the maximum
On Fri, Sep 22, 2017 at 6:11 PM, Mark Friedenbach

@_date: 2017-09-22 18:54:39
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
If the variable size increase is only a few bytes, then three possibilities
- one should allow signatures to be zero padded (to reach the maximum size)
and abandon strict DER encoding
- one should allow spare witness stack elements (to pad the size to match
the maximum size) and remove the cleanstack rule. But this is tricky
because empty stack elements must be counted as 1 byte.
- signers must loop the generation of signatures until the signature
generated is of its maximum size.
On Fri, Sep 22, 2017 at 6:39 PM, Mark Friedenbach

@_date: 2018-08-09 21:21:17
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Simple change to the "merkleblock" command to protect 
While fixing RSK's SPV bridge I came up with an idea to fix the
MERKLEBLOCK command to prevent rogue peers from attacking SPV peers using
Bitcoin's Merkle tree structure flaws. The most annoying attack is the one
that tries to confuse a victim peer into thinking a transaction is an inner
node, extending such node with a new right-sided branch with a fake
transaction (*) .
The old idea to soft-fork Bitcoin to make invalid 64-byte transactions is
attractive, but also a coordination problem that could be avoided with this
new proposal.
The idea is simple, and it's not a fork, but a network protocol improvement.
Let A be the hash digest that must be combined with the hash digest B, such
that the upper node hash is SHA256(SHA256(A | B)).
Therefore A = SHA256(SHA256(X)) and B = SHA256(SHA256(Y)), and X and Y are
either Bitcoin transactions or other inner nodes.
Instead of storing A, the merkleblock structure should store a pre-image of
A, or SHA256(X).
If the block only has the coinbase, nothing is done.
The pre-image change could be done to both left and right hashes, but it's
enough to do it to all left-side hashes that do not have children in the
partial merkle tree structure (let's call them terminal hahes. to avoid
confusion with leaf hashes).
Verifiers (SPV nodes) would apply a single SHA256() operation to the
left-sided terminal hashes before combining them. The cost to the verifier
is in the worse case only 33% more.
This basically limits the attacker's ability to supply chosen-hash digests
in order to build a transaction. Because the left side contains most of the
previn hash, the attacker would need to bruteforce a huge space (about 208
bits) in order to come up with a pre-image that maps to a owned previn.
Meet-in-the-middle attacks would be expensive as UTXOs are not free.
To implement this change, a new command MERKLEBLOCK2 could be implemented
or the protocol version could be used to differentiate between the two
modes of the MERKLEBLOCK command.
If the idea gets community support, I may write the BIP/code or invite
anyone to do it.
 (*)

@_date: 2018-08-14 12:26:25
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Fwd: Simple change to the "merkleblock" command to 
While fixing RSK's SPV bridge I came up with an idea to fix the
MERKLEBLOCK command to prevent rogue peers from attacking SPV peers using
Bitcoin's Merkle tree structure flaws. The most annoying attack is the one
that tries to confuse a victim peer into thinking a transaction is an inner
node, extending such node with a new right-sided branch with a fake
transaction (*) .
The old idea to soft-fork Bitcoin to make invalid 64-byte transactions is
attractive, but also a coordination problem that could be avoided with this
new proposal.
The idea is simple, and it's not a fork, but a network protocol improvement.
Let A be the hash digest that must be combined with the hash digest B, such
that the upper node hash is SHA256(SHA256(A | B)).
Therefore A = SHA256(SHA256(X)) and B = SHA256(SHA256(Y)), and X and Y are
either Bitcoin transactions or other inner nodes.
Instead of storing A, the merkleblock structure should store a pre-image of
A, or SHA256(X).
If the block only has the coinbase, nothing is done.
The pre-image change could be done to both left and right hashes, but it's
enough to do it to all left-side hashes that do not have children in the
partial merkle tree structure (let's call them terminal hahes. to avoid
confusion with leaf hashes).
Verifiers (SPV nodes) would apply a single SHA256() operation to the
left-sided terminal hashes before combining them. The cost to the verifier
is in the worse case only 33% more.
This basically limits the attacker's ability to supply chosen-hash digests
in order to build a transaction. Because the left side contains most of the
previn hash, the attacker would need to bruteforce a huge space (about 208
bits) in order to come up with a pre-image that maps to a owned previn.
Meet-in-the-middle attacks would be expensive as UTXOs are not free.
To implement this change, a new command MERKLEBLOCK2 could be implemented
or the protocol version could be used to differentiate between the two
modes of the MERKLEBLOCK command.
If the idea gets community support, I may write the BIP/code or invite
anyone to do it.
 (*)

@_date: 2018-02-12 21:54:00
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] [BIP] Stratum protocol specification 
When we worked on the extensions for RSK merge mining, I prepared an
internal document with the most up to date Straum protocol description I
could get.
This is the document:
On Fri, Feb 9, 2018 at 10:48 AM, Jules Lamur via bitcoin-dev <

@_date: 2018-06-09 13:03:53
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
Hi Peter,
We reported this as CVE-2017-12842, although it may have been known by
developers before us.
There are hundreds of SPV wallets out there, without even considering other
more sensitive systems relying on SPV proofs.
As I said we, at RSK, discovered this problem in 2017. For RSK it's very
important this is fixed because our SPV bridge uses SPV proofs.
I urge all people participating in this mailing list and the rest of the
Bitcoin community to work on this issue for the security and clean-design
of Bitcoin.
My suggestion is to use in the version bits 4 bits indicating the tree
depth (-1), as a soft-fork, so
00=1 depth,
0F = 16 depth (maximum 64K transactions). Very clean.
The other option is to ban transaction with size lower or equal to 64.
Best regards,
 Sergio.
On Sat, Jun 9, 2018 at 5:31 AM Bram Cohen via bitcoin-dev <

@_date: 2018-06-09 14:21:17
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
Also it must be noted that an attacker having only 1.3M USD that can
brute-force 72 bits (4 days of hashing on capable ASICs) can perform the
same attack, so the attack is entirely feasible and no person should accept
more than 1M USD using a SPV wallet.
Also the attack can be repeated: once you create the "extension point"
block, you can attack more and more parties without any additional
On Sat, Jun 9, 2018 at 1:03 PM Sergio Demian Lerner <

@_date: 2018-06-09 14:24:49
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
Here is our internal report, if you want more details on the problem.
 (our reported attack complexity may slightly differ from what Peter has
provided, but the attack complexity depends on the funds the attacker is
willing to lock).
 Sergio.
On Sat, Jun 9, 2018 at 2:21 PM Sergio Demian Lerner <

@_date: 2018-06-09 14:51:55
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Trusted merkle tree depth for safe tx inclusion 
Yo can fool a SPV wallet even if it requires a thousands confirmations
using this attack, and you don't need a Sybil attack, so yes, it impacts
SPV wallets also. The protections a SPV node should have to prevent this
attack are  different, so it must be considered separately.
It should be said that a SPV node can avoid accepting payments if any
Merkle node is at the same time a valid transaction, and that basically
almost eliminates the problem.
SPV Wallet would reject valid payments with a astonishingly low probability.

@_date: 2019-08-07 23:09:20
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Bitcoin vaults with anti-theft recovery/clawback 
Seems to be comparable to the proposed "Tick Method" from 2013:
However I remember that someone told me the tick method had a flaw..
On Wed, Aug 7, 2019 at 6:28 PM Dustin Dettmer via bitcoin-dev <

@_date: 2020-12-31 20:37:58
@_author: Sergio Demian Lerner 
@_subject: [bitcoin-dev] Softchains: Sidechains as a Soft Fork via 
Hi Roben,
 It's an interesting proposal, but I have two issues with it, one technical
and one philosophical.
On the technical side, I don't understand how your proposal prevents miners
proposing a peg-out for an invalid sidechain fork which is not made
available to the nodes (there are missing blocks). It seems that the system
would need to allow users to challenge miners to make available full
sidechain blocks that are missing, which really complicates the protocol.
On the philosophical side, as you mentioned, it is very limited in the
types of sidechains it can verify. I won't be able to verify RSK
(merge-mined with Bitcoin, but with different block format and different
functionality). It cannot verify a zCash-like sidechain for the same
reasons. Therefore it is strictly a payment scalability solution.
Drivechains, on the other hand, enable many new use cases apart from
scaling, which have a much lower level of complexity (if implemented
Since the inception of RSK sidechain, I suggested in its white-paper that
sidechains should be designed to support an hybrid peg-out system, based on
both a large multisig AND a drivechain, where both groups need to agree for
the peg-out to occur.  It's a censorship/security trade-off that most users
would be willing to accept until a trusted-setup-free SNARK-like based
solution is finally available.
Until we have a sidechain-selectable SNARK-like succinct verification of
any block state transition function, having a single succint proof to cover
the whole sidechain validity, as in Coda (now renamed Mina), drivechains
are the low-hanging-fruit.
On Thu, Dec 31, 2020 at 7:01 PM Ruben Somsen via bitcoin-dev <
