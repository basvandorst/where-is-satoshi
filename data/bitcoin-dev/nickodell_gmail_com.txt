
@_date: 2015-12-30 16:13:56
@_author: Nick ODell 
@_subject: [bitcoin-dev] How to preserve the value of coins after a fork. 
I have two technical criticisms of your proposal, and one economic criticism.
This seems to contradict a later section that says that users can use
Dee natively, without paying fees necessary to get a transaction into
Dum. You can't have this both ways - either you can get a transaction
into Dee without getting it into Dum first, or you can't.
What if some other group that wants to hurt both Dum and Dee were to
make a false-flag attack against Dee? Mutually assured destruction
doesn't work if you can't accurately attribute attacks.
I don't think a compromise would be reachable at that point - suppose
one had a market cap of 1.2 billion, and the other had a market cap of
0.8 billion. How would the coins on the unified chain be distributed?
You could give each chain an equal number of coins, but that's not
fair to the people holding the more valuable coins. You could give
each chain a number of coins proportional to the market cap, but that
invites price manipulation. In any case, if you had a way of reaching
compromise, why not use it instead of creating two chains?
Overall, I think this proposal is a bad idea.
That doesn't end up mattering, though, as I understand his proposal.
The unified client would just see that both validly spend an output
with a scriptPubKey of OP_HASH160 0xabcdef... OP_EQUAL.
On Wed, Dec 30, 2015 at 1:32 PM, Peter Todd via bitcoin-dev

@_date: 2015-11-06 02:22:27
@_author: Nick ODell 
@_subject: [bitcoin-dev] Dealing with OP_IF and OP_NOTIF malleability 
Your suggested modification seems sound.
Though, a script author could do something similar right now by
prefacing his IF with this:
    OP_DUP OP_DUP OP_0 OP_EQUAL OP_SWAP OP_1 OP_EQUAL OP_BOOLOR
OP_NOTIF OP_RETURN OP_ENDIF [actual OP_IF goes here]
That checks whether the input is 0 or 1, and runs OP_RETURN if not.
Your way is cleaner, though.
On Fri, Nov 6, 2015 at 1:13 AM, jl2012 via bitcoin-dev

@_date: 2016-12-23 10:35:49
@_author: Nick ODell 
@_subject: [bitcoin-dev] Multisig with hashes instead of pubkeys 
The first issue is that doing two OP_SWAP's in a row will just return
you to the original state. The second issue is that all of them end up
hashing the same key, so anyone on the network can spend this output.
(See  for a good resource on opcodes
and what each of them do. There are also a few simulators floating
around, but I can't recommend any in particular.)
Third, if you're concerned about exposing public keys, why not use a
P2SH script? That won't expose your public keys until you spend from
On Thu, Dec 22, 2016 at 11:29 AM, Andrew via bitcoin-dev

@_date: 2016-01-04 11:04:29
@_author: Nick ODell 
@_subject: [bitcoin-dev] An implementation of BIP102 as a softfork. 
How are you collecting fees from the transactions in the block?
On Sat, Jan 2, 2016 at 8:51 PM, joe2015--- via bitcoin-dev

@_date: 2016-07-26 16:03:41
@_author: Nick ODell 
@_subject: [bitcoin-dev] Reasons to add sync flags to Bitcoin 
Mining the sync flag isn't compatible with the payout structure of non
hot-wallet pools like Eligius or decentralized pools like p2pool.
Those need the ability to split a reward among multiple parties.
Instead of giving an address to send the funds to, you could include
the hash of the transaction allowed to spend the sync flag output.
You'd have to zero the previous outpoint of the transaction before
hashing, since you don't know what the hash of the coinbase ten blocks
from now will be.
On Tue, Jul 26, 2016 at 6:47 AM, Moral Agent via bitcoin-dev

@_date: 2016-06-28 18:06:41
@_author: Nick ODell 
@_subject: [bitcoin-dev] BIP 151 
Yeah, but not the same *sort* of authentication. As a trivial example,
you could have ten servers that sign long-term keys for nodes. All
that they need to check is that the node requesting a signature owns
the corresponding IP address. On the other hand, 'evil nodes' is a
subjective quality that is hard to assess automatically.
Bitcoin is designed with the assumption that some of the nodes you
connect to might be evil. Sure, if 100% of the nodes you're connected
to are evil, you're screwed. However, we shouldn't avoid protecting
people from someone on the same coffee-shop network, just because the
same mitigation won't work against a nation-state.
On Tue, Jun 28, 2016 at 5:29 PM, Eric Voskuil via bitcoin-dev

@_date: 2016-05-19 16:23:28
@_author: Nick ODell 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
What if two people create transactions from oupoints  within the same MMR
tree tip, at the same time?
For example, I create transaction A plus an MMR proof that MMR tip X will
become Y.
On the other side of the planet, someone else creates transaction B, plus
an MMR proof that tip X will become Z.
Can a miner who receives A and B put both into a block, without access to
the outputs that were pruned?
# Motivation
UTXO growth is a serious concern for Bitcoin's long-term decentralization.
run a competitive mining operation potentially the entire UTXO set must be
RAM to achieve competitive latency; your larger, more centralized,
will have the UTXO set in RAM. Mining is a zero-sum game, so the extra
of not doing so if they do directly impacts your profit margin. Secondly,
having possession of the UTXO set is one of the minimum requirements to run
full node; the larger the set the harder it is to run a full node.
Currently the maximum size of the UTXO set is unbounded as there is no
consensus rule that limits growth, other than the block-size limit itself;
of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,
which expands to significantly more in memory. UTXO growth is driven by a
number of factors, including the fact that there is little incentive to
inputs, lost coins, dust outputs that can't be economically spent, and
non-btc-value-transfer "blockchain" use-cases such as anti-replay oracles
We don't have good tools to combat UTXO growth. Segregated Witness proposes
give witness space a 75% discount, in part of make reducing the UTXO set
by spending txouts cheaper. While this may change wallets to more often
dust, it's hard to imagine an incentive sufficiently strong to discourage
let alone all, UTXO growing behavior.
For example, timestamping applications often create unspendable outputs due
ease of implementation, and because doing so is an easy way to make sure
the data required to reconstruct the timestamp proof won't get lost - all
Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay
use-cases like using the UTXO set for key rotation piggyback on the uniquely
strong security and decentralization guarantee that Bitcoin provides; it's
difficult - perhaps impossible - to provide these applications with
alternatives that are equally secure. These non-btc-value-transfer use-cases
can often afford to pay far higher fees per UTXO created than competing
btc-value-transfer use-cases; many users could afford to spend $50 to
a new PGP key, yet would rather not spend $50 in fees to create a standard
output transaction. Effective techniques to resist miner censorship exist,
without resorting to whitelists blocking non-btc-value-transfer use-cases as
"spam" is not a long-term, incentive compatible, solution.
A hard upper limit on UTXO set size could create a more level playing field
the form of fixed minimum requirements to run a performant Bitcoin node, and
make the issue of UTXO "spam" less important. However, making any coins
unspendable, regardless of age or value, is a politically untenable economic
# TXO Commitments
A merkle tree committing to the state of all transaction outputs, both spent
and unspent, we can provide a method of compactly proving the current state
an output. This lets us "archive" less frequently accessed parts of the UTXO
set, allowing full nodes to discard the associated data, still providing a
mechanism to spend those archived outputs by proving to those nodes that the
outputs are in fact unspent.
Specifically TXO commitments proposes a Merkle Mountain Range? (MMR), a
type of deterministic, indexable, insertion ordered merkle tree, which
new items to be cheaply appended to the tree with minimal storage
just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
never removed; if an output is spent its status is updated in place. Both
state of a specific item in the MMR, as well the validity of changes to
in the MMR, can be proven with log2(n) sized proofs consisting of a merkle
to the tip of the tree.
At an extreme, with TXO commitments we could even have no UTXO set at all,
entirely eliminating the UTXO growth problem. Transactions would simply be
accompanied by TXO commitment proofs showing that the outputs they wanted to
spend were still unspent; nodes could update the state of the TXO MMR purely
from TXO commitment proofs. However, the log2(n) bandwidth overhead per
txin is
substantial, so a more realistic implementation is be to have a UTXO cache
recent transactions, with TXO commitments acting as a alternate for the
event that an old txout needs to be spent.
Proofs can be generated and added to transactions without the involvement of
the signers, even after the fact; there's no need for the proof itself to
signed and the proof is not part of the transaction hash. Anyone with
access to
TXO MMR data can (re)generate missing proofs, so minimal, if any, changes
required to wallet software to make use of TXO commitments.
 Delayed Commitments
TXO commitments aren't a new idea - the author proposed them years ago in
response to UTXO commitments. However it's critical for small miners' orphan
rates that block validation be fast, and so far it has proven difficult to
create (U)TXO implementations with acceptable performance; updating and
recalculating cryptographicly hashed merkelized datasets is inherently more
work than not doing so. Fortunately if we maintain a UTXO set for recent
outputs, TXO commitments are only needed when spending old, archived,
We can take advantage of this by delaying the commitment, allowing it to be
calculated well in advance of it actually being used, thus changing a
latency-critical task into a much easier average throughput problem.
Concretely each block B_i commits to the TXO set state as of block B_{i-n},
other words what the TXO commitment would have been n blocks ago, if not for
the n block delay. Since that commitment only depends on the contents of the
blockchain up until block B_{i-n}, the contents of any block after are
irrelevant to the calculation.
 Implementation
Our proposed high-performance/low-latency delayed commitment full-node
implementation needs to store the following data:
1) UTXO set
    Low-latency K:V map of txouts definitely known to be unspent. Similar to
    existing UTXO implementation, but with the key difference that old,
    unspent, outputs may be pruned from the UTXO set.
2) STXO set
    Low-latency set of transaction outputs known to have been spent by
    transactions after the most recent TXO commitment, but created prior to
    TXO commitment.
3) TXO journal
    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
    must be low-latency; removals can be high-latency.
4) TXO MMR list
    Prunable, ordered list of TXO MMR's, mainly the highest pending
    backed by a reference counted, cryptographically hashed object store
    indexed by digest (similar to how git repos work). High-latency ok.
    cover this in more in detail later.
 Fast-Path: Verifying a Txout Spend In a Block
When a transaction output is spent by a transaction in a block we have two
1) Recently created output
    Output created after the most recent TXO commitment, so it should be in
    UTXO set; the transaction spending it does not need a TXO commitment
    Remove the output from the UTXO set and append it to the TXO journal.
2) Archived output
    Output created prior to the most recent TXO commitment, so there's no
    guarantee it's in the UTXO set; transaction will have a TXO commitment
    proof for the most recent TXO commitment showing that it was unspent.
    Check that the output isn't already in the STXO set (double-spent), and
    not add it. Append the output and TXO commitment proof to the TXO
In both cases recording an output as spent requires no more than two
updates, and one journal append. The existing UTXO set requires one
update per spend, so we can expect new block validation latency to be
within 2x
of the status quo even in the worst case of 100% archived output spends.
 Slow-Path: Calculating Pending TXO Commitments
In a low-priority background task we flush the TXO journal, recording the
outputs spent by each block in the TXO MMR, and hashing MMR data to obtain
TXO commitment digest. Additionally this background task removes STXO's that
have been recorded in TXO commitments, and prunes TXO commitment data no
Throughput for the TXO commitment calculation will be worse than the
UTXO only scheme. This impacts bulk verification, e.g. initial block
That said, TXO commitments provides other possible tradeoffs that can
impact of slower validation throughput, such as skipping validation of old
history, as well as fraud proof approaches.
 TXO MMR Implementation Details
Each TXO MMR state is a modification of the previous one with most
shared, so we an space-efficiently store a large number of TXO commitments
states, where each state is a small delta of the previous state, by sharing
unchanged data between each state; cycles are impossible in merkelized data
structures, so simple reference counting is sufficient for garbage
Data no longer needed can be pruned by dropping it from the database, and
unpruned by adding it again. Since everything is committed to via
hash, we're guaranteed that regardless of where we get the data, after
unpruning we'll have the right data.
Let's look at how the TXO MMR works in detail. Consider the following TXO
with two txouts, which we'll call state       0
     / \
    a   b
If we add another entry we get state         1
       / \
      0   \
     / \   \
    a   b   c
Note how it 100% of the state  data was reused in commitment  Let's
add two more entries to get state             2
           / \
          2   \
         / \   \
        /   \   \
       /     \   \
      0       2   \
     / \     / \   \
    a   b   c   d   e
This time part of state  wasn't reused - it's wasn't a perfect binary
tree - but we've still got a lot of re-use.
Now suppose state  is committed into the blockchain by the most recent
Future transactions attempting to spend outputs created as of state  are
obliged to prove that they are unspent; essentially they're forced to
part of the state  MMR data. This lets us prune that data, discarding it,
leaving us with only the bare minimum data we need to append new txouts to
TXO MMR, the tips of the perfect binary trees ("mountains") within the MMR:
            2
           / \
          2   \
               \
                \
                 \
                  \
                   \
                    e
Note that we're glossing over some nuance here about exactly what data
needs to
be kept; depending on the details of the implementation the only data we
for nodes "2" and "e" may be their hash digest.
Adding another three more txouts results in state                   3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
                         / \
                        /   \
                       /     \
                      3       3
                     / \     / \
                    e   f   g   h
Suppose recently created txout f is spent. We have all the data required to
update the MMR, giving us state  It modifies two inner nodes and one leaf
                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
                         / \
                        /   \
                       /     \
                      4       3
                     / \     / \
                    e  (f)  g   h
If an archived txout is spent requires the transaction to provide the merkle
path to the most recently committed TXO, in our case state  If txout b is
spent that means the transaction must provide the following data from state
            2
           /
          2
         /
        /
       /
      0
       \
        b
We can add that data to our local knowledge of the TXO MMR, unpruning part
                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
         /               / \
        /               /   \
       /               /     \
      0               4       3
       \             / \     / \
        b           e  (f)  g   h
Remember, we haven't _modified_ state  yet; we just have more data about
When we mark txout b as spent we get state                   5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               / \
        /               /   \
       /               /     \
      5               4       3
       \             / \     / \
       (b)          e  (f)  g   h
Secondly by now state  has been committed into the chain, and transactions
that want to spend txouts created as of state  must provide a TXO proof
consisting of state  data. The leaf nodes for outputs g and h, and the
node above them, are part of state  so we prune them:
                  5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               /
        /               /
       /               /
      5               4
       \             / \
       (b)          e  (f)
Finally, lets put this all together, by spending txouts a, c, and g, and
creating three new txouts i, j, and k. State  was the most recently
state, so the transactions spending a and g are providing merkle paths up to
it. This includes part of the state  data:
                  3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
         / \               \
        /   \               \
       /     \               \
      0       2               3
     /       /               /
    a       c               g
After unpruning we have the following data for state                   5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         / \             / \
        /   \           /   \
       /     \         /     \
      5       2       4       3
     / \     /       / \     /
    a  (b)  c       e  (f)  g
That's sufficient to mark the three outputs as spent and add the three new
txouts, resulting in state                         6
                       / \
                      /   \
                     /     \
                    /       \
                   /         \
                  6           \
                 / \           \
                /   \           \
               /     \           \
              /       \           \
             /         \           \
            /           \           \
           /             \           \
          6               6           \
         / \             / \           \
        /   \           /   \           6
       /     \         /     \         / \
      6       6       4       6       6   \
     / \     /       / \     /       / \   \
   (a) (b) (c)      e  (f) (g)      i   j   k
Again, state  related data can be pruned. In addition, depending on how
STXO set is implemented may also be able to prune data related to spent
after that state, including inner nodes where all txouts under them have
spent (more on pruning spent inner nodes later).
 Consensus and Pruning
It's important to note that pruning behavior is consensus critical: a full
that is missing data due to pruning it too soon will fall out of consensus,
a miner that fails to include a merkle proof that is required by the
is creating an invalid block. At the same time many full nodes will have
significantly more data on hand than the bare minimum so they can help
make transactions spending old coins; implementations should strongly
separating the data that is, and isn't, strictly required for consensus.
A reasonable approach for the low-level cryptography may be to actually
the two cases differently, with the TXO commitments committing too what data
does and does not need to be kept on hand by the UTXO expiration rules. On
other hand, leaving that uncommitted allows for certain types of soft-forks
where the protocol is changed to require more data than it previously did.
 Consensus Critical Storage Overheads
Only the UTXO and STXO sets need to be kept on fast random access storage.
Since STXO set entries can only be created by spending a UTXO - and are
than a UTXO entry - we can guarantee that the peak size of the UTXO and STXO
sets combined will always be less than the peak size of the UTXO set alone
the existing UTXO-only scheme (though the combined size can be temporarily
higher than what the UTXO set size alone would be when large numbers of
archived txouts are spent).
TXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum
overhead per entry: a unique merkle path to a TXO commitment (by "unique" we
mean that no other entry shares data with it). On a reasonably fast system
TXO journal will be flushed quickly, converting it into TXO MMR data; the
journal will never be more than a few blocks in size.
Transactions spending non-archived txouts are not required to provide any
commitment data; we must have that data on hand in the form of one TXO MMR
entry per UTXO. Once spent however the TXO MMR leaf node associated with
non-archived txout can be immediately pruned - it's no longer in the UTXO
so any attempt to spend it will fail; the data is now immutable and we'll
need it again. Inner nodes in the TXO MMR can also be pruned if all leafs
them are fully spent; detecting this is easy the TXO MMR is a merkle-sum
with each inner node committing to the sum of the unspent txouts under it.
When a archived txout is spent the transaction is required to provide a
path to the most recent TXO commitment. As shown above that path is
information to unprune the necessary nodes in the TXO MMR and apply the
immediately, reducing this case to the TXO journal size question
critical overhead is a different question, which we'll address in the next
Taking all this into account the only significant storage overhead of our
commitments scheme when compared to the status quo is the log2(n) merkle
overhead; as long as less than 1/log2(n) of the UTXO set is active,
non-archived, UTXO's we've come out ahead, even in the unrealistic case
all storage available is equally fast. In the real world that isn't yet the
case - even SSD's significantly slower than RAM.
 Non-Consensus Critical Storage Overheads
Transactions spending archived txouts pose two challenges:
1) Obtaining up-to-date TXO commitment proofs
2) Updating those proofs as blocks are mined
The first challenge can be handled by specialized archival nodes, not unlike
how some nodes make transaction data available to wallets via bloom filters
the Electrum protocol. There's a whole variety of options available, and the
the data can be easily sharded to scale horizontally; the data is
self-validating allowing horizontal scaling without trust.
While miners and relay nodes don't need to be concerned about the initial
commitment proof, updating that proof is another matter. If a node
prunes old versions of the TXO MMR as it calculates pending TXO
commitments, it
won't have the data available to update the TXO commitment proof to be
the next block, when that block is found; the child nodes of the TXO MMR tip
are guaranteed to have changed, yet aggressive pruning would have discarded
Relay nodes could ignore this problem if they simply accept the fact that
they'll only be able to fully relay the transaction once, when it is
broadcast, and won't be able to provide mempool functionality after the
relay. Modulo high-latency mixnets, this is probably acceptable; the author
previously argued that relay nodes don't need a mempool? at all.
For a miner though not having the data necessary to update the proofs as
are found means potentially losing out on transactions fees. So how much
data is necessary to make this a non-issue?
Since the TXO MMR is insertion ordered, spending a non-archived txout can
invalidate the upper nodes in of the archived txout's TXO MMR proof (if this
isn't clear, imagine a two-level scheme, with a per-block TXO MMRs,
by a master MMR for all blocks). The maximum number of relevant inner nodes
changed is log2(n) per block, so if there are n non-archival blocks between
most recent TXO commitment and the pending TXO MMR tip, we have to store
log2(n)*n inner nodes - on the order of a few dozen MB even when n is a
(seemingly ridiculously high) year worth of blocks.
Archived txout spends on the other hand can invalidate TXO MMR proofs at any
level - consider the case of two adjacent txouts being spent. To guarantee
success requires storing full proofs. However, they're limited by the
limit, and additionally are expected to be relatively uncommon. For
example, if
1% of 1MB blocks was archival spends, our hypothetical year long TXO
delay is only a few hundred MB of data with low-IO-performance requirements.
 Security Model
Of course, a TXO commitment delay of a year sounds ridiculous. Even the
imaginable computer isn't going to need more than a few blocks of TXO
commitment delay to keep up ~100% of the time, and there's no reason why we
can't have the UTXO archive delay be significantly longer than the TXO
commitment delay.
However, as with UTXO commitments, TXO commitments raise issues with
security model by allowing relatively miners to profitably mine transactions
without bothering to validate prior history. At the extreme, if there was no
commitment delay at all at the cost of a bit of some extra network bandwidth
"full" nodes could operate and even mine blocks completely statelessly by
expecting all transactions to include "proof" that their inputs are
unspent; a
TXO commitment proof for a commitment you haven't verified isn't a proof
that a
transaction output is unspent, it's a proof that some miners claimed the
was unspent.
At one extreme, we could simply implement TXO commitments in a "virtual"
fashion, without miners actually including the TXO commitment digest in
blocks at all. Full nodes would be forced to compute the commitment from
scratch, in the same way they are forced to compute the UTXO state, or total
work. Of course a full node operator who doesn't want to verify old history
get a copy of the TXO state from a trusted source - no different from how
could get a copy of the UTXO set from a trusted source.
A more pragmatic approach is to accept that people will do that anyway, and
instead assume that sufficiently old blocks are valid. But how old is
"sufficiently old"? First of all, if your full node implementation comes
the factory" with a reasonably up-to-date minimum accepted total-work
threshold? - in other words it won't accept a chain with less than that
of total work - it may be reasonable to assume any Sybil attacker with
sufficient hashing power to make a forked chain meeting that threshold with,
say, six months worth of blocks has enough hashing power to threaten the
chain as well.
That leaves public attempts to falsify TXO commitments, done out in the
open by
the majority of hashing power. In this circumstance the "assumed valid"
threshold determines how long the attack would have to go on before full
start accepting the invalid chain, or at least, newly installed/recently
full nodes. The minimum age that we can "assume valid" is tradeoff between
political/social/technical concerns; we probably want at least a few weeks
guarantee the defenders a chance to organise themselves.
With this in mind, a longer-than-technically-necessary TXO commitment delay?
may help ensure that full node software actually validates some minimum
of blocks out-of-the-box, without taking shortcuts. However this can be
achieved in a wide variety of ways, such as the author's prev-block-proof
proposal?, fraud proofs, or even a PoW with an inner loop dependent on
blockchain data. Like UTXO commitments, TXO commitments are also potentially
very useful in reducing the need for SPV wallet software to trust third
providing them with transaction data.
i) Checkpoints that reject any chain without a specific block are a more
   common, if uglier, way of achieving this protection.
j) A good homework problem is to figure out how the TXO commitment could be
   designed such that the delay could be reduced in a soft-fork.
 Further Work
While we've shown that TXO commitments certainly could be implemented
increasing peak IO bandwidth/block validation latency significantly with the
delayed commitment approach, we're far from being certain that they should
implemented this way (or at all).
1) Can a TXO commitment scheme be optimized sufficiently to be used directly
without a commitment delay? Obviously it'd be preferable to avoid all the
complexity entirely.
2) Is it possible to use a metric other than age, e.g. priority? While this
complicates the pruning logic, it could use the UTXO set space more
efficiently, especially if your goal is to prioritise bitcoin value-transfer
over other uses (though if "normal" wallets nearly never need to use TXO
commitments proofs to spend outputs, the infrastructure to actually do this
3) Should UTXO archiving be based on a fixed size UTXO set, rather than an
age/priority/etc. threshold?
4) By fixing the problem (or possibly just "fixing" the problem) are we
encouraging/legitimising blockchain use-cases other than BTC value transfer?
Should we?
5) Instead of TXO commitment proofs counting towards the blocksize limit,
we use a different miner fairness/decentralization metric/incentive? For
instance it might be reasonable for the TXO commitment proof size to be
discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.
thinblocks) is used to ensure all miners have received the proof in advance.
6) How does this interact with fraud proofs? Obviously furthering
dependency on
non-cryptographically-committed STXO/UTXO databases is incompatible with the
modularized validation approach to implementing fraud proofs.
# References
1) "Merkle Mountain Ranges",
   Peter Todd, OpenTimestamps, Mar 18 2013,
2) "Do we really need a mempool? (for relay nodes)",
   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,
3) "Segregated witnesses and validationless mining",
   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,
 'peter'[:-1]
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2016-10-14 13:01:13
@_author: Nick ODell 
@_subject: [bitcoin-dev] DPL is not only not enough, 
Pledging to not use patents offensively defeats the point of owning patents.
The point of owning a patent is so that you can use it offensively, either to
prevent competition, or get licensing fees.
Obtaining a patent for defense doesn't make sense. The litigants you need to
worry about do not produce or make anything. Their 'product' is patent lawsuits.
Unless you have a patent on using a mail-merge program to sue people, your
defensive patents are useless in that situation.
On Fri, Oct 14, 2016 at 4:57 AM, Peter Todd via bitcoin-dev

@_date: 2016-09-17 16:34:43
@_author: Nick ODell 
@_subject: [bitcoin-dev] Simple tx ID malleability fix, 
Then you have a new problem. Hash1 must contain Hash2 and the
transaction, but Hash2 must contain Hash1 and the transaction. A
circular dependency.
On Sat, Sep 17, 2016 at 3:14 PM, Rune K. Svendsen via bitcoin-dev

@_date: 2017-08-09 14:14:18
@_author: Nick ODell 
@_subject: [bitcoin-dev] Structure for Trustless Hybrid Bitcoin Wallets 
1) This is a good start for a BIP, but it's missing details. For example,
the nonce is encrypted by the server. What key is it encrypted with?
Clarifying ambiguities like this can sometimes reveal weaknesses that you
wouldn't otherwise think of.
2) What kind of recovery questions are asked? If it's something like "What
was the name of your first pet?" then what prevents the server from
stealing the wallet by trying a dictionary of the most common pet names? Is
there a mitigation to this, besides picking cryptographically secure
identifiers for my pets?
On Wed, Aug 9, 2017 at 12:49 PM, Colin Lacina via bitcoin-dev <

@_date: 2017-08-16 10:52:01
@_author: Nick ODell 
@_subject: [bitcoin-dev] Fwd: Proposal of a new BIP : annual splitting 
What makes this approach better than the prune option of Bitcoin?
On Wed, Aug 16, 2017 at 10:20 AM, ??????? ???????? via bitcoin-dev <

@_date: 2017-03-05 14:31:26
@_author: Nick ODell 
@_subject: [bitcoin-dev] Moving towards user activated soft fork activation 
the UASF has the higher coin price, the other chain will be annihilated. If
the UASF has a lower coin price, the user activated chain can still exist
(though their coins can be trivially stolen on the majority chain).
I don't think that's true. Say there are two forks of Blahcoin. Alice
thinks there's a 55% chance that Fork A will succeed. Bob thinks there's a
55% chance that Fork B will succeed. Alice trades all of her Fork B coins
for all of Bob's Fork A coins. Now, Bob and Alice both have a stake in one
fork or the other succeeding. Alice starts spending more time around Fork A
users; Bob starts spending his time with Fork B users.
A year passes, and Alice and Bob meet again. Bob tells Alice that Fork B
has been doing much better than Fork A, and is trading at ten times the
price. Alice replies that it doesn't matter, since Fork B will soon split
into B1 and B2. After all, if Fork B surrendered its principles once, it
can do so again. Bob replies that Fork B represents the true spirit of
Blahcoin. Alice replies that all of the people whose opinion she respects
believe that Fork B violates principles set down by the Founder (peace be
upon him.)
Bob disagrees, and cites an annotated collection of the Founder's writings,
which clearly show that if a situation like what provoked the Great Fork
happens, Fork B is in the right. All of the people Bob knows (except Alice)
agree that this shows Fork A is invalid. Alice replies that Bob is
committing the bandwagon fallacy; even if a thousand people believe that
red is green, that does not make it true. Also, the collection takes
several of the Founder's comments out of context. If one looks at comments
made prior to the release of Blahcoin, they lay out a framework that
envisions Fork A. Bob replies that Alice can't use statements made prior to
the release of Blahcoin to establish original intent; the system hadn't
been designed yet.
Bob points out that Fork B has a higher total chainwork. Alice scoffs. She
posits a Fork C, which is exactly like Fork A, except that chainwork is
defined to be the previous definition plus a quadrillion. Bob finds that
ridiculous. Fork C would transgress upon intrinsic principles of Blahcoin.
No more than Fork B does, Alice replies.
Each sentence above is true from some point of view. Each person sincerely
believes in the rightness of their position. Is there some objective
measure, that both Alice and Bob can agree on, that resolves this? I don't
think there is. Bob and Alice will sneer at each other for being idiots
forever. The schism will never resolve.
Satoshi Bless,
On Sun, Mar 5, 2017 at 11:10 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-12 22:59:10
@_author: Nick ODell 
@_subject: [bitcoin-dev] Flag day activation of segwit 
The problem with modifying Bitcoin to work around community norms is that
it's a two-way street. Other people can do it too.
Let me propose a counter-fork, or a "Double UASF." This is also a BIP9
fork, and it uses, say, bit 2. starttime is 1489449600, and the end time is
1506812400. It enforces every rule that UASF enforces, plus one additional
rule. If 60% of blocks in any retargeting period signal for Double UASF,
any descendant block that creates or spends a segregated witness output is
Double UASF signaling never coincides with UASF signaling, because the
signaling periods don't overlap. Full nodes happily validate the chain,
because Double UASF doesn't break any rules; it just adds new ones. Miners
who adopt Double UASF don't need to understand segwit, because all segwit
transactions are banned. Miners don't need to commit to a wtxid structure,
either. Per BIP 141, "If all transactions in a block do not have witness
data, the commitment is optional." Segwit is activated, but useless. Miners
who *do* adopt segwit will lose money, as their blocks are orphaned.
On Sun, Mar 12, 2017 at 9:50 AM, shaolinfry via bitcoin-dev <

@_date: 2017-03-13 16:18:15
@_author: Nick ODell 
@_subject: [bitcoin-dev] Flag day activation of segwit 
This has a different start time from the first post.
pindex->GetMedianTimePast() <= 1510704000 ...
On Mon, Mar 13, 2017 at 4:36 AM, shaolinfry via bitcoin-dev <

@_date: 2017-03-20 12:02:52
@_author: Nick ODell 
@_subject: [bitcoin-dev] Malice Reactive Proof of Work Additions (MR 
Chain work currently means the expected number of sha256d evaluations
needed to build a chain. Given that these hash functions are not equally
hard, what should the new definition of chain work be?
On Mon, Mar 20, 2017 at 9:38 AM, Andrew Johnson via bitcoin-dev <

@_date: 2017-03-24 11:29:54
@_author: Nick ODell 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
Two concerns:
1) This makes block validity depend on things that aren't in the
blockchain. If you and I have different minrelaytxfee's, we will have
different mempool sizes. Your node will discard a block, but my node will
think it is valid, and our nodes will not come to consensus.
2) This is trivially bypassed. A miner can take some coins they own
already, and create a zero-value transaction that has a scriptPubKey of
OP_1. (i.e. anyone-can-spend.) Then, they create another transaction
spending the first transaction, with an empty scriptSig, and the same
scriptPubKey. They do this over and over until they fill up the block.
The last OP_1 output can be left for the next miner. Since the above
algorithm is deterministic, a merkle tree containing every transaction
except the coinbase can be precomputed. The 'malicious' miners do not need
to store this fake block.
On Fri, Mar 24, 2017 at 10:03 AM, CANNON via bitcoin-dev <
