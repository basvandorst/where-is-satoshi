
@_date: 2015-08-01 16:23:23
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP draft: Hardfork bit 
Although the chance is very slim, it is possible to have multiple hardforks sharing the same flag block. For example, different proposals may decide the flag time based on voting result and 2 proposals may have the same flag time just by chance. The coinbase message is to preclude any potential ambiguity. It also provides additional info to warning system of non-upgrading nodes.
If we are pretty sure that there won't be other hardfork proposal at the same time, the coinbase message may not be necessary. With some prior collaboration, this may also be avoided (e.g. no sharing flag block allowed as consensus rules of the hardforks)
The "version 0" idea is not compatible with the version bits voting system, so I have this hardfork bit BIP after thinking more carefully. Otherwise they are technically similar.
Michael Ruddy ? 2015-08-01 09:05 ??:

@_date: 2015-08-02 03:16:54
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
Pieter Wuille ? 2015-08-01 16:45 ??:
Since I'm using "30 days after 75% miner support", the actual deployment period will be longer than 30 days. Anyway, if all major exchanges and merchants agree to upgrade, people are forced to upgrade immediately or they will follow a worthless chain.
Since the block reward is miners' major income source, no rational miner would create mega blocks unless the fee could cover the extra orphaning risk. Blocks were not constantly full until recent months, and many miners are still keeping the 750kB soft limit. This strongly suggests that we won't have 4MB blocks now even Satoshi set a 8MB limit.
I don't have the data now but I believe the Satoshi Dice model failed not primarily due to the 1MB cap, but the raise in BTC/USD rate. Since minting reward is a fixed value in BTC, the tx fee must also be valued in BTC as it is primarily for compensating the extra orphaning risk. As the BTC/USD rate increases, the tx fee measured in USD would also increase, making micro-payment (measured in USD) unsustainable.
We might have less full nodes, but it was Satoshi's original plan: "At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware. A server farm would only need to have one node on the network and the rest of the LAN connects with that one node." Theoretically, we only require one honest full node to prove wrongdoing on the blockchain and tell every SPV nodes to blacklist the invalid chain.
I think SPV mining exists long before the 1MB block became full, and I don't think we could stop this trend by artificially suppressing the block size. Miners should just do it properly, e.g. stop mining until the grandparent block is verified, which would make sure an invalid fork won't grow beyond 2 blocks.
If we could have a longer initial ramp up period, we may adopt a slower long term parameter. I think we should at least restore the original 32MB limit in a reasonable time frame, say 6-8 years, instead of 20 years in your proposal. If you believe Bitcoin should become a global settlement network, 32MB would be the very minimum as that is only 75% of current SWIFT traffic.

@_date: 2015-08-03 04:54:27
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP draft: Hardfork bit 
I have put it on the github: I removed the specification of coinbase message to make it simpler. Instead, it requires that a flag block must not be shared by multiple hardfork proposals.
I'm not sure whether it is a Standard, Informational, or Process BIP
I'm also thinking whether we should call it "hardfork bit", "hardfork flag", or with other name.
Michael Ruddy ? 2015-08-02 06:53 ??:

@_date: 2015-08-04 03:50:43
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
As now we have some concrete proposals ( I think we should wrap up the endless debate with voting by different stakeholder groups.
Voting system
Single transferable vote is applied. ( Voters are required to rank their preference with ?1?, ?2?, ?3?, etc, or use ?N? to indicate rejection of a candidate.
Vote counting starts with every voter?s first choice. The candidate with fewest votes is eliminated and those votes are transferred according to their second choice. This process repeats until only one candidate is left, which is the most popular candidate. The result is presented as the approval rate: final votes for the most popular candidate / all valid votes
After the most popular candidate is determined, the whole counting process is repeated by eliminating this candidate, which will find the approval rate for the second most popular candidate. The process repeats until all proposals are ranked with the approval rate calculated.
Technical issues:
Voting by the miners, developers, exchanges, and merchants are probably the easiest. We need a trusted person to verify the voters? identity by email, website, or digital signature. The trusted person will collect votes and publish the named votes so anyone could verify the results.
For full nodes, we need a trusted person to setup a website as an interface to vote. The votes with IP address will be published.
For bitcoin holders, the workload could be very high and we may need some automatic system to collect and count the votes. If people are worrying about reduced security due to exposed raw public key, they should move their bitcoin to a new address before voting.
Double voting: people are generally not allowed to change their mind after voting, especially for anonymous voters like bitcoin holders and solo miners. A double voting attempt from these classes will invalidate all related votes.
Multiple identity: People may have multiple roles in the Bitcoin ecology. I believe they should be allowed to vote in all applicable categories since they are contributing more than other people.

@_date: 2015-08-04 05:22:46
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
What is your definition of "consensus"? Do you mean 100% agreement? Without a vote how do you know there is 100% (or whatever percentage) Who are the "everyone"?
Pieter Wuille ? 2015-08-04 05:03 ??:

@_date: 2015-08-04 05:44:54
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
As I mentioned, the candidate proposals must go through usual peer review process, which includes proper testing, I assume.
Scaling down is always possible with softforks, or miners will simply produce smaller blocks. BIP100 has a scaling down mechanism but it still requires miners to vote so it doesn't really make much difference
But anyway, this is off-topic, as candidate proposals may include mechanism for scaling down.
Venzen Khaosan ? 2015-08-04 05:23 ??:

@_date: 2015-08-07 01:20:21
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Block size implementation using Game Theory 
It won't work as you thought. If a miner has 95% of hashing power, he would have 95% of chance to find the next block and collect the penalty. In long term, he only needs to pay 5% penalty. It's clearly biased against small miners.
Instead, you should require the miners to burn the penalty. Whether this is a good idea is another issue.
Wes Green via bitcoin-dev ? 2015-08-06 19:52 ??:

@_date: 2015-08-07 03:14:49
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Idea: Efficient bitcoin block propagation 
Your proposal fails here:
"If the block defined in the Guarantee Message has not been shown"
What is blockchain? You can see blockchain as a mechanism to prove something has been shown by certain order. Therefore, it is not possible to prove something has not been shown with blockchain.
Your proposal works only with a centralized trusted party.
Arnoud Kouwenhoven - Pukaki Corp via bitcoin-dev ? 2015-08-05 15:07 ??:

@_date: 2015-08-07 14:17:32
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Pieter Wuille via bitcoin-dev ? 2015-08-07 12:28 ??:
What if we reduce the block size to 0.125MB? That will allow 0.375tx/s. If 3->24 sounds "almost the same", 3->0.375 also sounds almost the same. We will have 50000 full nodes, instead of 5000, since it is so affordable to run a full node.
If 0.125MB sounds too extreme, what about 0.5/0.7/0.9MB? Are we going to have more full nodes?
No, I'm not trolling. I really want someone to tell me why we should/shouldn't reduce the block size. Are we going to have more or less full nodes if we reduce the block size?

@_date: 2015-08-08 14:51:17
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] The use of tx version field in BIP62 and 68 
BIP68 rules and some of the BIP62 rules are applied only if the tx version is >=2 and >=3 respectively. Therefore, it is not possible to create a tx which follows BIP62 but not BIP68. If we introduce v4 tx later, BIP62 and BIP68 will all become mandatory.
Some rules, e.g. "scriptPubKey evaluation will be required to result in a single non-zero value" in BIP62, will cause trouble when we try to introduce a new script system with softfork.
I suggest to divide the tx version field into 2 parts: the higher 4 bits and lower 28 bits.
BIP62 is active for a tx if its highest bits are 0000, and the second lowest bit is 1.
BIP68 is active for a tx if its highest bits are 0000, and the third lowest bit is 1.
So it will be easier for us to re-purpose the nSequence, or to take advantage of malleability in the future. If this is adopted, the nSequence high bit requirement in BIP68 becomes unnecessary as we could easily switch it off.
The low bits will allow 28 independent BIPs and should be ample for many years. When they are exhausted, we can switch the high bits to a different number (1-15) and redefine the meaning of low bits. By that time, some of the 28 BIPs might have become obsoleted or could be (I'm not sure if there are other draft BIPs with similar interpretation of tx version but the comments above should also apply to them)

@_date: 2015-08-08 15:18:54
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] The use of tx version field in BIP62 and 68 
I think I have explained my motivation but let me try to make it For example, BIP62 says "scriptPubKey evaluation will be required to result in a single non-zero value". If we had BIP62 before BIP16, P2SH could not be done in the current form because BIP16 leaves more than one element on the stake for non-upgrading nodes. BIP17 also violates BIP62.
BIP68 is "only enforced if the most significant bit of the sequence number field is set.", so it is optional, anyway. All I do is to move the flag from sequence number to version number.
The blocksize debate shows how a permanent softfork may cause trouble later. We need to be very careful when doing further softforks, making sure we will have enough flexibility for further development.
Mark Friedenbach ? 2015-08-08 14:56 ??:

@_date: 2015-08-15 15:10:26
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Sign with the key 5EC948A1 or shut up, you scammer
Satoshi Nakamoto via bitcoin-dev ? 2015-08-15 13:43 ??:

@_date: 2015-08-16 23:10:48
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
Thanks to mining centralization, such attempts won't be successful. Asking mining pools to mine spoofing blocks in their real name is even harder than asking them to run the real BitcoinXT
Node count is always manipulable, there is nothing new. People running this will only be interpreted as XT-supporters.
Julie via bitcoin-dev ? 2015-08-16 18:34 ??:

@_date: 2015-08-17 05:15:15
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Miners are struggling with blocks far smaller 
The traffic between the pool server and individual hashers is far busier than 50kB/30s. If their bandwidth is so limited, hashers would have switched to other pools already.
All these data may prove is they have very bad mining codes. For example, their hashers may not be required to update the transaction list regularly. I don't think they are struggling. They are just too lazy or think that's too risky to improve their code. After all, they are generating half million USD per day and a few seconds of downtime would hurt.
By the way, vast majority of the full blocks (>0.99MB) on the blockchain are generated by Chinese pools.
Luv Khemani via bitcoin-dev ? 2015-08-17 04:42 ??:

@_date: 2015-08-18 05:54:56
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] =?utf-8?q?Bitcoin_is_an_experiment=2E_Why_don=27t_w?= 
As I understand, there is already a consensus among core dev that block size should/could be raised. The remaining questions are how, when, how much, and how fast. These are the questions for the coming Bitcoin Scalability Workshops but immediate consensus in these issues are not Could we just stop the debate for a moment, and agree to a scheduled experimental hardfork?
Objectives (by order of importance):
1. The most important objective is to show the world that reaching consensus for a Bitcoin hardfork is possible. If we could have a successful one, we would have more in the future
2. With a slight increase in block size, to collect data for future 3. To slightly relieve the pressure of full block, without minimal adverse effects on network performance
With the objectives 1 and 2 in mind, this is to NOT intended to be a kick-the-can-down-the-road solution. The third objective is more like a side effect of this experiment.
Proposal (parameters in ** are my recommendations but negotiable):
1. Today, we all agree that some kind of block size hardfork will happen on t1=*1 June 2016*
2. If no other consensus could be reached before t2=*1 Feb 2016*, we will adopt the backup plan
3. The backup plan is: t3=*30 days* after m=*80%* of miner approval, but not before t1=*1 June 2016*, the block size is increased to s=*1.5MB*
4. If the backup plan is adopted, we all agree that a better solution should be found before t4=*31 Dec 2017*.
t1 = 1 June 2016 is chosen to make sure everyone have enough time to prepare for a hardfork. Although we do not know what actually will happen but we know something must happen around that moment.
t2 = 1 Feb 2016 is chosen to allow 5 more months of negotiations (and 2 months after the workshops). If it is successful, we don't need to activate the backup plan
t3 = 30 days is chosen to make sure every full nodes have enough time to upgrade after the actual hardfork date is confirmed
t4 = 31 Dec 2017 is chosen, with 1.5 year of data and further debate, hopefully we would find a better solution. It is important to acknowledge that the backup plan is not a final solution
m = 80%: We don't want a very small portion of miners to have the power to veto a hardfork, while it is important to make sure the new fork is secured by enough mining power. 80% is just a compromise.
s = 1.5MB. As the 1MB cap was set 5 years ago, there is no doubt that all types of technology has since improved by >50%. I don't mind making it a bit smaller but in that case not much valuable data could be gathered and the second objective of this experiment may not be If the community as a whole could agree with this experimental hardfork, we could announce the plan on bitcoin.org and start coding of the patch immediately. At the same time, exploration for a better solution continues. If no further consensus could be reached, a new version of Bitcoin Core with the patch will be released on or before 1 Feb 2016 and everyone will be asked to upgrade immediately.

@_date: 2015-08-19 06:34:38
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] 
=?utf-8?q?e_have_an_experimental_hardfork=3F?=
Jorge Tim?n ? 2015-08-19 05:24 ??:
You misunderstand my intention. The experiment is not about a random hardfork. It's about a block size increase hardfork. The data will help us to design further hardfork on block size.
To make it less controversial, the size must not be too big.
To allow a meaningful experiment, the size must not be too small. Technically we could make it 1.01MB but that defeats all objectives I listed and there is no point to do it.
That's why I suggest 1.5MB.
I hope the fork could be done before the halving, which (hopefully) we may have a new bitcoin rush
There was only 2 months for the BIP50 hardfork. You may argue that's a "bug fix" but practically there is no difference: people not fixing the bug in 2 months was forked off. Four months of grace period (Feb to June 2016) is already a double of that.
Also, if we could have zero grace period for softfork, why must we have a ultra-long period for hardfork? (Unless you also agree to have an 1-year grace period for softfork. I don't buy the "softfork is safer than hardfork" theory. The recent BIP66 fork has clearly shown why it is wrong: non-upgrading full nodes are not full nodes)
The problem is many people won't update until they must do so. So 4 months or 1 year make little difference

@_date: 2015-08-19 11:22:43
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] 
=?utf-8?q?e_have_an_experimental_hardfork=3F?=
odinn via bitcoin-dev ? 2015-08-19 07:25 ??:
No, BitcoinXT won't become a Schism hardfork, or may be just for a few days, at most.
There is one, and only one scenario that BitcoinXT will win: it is supported by major exchanges, merchants, and investors, and they request miners to support it. When BIP101 is activated, these exchanges will refuse to accept or exchange tokens from the old chain. Miners in the old chain can't sell their newly generated coins and can't pay the electricity bill. They will soon realize that they are mining fool's gold and will be forced to switch to the new chain or sell their ASIC. The old chain will be abandoned and has no hope to revive without a hardfork to decrease the difficulty. The dust will settle in days if not Will the adoption of BitcoinXT lead by miners? No, it won't. Actually, Chinese miners who control 60% of the network has already said that they would not adopt XT. So they must not be the leader in this revolution. Again, miners need to make sure they could sell their bitcoin in a good price, and that's not possible without support of exchanges and What about that Not-Bitcoin-XT? The creator of the spoof client may stay anonymous, but the miners cannot. 95% of the blocks come from known entities and they have to be responsible to their actions. And again, they have real money in stake. If bitcoin is destroyed, their ASIC serves at best as very inefficient heaters.
So Bitcoin-XT is basically in a win-all-or-lose-all position. It all relies on one condition: the support of major exchanges, merchants, and investors. Their consensus is what really matters. With their consensus, that could not be a Schism hardfork. Without their consensus, nothing will happen.
Or let me analyse in a different angle. BitcoinXT is in no way similar to your examples of Schism hardforks. All of your examples, "ASIC-reset hardfork", "Anti-Block-creator hardfork", and "Anti-cabal hardfork", are hostile to the current biggest miners and will destroy their investment. These miners have no choice but stick to the original protocol so 2 chains MUST coexist. However, BIP101 has no such effect at all and miners may freely switch between the forks. They will always choose the most valuable fork, so only one fork will survive.

@_date: 2015-08-20 13:32:02
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
Peter Todd via bitcoin-dev ? 2015-08-19 01:50 ??:
If you are going to mask bits, would you consider to mask all bits except the 4th bit? So other fork proposals may use other bits for voting concurrently.
And as I understand, the masking is applied only during the voting stage? After the softfork is fully enforced with 95% support, the nVersion will be simply >=8, without any masking?

@_date: 2015-08-23 14:08:41
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] =?utf-8?q?Encouraging_mining_of_the_first_few_big_b?= 
Someone is going to burn 150BTC to create a backlog of 30-day in September.  However, the money could be spent more wisely by encouraging mining of the first few big blocks
1. OP_CSV and BIP68 are enabled
2. Max tx size remains 1MB
The donor will create a transaction, with an input of 150BTC, and 10 1. 0 BTC to "OP_RETURN "
2. 42 BTC to "OP_1 OP_CSV"
3. 21 BTC to "OP_2 OP_CSV"
4. 10.5 BTC to "OP_3 OP_CSV"
5. 5.25 BTC to "OP_4 OP_CSV"
6. 2.625 BTC to "OP_5 OP_CSV"
7. 1.3125 BTC to "OP_6 OP_CSV"
8. 0.65625 BTC to "OP_7 OP_CSV"
9. 0.328125 BTC to "OP_8 OP_CSV"
10. 0.328125 BTC to "OP_9 OP_CSV"
The first output will fill up the size to 1MB.
This tx could not be confirmed by a pre-hardfork miner because the coinbase tx will consume some block space. The first big block miner will be able to collect 66BTC of fee. The block confirming the first big block will collect 42BTC of fee, etc. This will create a long enough chain to bootstrap the hardfork.
The amount is chosen to make sure the difference is <25BTC, so miners would have less incentive to create a fork instead of confirming other's block. However, a miner cartel may launch a 51% attack to collect all money. Such incentive may be reduced by adjusting the distribution of donation. (Actually, such cartel may be formed anytime, just for collecting more block reward)

@_date: 2015-08-23 22:40:56
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Gregory Maxwell via bitcoin-dev ? 2015-08-23 21:01 ??:
I think this comment is more related to BIP68 instead of OP_CSV? Without further complicating the BIP68, I believe the best way to leave room for improvement is to spend a bit in tx nVersion to indicate the activation of BIP68. I have raised this issue before with  However, it seems Mark isn't in favor of my proposal
The idea is not to permanently change the meaning of nSequence. Actually, BIP68 is "only enforced if the most significant bit of the sequence number field is set." So BIP68 is optional, anyway. All I suggest is to move the flag from nSequence to nVersion. However, this will leave much bigger room for using nSequence for other purpose in the AFAIK, nSequence is the only user definable and signed element in TxIn. There could be more interesting use of this field and we should not change its meaning permanently. (e.g. if nSequence had 8 bytes instead of 4 bytes, it could be used to indicate the value of the input to fix this problem:  )

@_date: 2015-08-24 03:00:37
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Your proposal also permanently burns a sequence bit. It depends on how we value a nSequence bit and a nVersion bit. I think there is a trade-off here:
1. nSequence is signed by each TxIn individually, while all TxIns must share the same nVersion
2. If nVersion is used to indicate the meaning of nSequence (as I It saves a nSequence bit and allows more space for redefining the It burns a nVersion bit.
All TxIns in a tx must share the same meaning for their nSequence
3. If nSequence is used to indicate the meaning of itself (as you It saves a nVersion bit
Different TxIn may have different meaning with their nSequence
It burns a nSequence bit, thus less space for extension
I don't think there is a perfect choice. However, I still prefer my proposal because:
1. nSequence is signed by each TxIn individually and could be more interesting than nVersion.
2. If nVersion is expected to be a monotonic number, 2 bytes = 65536 versions is enough for 65 millenniums if it ticks once per year. 4 bytes is an overkill. Why don't we spend a bit if there is a good reason? Most softforks (e.g. OP_CLTV, OP_CSV, BIP66) are not optional. These kind of optional new functions would not be common and should never use up the version bits. (or, could you suggest a better use of the tx version Mark Friedenbach ? 2015-08-23 22:54 ??:

@_date: 2015-08-27 08:14:47
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIPS proposal for implementing AML-KYC in bitcoin 
Very good, I can't wait to see it. Please code it up and submit a pull request to github. Don't expect someone will do it for you.
prabhat via bitcoin-dev ? 2015-08-27 08:06 ??:

@_date: 2015-08-27 08:29:06
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Rusty Russell ? 2015-08-26 23:08 ??:
This is not needed because BIP68 is not active for version 1 tx. No existing wallet would be affected.
Do you mean "have nLockTime apply even if nSequence = 0xFFFFFFFF"? This is a softfork. Should we do this together with BIP65, BIP68 and BIP112?

@_date: 2015-08-27 23:02:47
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Questiosn about BIP100 
Mode could be ruled out immediately. Just consider this: 34% 8MB, 33% 1.5MB, 33% 1.2MB
I personally believe the median is the most natural and logical choice. 51% of miners can always force the 49% to follow the simple majority choice through a 51% attack. Using median will eliminate the incentive to 51% attack due to this reason. The incentive to 51% attack will exist when you use any value other than 50-percentile. The further it is from 50, the bigger the incentive.
Having said that, I don't think it is an absolutely bad idea to use a value other than 50-percentile. The exact value is debatable.
However, if you use something other than median, you should make it symmetrical. For example, the block size will increase if the 20-percentile is bigger than the current limit, and the block size will decrease if the 80-percentile is smaller than the current limit.
Jeff Garzik via bitcoin-dev ? 2015-08-27 16:49 ??:

@_date: 2015-08-28 11:27:23
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP: Using Median time-past as endpoint for 
I have an ugly solution to this problem, with minimal change to the current softfork logic, and still allows all features described in sipa's Version bits BIP
1. xVersion = nVersion AND 0b10000000000000000000000000011000
2. miners supporting BIP65 will set xVersion = 8 or greater
3. If 750 of the last 1000 blocks are xVersion >= 8, reject invalid xVersion 8 (or greater) blocks
4. If 950 of the last 1000 blocks are xVersion >= 8, reject all blocks with xVersion < 8
So the logic is exactly the same as BIP66, with the AND masking in step 1. After the BIP65 softfork is implied, xVersion may take only one of the following values: 8, 16, 24
This is basically moving the "high bits" in sipa's proposal to the middle of the nVersion field. After the BIP65 softfork, this will still leave 29 available bits for parallel soft forks, just in different This is ugly, but I believe this is the easiest solution
Ref: Peter Todd via bitcoin-dev ? 2015-08-27 19:19 ??:

@_date: 2015-08-29 05:38:17
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
We need some game theory experts to analyse this but I see there are 3 major problems:
1. Tragedy of the commons: Some miners have to scarify their income to increase the block size, and all miners will share the beneficial outcome of the increase. All miners would like to be free riders.
2. Promote the formation of mining cartel: miners have to make sure that their vote for increase is supported by at least 50% of miners, or they will lose income for nothing. A miner also needs to make sure that he is not voting "excessively" (in terms of size and vote count), as the excessive part will never be counted due to the use of median. So basically you will either have exactly 1009 miners voting for the same size increase in a cycle, or have no vote for increase at all. That will require a lot of offline negotiations. Such cartel may work ONLY if mining is highly centralized, and miners trust each other. Also, there is no mechanism to punish those who betray the cartel.
3. Imbalance of power: it is costly to increase the size, while totally free to decrease the size
Btc Drak via bitcoin-dev ? 2015-08-28 16:28 ??:

@_date: 2015-08-29 15:03:57
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
I am quite skeptical about any pay-to-increase proposal because it is difficult to predict the game dynamics and determine the right amount of penalty. But anyway, here is my response to your revised proposal:
1. I agree with you that there should be a cap in the rate of change, and also the maximum possible size. This is already part of BIP100
2. Requiring a higher difficulty is bad for everyone:
  a) it increases the variance of the miner;
  b) average confirmation time for all tx are increased. It may even cause a feedback: many tx in mempool -> increase block size -> wait longer for confirmation -> more tx in mempool;
  c) difficulty of the next round will be decreased, leading to a greater fluctuation in confirmation time.
Instead, you should require miners to burn their coinbase reward. This is effectively same as higher difficulty but is good for everyone: a) mining variance and confirmation time unchanged; b) all bitcoin holders become relatively richer
If you don't want to burn any bitcoin, you may require the miner the send to penalty to <840000 + current height> OP_CHECKLOCKTIMEVERIFY, which will subsidize the mining when the block reward drops below 1 BTC
3. It is a better idea to allow mining of a bigger block immediately, which reduces (but not eliminates) the problem of tragedy of the commons. However, you can't use the blocksize as the vote. Mining an empty block doesn't mean the miner wants to decrease the block size to 200 bytes. That will just encourage some miners to fill up a block with garbage which does no good for anyone. Therefore, you need to look at both the actual block size and the coinbase vote, and always take the bigger value to determine the penalty and the max block size of next round. If a miner includes nothing in the coinbase, it should be consider as a vote for the current max block size.
Btc Drak via bitcoin-dev ? 2015-08-29 06:15 ??:

@_date: 2015-08-30 11:56:59
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] [META] Mailing list etiquette Re: BIPS proposal for 
Sorry to be off-topic but SNR of the mailing list is really getting Stop trolling and feeding the trolls.
Before you click "send", remember that your message will be sent to the inbox of hundreds or thousands of people.
Ref: Vinyas via bitcoin-dev ? 2015-08-30 11:16 ??:

@_date: 2015-08-30 13:13:54
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
Jorge Tim?n via bitcoin-dev ? 2015-08-29 16:41 ??:
This is based on the assumption that miners would always like to use up the last byte of the available block size. However, this is just not 1. The 6 year blockchain history has shown that most miners have a soft cap with their block size.
2. Chinese miners, controlling 60% of the network, rejected Gavin's initial 20MB proposal and asked for 8MB: 3. BTCChina supports BIP100 and will vote for 2MB at the beginning, with 8MB as a mid-term goal: BTCChina is controlling 12% of the network in the past month. If BIP100 uses the 20-percentile vote as the block size, it takes only 8% more vote to keep the size at 2MB
For many reasons miners may want to have a smaller block size, which we don't need to list them here. Although they can limit it by a softfork or even 51% attack, it is a very violent process. Why don't we just allow them to vote for a lower limit?
So I think the right way is to choose a mining-centralization-safe limit, and let it free float within a range based on miner's vote. If we are lucky enough to have some responsible miners, they will keep it as low as possible, until the legitimate tx volume catches up. Even in the worst case, the block size is still mining-centralization-safe. The upper limit may increase linearly, if not exponentially, until we find a better long-term solution. (sort of a combination of BIP100 and 101, with different parameters)
For the matter of "urgency", I agree with you that there is no actual urgency AT THIS MOMENT. However, if a hardfork may take 5 years to deploy (as you suggested), we really have the urgency to make a decision now. Actually, the main point is not urgency but uncertainty. We have debated for 5 years. Why won't we have 5 more years of debate, plus 5 years of deployment delay? Are we sticking to 1MB for 10 years? In that case Bitcoin Core must be abandoned by the economic majority and a Schism fork must occur.

@_date: 2015-08-31 12:24:21
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Short review of previously-proposed exotic 
Bryan Bishop via bitcoin-dev ? 2015-08-30 14:56 ??:
Thanks for your summary. This one seems particularly interesting. However, it does not allow fine adjustment for each input and output separately, so I wonder if it really "fully enable any seen or unforseen use case of the CTransactionSignatureSerializer." as it claims.

@_date: 2015-08-31 14:50:25
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
Jorge Tim?n ? 2015-08-30 14:56 ??:
The reason for 60% of block were generated in China is same as the reason for 60% of your clothes were made in China. The electricity there is the cheapest on the planet. Many dams were built in the past 10 years and now they have huge amount of surplus electricity due to economic Not sure if you are aware of this thread:  . Could you imagine this in any developed country? As long as mining is largely dependent on energy, there is no hope to break the balance/imbalance.
Bandwidth is probably only a few percent of miners' cost. There is no evidence that the current level of centralization is a result of block size. Instead, clear evidence has shown that centralization is a result of pool mining*, invention of ASIC, and disparity of energy cost. (* People started pool mining in 2010 because they wanted lower variance, not because of the inability to run a full node)
Even if we could quantify the level of centralization, it is a continuum and we must compromise between utility and centralization. Unless BIP101/103 is adopted, adjusting the hard cap always require a hardfork. For obvious technical and political reasons we can't have hardfork too frequently. Therefore, we need to leave some leeway: the hard cap may be a bit too high for today, but we are sure that technology will catch up in the near future.
Assuming we have plenty amount of "benevolent" miners, they will keep the block size low unless there is a real demand for larger block space. This is different from setting an individual soft limit, as that will lead to block size scarcity and therefore higher tx fee, which may be good for all miners. And as we say "miners can always decrease the block size with softfork or 51% attack", BIP100 materializes this possibility in a much smoother way.
I say "lucky" because I wholeheartedly believe it is good to keep the block as small as we really need. We can't do this by an equation so I would prefer to leave the power to miners (and they always have this power, anyway).
Jeff and Satoshi discussed this in 2010, although the flame throwing debate did not start until 2013.
The problem is the definition of "urgency" itself is controversial. And I believe an urgent hardfork should only be done as a bug fix, not implementation of a new feature. Block size increase should be a planned feature, as we don't want the tx fee raised to 10USD, before suddenly dropping to 0.01USD with the hardfork.
I don't think it's urgent today because my free tx always get mined. I don't know what is urgent and different people have different definition, but in general I think that should be measured by tx fee in USD. 0.001USD/byte may be intolerable for many people. (It's about $0.0001/byte now, and $0.0005/byte when it was $1200/BTC). It's not difficult to reach this level given the halving and potential bull market is coming.
As I argued above, it's already too late when things become really urgent. That will lead to serious market disruption, and the uncertainty could be very harmful to the development of the bitcoin economy.
As I said above, strictly limiting the block size may have little effect on mining centralization (because block size at this level is not a determining factor), while it may seriously suppress the utility.
I'm all for mining decentralization but block size is just not the right way to improve the situation.
I agree with you, but the balance between centralization and utility is also important. (and I believe the difference between 1MB and 8MB is tolerable, at least I must keep my full node running at this level)
I also have an idea to have a "decentralizedcoin", with very small blocks and everyone could mine with a CPU. That would be interesting if it is backed by famous devs in this area and is not

@_date: 2015-12-09 09:30:23
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Impacts of Segregated Witness softfork 
Although the plan is to implement SW with softfork, I think many important (but non-consensus critical) components of the network would be broken and many things have to be redefined.
1. Definition of "Transaction ID". Currently, "Transaction ID" is simply a hash of a tx. With SW, we may need to deal with 2 or 3 IDs for each tx. Firstly we have the "backward-compatible txid" (bctxid), which has exactly the same meaning of the original txid. We also have a "witness ID" (wid), which is the hash of the witness. And finally we may need a "global txid" (gtxid), which is a hash of bctxid|wid. A gtxid is needed mainly for the relay of txs between full nodes. bctxid and wid are consensus critical while gtxid is for relay network only.
2. IBLT / Bitcoin relay network: As the "backward-compatible txid" defines only part of a tx, any relay protocols between full nodes have to use the "global txid" to identify a tx. Malleability attack targeting relay network is still possible as the witness is malleable.
3. getblocktemplete has to be upgraded to deal with witness data and witness IDs. (Stratum seems to be not affected? I'm not sure)
4. Protocols relying on the coinbase tx (e.g. P2Pool, merged mining): depends on the location of witness commitment, these protocols may be Feel free to correct me and add more to the list.

@_date: 2015-12-10 01:47:35
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
It seems the current consensus is to implement Segregated Witness. SW opens many new possibilities but we need a balance between new features and deployment time frame. I'm listing by my priority:
1-2 are about scalability and have highest priority
1. Witness size limit: with SW we should allow a bigger overall block size. It seems 2MB is considered to be safe for many people. However, the exact size and growth of block size should be determined based on testing and reasonable projection.
2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented. This is not only a technical issue but also a response to the community which has been waiting for a scaling solution for years
3-6 promote safety and reduce level of trust (higher priority)
3. SIGHASH_WITHINPUTVALUE [1]: there are many SIGHASH proposals but this one has the highest priority as it makes offline signing much easier.
4. Sum of fee, sigopcount, size etc as part of the witness hash tree: for compact proof of violations in these parameters. I prefer to have this feature in SWv1. Otherwise, that would become an ugly softfork in SWv2 as we need to maintain one more hash tree
5. Height and position of an input as part of witness will allow compact proof of non-existing UTXO. We need this eventually. If it is not done in SWv1, we could softfork it nicely in SWv2. I prefer this earlier as this is the last puzzle for compact fraud proof.
6. BIP62 and OP_IF malleability fix [2] as standardness rules: involuntary malleability may still be a problem in the relay network and may make the relay less efficient (need more research)
7-15 are new features and long-term goals (lower priority)
7. Enable OP_CAT etc:
OP_CAT will allow tree signatures described by [3]. Even without Schnorr signature, m-of-n multisig will become more efficient if m < n.
OP_SUBSTR/OP_LEFT/OP_RIGHT will allow people to shorten a payment address, while sacrificing security.
I'm not sure how those disabled bitwise logic codes could be useful
Multiplication and division may still considered to be risky and not very useful?
8. Schnorr signature: for very efficient multisig [3] but could be introduced later.
9. Per-input lock-time and relative lock-time: define lock-time and relative lock-time in witness, and signed by user. BIP68 is not a very ideal solution due to limited lock time length and resolution
10. OP_PUSHLOCKTIME and OP_PUSHRELATIVELOCKTIME: push the lock-time and relative lock-time to stack. Will allow more flexibility than OP_CLTV and OP_CSV
11. OP_RETURNTURE which allows softfork of any new OP codes [4]. It is not really necessary with the version byte design but with OP_RETURNTURE we don't need to pump the version byte too frequently.
12. OP_EVAL (BIP12), which enables Merkleized Abstract Syntax Trees (MAST) with OP_CAT [5]. This will also obsolete BIP16. Further restrictions should be made to make it safe [6]:
a) We may allow at most one OP_EVAL in the scriptPubKey
b) Not allow any OP_EVAL in the serialized script, nor anywhere else in the witness (therefore not Turing-complete)
c) In order to maintain the ability to statically analyze scripts, the serialized script must be the last push of the witness (or script fails), and OP_EVAL must be the last OP code in the scriptPubKey
13. Combo OP codes for more compact scripts, for example:
OP_MERKLEHASH160, if executed, is equivalent to OP_SWAP OP_IF OP_SWAP OP_ENDIF OP_CAT OP_HASH160 [3]. Allowing more compact tree-signature and MAST scripts.
OP_DUPTOALTSTACK, OP_DUPFROMALTSTACK: copy to / from alt stack without removing the item
14. UTXO commitment: good but not in near future
15. Starting as a softfork, moving to a hardfork? SW Softfork is a quick but dirty solution. I believe a hardfork is unavoidable in the future, as the 1MB limit has to be increased someday. If we could plan it ahead, we could have a much cleaner SW hardfork in the future, with codes pre-announced for 2 years.
[1] [2] [3] [4] [5] [6]

@_date: 2015-12-12 15:09:03
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
It is a common practice in commercial banks that a dormant account might be confiscated. Confiscating or deleting dormant UTXOs might be too controversial, but allowing the UTXOs set growing without any limit might not be a sustainable option. People lose their private keys. People do stupid things like sending bitcoin to 1BitcoinEater. We shouldn?t be obliged to store everything permanently. This is my Dormant UTXOs are those UTXOs with 420000 confirmations. In every block X after 420000, it will commit to a hash for all UTXOs generated in block X-420000. The UTXOs are first serialized into the form: txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated. After some confirmations, nodes may safely delete the UTXO records of block X permanently.
If a user is trying to redeem a dormant UTXO, in addition the signature, they have to provide the scriptPubKey, height (X), and UTXO value as part of the witness. They also need to provide the Merkle path to the dormant UTXO commitment.
To confirm this tx, the miner will calculate a new Merkle hash for the block X, with the hash of the spent UTXO replaced by 1, and commit the hash to the current block. All full nodes will keep an index of latest dormant UTXO commitments so double spending is not possible. (a "meta-UTXO set")
If all dormant UTXOs under a Merkle branch are spent, hash of the branch will become 1. If all dormant UTXOs in a block are spent, the record for this block could be forgotten. Full nodes do not need to remember which particular UTXO is spent or not, since any person trying to redeem a dormant UTXO has to provide such information.
It becomes the responsibility of dormant coin holders to scan the blockchain for the current status of the UTXO commitment for their coin. They may also need to pay extra fee for the increased tx size.
This is a softfork if there is no hash collision but this is a fundamental assumption in Bitcoin anyway. The proposal also works without segregated witness, just by replacing "witness" with "scriptSig"

@_date: 2015-12-13 10:25:10
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
Hash: SHA256
I'm trying to list the minimal consensus rule changes needed for segwit softfork. The list does not cover the changes in non-consensus critical behaviors, such as relay of witness data.
1. OP_NOP4 is renamed as OP_SEGWIT
2. A script with OP_SEGWIT must fail if the scriptSig is not completely 3. If OP_SEGWIT is used in the scriptPubKey, it must be the only and the last OP code in the scriptPubKey, or the script must fail
4. The OP_SEGWIT must be preceded by exactly one data push (the "serialized script") with at least one byte, or the script must fail
5. The most significant byte of serialized script is the version byte, an unsigned number
6. If the version byte is 0x00, the script must fail
7. If the version byte is 0x02 to 0xff, the rest of the serialized script is ignored and the output is spendable with any form of witness (even if the witness contains something invalid in the current script system, e.g. OP_RETURN)
8. If the version byte is 0x01,
8a. rest of the serialized script is deserialized, and be interpreted as the scriptPubKey.
8b. the witness is interpreted as the scriptSig.
8c. the script runs with existing rules (including P2SH)
9. If the script fails when OP_SEGWIT is interpreted as a NOP, the script must fail. However, this is guaranteed by the rules 2, 3, 4, 6 so no additional check is needed.
10. The calculation of Merkle root in the block header remains unchanged
11. The witness commitment is placed somewhere in the block, either in coinbase or an OP_RETURN output in a specific tx
Format of the witness commitment:
The witness commitment could be as simple as a hash tree of all witness in a block. However, this is bad for further development of sum tree for compact SPV fraud proof as we have to maintain one more tree in the future. Even if we are not going to implement any sum checking in first version of segwit, it's better to make it easier for future softforks. (credit: gmaxwell)
12. The block should indicate how many sum criteria there are by committing the number following the witness commitment
13. The witness commitment is a hash-sum tree with the number of sum criteria committed in rule 12
14. Each sum criterion is a fixed 8 byte signed number (Negative value is allowed for use like counting delta-UTXO. 8 bytes is needed for fee commitment. Multiple smaller criteria may share a 8 byte slot, as long as they do not allow negative value)
15. Nodes will ignore the sum criteria that they do not understand, as long as the sum is correctly calculated
Size limit:
16. We need to determine the size limit of witness
17. We also need to have an upper limit for the number of sum criteria, or a malicious miner may find a block with many unknown sum criteria and feed unlimited amount of garbage to other nodes.
All other functions I mentioned in my wish list could be softforked later to this system
To mitigate the risk described in rule 17, we may ask miners to vote for an increase (but not decrease) in the number of sum criteria. Initially there should be 0 sum criteria. If we would like to introduce a new criteria, miners will support the proposal by setting the number of sum criteria as 1. However, until 95% of miners support the increase, all values of the extra sum criteria must be 0. Therefore, even a malicious miner may declare any amount of sum criteria, those criteria unknown to the network must be 0, and no extra data is needed to construct the block. This voting mechanism could be a softfork over rule 12 and 13, and is not necessary in the initial segwit deployment.
Gregory Maxwell ? 2015-12-10 04:51 ??:

@_date: 2015-12-13 13:11:41
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
Hash: SHA256
On Mon, Dec 14, 2015 at 12:14 AM, Danny Thorpe  I don't see why it must be kept in memory. But storage is still a problem. With the 8 year limit and a fixed max block size, it indirectly sets an upper limit for UTXO set.
Chris Priest via bitcoin-dev :
Do you believe that thousands of volunteer full nodes are obliged to store an UTXO record, just because one paid US$0.01 to an anonymous miner 100 years ago? It sounds insanely cheap, isn't it? My proposal (or similar proposal by Peter Todd) is to solve this problem. Many commercial banks have a dormant threshold less than 8 years so I believe it is a balanced choice.
Back to the topic, I would like to further elaborate my proposal.
We have 3 types of full nodes:
Archive nodes: full nodes that store the whole blockchain
Full UTXO nodes: full nodes that fully store the latest UTXO state, but not the raw blockchain
Lite UTXO nodes: full nodes that store only UTXO created in that past 420000 blocks
Currently, if one holds nothing but a private key, he must consult either an archive node or a full UTXO node for the latest UTXO state to spend his coin. We currently do not have any lite UTXO node, and such node would not work properly beyond block 420000.
With the softfork I described in my original post, if the UTXO is created within the last 420000 blocks, the key holder may consult any type of full node, including a lite UTXO node, to create the If the UTXO has been confirmed by more than 420000 blocks, a lite UTXO node obviously can't provide the necessary information to spend the coin. However, not even a full UTXO node may do so. A full UTXO node could tell the position of the UTXO in the blockchain, but can't provide all the information required by my specification. Only an archive node may do so.
What extra information is needed?
(1) If your UTXO was generated in block Y, you first need to know the TXO state (spent / unspent) of all outputs in block Y at block (Y + 420000). Only UTXOs at that time are relevant.
(2) You also need to know if there was any spending of any block Y UTXOs after block (Y + 420000).
It is not possible to construct the membership prove I require without these information. It is designed this way, so that lite UTXO nodes won't need to store any dormant UTXO records: not even the hash of individual dormant UTXO records. If the blockchain grows to insanely big, it may take days or weeks to retrieve to records. However, I don't think this is relevant as one has already left his coins dormant for >8 years. Actually, you don't even need the full blockchain. For (1), all you need is the 420000 blocks from Y to Y+420000 minus any witness data, as you don't need to do any validation. For (2), you just need the coinbase of Y+420001 to present, where any spending would have been committed, and retrieve the full block only if a spending is found.
So the Bitcoin Bank (miners) is not going to shred your record and confiscate your money. Instead, the Bank throws your record to the garage (raw blockchain). You can search for your record by yourself, or employ someone (archive node) to search it for you. In any case it incurs costs. But as thousands of bankers have kept your record on their limited desk space for 8 years for free (though one of them might receive a fraction of a penny from you), you shouldn't complain with any moral, technical, or legal reason. And no matter what users say, I believe something like this will happen when miners and full nodes can't handle the UTXO set.
I'd like to see more efficient proposals that archive the same goals.
p.s. there were some typos in my original. The second sentence of the second paragraph should be read as "For every block X+420000, it will commit to a hash for all UTXOs generated in block X."

@_date: 2015-12-13 13:41:44
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Pieter Wuille 2015-12-13 13:07 :
Just to make sure a script like OP_0 OP_SEGWIT will fail.
Anyway, your design may be better so forget it
I am not referring to the serialized script, but the witness. Basically,
it doesn't care what the content look like.
Why 41 bytes? Do you expect all witness program to be P2SH-like?
Could we just implement this as standardness rule? It is always possible
to stuff the scriptSig with pointless data so I don't think it's a new
attack vector. What if we want to include the height and tx index of
the input for compact fraud proof? Such fraud proof should not be an
opt-in function and not be dependent on the version byte
For the same reason, we should also allow traditional tx to have data
in the witness field, for any potential softfork upgrade

@_date: 2015-12-16 13:36:00
@_author: jl2012 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
I would also like to summarize my observation and thoughts after the Hong Kong workshop.
1. I'm so glad that I had this opportunity to meet so many smart developers who are dedicated to make Bitcoin better. Regular conference like this is very important for a young project, and it is particularly important for Bitcoin, with consensus as the core value. I hope such a conference could be conducted at least once in 2 years in Hong Kong, which is visa-friendly for most people in both East and West.
2. I think some consensus has emerged at/after the conference. There is no doubt that segregated witness will be implemented. For block size, I believe 2MB as the first step is accepted by the super majority of miners, and is generally acceptable / tolerable for devs.
3. Chinese miners are requesting consensus among devs nicely, instead of using their majority hashing power to threaten the community. However, if I were allowed to speak for them, I think 2MB is what they really want, and they believe it is for the best interest of themselves and the whole community
4. In the miners round table on the second day, one of the devs mentioned that he didn't want to be seen as the decision maker of Bitcoin. On the other hand, Chinese miners repeatedly mentioned that they want several concrete proposals from devs which they could choose. I see no contradiction between these 2 viewpoints.
Below are some of my personal views:
5. Are we going to have a "Fee Event" / "Economic Change Event" in 2-6 months as Jeff mentioned? Frankly speaking I don't know. As the fee starts to increase, spammers will first get squeezed --- which could be a good thing. However, I have no idea how many txs on the blockchain are spam. We also need to consider the effect of halving in July, which may lead to speculation bubble and huge legitimate tx volume.
6. I believe we should avoid a radical "Economic Change Event" at least in the next halving cycle, as Bitcoin was designed to bootstrap the adoption by high mining reward in the beginning. For this reason, I support an early and conservative increase, such as BIP102 or 2-4-8. 2MB is accepted by most people and it's better than nothing for BIP101 proponents. By "early" I mean to be effective by May, at least 2 months before the halving.
7. Segregated witness must be done. However, it can't replace a short-term block size hardfork for the following reasons:
(a) SW softfork does not allow higher volume if users are not upgrading. In order to bootstrap the new tx type, we may need the help of altruistic miners to provide a fee discount for SW tx.
(b) In terms of block space saving, SW softfork is most efficient for multisig tx, which is still very uncommon
(c) My most optimistic guess is SW will be ready in 6 months, which will be very close to halving and potential tx volume burst. And it may not be done in 2016, as it does not only involve consensus code, but also change in the p2p protocol and wallet design
8. Duplex payment channel / Lightning Network may be viable solutions. However, they won't be fully functional until SW is done so they are irrelevant in this discussion
9. No matter what is going to be done / not done, I believe we should now have a clear road map and schedule for the community: a short-term hardfork or not? The timeline of SW? It is bad to leave everything uncertain and people can't well prepared for any potential radical 10. Finally, I hope this discussion remains educated and evidence-based, and no circling

@_date: 2015-12-17 00:32:11
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
There are at least 2 proposals on the table:
1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately equals to 2MB actual limit
2. BIP102: 2MB actual limit
Since the actual limits for both proposals are approximately the same, it is not a determining factor in this discussion
The biggest advantage of SWSF is its softfork nature. However, its complexity is not comparable with any previous softforks we had. It is reasonable to doubt if it could be ready in 6 months
For BIP102, although it is a hardfork, it is a very simple one and could be deployed with ISM in less than a month. It is even simpler than BIP34, 66, and 65.
So we have a very complicated softfork vs. a very simple hardfork. The only reason makes BIP102 not easy is the fact that it's a hardfork.
The major criticism for a hardfork is requiring everyone to upgrade. Is that really a big problem?
First of all, hardfork is not a totally unknown territory. BIP50 was a hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was released on 18 March, which only gave 2 months of grace period for everyone to upgrade. The actual hardfork happened on 16 August. Everything completed in 5 months without any panic or chaos. This experience strongly suggests that 5 months is already safe for a simple hardfork. (in terms of simplicity, I believe BIP102 is even simpler than Another experience is from BIP66. The 0.10.0 was released on 16 Feb 2015, exactly 10 months ago. I analyze the data on  and found that 4600 out of 5090 nodes (90.4%) indicate BIP66 support. Considering this is a softfork, I consider this as very good adoption already.
With the evidence from BIP50 and BIP66, I believe a 5 months pre-announcement is good enough for BIP102. As the vast majority of miners have declared their support for a 2MB solution, the legacy 1MB fork will certainly be abandoned and no one will get robbed.
My primary proposal:
Now - 15 Jan 2016: formally consult the major miners and merchants if they support an one-off rise to 2MB. I consider approximately 80% of mining power and 80% of trading volume would be good enough
16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80% of hashing power
1 Jun 2016: the first day a 2MB block may be allowed
Before 31 Dec 2016: release SWSF
My secondary proposal:
Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016
1 Jun 2016: release SWSF
What if the deadline is not met? Maybe pushing an urgent BIP102 if things become really bad.
In any case, I hope a clear decision and road map could be made now. This topic has been discussed to death. We are just bringing further uncertainty if we keep discussing.
Matt Corallo via bitcoin-dev ? 2015-12-16 15:50 ??:

@_date: 2015-12-17 05:00:24
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
I know my reply is a long one but please read before you hit send. I have 2 proposals: fast BIP102 + slow SWSF and fast SWSF only. I guess no one here is arguing for not doing segwit; and it is on the top of my wish list. My main argument (maybe also Jeff's) is that segwit is too complicated and may not be a viable short term solution (with the reasons I listed that I don't want to repeat)
And also I don't agree with you that BIP102 is *strictly* inferior than segwit. We never had a complex softfork like segwit, but we did have a successful simple hardfork (BIP50), and BIP102 is very simple. (Details in my last post. I'm not going to repeat)
Mark Friedenbach ? 2015-12-17 04:33 ??:

@_date: 2015-12-17 13:46:06
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
This is not correct.
As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx are less secure than others? I don't think so. Since one invalid CLTV tx will make the whole block invalid. Having more nodes to fully validate non-CLTV txs won't make them any safer. The same logic also applies to SW softfork.
You may argue that a softfork would make the network as a whole less secure, as old nodes have to trust new nodes. However, the security of all content in the same block must be the same, by definition.
Anyway, I support SW softfork at the beginning, and eventually (~2 years) moving to a hardfork with higher block size limit and better commitment structure.
Jeff Garzik via bitcoin-dev ? 2015-12-17 13:27 ??:

@_date: 2015-12-17 22:10:02
@_author: jl2012 
@_subject: [bitcoin-dev] On the security of softforks 
Jonathan Toomim via bitcoin-dev ? 2015-12-17 21:47 ??:
You miss the fact that 0-conf is not safe, neither 1-conf. What you are suggesting is just a variation of Finney attack.

@_date: 2015-12-19 11:49:25
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated witness softfork with moderate adoption 
I have done some calculation for the effect of a SW softfork on the actual total block size.
Core block size (CBS): The block size as seen by a non-upgrading full Witness size (WS): The total size of witness in a block
Total block size (TBS): CBS + WS
Witness discount (WD): A discount factor for witness for calculation of VBS (1 = no discount)
Virtual block size (VBS): CBS + (WS * WD)
Witness adoption (WA): Proportion of new format transactions among all Prunable ratio (PR): Proportion of signature data size in a transaction
With some transformation it could be shown that:
  TBS = CBS / (1 - WA * PR) = VBS / (1 - WA * PR * (1 - WD))
sipa suggested a WD of 25%.
The PR heavily depends on the transaction script type and input-output ratio. For example, the PR of 1-in 2-out P2PKH and 1-in 1-out 2-of-2 multisig P2SH are about 47% and 72% respectively. According to sipa's presentation, the current average PR on the blockchain is about 60%.
Assuming WD=25% and PR=60%, the MAX TBS with different MAX VBS and WA is listed at:
The highlight indicates whether the CBS or VBS is the limiting factor.
With moderate SW adoption at 40-60%, the total block size is 1.32-1.56MB when MAX VBS is 1.25MB, and 1.22-1.37MB when MAX VBS is 1.00MB.
P2SH has been introduced for 3.5 years and only about 10% of bitcoin is stored this way (I can't find proportion of existing P2SH address). A 1-year adoption rate of 40% for segwit is clearly over-optimistic unless the tx fee becomes really high.
(btw the PR of 60% may also be over-optimistic, as using SW nested in P2SH will decrease the PR, and therefore TBS becomes even lower)
I am not convinced that SW softfork should be the *only* short term scalability solution

@_date: 2015-12-19 15:03:28
@_author: jl2012 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
After the meeting I find a softfork solution. It is very inefficient and I am leaving it here just for record.
1. In the first output of the second transaction of a block, mining pool will commit a random nonce with an OP_RETURN.
2. Mine as normal. When a block is found, the hash is concatenated with the committed random nonce and hashed.
3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the block is invalid. That means about 1% of blocks are discarded.
4. For each difficulty retarget, the secondary target is decreased by 2 ^ 1/64.
5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ 252. Therefore only 1 in 16 hash returned by hasher is really valid. This should make the detection of block withholding attack much easier.
All miners have to sacrifice 1% reward for 10 years. Confirmation will also be 1% slower than it should be.
If a node (full or SPV) is not updated, it becomes more vulnerable as an attacker could mine a chain much faster without following the new rules. But this is still a softfork, by definition.
ok, back to topic. Do you mean this? Peter Todd via bitcoin-dev ? 2015-12-19 13:42 ??:

@_date: 2015-12-19 22:40:06
@_author: jl2012 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Chris Priest via bitcoin-dev ? 2015-12-19 22:34 ??:
This is not true. For a pool with 5% total hash rate, an attacker only
needs 0.5% of hash rate to sabotage 10% of their income. It's already
enough to kill the pool

@_date: 2015-12-19 23:24:52
@_author: jl2012 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Chris Priest ? 2015-12-19 22:47 ??:
It did happen: The worst thing is that the proof for such attack is probabilistic, not
A smarter attacker may even pretend to be many small miners, make it
even more difficult or impossible to prove who are attacking.
The only solution is to ask for KYC registration, unless one could a cryptographic solution that does not require a consensus fork.

@_date: 2015-12-20 14:16:29
@_author: jl2012 
@_subject: [bitcoin-dev] On the security of softforks 
Rusty Russell via bitcoin-dev ? 2015-12-19 23:14 ??:
I think he means Mallory is paying with an invalid Segwit input, not output (there is no "invalid output" anyway). However, this is not a issue if Bob waits for a few confirmations.

@_date: 2015-12-20 23:23:32
@_author: jl2012 
@_subject: [bitcoin-dev] Increasing the blocksize as a (generalized) 
I proposed something very similar 2 years ago:
This is an interesting academic idea. But the way you implement it will immediately kill all existing full and SPV nodes (not really dead, rather like zombie as they can't send and receive any tx).
joe2015--- via bitcoin-dev ? 2015-12-20 05:56 ??:

@_date: 2015-12-21 00:14:12
@_author: jl2012 
@_subject: [bitcoin-dev] =?utf-8?q?A_new_payment_address_format_for_segregat?= 
On the -dev IRC I asked the same question and people seem don't like it. I would like to further elaborate this topic and would like to consult merchants, exchanges, wallet devs, and users for their preference
People will be able to use segregated witness in 2 forms. They either put the witness program directly as the scriptPubKey, or hide the witness program in a P2SH address. They are referred as "native SW" and "SW in P2SH" respectively
Examples could be found in the draft BIP: As a tx malleability fix, native SW and SW in P2SH are equally good.
The SW in P2SH is better in terms of:
1. It allows payment from any Bitcoin reference client since version 2. Slightly better privacy by obscuration since people won't know whether it is a traditional P2SH or a SW tx before it is spent. I don't consider this is important since the type of tx will be revealed eventually, and is irrelevant when native SW is more popular
The SW in P2SH is worse in terms of:
1. It requires an additional push in scriptSig, which is not prunable in transmission, and is counted as part of the core block size
2. It requires an additional HASH160 operation than native SW
3. It provides 160bit security, while native SW provides 256bit
4. Since it is less efficient, the tx fee is likely to be higher than native SW (but still lower than non-SW tx)
So I'd like to suggest 2 proposals:
Proposal 1:
To define a native SW address format, while people can still use payment protocol or SW in P2SH if the want
Proposal 2:
No new address format is defined. If people want to pay as lowest fee as possible, they must use payment protocol. Otherwise, they may use SW in Since this topic is more relevant to user experience, in addition to core devs, I would also like to consult merchants, exchanges, wallet devs, and users for their preferences.

@_date: 2015-12-24 09:22:01
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness BIPs 
The SW payment address format BIP draft is ready and is pending BIP number assignment:
This is the 3rd BIP for segwit. The 2nd one for Peer Services is being prepared by Eric Lombrozo
Eric Lombrozo via bitcoin-dev ? 2015-12-23 10:22 ??:

@_date: 2015-12-27 03:26:21
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness BIPs 
The SW payment address format BIP is completely rewritten to introduce 2 types of new addresses:
jl2012 via bitcoin-dev ? 2015-12-24 09:22 ??:

@_date: 2015-12-29 02:47:22
@_author: jl2012 
@_subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a 
Do we need to consider that someone may have a timelocked big tx, with private key lost?
I think we need to tell people not to do this. Related discussion:
Peter Todd via bitcoin-dev ? 2015-12-29 00:35 ??:

@_date: 2015-12-29 07:55:28
@_author: jl2012 
@_subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a 
What if someone complains? We can't even tell whether a complaint is legit or just trolling. That's why I think we need some general consensus rules which is not written in code, but as a social contract. Breaking those rules would be considered as a hardfork and is allowed only in exceptional situation.
Jonathan Toomim via bitcoin-dev ? 2015-12-29 07:42 ??:

@_date: 2015-07-22 17:00:41
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
Quoting Peter Todd via bitcoin-dev :
To avoid any risk of reorg, the hardfork may require that the first  block with GetMedianTimePast() after a pre-determined time (the "flag  block") MUST be version 0. The exception is applied ONLY to the flag  Alternatively, the hardfork may require that the flag block MUST be  larger than 1MB. Comparing with exploiting the block version, this  does not require additional exceptions in consensus rules. However,  miner may need to artificially inflate the size of the flag block and  that could be trouble in coding. I don't have any preference.
Old nodes will not accept the new chain because it violates BIP66 /  block size limit. New nodes will not accept the old chain because its  flag block is not version 0 / not larger than 1MB.
This is actually checkpointing in a decentralized way. In that case,  we can say goodbye to the old chain forever, as long as all major  merchants and exchanges agree to upgrade. Miner support is less  relevant. It is a no-brainer for miners to support the new chain,  unless they don't want to sell or spend their bitcoin, or just give up  mining after heavily investing in ASIC.

@_date: 2015-07-23 04:55:46
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP: Short Term Use Addresses for Scalability 
Quoting Gavin Andresen via bitcoin-dev  I think it would only save ~5% with all overhead (value, sequence,  locktime, version, etc.) counted
A better way is to introduce shorter ECDSA keys, which will save a lot  of space for public key and signature. It is safe as long as the  output value is much lower than the cost of attack.
If this happens, I think it will be part of the OP_MAST which will  require a new address type anyway.

@_date: 2015-07-23 05:39:20
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP 102 - kick the can down the road to 2MB 
Quoting Peter Todd via bitcoin-dev :
I think I have already answered this with my previous mail. If there  is consensus among major exchanges and merchants, the preference of  miners are not particularly relevant. A checkpoint could be  implemented in a decentralized way to make sure miners of the original  chain won't be able to overtake the new chain.
Bitcoin has no intrinsic value. Bitcoin has value because people are  willing to exchange it with something really valuable (e.g. a pizza;  or USD which could buy a pizza). If most bitcoin-accepting business  agree to follow BIP102 and ONLY BIP102, then BIP102 is THE Bitcoin,  and the original chain is just a dSHA256 alt-coin which one can't even  merge mine with BIP102. Switching to BIP102 is the only economically  viable choice for miners.
Having said that, a miner voting may still be useful. It is just to  make sure enough miners are ready for the change, instead of measuring  their consensus. For example, the new rule will be implemented 1) 1  week after 70% of miners are ready; or 2) on 1 Feb 2016, whichever  happens first.
For SPV wallets, they have to strengthen their security model after  the BIP66 fork, anyway. They should be able to identify potential  consensus fork in the network and stop accepting incoming txs when it  is in doubt. My "version 0 flag block" proposal could be a good  generic way to indicate a hardfork to SPV wallets. (see my previous  email on this topic)

@_date: 2015-07-23 16:23:21
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP draft: Hardfork bit 
Please feel free to comment, for technical issues and language
BIP: ??
Title: Hardfork bit
Author: jl2012 Status: Draft
Type: Standard Track
Created: 2015-07-23
This document specifies a proposed change to the semantics of the most  significant bit of the ?version? field in Bitcoin block headers, as a  mechanism to indicate a hardfork is deployed. It alleviates certain  risks related to a hardfork by introducing an explicit ?point of no  return? in the blockchain. This is a general mechanism which should be  employed by any planned hardfork in the future.
Hardforks in Bitcoin are usually considered as difficult and risky, because:
1) Hardforks require not only support of miners, but also, most  importantly, supermajority support of the Bitcoin economy. As a  result, softfork deployment mechanisms described in BIP 34 or BIP XX  ?Version bits? ( are  not enough for introducing hardforks safely.
2) Full nodes and SPV nodes following original consensus rules may not  be aware of the deployment of a hardfork. They may stick to an  economic-minority fork and unknowingly accept devalued legacy tokens.
3) In the case which the original consensus rules are also valid under  the new consensus rules, users following the new chain may  unexpectedly reorg back to the original chain if it grows faster than  the new one. People may find their confirmed transactions becoming  unconfirmed and lose money.
The first issue involves soliciting support for a hardfork proposal,  which is more a political topic than a technical one. This proposal  aims at alleviating the risks related to the second and third issues.  It should be employed by any planned hardfork in the future.
See BIP YY ?Motivation and deployment of consensus rules changes?
Hardfork bit: The most significant bit in nVersion is defined as the  hardfork bit. Currently, blocks with this header bit setting to 1 are  invalid, since BIP34 interprets nVersion as a signed number and  requires it to be >=2 (with BIP66, >=3). Among the 640 bits in the  block header, this is the only one which is fixed and serves no  purpose, and therefore the best way to indicate the deployment of a  Flag block: Any planned hardfork must have one and only one flag block  which is the ?point of no return?. To ensure monotonicity, flag block  should be determined by block height, or as the first block with  GetMedianTimePast() greater than a threshold. Other mechanisms could  be difficult for SPV nodes to follow. The height/time threshold could  be a predetermined value or relative to other events (e.g. 1000 blocks  the scope of this BIP. No matter what mechanism is used, the threshold  is consensus critical. It must be publicly verifiable with only  blockchain data and the programme source code, and preferably  Flag block is constructed in a way that nodes with the original  consensus rules must reject. On the other hand, nodes with the new  consensus rules must reject a block if it is not a flag block while it  is supposed to be. To achieve these goals, the flag block must 1) have  the hardfork bit setting to 1, 2) include a short predetermined unique  description of the hardfork anywhere in its coinbase, and 3) follow  any other rules required by the hardfork. If these conditions are not  fully satisfied, upgraded nodes shall reject the block.
The hardfork bit must be turned off in the decedents of the flag  block, until the deployment of the next hardfork. The requirement of  coinbase message is also limited to the flag block. In the rare case  that multiple hardforks share the same flag block, the coinbase shall  include all relevant messages and the order/position of the messages  shall not be consensus critical.
Although a hardfork is officially deployed after the flag block, the  exact behavioural change is out of the scope of this BIP. For example,  a hardfork may not be fully active until certain time after the flag  Automatic warning system: When a flag block is found on the network,  full nodes and SPV nodes should look into its coinbase. They should  alert their users and/or stop accepting incoming transactions if it is  an unknown hardfork. It should be noted that the warning system could  become a DoS vector if the attacker is willing to give up the block  reward. Therefore, the warning may be issued only if a few blocks are  built on top of the flag block in a reasonable time frame. This will  in turn increase the risk in case of a real planned hardfork so it is  up to the wallet programmers to decide the optimal strategy. Human  warning system (e.g. the emergency alert system in Bitcoin Core) could  fill the gap.
As a mechanism to indicate hardfork deployment, this BIP breaks  backward compatibility intentionally. However, without further changes  in the block header format, full nodes and SPV nodes could still  verify the PoW of a flag block and its descendants.
This proposal is also compatible with the BIP XX ?Version bits?. The  version bits mechanism could be employed to measure miner support  towards a hardfork proposal, and to determine the height or time  threshold of the flag block. Also, miners of the flag block may still  cast votes for other concurrent softfork or hardfork proposals as  After the flag block is generated, a miner may support either fork but  not both. It is not possible for miners in one fork to attack or  overtake the other fork because the forks are mutually exclusive.

@_date: 2015-07-23 19:26:33
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP draft: Hardfork bit 
Quoting Tier Nolan via bitcoin-dev :
I refrain from calling it the "main chain". I use "original chain" and  "new chain" instead as I make no assumption about the distribution of  mining power. This BIP still works when we have a 50/50 hardfork. The  main point is to protect all users on both chains, and allow them to  make an informed choice.
Again, as I make no assumption about the mining power distribution,  the new chain may actually have less miner support. Without any  protection (AFAIK, for example, BIP100, 101, 102), the weaker new  chain will get 51%-attacked by the original chain constantly.
I guess the git hash is not known until the code is written? (correct  me if I'm wrong) As the coinbase message is consensus-critical, it  must be part of the source code and therefore you can't use any kind  of hash of the code itself (a chicken-and-egg problem)
This may not be compatible with the other version bits voting mechanisms.
The flag block itself is a hardfork already and old miners will not  mine on top of the flag block. So your suggestion won't be helpful in  this situation.
To make it really meaningful, we need to consume one more bit of the  'version' field ("notice bit"). Supporting miners will turn on the  notice bit, and include a message in coinbase ("notice block"). When a  full node/SPV node find many notice blocks with the same coinbase  message, they could bet that the subsequent flag block is a legit one.  However, an attacker may still troll you by injecting an invalid flag  block after many legit notice blocks. So I'm not sure if it is worth  the added complexity.

@_date: 2015-07-30 18:14:50
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A summary of block size hardfork proposals 
Currently, there are 4 block size BIP by Bitcoin developers:
BIP100 by Jeff:  BIP101 by Gavin:  BIP102 by Jeff: BIP??? by Pieter (called "BIP103" below):  To facilitate further discussion, I'd like to summarize these  proposals by a series of questions. Please correct me if I'm wrong.  Something like sigop limit are less controversial and are not shown.
Should we use a BIP34-like voting mechanism to initiate the hardfork?
BIP100, 102, 103: No
BIP101: Yes
When should we initiate the hardfork?
BIP100: 2016-01-11
BIP101: 2 weeks after 75% miner support, but not before 2016-01-11
BIP102: 2015-11-11
BIP103: 2017-01-01
What should be the block size at initiation?
BIP100: 1MB
BIP101: 8MB
BIP102: 2MB
BIP103: 1MB
Should we allow further increase / decrease?
BIP100: By miner voting, 0.5x - 2x every 12000 blocks (~3 months)
BIP101: Double every 2 years, with interpolations in between (41.4% p.a.)
BIP102: No
BIP103: +4.4% every 97 days (double every 4.3 years, or 17.7% p.a.)
The earliest date for a >=2MB block?
BIP100: 2016-04-03 (assuming 10 minutes block)
BIP101: 2016-01-11
BIP102: 2015-11-11
BIP103: 2020-12-27
What should be the final block size?
BIP100: 32MB is the max, but it is possible to reduce by miner voting
BIP101: 8192MB
BIP102: 2MB
BIP103: 2048MB
When should we have the final block size?
BIP100: Decided by miners
BIP101: 30 years after initiation
BIP102: 2015-11-11
BIP103: 2063-07-09

@_date: 2015-07-31 03:10:34
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] CORRECTIONS: A summary of block size hardfork 
I am making some corrections to my previous summary
Currently, there are 4 block size BIP by Bitcoin developers:
BIP100 by Jeff:  BIP101 by Gavin:  BIP102 by Jeff: BIP??? by Pieter (called "BIP103" below):  To facilitate further discussion, I'd like to summarize these  proposals by a series of questions. Please correct me if I'm wrong.  Something like sigop limit are less controversial and are not shown.
Should we use a miner voting mechanism to initiate the hardfork?
BIP100: Yes, support with 10800 out of last 12000 blocks (90%)
BIP101: Yes, support with 750 out of last 1000 blocks (75%)
BIP102: No
BIP103: No
When should we initiate the hardfork?
BIP100: 2016-01-11#
BIP101: 2 weeks after 75% miner support, but not before 2016-01-11
BIP102: 2015-11-11
BIP103: 2017-01-01
# The network does not actually fork until having 90% miner support
What should be the block size at initiation?
BIP100: 1MB
BIP101: 8MB*
BIP102: 2MB
BIP103: 1MB
* It depends on the exact time of initiation, e.g. 8MB if initiated on  2016-01-11, 16MB if initiated on 2018-01-10.
Should we allow further increase / decrease?
BIP100: By miner voting, 0.5x - 2x every 12000 blocks (~3 months)
BIP101: Double every 2 years, with linear interpolations in between  (41.4% p.a.)
BIP102: No
BIP103: +4.4% every 97 days (double every 4.3 years, or 17.7% p.a.)
The earliest date for a >=2MB block?
BIP100: 2016-04-03^
BIP101: 2016-01-11
BIP102: 2015-11-11
BIP103: 2020-12-27
^ Assuming 10 minutes blocks and votes cast before 2016-01-11 are not counted
What should be the final block size?
BIP100: 32MB is the max, but it is possible to reduce by miner voting
BIP101: 8192MB
BIP102: 2MB
BIP103: 2048MB
When should we have the final block size?
BIP100: Decided by miners
BIP101: 2036-01-06
BIP102: 2015-11-11
BIP103: 2063-07-09

@_date: 2015-07-31 08:39:43
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
There is a summary of the proposals in my previous mail at  I think there could be a compromise between Gavin's BIP101 and  Pieter's proposal (called "BIP103" here). Below I'm trying to play  with the parameters, which reasons:
1. Initiation: BIP34 style voting, with support of 750 out of the last  1000 blocks. The "hardfork bit" mechanism might be used:  Rationale: This follows BIP101, to make sure the new chain is secure.  Also, no miner would like to be the first one to mine a large block if  they don't know how many others would accept it.
2. Starting date: 30 days after 75% miner support, but not before  2016-01-12 00:00 UTC
Rationale: A 30-day grace period is given to make sure everyone has  enough time to follow. This is a compromise between 14 day in BIP101  and 1 year in BIP103. I tend to agree with BIP101. Even 1 year is  given, people will just do it on the 364th day if they opt to  2016-01-12 00:00 UTC is Monday evening in US and Tuesday morning in  China. Most pool operators and devs should be back from new year  holiday and not sleeping. (If the initiation is delayed, we may  require that it must be UTC Tuesday midnight)
3. The block size at 2016-01-12 will be 1,414,213 bytes, and  multiplied by 1.414213 by every 2^23 seconds (97 days) until exactly  8MB is reached on 2017-05-11.
Rationale: Instead of jumping to 8MB, I suggest to increase it  gradually to 8MB in 16 months. 8MB should not be particularly painful  to run even with current equipment (you may see my earlier post on  bitctointalk:  8MB  is also agreed by Chinese miners, who control >60% of the network.
4. After 8MB is reached, the block size will be increased by 6.714%  every 97 days, which is equivalent to exactly octuple (8x) every 8.5  years, or double every 2.9 years, or +27.67% per year. Stop growth at  4096MB on 2042-11-17.
Rationale: This is a compromise between 17.7% p.a. of BIP103 and 41.4%  p.a. of BIP101. This will take us almost 8 years from now just to go  back to the original 32MB size (4 years for BIP101 and 22 years for  SSD price is expected to drop by >50%/year in the coming years. In  2020, we will only need to pay 2% of current price for SSD. 98% price  reduction is enough for 40 years of 27.67% growth.
Source: Global bandwidth is expected to grow by 37%/year until 2021 so 27.67%  should be safe at least for the coming 10 years.
Source:  The final cap is a compromise between 8192MB at 2036 of BIP101 and  2048MB at 2063 of BIP103
Generally speaking, I think we need to have a faster growth in the  beginning, just to normalize the block size to a more reasonable one.  After all, the 1MB cap was introduced when Bitcoin was practically  worthless and with inefficient design. We need to decide a new  "optimal" size based on current adoption and technology.
About "fee market": I do agree we need a fee market, but the fee  pressure must not be too high at this moment when block reward is  still miner's main income source. We already have a fee market: miners  will avoid building big blocks with low fee because that will increase  the orphan risk for nothing.
About "secondary layer": I respect everyone building secondary layer  over the blockchain. However, while the SWIFT settlement network is  processing 300tps, Bitcoin's current 7tps is just nothing more than an  experiment. If the underlying settlement system does not have enough  capacity, any secondary layer built on it will also fall apart. For  example, people may DoS attack a Lightening network by provoking a  huge amount of settlement request which some may not be confirmed on  time. Ultimately, this will increase the risk of running a LN service  and increase the tx fee inside LN. After all, the value of secondary  layer primarily comes from instant confirmation, not scarcity of the  block space.

@_date: 2015-07-31 13:07:14
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
Yes, data-center operators are bound to follow laws, including NSLs  and gag orders. How about your ISP? Is it bound to follow laws,  including NSLs and gag orders?
Do you think everyone should run a full node behind TOR? No way, your  repressive government could just block TOR:
Or they could raid your home and seize your Raspberry Pi if they  couldn't read your encrypted internet traffic. You will have a hard  time proving you are not using TOR for child porn or cocaine.
If you are living in a country like this, running Bitcoin in an  offshore VPS could be much easier. Anyway, Bitcoin shouldn't be your  first thing to worry about. Revolution is probably your only choice.
Data-centers would get hacked. How about your Raspberry Pi?
Corrupt data-center employee is probably the only valid concern.  However, there is nothing (except cost) to stop you from establishing  multiple full nodes all over the world. If your Raspberry Pi at home  could no longer fully validate the chain, it could become a  header-only node to make sure your VPS full nodes are following the  correct chaintip. You may even buy hourly charged cloud hosting in  different countries to run header-only nodes at negligible cost.
There is no single point of failure in a decentralized network. Having  multiple nodes will also save you from Sybil attack and geopolitical  risks. Again, if all data-centres and governments in the world are  turning against Bitcoin, it is delusional to think we could fight  against them without using any real weapon.
By the way, I'm quite confident that my current full node at home are  capable of running at 8MB blocks.
Quoting Adam Back :

@_date: 2015-11-01 12:28:39
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Compatibility requirements for hard or soft forks 
My answer is simply "No", you don't have to maintain backward compatibility for non-standard tx.
The same question applies to P2SH. Before the deployment of BIP16, one could have created a time-locked tx with one of the output was in the form of HASH160  EQUAL. The , however, is not a hash of a valid serialized script, so the output is now permanently frozen.
It also applies to all the OP codes disabled by Satoshi: one could have created a time-locked tx with those now disabled OP codes.
Same for BIP65 with the use of OP_NOP2. Following your logic, we can't make any softfork related to the script system.
I think it is very important to make it clear that non-standard txs and non-standard scripts may become invalid in the future
Gavin Andresen via bitcoin-dev ? 2015-10-28 10:06 ??:

@_date: 2015-11-01 23:27:50
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP 113: Median time-past is a HARDfork, 
Currently, a tx maybe included in a block only if its locktime (x) is smaller than the timestamp of a block (y)
BIP113 says that a tx maybe included in a block only if x is smaller than the median-time-past (z)
It is already a consensus rule that y > z. Therefore, if x < z, x < y
The new rule is absolutely stricter than the old rule, so it is a softfork. Anything wrong with my interpretation?
Luke Dashjr via bitcoin-dev ? 2015-11-01 14:06 ??:

@_date: 2015-11-03 00:32:18
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Compatibility requirements for hard or soft forks 
The other strategy is to have an informational BIP to define "safe" use of Bitcoin.
1. scriptPubKey must be one of the following types: P2PK, P2PKH, P2SH, n-of-m multisig with m < 4 (with or without CLTV or CSV, we should define standard use of CLTV and CSV)
2. For P2SH, the serialized script must be one of the standard type
3. No use of unknown transaction version
4. Tx size < 100k
5. If conditions 1-4 are all satisfied, the locktime must not be longer than 4 years from the creation of the tx
6. If at least one of the conditions 1-4 is not satisfied, the lock time must not be longer than 6 months from the creation of the tx
7. A chain of unconfirmed transactions is unsafe due the malleability
8. Tx created by wallet software last updated 1 year ago is unsafe
9. Permanently deleting a private key is unsafe (that might be safe if stricter practice is followed)
We must not introduce a new rule that may permanently invalidate a safe tx, unless in emergency. Even in emergency, we should try to preserve backward compatibility as much as possible (see the example at the end of my message)
Being unsafe means there is a chance that the tx may become unconfirmable, outputs become unspendable, or funds may be stolen without a private key.
A grace period of 5 years must be given for "soft-fork" type change of any of the rules. For example it is ok to introduce new standard script anytime but not to remove.
Back to Gavin's original question. If you want to somehow keep backward compatibility for expensive-to-validate transactions in the future, you may have rules like there could only be at most one expensive-to-validate transaction in every 10 blocks, until year 2025. I know this is over-complicated but it's a possible way to address your Gavin Andresen via bitcoin-dev ? 2015-11-02 15:33 ??:

@_date: 2015-11-06 03:13:10
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] =?utf-8?q?Dealing_with_OP=5FIF_and_OP=5FNOTIF_malle?= 
I have a new BIP draft for fixing OP_IF and OP_NOTIF malleability. Please comment: Copied below:
BIP: x
   Title: Dealing with OP_IF and OP_NOTIF malleability
   Author: jl2012    Status: Draft
   Type: Standards Track
   Created: 2015-11-06
As an supplement to BIP62, this document specifies proposed changes to the Bitcoin transaction validity rules in order to make malleability of transactions with OP_IF and OP_NOTIF impossible.
OP_IF and OP_NOTIF are flow control codes in the Bitcoin script system. The programme flow is decided by whether the top stake value is 0 or not. However, this behavior opens a source of malleability as a third party may alter a non-zero flow control value to any other non-zero value without invalidating the transaction.
As of November 2015, OP_IF and OP_NOTIF are not commonly used in the blockchain. However, as more sophisticated functions such as OP_CHECKLOCKTIMEVERITY are being introduced, OP_IF and OP_NOTIF will become more popular and the related malleability should be fixed. This proposal serves as a supplement to BIP62 and should be implemented with other malleability fixes together.
If the transaction version is 3 or above, the flow control value for OP_IF and OP_NOTIF must be either 0 or 1, or the transaction fails.
This is to be implemented with BIP62.
This is a softfork. To ensure OP_IF and OP_NOTIF transactions created before the introduction of this BIP will still be accpeted by the network, the new rules only apply to transactions of version 3 or above.
For people who want to preserve the original behaviour of OP_IF and OP_NOTIF, an OP_0NOTEQUAL could be  used before the flow control code to transform any non-zero value to 1.
BIP62:

@_date: 2015-11-06 05:16:46
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] 
=?utf-8?q?ability?=
I assume this proposal is implemented at the same time as BIP62. As long as OP_IF/OP_NOTIF interprets the argument as a number, zero-padded number and negative zero are already prohibited in BIP62
Tier Nolan via bitcoin-dev ? 2015-11-06 04:37 ??:

@_date: 2015-10-02 09:14:11
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] 
=?utf-8?q?=27s_deploy_BIP65_CHECKLOCKTIMEVERIFY!=5D?=
According to the Oxford Dictionary, "coin" as a verb means "invent (a new word or phrase)". Undoubtedly you created the first functional SPV client but please retract the claim "I coined the term SPV" or that's And I'd like to highlight the following excerpt from the whitepaper: "the simplified method can be fooled by an attacker's fabricated transactions for as long as the attacker can continue to overpower the network. One strategy to protect against this would be to accept alerts from network nodes when they detect an invalid block, prompting the user's software to download the full block and alerted transactions to confirm the inconsistency."
Header only clients without any fraud detecting mechanism are functional but incomplete SPV implementations, according to Sathoshi's original definition. This might be good enough for the first generation SPV wallet, but eventually SPV clients should be ready to detect any rule violation in the blockchain, including things like block size (as Satoshi mentioned "invalid block", not just "invalid transaction").
Mike Hearn via bitcoin-dev ? 2015-10-02 08:23 ??:

@_date: 2015-10-03 14:49:20
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
BIP68 allows per-input locktime, though I don't know how this could be BIP68 and BIP112 are mostly ready. If we try to reimplement relative-locktime without using nSequence, we may need to wait for another year for deployment.
A compromise is to make BIP68 optional, indicated by a bit in tx nVersion, as I suggested earlier (1). This will allow deploying relative-locktime without further delay while not permanently limiting future upgrades.
(1) Peter Todd via bitcoin-dev ? 2015-10-03 10:30 ??:

@_date: 2015-10-09 00:16:50
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] 
You are mixing multiple issues.
1. It is not possible to "checkpoint" in a totally decentralized and trustless way. You need the whole blockchain to confirm its validity, as a single invalid tx in the history will invalidate ALL blocks after it, even if the invalid tx is irrelevant to you.
2. Downloading the whole blockchain does not mean you need to store the whole blockchain. Spent transactions outputs can be safely removed from your harddrive. Please read section 7 of Satoshi's paper:  . This function is already implemented in Bitcoin Core 0.11
3. If you don't even want to download the whole blockchain, you can download and validate the portions that your are interested. Satoshi called it Simplified Payment Verification (SPV), the section 8 of his paper. It is secure as long as >50% of miners are honest. Android Bitcoin Wallet is an SPV wallet based on bitcoinj.
Finally, I think this kind of question would be better asked on the bitcointalk forum. The mailing list should be more specific to development, not merely some vague idea.
telemaco via bitcoin-dev ? 2015-10-08 23:18 ??:

@_date: 2015-09-03 00:55:35
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] block size - pay with difficulty 
Jeff Garzik via bitcoin-dev ? 2015-09-03 00:05 ??:
Ref: I explained here why pay with difficulty is bad for everyone: miners and users, and described the use of OP_CLTV for pay-to-future-miner
However, a general problem of pay-to-increase-block-size scheme is it indirectly sets a minimal tx fee, which could be difficult and arbitrary, and is against competition

@_date: 2015-09-03 03:57:09
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP 100 specification 
Some comments:
 	* The 75% rule is meaningless here. Since this is a pure relaxation of
rules, there is no such thing as "invalid version 4 blocks"
The implication threshold is unclear. Is it 95% or 80%?
 	* Softfork requires a very high threshold (95%) to "attack" the
original fork. This makes sure that unupgraded client will only see the
new fork.
 	* In the case of hardfork, however, the new fork is unable to attack
the original fork, and unupgraded client will never see the new fork.
The initiation of a hardfork should be based on its acceptance by the
economic majority, not miner support. 95% is an overkill and may
probably never accomplished. I strongly prefer a 80% threshold rather
than 95%.
 	* As I've pointed out, using 20-percentile rather than median creates
an incentive to 51% attack the uncooperative minority.
Having said that, I don't have a strong feeling about the use of
20-percentile as threshold to increase the block size. That means the
block size is increased only when most miners agree, which sounds ok to
me. However, using 20-percentile as threshold to DECREASE the block size
could be very dangerous. Consider that the block size has been stable at
8MB for a few years. Everyone are happy with that. An attacker would
just need to acquire 21% of mining power to break the status quo and
send us all the way to 1MB. The only way to stop such attempt is to 51%
attack the attacker. That'd be really ugly. For technical and ethical reasons, I believe the thresholds for increase
and decrease must be symmetrical: increase the block size when the
x-percentile is bigger than the current size, decrease the block size
when the (100-x)-percentile is smaller than the current size. The
overall effect is: the block size remains unchanged unless 80% of miners
agree to.  	* Please consider the use of "hardfork bit" to signify the hardfork:
  	* Or, alternatively, please combine the hardfork with a softfork. I'm
rewriting the specification as follow (changes underlined):
 	* Replace static 1M block size hard limit with a floating limit
hardLimit floats within the range 1-32M, inclusive.
Initial value of hardLimit is 1M, preserving current system.
 	* Changing hardLimit is accomplished by encoding a proposed value
within a block's coinbase scriptSig.
 	* Votes refer to a byte value, encoded within the pattern "/BVd+/"
Example: /BV8000000/ votes for 8,000,000 byte hardLimit. If there is
more than one match with with pattern, the first match is counted.
 	* Absent/invalid votes and votes below minimum cap (1M) are counted as
1M votes. Votes above the maximum cap (32M) are counted as 32M votes.
 	* A new hardLimit is calculated at each difficult adjustment period
(2016 blocks), and applies to the next 2016 blocks.
 	* Calculate hardLimit by examining the coinbase scriptSig votes of the
previous 12,000 blocks, and taking the 20th percentile and 80th
 	* New hardLimit is the median of the followings:
 	* min(current hardLimit * 1.2, 20-percentile)
 	* max(current hardLimit / 1.2, 80-percentile)
 	* current hardLimit
 	* version 4 block: the coinbase of a version 4 block must match this
pattern: "/BVd+/"
 	* 70% rule: If 8,400 of the last 12,000 blocks are version 4 or
greater, reject invalid version 4 blocks. (testnet4: 501 of last 1000)
 	* 80% rule ("Point of no return"): If 9,600 of the last 12,000 blocks
are version 4 or greater, reject all version <= 3 blocks. (testnet4: 750
of last 1000)
 	* Block version number is calculated after masking out high 16 bits
(final bit count TBD by versionBits outcome).
Jeff Garzik via bitcoin-dev ? 2015-09-02 23:33 ??:

@_date: 2015-09-03 12:32:15
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP 100 specification 
1. I think there is no need to have resolution at byte level, while resolution at MB level is not enough. kB would be a better choice.
2. In my specification a v4 block without a vote is invalid, so there is no need to consider absent or invalid votes
3. We should allow miners to explicitly vote for the status quo, so they don't need to change the coinbase vote every time the size is changed. They may indicate it by /BV/ in the coinbase, and we should look for the first "/BVd*/" instead of "/BVd+/"
4. Alternatively, miners may vote in different styles: /BV1234567/, 1.5MB, the last one is 3MB. The pattern is "/BV(\d+[KM]?)?/"
Tier Nolan via bitcoin-dev ? 2015-09-03 07:59 ??:

@_date: 2015-09-03 14:24:26
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] block size - pay with difficulty 
Assuming that:
1. The current block size is 1MB
2. The block reward for a full block is 25.5BTC including tx fee
3. Miner is required to pay x% of reward penalty if he is trying to increase the size of the next block by x%
If a miner wants to increase the block size by 1 byte, the block size has to increase by 0.0001%, and the penalty will be 0.0000255BTC/byte. For a typical 230byte tx that'd be 0.005865BTC, or 1.35USD at current rate. This is the effective minimum tx fee.
Jeff Garzik ? 2015-09-03 10:18 ??:

@_date: 2015-09-10 13:18:52
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] =?utf-8?q?MAST_with_OP=5FEVAL_and_OP=5FCAT?= 
Inspired by Pieter's Tree Signatures, I believe Merkleized Abstract Syntax Trees (MAST) could be implemented with only OP_CAT and OP_EVAL The idea is very simple. Using a similar example in Pieter's paper,
scriptSig =   Z1 0 1 1 X6 1 K9 0 scriptPubKey = DUP HASH160  EQUALVERIFY EVAL
serialized script = 8 PICK SHA256 (SWAP IF SWAP ENDIF CAT SHA256)*4  EQUALVERIFY EVAL
This will run the 10-th sub-script, when there are 11 sub-scripts in the I think this is the easiest way to enable MAST since the reference implementation for BIP12 is already there. We could enable OP_CAT only inside OP_EVAL so this will be a pure softfork.
Tree Signatures: BIP12:

@_date: 2015-09-17 03:43:02
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
How many years of relative lock time do we need? It really depends why we need a relative lock time in the first place, what what does it offer in addition to CHECKLOCKTIMEVERIFY. The only case I know is when the confirmation taking too long, CLTV may expire before the tx is confirmed. For use case like this, 1 year of relative lock time is much more than enough, since Bitcoin is basically worthless if it takes months to confirm a tx with a reasonable fee.
Is there any other use case of CSV that is irreplaceable by CLTV? There is only one example in the BIP CSV draft.
For the timebased relative lock time, 256 seconds of granularity is more than enough since the block interval is 600s. Although it is not impossible to reduce the block interval in the future, that will be a hardfork anyway and we may just hardfork BIP68/CSV at the same time.
Mark Friedenbach via bitcoin-dev ? 2015-08-27 19:32 ??:

@_date: 2015-09-17 14:41:39
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Fill-or-kill tx is not a new idea and is discussed in the Scaling Bitcoin workshop. In Satoshi's implementation of nLockTime, a huge range of timestamp (from 1970 to 2009) is wasted. By exploiting this unused range and with compromise in the time resolution, a fill-or-kill system could be built with a softfork.
Two new parameters, nLockTime2 and nKillTime are defined:
nLockTime2 (Range: 0-1,853,010)
0: Tx could be confirmed at or after block 420,000
1: Tx could be confirmed at or after block 420,004
719,999: Tx could be confirmed at or after block 3,299,996 (about 55 years from now)
720,000: Tx could be confirmed if the median time-past >= 1,474,562,048 720,001: Tx could be confirmed if the median time-past >= 1,474,564,096 1,853,010 (max): Tx could be confirmed if the median time-past >= 3,794,966,528 (2090-04-04)
nKillTime (Range: 0-2047)
if nLockTime2 < 720,000, the tx could be confirmed at or before block (nLockTime2 + nKillTime * 4)
if nLockTime2 >= 720,000, the tx could be confirmed if the median time-past <= (nLockTime2 - 720,001 + nKillTime) * 2048
Finally, nLockTime = 500,000,000 + nKillTime + nLockTime2 * 2048
Setting a bit flag in tx nVersion will activate the new rules.
The resolution is 4 blocks or 2048s (34m)
The maximum confirmation window is 8188 blocks (56.9 days) or 16,769,024s (48.5 days)
For example:
With nLockTime2 = 20 and nKillTime = 100, a tx could be confirmed only between block 420,080 and 420,480
With nLockTime2 = 730,000 and nKillTime = 1000, a tx could be confirmed only between median time-past of 1,495,042,048 and 1,497,090,048
User's perspective:
A user wants his tx either filled or killed in about 3 hours. He will set a time-based nLockTime2 according to the current median time-past, and set nKillTime = 5
A user wants his tx get confirmed in the block 630000, the first block with reward below 10BTC. He is willing to pay high fee but don't want it gets into another block. He will set nLockTime2 = 210,000 and nKillTime = 0
55 years after?
The height-based nLockTime2 will overflow in 55 years. It is very likely a hard fork will happen to implement a better fill-or-kill system. If not, we could reboot everything with another tx nVersion for another 55

@_date: 2015-09-17 23:27:54
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Peter Todd via bitcoin-dev ? 2015-09-17 18:44 ??:
I think this is the cleanest way to implement the maturity requirement. I understand why we need maturity, However, requiring 100 block maturity will unfortunately make the system much less appealing since the recipient may not like it. A fill-or-kill tx may still be used as the initial funding tx to the Lightning Network, as long as the counterparty is willing to take the extra risk.
Actually, a fill-or-kill tx is slight safer than a coinbase tx, depending on the difference between the absolute kill time and actual confirmation time. In a re-org, an orphaned coinbase tx is permanently invalidated and has no hope to be included again. However, an orphaned fill-or-kill tx may still be confirmed by another miner. If there is still a few days until the absolute kill time, a fill-or-kill tx is basically as safe as a normal tx.
With possibility of re-org and unpredictable block interval in mind, height-based fill-or-kill is not very useful since it is difficult for users to determine the actual kill time. If we could abolish the idea of height-based fill-or-kill, the resolution of time-based fill-or-kill might be improved.
I made a mistake in this example:
The correct nLockTime2 for this example should be 210000/4 = 52500

@_date: 2015-09-18 05:12:27
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Btc Drak ? 2015-09-18 02:42 ??:
If you simply redefine a range of unused nLockTime as nKillTime, users will be constrained to use either nLockTime or nKillTime, but not both in the same tx.
If we are willing to scarify a large range of tx nVersion, say 10-15bits, the nKillTime data could be embedded there.
Another option is nSequence, which will allow per-input nKillTime and The cleanest way, of course, is a hardfork to add a new nKillTime field to the tx so people could use nLockTime and nKillTime in parallel.

@_date: 2015-09-21 09:01:52
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
It is possible to softfork. Just use Iceland time. Iceland time = UTC without DST
Btc Drak via bitcoin-dev ? 2015-09-18 16:34 ??:

@_date: 2015-09-23 07:01:12
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Weekly development meetings on IRC: schedule 
There could not be a worse timing than this for those in China (3-4am), Japan/Korea (4-5am), and Australia (3-6am depends on which part of the country). Maybe we have no dev in this part of the planet? Is there any chance to review the timing in a weekly or monthly basis (also with a doodle vote?)
Will there be any agenda published before the meetings? If I'm really interested in the topics, I'll have some reasons to get up in the middle of the night.
Wladimir J. van der Laan via bitcoin-dev ? 2015-09-22 10:36 ??:

@_date: 2015-09-27 16:26:12
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
+1 for deploying BIP65 immediately without further waiting. Agree with all Peter's points.
If BIP65 has to follow the 0.12 schedule, it will take almost 9 months from now to complete the softfork. I don't see any good reason to wait for that long. We have too much talk, too little action.
Some mining pools hinted that they may adopt BitcoinXT at the end of 2015. If we could start deploying BIP65 earlier, they will have a patched version by the time they switch. Gavin has agreed to support BIP65 in XT.
By the way, is there any chance to backport it to 0.9? In the deployment of BIP66 some miners requested a backport to 0.9 and that's why we have

@_date: 2015-09-28 12:52:51
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Mike Hearn via bitcoin-dev ? 2015-09-28 11:38 ??:
Bypassing IsStandard should be considered as an "expert mode". The message should be "don't bypass it unless you understand what you are By the way, miners are PAID to protect the network. It is their greatest responsibility to follow the development and keep their software up to Let me try to answer this question. Softfork is beneficial to non-mining full nodes as they will follow the majority chain. In the case of a hardfork (e.g. BIP101), non-upgrading full nodes will insist to follow the minority chain. (unless you believe that all non-miner should use an SPV client)
Put it in a different angle. In a softfork, the new fork is a persistent 95% attack against the old fork, which will force all in-cooperating miners to join (or leave). In a hardfork, however, there is no mechanism to stop the old fork and we may have 2 chains co-exist for a long time.
Although it is not mentioned in the whitepaper, the ability to softfork is a feature of Bitcoin. Otherwise, we won't have these OP_NOPs and the original OP_RETURN.

@_date: 2015-09-29 11:59:05
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Jonathan Toomim (Toomim Bros) via bitcoin-dev ? 2015-09-29 09:30 ??:
1. Who told you to accept 1-confirmation tx? Satoshi recommended 6 confirmations in the whitepaper. Take your own risk if you do not follow his advice.
2. This is true only if your SPV client naively follows the longest chain without even looking at the block version. This might be good enough for the 1st generation SPV client, but future generations should at least have basic fraud detecting mechanism.
This point is totally irrelevant. No matter there is a softfork or not, SPV users are always vulnerable to such double-spending attack if they blindly follow the longest chain AND accept 1-confirmation. The fiat currency system might be safer for them.

@_date: 2016-04-02 01:19:53
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP draft: Merkelized Abstract Syntax Tree 
BIP draft: Reference implementation:
This BIP defines a new witness program type that uses a Merkle tree to
encode mutually exclusive branches in a script. This enables complicated
redemption conditions that are currently not possible, improves privacy by
hiding unexecuted scripts, and allows inclusion of non-consensus enforced
data with very low or no additional cost.
The reference implementation is a small and simple patch on top of BIP141
(segwit), however, I have no intention to push this before segwit is
enforced. Instead, I hope the MAST will come with many new op codes,
particularly Schnorr signature.

@_date: 2016-04-19 03:03:07
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
I just realize that if we have OP_CAT, OP_CHECKPRIVATEKEYVERIFY (aka OP_CHECKPRIVPUBPAIR) is not needed (and is probably better for privacy)
Bob has the prikey-x for pubkey-x. Alice and Bob will agree to a random secret nonce, k. They calculate r, in the same way as signing a transaction.
The script is:
SIZE  ADD <0x30> SWAP CAT <0x02|r-length|r> CAT SWAP CAT  CECHKSIGVERIFY  CHECKSIG
To redeem, Bob has to provide:
 <0x02|s-length|s|sighashtype>
With k, s and sighash, Alice (and only Alice) can recover the prikey-x with the well-known k-reuse exploit
(  )
The script will be much cleaner if we remove the DER encoding in the next generation of CHECKSIG
The benefit is prikey-x remains a secret among Alice and Bob. If they don?t mind exposing the prikey-x, they could use r = x coordinate of pubkey-x, which means k = prikey-x ( This would reduce the witness size a little bit as a DUP may be used
Sent: Monday, 29 February, 2016 19:53
This is actually very useful for LN too, see relevant discussion here
Is there much demand for trying to code up a patch to the reference client?  I did a basic one, but it would need tests etc. added.
I think that segregated witness is going to be using up any potential soft-fork slot for the time being anyway.

@_date: 2016-04-23 16:59:32
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Recent editing for the consensus BIPs of segregated 
In the past month, there are a number of revisions in BIP141 and 143. Except
the use of BIP9 for deployment, none of these edits was a result of a change
of consensus behavior of the reference implementation. Instead, the text
were edited for clarifications or corrections to match the reference
implementation. The consensus specification (except the use of BIP9) has not
changed since early Feb 2016, if not earlier.
History of BIP141 and revision proposals
History of BIP143

@_date: 2016-04-29 07:48:10
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP draft: Scripting System in Merkelized Abstract 
This BIP defines the scripting system in Merkelized Abstract Syntax Tree (BIP114). It re-enables some of the previously disabled opcodes, introduces new opcodes, and defines expandable opcodes for future extension.
It will:
This BIP is based on the BIP114 MAST: Reference implementation, including the BIP9 logic and script tests, could be found at  . This branch is rebased on top of the  segwit PR. However, I have not tested the BIP9 activation.
The implementation of the re-enabled opcode are mostly taken from the Elements Project.
This BIP does not describe changes in CHECKSIG (e.g. new hash type, Schnorr sig), which I think should be another BIP.
I have also considered more radical changes. For example, make all comparison opcode to be ?VERIFY? type, and a script passes if and only if the stack is exactly empty after evaluation.

@_date: 2016-04-29 09:25:28
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP144: use of 1<<3 service bit 
Please note that the BIP144 has just been revised to match the implementation in  We will use the 1<<3 service bit (NODE_WITNESS) to signal the readiness for segwit.

@_date: 2016-08-16 06:10:01
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Low S values signatures 
Hash: SHA512
As discussed in the 11 Aug 2016 IRC meeting ( a new BIP with implementation is prepared to make low S value signature as a consensus rule:
The softfork is proposed to be deployed with segwit (BIP141), likely in v0.13.1
The text is copied below
  BIP: ?
  Title: Low S values signatures
  Author: Pieter Wuille           Johnson Lau   Status: Draft
  Type: Standards Track
  Created: 2016-08-16
This document specifies proposed changes to the Bitcoin transaction validity rules to restrict signatures to using low S values.
ECDSA signatures are inherently malleable as taking the negative of the number S inside (modulo the curve order) does not invalidate it. This is a nuisance malleability vector as any relay node on the network may transform the signature, with no access to the relevant private keys required. For non-segregated witness transactions, this malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).
To fix this malleability, we require that the S value inside ECDSA signatures is at most the curve order divided by 2 (essentially restricting this value to its lower half range). The value S in signatures must be between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive). If S is too high, simply replace it by S' = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141 - S.
Every signature passed to OP_CHECKSIG, OP_CHECKSIGVERIFY, OP_CHECKMULTISIG, or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied, MUST use a S value between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive) with strict DER encoding (see BIP66).
These operators all perform ECDSA verifications on pubkey/signature pairs, iterating from the top of the stack backwards. For each such verification, if the signature does not pass the IsLowDERSignature check, the entire script evaluates to false immediately. If the signature is valid DER with low S value, but does not pass ECDSA verification, opcode execution continues as it used to, causing opcode execution to stop and push false on the stack (but not immediately fail the script) in some cases, which potentially skips further signatures (and thus does not subject them to IsLowDERSignature).
This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.
For Bitcoin mainnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).
For Bitcoin testnet, the BIP9 starttime will be midnight 1 May 2016 UTC (Epoch timestamp 1462060800) and BIP9 timeout will be midnight 1 May 2017 UTC (Epoch timestamp 1493596800).
The reference client has produced compatible signatures since v0.9.0, and the requirement to have low S value signatures has been enforced as a relay policy by the reference client since v0.11.1. As of August 2016, very few transactions violating the requirement are being added to the chain. In addition, every non-compliant signature can trivially be converted into a compliant one, so there is no loss of functionality by this requirement. This proposal has the added benefit of reducing transaction malleability.
An implementation for the reference client is available at This document is extracted from the previous BIP62 proposal which had input from various people.
This document is placed in the public domain.

@_date: 2016-08-16 13:46:11
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Low S values signatures 
That's an implicit CHECKSIG. Will clarify.
IsLowDERSignature is the function in Bitcoin Core. That's a bit complicated as the real checking function is not directly called. I'll clarify.

@_date: 2016-08-16 13:53:08
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF malleability 
Hash: SHA512
A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in P2WSH:
   BIP: x
   Title: Dealing with OP_IF and OP_NOTIF malleability in P2WSH
   Author: Johnson Lau    Status: Draft
   Type: Standards Track
   Created: 2016-08-17
This document specifies proposed changes to the Bitcoin script validity rules in order to make transaction malleability related to OP_IF and OP_NOTIF impossible in pay-to-witness-script-hash (P2WSH) scripts.
OP_IF and OP_NOTIF are flow control codes in the Bitcoin script system. The programme flow is decided by whether the top stake value is True or False. However, this behaviour opens a source of malleability as a third party may replace a True (False) stack item with any other True (False) value without invalidating the transaction.
The proposed rules apply only to pay-to-witness-script-hash (P2WSH) scripts described in BIP141, which has not been activated on the Bitcoin mainnet as of writing. To ensure OP_IF and OP_NOTIF transactions created before the introduction of this BIP will still be accepted by the network, the new rules are not applied to non-segregated witness scripts.
In P2WSH, the argument for OP_IF and OP_NOTIF MUST be exactly an empty vector or 0x01, or the script evaluation fails immediately.
This is deployed using BIP9 after segregated witness (BIP141) is activated. Details TBD.
This is a softfork on top of BIP141. The rules are enforced as a relay policy by the reference client since the first release of BIP141 (v0.13.1). To avoid risks of fund loss, users MUST NOT create P2WSH scripts that are incompatible with this BIP. An OP_0NOTEQUAL may be used before OP_IF or OP_NOTIF to imitate the original behaviour (which may also re-enable the malleability vector depending on the exact script).
This work is placed in the public domain.

@_date: 2016-08-16 23:02:53
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
Not really. I think the goal is to protect as many common scripts as possible.
For example:
1) BIP146 (Low S values signatures) will eliminate all malleability for P2WPKH
2) BIP146 + null dummy value for CHECKMULTISIG ("NULLDUMMY") will eliminate all malleability for simple multi-sig in P2WSH. This is particularly interesting since without NULLDUMMY, attackers are able to replace the dummy value with anything.
3) BIP146 + NULLDUMMY + minimal IF argument ("MINIMALIF") will eliminate malleability for any Lightening Network scripts that I'm aware of.
With 3), 99.99% of segwit transactions in foreseeable future should be fully protected.
The plan is to implement MINIMALIF as a relay policy first, and enforce the softfork after further risks assessment. This BIP serves as a warning to users for not using incompatible script.
Peter Todd:
I believe all Lightening Network scripts (the only real users of IF/NOTIF in foreseeable future) are already compatible with MINIMALIF. It may not be a good idea for them to spend 1 more byte to get protected.
If people want to have the original OP_IF behaviour, a simple way would be using "0NOTEQUAL IF". However, this works only if the argument is a valid number (also beware of MINIMALDATA rule in BIP62).
To completely replicate the original behaviour, one may use:
"DEPTH TOALTSTACK IFDUP DEPTH FROMALTSTACK NUMNOTEQUAL IF 2DROP {if script} ELSE DROP {else script} ENDIF"
This is because we don't have a simple OP_CASTTOBOOL, and IFDUP is 1 of the 4 codes that perform CastToBool on top stack item (the others are VERIFY, IF, and NOTIF; and VERIFY can't be used here since it terminates the script with a False).

@_date: 2016-08-17 06:00:37
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP draft: HTLC transactions 
This is incompatible with my proposal for fixing the OP_IF/NOTIF malleability in segwit ("MINIMALIF"). In this case only the timeout branch may be executed.
To make it compatible, you may use one of the following 2 scripts:
OP_SIZE OP_0NOTEQUAL
 [HASHOP]  OP_EQUALVERIFY
  [TIMEOUTOP] OP_DROP
 [HASHOP]  OP_EQUALVERIFY
  [TIMEOUTOP] OP_DROP
The overall witness size are the same for these scripts. They are 1 byte larger than Luke's script, in case MINIMALIF is not enforced.
(btw, the OP_DROP after TIMEOUTOP is missing in Luke's script)

@_date: 2016-08-17 06:15:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
It is ugly only if you want to faithfully replicate the behaviour. I'd argue that in no real use case you need to do this. For example, "OP_SIZE OP_IF" could just become "OP_SIZE OP_0NOTEQUAL OP_IF", since OP_SIZE must return a valid MINIMALDATA number.
And your workaround does not fix malleability, since any non-0x01 values are valid FALSE
However, in some case, enforcing MINIMALIF does require 1 more witness byte: I think the best strategy is to make it a relay policy first, and decide whether we want a softfork later.

@_date: 2016-08-17 08:43:47
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Low S values signatures 
The BIP146 has been updated to include NULLDUMMY* as part of the softfork:
NULLDUMMY is a trivial softfork to fix malleability related to the extra stack element consumed by CHECKMULTISIG(VERIFY). It is probably more important than LOW_S since without that an attacker may replace the stack element with any value.

@_date: 2016-08-24 21:49:34
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Attack by modifying non-segwit transactions after 
Adding witness data to a non-segwit script is invalid by consensus:
This PR will detect such violation early and ban the peer:
Another approach is to run the scripts of all incoming transactions. That's not too bad as you have already fetched the utxos which is a major part of validation.

@_date: 2016-12-02 01:20:31
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Hardfork warning system 
This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.
For better formatting, visit github
BIP: ?
Title: Hardfork warning system
Author: Johnson Lau Status: Draft
Type: Standard
Created: 2016-12-01
This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.
Softfork and hardfork are the 2 majors categories of consensus rules change. Generally, softforks make some previously valid blocks invalid, while hardforks make some previously invalid blocks valid. Bitcoin has successfully introduced a number of new functions through softforks. A built-in warning system is also available in many implementations to warn users for the activation of any unknown softforks.
Some features, however, may not be easily introduced with a softfork. Examples include expanding maximum block resources limits, and changing the average block time interval. When such features are implemented with a hardfork, existing full node implementations would consider such blocks as invalid, and may even ban a peer for relaying such blocks. They are effectively blind to such hardfork rule changes, leaving users to unknowingly transact on a system with potentially different token value. On the other hand, light nodes may blindly follow a hardfork with unknown rule changes and lose the right to choose the previous system.
This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.
Valid block
A block that satisfies all the consensus rules being enforced by a bitcoin protocol implementation. An implementation may intentionally (e.g. a light node) or unintentionally (e.g. unaware of a softfork) not enforcing any part of the current netwrok rules.
Valid blockchain
A blockchain constituting of only valid blocks.
Best valid blockchain
The valid blockchain with highest total proof-of-work.
Valid blockchain fork
A valid blockchain sharing a common ancestral block with the best valid blockchain, but with less total proof-of-work
Generalized block header
Any serialized hexadecimal data with exactly 80 bytes (byte 0 to byte 79). The bytes 4 to 35 are the double-SHA256 hash of another generalized block header. The bytes 72 to 75 are nBits, the target of this generalized block header encoded in the same way as normal bitcoin block header. The 2 most significant bits of the byte 3 are the hardfork notification bits. The semantics of other data in a generalized block header is not defined in any general way. It should be noted that a normal bitcoin block header is a special case of generalized block header.
Generalized block header chain
A chain of generalized block header. A header chain of valid blocks is a special case of a generalized block header chain.
Block nVersion softfork
A softfork is deployed to restrict the valid value of block nVersion. Upon activation, any block with the second highest nVersion bit set becomes invalid (nVersion & 0x40000000)
This softfork will be deployed by "version bits" BIP9 with the name "hfbit" and using bit 2.
For Bitcoin mainnet, the BIP9 starttime will be midnight TBC UTC (Epoch timestamp TBC) and BIP9 timeout will be midnight TBC UTC (Epoch timestamp TBC).
For Bitcoin testnet, the BIP9 starttime will be midnight TBC UTC (Epoch timestamp TBC) and BIP9 timeout will be midnight TBC UTC (Epoch timestamp TBC).
Any bitcoin implementation (full nodes and light nodes) supporting this softfork should also implement a hardfork warning system described below.
Validation of generalized block header
A bitcoin protocol implementation should consider a generalized block header as valid if it satisfies all of the following criteria:
In general, a bitcoin protocol implementation should keep an index of all known generalized block header chains, along with the valid blockchain(s). However, if a generalized block header chain is grown on top of a very old valid block, with total proof-of-work much lower than the current best valid bloackchain, it may be safely discarded.
Hardfork warning system in full nodes
Hardfork with unknown rules
If a generalized block header chain with non-trivial total proof-of-work is emerging, and is not considered as a valid blockchain, a hardfork with unknown rules may be happening.
A wallet implementation should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given. It should not attempt to conduct transactions on or otherwise interpreting any block data of the hardfork with unknown rules.
A mining implementation should issue a warning to its operator. Until further instructions are given, it may either stop mining, or ignore the hardfork with unknown rules. It should not attempt to confirm a generalized block header with unknown rules.
Setting of one or both hardfork notification bits is, as defined by BIP34 and this BIP, a hardfork, and should be considered as an indication of a planned hardfork. If a hardfork with unknown rules is happening without any hardfork notification bits set, it is probably an accidental consensus failure, such as the March 2013 fork due to a block database bug (BIP50), and the July 2015 fork following the BIP66 activation.[2]
Hardfork with multiple valid blockchains
If a valid blockchain fork is emerging with non-trivial total proof-of-work, a consensus disagreement may be happening among different miners.
A wallet implementation should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given.
A mining implementation should issue a warning to its operator. Until further instructions are given, it may either stop mining, or mine on top of the best valid chain by its own standard.
Hardfork warning system in light nodes
Light node (usually wallet implementations) is any bitcoin protocol implementations that intentionally not fully enforcing the network rules. As an important part of the hardfork warning system, a light node should observe the hardfork notification bits in block header, along with any other rules it opts to validate. If any of the hardfork notification bits is set, it should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given. It should not attempt to conduct transactions on or otherwise interpreting any block data of the hardfork blockchain, even if it might be able to decode the block data.
Hardfork notification bits
There are 2 hardfork notification bits defined in this BIP. The higher bit has been forbidden since BIP34, and the lower bit is disabled by this BIP. For nodes supporting this BIP, the semantics of the 2 bits are the same: a hardfork is happening. For legacy node, however, setting the higher bit would make them fail to follow the hardforking chain. In a soft-hardfork design (described below), the lower notification bit should be used.
The hardfork warning system is able to detect the following types of hardforks:
Soft-hardfork (with the lower hardfork notification bit)
A soft-hardfork is a technique to implement a hardfork by pretending to create blocks with only a zero output value coinbase transaction, and commit the real transaction Merkle root in the coinbase scriptSig field. With the lower hardfork notification bit set, a node following this BIP will consider this as a hardfork and enter the safe mode, while a legacy node not following this BIP will be effectively broken due to seeing the continuously empty blockchain.
Redefining the nTime field
As the warning system does not interpret the nTime field, redefining it through a hardfork would be detectable. For example, overflow may be allowed to overcome the year 2106 problem.
Redefining the Merkle root hash field and changing block content validation rules
The 32-byte Merkle root hash could be redefined, for example, with a different hashing algorithm. Any block resources limitation and transaction validation rules may also be changed. All such hardforks would be detected by the warning system.
Changing average block interval or difficulty reset
Since the warning system is not bound to a particular proof-of-work target adjustment schedule, a hardfork changing the average block interval or resetting the difficulty will be detectable.
Introducing secondary proof-of-work
Introducing secondary proof-of-work (with non-SHA256 algorithm or fixing the block withholding attack against mining pools) may be detectable, as long as the generalized block header format is preserved.
Accidental hardfork
An accidental hardfork may be detectable, if the generalized block headers in both forks are valid but no hardfork notification bit is set.
The only function of this system is to inform the users that a hardfork might be happening and prompt for further instructions. It does not guarantee that the hardfork will be successful and not end up with two permanent incompatible forks. This requires broad consensus of the whole community and is not solvable with technical means alone.
The following types of hardfork are not detectable with this warning system:
Backward compatibility
The softfork described in the BIP would only affect miners. As the disabled nVersion bit is never used in the main network, it is unlikely that any miner would unintentionally find an invalid block due to the new rules.
BIP9 is disabled when any of the hardfork notification bits is set, which may interrupt any ongoing softfork support signalling process. Developers should pay attention to this when desinging a hardfork. For example, they may redefine the counting of signal, or move the signalling bitfield to a different location.
Legacy nodes would not be benefited from this softfork and warning system. However, no additional risks are introduced to legacy node either.
Reference implementation
To be done
This document is placed in the public domain.

@_date: 2016-12-05 03:34:00
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new header 
Based on Luke Dashjr?s code and BIP:  , I created an experimental network to show how a new header format may be implemented.
Basically, the header hash is calculated in a way that non-upgrading nodes would see it as a block with only the coinbase tx and zero output value. They are effectively broken as they won?t see any transactions confirmed. This allows rewriting most of the rules related to block and transaction validity. Such technique has different names like soft-hardfork, firmfork, evil softfork, and could be itself a controversial topic. However, I?d rather not to focus on its soft-hardfork property, as that would be trivial to turn this into a true hardfork (e.g. setting the sign bit in block nVersion, or setting the most significant bit in the dummy coinbase nLockTime)
Instead of its soft-HF property, I think the more interesting thing is the new header format. The current bitcoin header has only 80 bytes. It provides only 32bits of nonce space and is far not enough for ASICs. It also provides no room for committing to additional data. Therefore, people are forced to put many different data in the coinbase transaction, such as merge-mining commitments, and the segwit commitment. It is not a ideal solution, especially for light wallets.
Following the practice of segwit development of making a experimental network (segnet), I made something similar and call it the Forcenet (as it forces legacy nodes to follow the post-fork chain)
The header of forcenet is mostly described in Luke?s BIP, but I have made some amendments as I implemented it. The format is (size in parentheses; little endian):
Height (4), BIP9 signalling field (4), hardfork signalling field (3), merge-mining hard fork signalling field (1), prev hash (32), timestamp (4), nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), Hash TMR (32), Hash WMR (32), total tx size (8) , total tx weight (8), total sigops (8), number of tx (4), merkle branches leading to header C (compactSize + 32 bit hashes)
In addition to increasing the max block size, I also showed how the calculation and validation of witness commitment may be changed with a new header. For example, since the commitment is no longer in the coinbase tx, we don?t need to use a 0000?.0000 hash for the coinbase tx like in BIP141.
Something not yet done:
1. The new merkle root algorithm described in the MMHF BIP
2. The nTxsSigops has no meaning currently
3. Communication with legacy nodes. This version can?t talk to legacy nodes through the P2P network, but theoretically they could be linked up with a bridge node
4. A new block weight definition to provide incentives for slowing down UTXO growth
5. Many other interesting hardfork ideas, and softfork ideas that works better with a header redesign
For easier testing, forcenet has the following parameters:
Hardfork at block 200
Segwit is always activated
1 minutes block with 40000 (prefork) and 80000 (postfork) weight limit
50 blocks coinbase maturity
21000 blocks halving
144 blocks retarget
How to join: codes at  , start with "bitcoind ?forcenet" .
Connection: I?m running a node at 8333.info with default port (38901)
Mining: there is only basic internal mining support. Limited GBT support is theoretically possible but needs more hacking. To use the internal miner, writeup a shell script to repeatedly call ?bitcoin-cli ?forcenet generate 1?
New RPC commands: getlegacyblock and getlegacyblockheader, which generates blocks and headers that are compatible with legacy nodes.
This is largely work-in-progress so expect a reset every couple weeks

@_date: 2016-12-14 18:55:39
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
I think the biggest problem of sum tree is the lack of flexibility to redefine the values with softforks. For example, in the future we may want to define a new CHECKSIG with witness script version 1. That would be counted as a SigOp. Without a sum tree design, that?d be easy as we could just define new SigOp through a softfork (e.g. the introduction of P2SH SigOp, and the witness v0 SigOp). In a sum tree, however, since the nSigOp is implied, any redefinition requires either a hardfork or a new sum tree (and the original sum tree becomes a placebo for old nodes. So every softfork of this type creates a new tree)
Similarly, we may have secondary witness in the future, and the tx weight would be redefined with a softfork. We will face the same problem with a sum tree
The only way to fix this is to explicitly commit to the weight and nSigOp, and the committed value must be equal to or larger than the real value. Only in this way we could redefine it with softfork. However, that means each tx will have an overhead of 16 bytes (if two int64 are used)
You could find related discussion here:  Maybe we could make this optional: for nodes running exactly the same rules, they could omit the weight and nSigOp value in transmission. To talk to legacy nodes, they need to transmit the newly defined weight and nSigOp. But this makes script upgrade much complex.

@_date: 2016-12-14 19:01:58
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
There is no reason to use a timestamp beyond 4 bytes. Just let it overflow. If a blockchain is stopped for more than 2^31 seconds, it?s just dead.

@_date: 2016-12-14 19:11:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
I foresee there will be 2 types of headers under this system: the 80 bytes short header and the variable length full header. Short headers are enough to link everything up. SPV needs the full header only if they are interested in any tx in a block. you could omit the transmission of nHeight, as it is implied (saving 4bytes). Storing nHeight of headers is what every full and SPV nodes would do anyway
Yes, I agree with you that these are not particularly useful. Sum tree is more useful but it has other problems (see my other reply)
Related discussion:  No need. See my other reply

@_date: 2016-12-14 19:12:45
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
You could steal a few bits form tx nVersion through a softfork

@_date: 2016-12-14 23:45:37
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
I think that?s too much tech debt just for softforkability.
The better way would be making the sum tree as an independent tree with a separate commitment, and define a special type of softfork (e.g. a special BIP9 bit). When the softfork is activated, the legacy full node will stop validating the sum tree. This doesn?t really degrade the security by more than a normal softfork, as the legacy full node would still validate the total weight and nSigOp based on its own rules. The only purpose of the sum tree is to help SPV nodes to validate. This way we could even completely redefine the structure and data committed in the sum tree.
I?d like to combine the size weight and sigOp weight, but not sure if we could. The current size weight limit is 4,000,000 and sigop limit is 80,000. It?s 50:1. If we maintain this ratio, and define
weight = n * (total size +  3 * base size) + sigop , with n = 50
a block may have millions of sigops which is totally unacceptable.
On the other hand, if we make n too low, we may allow either too few sigop, or a too big block size.
Signature aggregation will make this a bigger problem as one signature may spend thousands of sigop

@_date: 2016-02-04 12:14:49
@_author: jl2012 
@_subject: [bitcoin-dev] Hardfork bit BIP 
This document specifies a proposed change to the semantics of the sign
bit of the "version" field in Bitcoin block headers, as a mechanism to
indicate a hardfork is deployed. It alleviates certain risks related to
a hardfork by introducing an explicit "point of no return" in the
blockchain. This is a general mechanism which should be employed by any
planned hardfork in the future.  [1]MOTIVATION
Hardforks in Bitcoin are usually considered as difficult and risky,
because:  	* Hardforks require not only support of miners, but also, most
importantly, supermajority support of the Bitcoin economy. As a result,
softfork deployment mechanisms described in BIP 34 [2] or BIP 9 [3] are
not enough for introducing hardforks safely.
 	* Full nodes and SPV nodes following original consensus rules may not
be aware of the deployment of a hardfork. They may stick to an
economic-minority fork and unknowingly accept devalued legacy tokens.
 	* In the case which the original consensus rules are also valid under
the new consensus rules, users following the new chain may unexpectedly
reorg back to the original chain if it grows faster than the new one.
People may find their confirmed transactions becoming unconfirmed and
lose money.
The first issue involves soliciting support for a hardfork proposal,
which is more a political topic than a technical one. This proposal aims
at alleviating the risks related to the second and third issues. It
should be employed by any planned hardfork in the future.
 [4]DEFINITIONS
See BIP99 [5]  [6]SPECIFICATION
HARDFORK BIT The sign bit in nVersion is defined as the hardfork bit.
Currently, blocks with this header bit setting to 1 are invalid, since
BIP65 [7] interprets nVersion as a signed number and requires it to be ?
4. Among the 640 bits in the block header, this is the only one which is
fixed and serves no purpose, and therefore the best way to indicate the
deployment of a hardfork. FLAG BLOCK Any planned hardfork must have one and only one flag block
which is the "point of no return". To ensure monotonicity, flag block
should be determined by block height, or as the first block with
GetMedianTimePast() greater than a threshold. Other mechanisms could be
difficult for SPV nodes to follow. The height/time threshold could be a
predetermined value or relative to other events (e.g. 10000 blocks / 100
days after 95% of miner support). The exact mechanism is out of the
scope of this BIP. No matter what mechanism is used, the threshold is
consensus critical. It must be publicly verifiable with only blockchain
data, and preferably SPV-friendly (i.e. verifiable with block headers
only, without downloading any transaction). Flag block is constructed in a way that nodes with the original
consensus rules must reject. On the other hand, nodes with the new
consensus rules must reject a block if it is not a flag block while it
is supposed to be. To achieve these goals, the flag block must  	* have the hardfork bit setting to 1, and
 	* follow any other rules required by the hardfork
If these conditions are not fully satisfied, upgraded nodes shall reject
the block.
The hardfork bit must be turned off in the successors of the flag block,
until the deployment of the next hardfork. Although a hardfork is officially deployed when flag block is generated,
the exact behavioural change is out of the scope of this BIP. For
example, a hardfork may not be fully active until certain time after the
flag block. CONCURRENT HARDFORK PROPOSALS To avoid confusion and unexpected
behaviour, a flag block should normally signify the deployment of only
one hardfork. Therefore, a hardfork proposal has to make sure that its
flag block threshold is not clashing with other ongoing hardfork
proposals. In the case that the version bits mechanism is used in deploying a
hardfork, height of the flag block should take a value of32N + B, where
N is a positive integer and B is the position of bit B defined in BIP9
[8]. This guarantees that no clash may happen with another hardfork
proposal using BIP9. UNCONTROVERSIAL SUBTLE HARDFORKS Hardforks may sometimes be totally
uncontroversial and make barely noticeable change (BIP50 [9], for
example). In such cases, the use of hardfork bit may not be needed as it
may cause unnecessary disruption. The risk and benefit should be
evaluated case-by-case. AUTOMATIC WARNING SYSTEM When a flag block for an unknown hardfork is
found on the network, full nodes and SPV nodes should alert their users
and/or stop accepting/sending transactions. It should be noted that the
warning system could become a denial-of-service vector if the attacker
is willing to give up the block reward. Therefore, the warning may be
issued only if a few blocks are built on top of the flag block in a
reasonable time frame. This will in turn increase the risk in case of a
real planned hardfork so it is up to the wallet programmers to decide
the optimal strategy. Human warning system (e.g. the emergency alert
system in Bitcoin Core) could fill the gap.  [10]COMPATIBILITY
As a mechanism to indicate hardfork deployment, this BIP breaks backward
compatibility intentionally. However, without further changes in the
block header format, full nodes and SPV nodes could still verify the
Proof-of-Work of a flag block and its successors. HARDFORK INVOLVING CHANGE IN BLOCK HEADER FORMAT If a hardfork involves
a new block header format, the original format should still be used for
the flag block and a reasonable period afterwards, to make sure existing
nodes realize that an unknown hardfork has been deployed. VERSION BITS This proposal is also compatible with the BIP9. The version
bits mechanism could be employed to measure miner support towards a
hardfork proposal, and to determine the height or time threshold of the
flag block. Also, miners of the flag block may still cast votes for
other concurrent softfork or hardfork proposals as normal. POINT OF NO RETURN After the flag block is generated, a miner may
support either the original rules or the new rules, but not both. It is
not possible for miners in one fork to attack or overtake the other fork
without giving up the mining reward of their preferred fork.  [11]COPYRIGHT
This document is placed in the public domain. [2] [3] [5] [7] [9]

@_date: 2016-02-04 12:56:42
@_author: jl2012 
@_subject: [bitcoin-dev] Hardfork bit BIP 
Gavin Andresen ? 2016-02-04 12:36 ??:
Thanks for your comments.
In the case of a softfork, as long as an user waits for a few confirmations, the risk of money loss is very low. In the worst case they run a full node with SPV security. In the case of a hardfork, the consequence of failing to upgrade to the economic majority fork *is* fatal, even if an user waits for 1000 confirmations. Not to mention the risk of having 2 economically active forks. That's why wallets should STOP accepting and sending tx after a hardfork bit is detected and wait for users' instructions.
No, the "triggering block" you mentioned is NOT where the hardfork starts. Using BIP101 as an example, the hardfork starts when the first  >1MB is mined. For people who failed to upgrade, the "grace period" is always zero, which is the moment they realize a hardfork.

@_date: 2016-02-05 13:40:57
@_author: jl2012 
@_subject: [bitcoin-dev] BIP draft: Hard fork opt-in mechanism for SPV nodes 
BIP draft: Hard fork opt-in mechanism for SPV nodes:
This is a supplement, instead of a replacement, of the hardfork bit BIP:
They solves different problems:
The hardfork bit tells full and SPV that a planned hardfork (instead of
a softfork) has happened.
This BIP makes sure SPV nodes won't lose any money in a hardfork, even
if they do not check the hardfork bit.
BIP: ?
Title: Hard fork opt-in mechanism for SPV nodes
Author: Johnson Lau Status: Draft
Type: Standard Track
Created: 2016-02-05
This document specifies a new algorithm for the transaction commitment
in block header, to ensure that SPV nodes will not automatically follow
a planned hard fork without explicit opt-in consent.  [1]MOTIVATION
A hard fork in Bitcoin is a consensus rule change where previously
invalid blocks become valid. For the operators of fully validating
nodes, migration to the new fork requires conscious actions. However,
this may not be true for SPV node, as many consensus rules are
transparent to them. SPV nodes may follow the chain with most
proof-of-work, even if the operators do not agree with the economical or
ideological properties of the chain. By specifying a new algorithm for the transaction commitment in block
header, migration to the new fork requires explicit opt-in consent for
SPV nodes. It is expected that this proposal will be implemented with
other backward-incompatible consensus rule changes at the same time.  [2]SPECIFICATION
The calculation of Merkle root remains unchanged. Instead of directly
committing the Merkle root to the header, we commit  Double-SHA256(zero|merkle_root|zero)
where zero is 0x0000....0000 with 32 bytes.  [3]RATIONALE
Since the header structure is not changed, non-upgraded SPV nodes will
still be able to verify the proof-of-work of the new chain, and they
will follow the new chain if it has most proof-of-work. However, they
will not be able to the accept any incoming transactions on the new
chain since they cannot verify them with the new commitment format. At
the same time, SPV nodes will not accept any new transactions on the old
chain, as they find it has less proof-of-work. Effectively, SPV nodes
stop accepting any transactions, until their operators take further
actions. Zero-padding is applied before and after the merkle_root, so it is not
possible to circumvent the rule change with any current implementations,
even for faulty ones. A future hard fork should change the padding value to stop non-upgraded
SPV nodes from processing new transactions. Hard forks may sometimes be totally uncontroversial and make barely
noticeable change (BIP50 [4], for example). In such cases, changing the
padding value may not be needed as it may cause unnecessary disruption.
The risk and benefit should be evaluated case-by-case.  [5]COMPATIBILITY
As a mechanism to indicate hard fork deployment, this BIP breaks
backward compatibility intentionally. However, without further changes
in the block header format, non-upgraded full nodes and SPV nodes could
still verify the proof-of-work of upgraded blocks. INTERACTION WITH FRAUD PROOF SYSTEM A fraud proof system is full nodes
that will generate compact proofs to testify invalid blocks on the
blockchain, verifiable by SPV nodes. Hard forks without any malicious
intention may also be considered as a "fraud" among non-upgraded nodes.
This may not be desirable, as the SPV node may accept devalued tokens on
the old chain with less proof-of-work. With this BIP, non-upgraded SPV
nodes will always believe the new chain is valid (since they cannot
verify any fraud proof), while cannot be defrauded as they will not see
any incoming transactions.  [6]COPYRIGHT
This document is placed in the public domain. [4]

@_date: 2016-02-08 01:24:25
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP proposal: Increase block size limit to 
You are making a very na?ve assumption that miners are just looking for
profit for the next second. Instead, they would try to optimize their short
term and long term ROI. It is also well known that some miners would mine at
a loss, even not for ideological reasons, if they believe that their action
is beneficial to the network and will provide long term ROI. It happened
after the last halving in 2012. Without any immediate price appreciation,
the hashing rate decreased by only less than 10%
[mailto:bitcoin-dev-bounces at lists.linuxfoundation.org] On Behalf Of Jonathan
Toomim via bitcoin-dev
Sent: Monday, 8 February, 2016 01:11
On Feb 7, 2016, at 7:19 AM, Anthony Towns via bitcoin-dev
The stated reasoning for 75% versus 95% is "because it gives "veto power"
to a single big solo miner or mining pool". But if a 20% miner wants to
"veto" the upgrade, with a 75% threshold, they could instead simply use
their hashpower to vote for an upgrade, but then not mine anything on
the new chain. At that point there'd be as little as 55% mining the new
2MB chain with 45% of hashpower remaining on the old chain. That'd be 18
minute blocks versus 22 minute blocks, which doesn't seem like much of
a difference in practice, and at that point hashpower could plausibly
end up switching almost entirely back to the original consensus rules
prior to the grace period ending.
Keep in mind that within a single difficulty adjustment period, the
difficulty of mining a block on either chain will be identical. Even if the
value of a 1MB branch coin is $100 and the hashrate on the 1 MB branch is
100 PH/s, and the value of a 2 MB branch coin is $101 and the hashrate on
the 2 MB branch is 1000 PH/s, the rational thing for a miner to do (for the
first adjustment period) is to mine on the 2 MB branch, because the miner
would earn 1% more on that branch.
So you're assuming that 25% of the hashrate chooses to remain on the
minority version during the grace period, and that 20% chooses to switch
back to the minority side. The fork happens. One branch has 1 MB blocks
every 22 minutes, and the other branch has 2 MB blocks every 18 minutes. The
first branch cannot handle the pre-fork transaction volume, as it only has
45% of the capacity that it had pre-fork. The second one can, as it has 111%
of the pre-fork capacity. This makes the 1 MB branch much less usable than
the 2 MB branch, which in turn causes the market value of newly minted coins
on that branch to fall, which in turn causes miners to switch to the more
profitable 2MB branch. This exacerbates the usability difference, which
exacerbates the price difference, etc. Having two competing chains with
equal hashrate using the same PoW function and nearly equal features is not
a stable state. Positive feedback loops exist to make the vast majority of
the users and the hashrate join one side.
Basically, any miners who stick to the minority branch are going to lose a
lot of money.

@_date: 2016-02-08 01:53:53
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Pre-BIP Growth Soft-hardfork 
This looks very interesting. The first time implementing it might be more
painful but that will make subsequent hardforks a lot easier.
Do you think it's good to include the median timestamp of the past 11 blocks
after the block height in coinbase? That would make it easier to use it as
activation threshold of consensus rule changes.
For the witness commitment, it will also be treated as a merge mined
It is also good to emphasize that it is the responsibility of miners, not
devs, to ensure that the hardfork is accepted by the supermajority of the
-----Original Message-----
[mailto:bitcoin-dev-bounces at lists.linuxfoundation.org] On Behalf Of Luke
Dashjr via bitcoin-dev
Sent: Sunday, 7 February, 2016 17:53
Here's a draft BIP I wrote almost a year ago. I'm going to look into
revising and completing it soon, and would welcome any suggestions for doing
This hardfork BIP aims to accomplish a few important things:
- Finally deploying proper merge-mining as Satoshi suggested before he left.
- Expanding the nonce space miners can scan in-chip, avoiding expensive
  calculations on the host controller as blocks get larger.
- Provide a way to safely deploy hardforks without risking leaving old nodes
  vulnerable to attack.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2016-02-08 03:27:48
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] Hardfork bit BIP 
Sent: Friday, 5 February, 2016 06:16
It is unfortunate SPV clients are not following that. However, they SHOULD follow that. It becomes a self fulfilling prophecy if we decide not to do that if SPV are not following that.
It will distinguish between a planned hardfork and an accidental hardfork, and full nodes may react differently. Particularly, a planned unknown hardfork is a strong indication that the original chain has become economic minority and the non-upgraded full node should stop accepting incoming tx immediately.
Same for not-upgraded full nodes following not-upgraded full nodes. Anyway, the header with enough PoW should still be propagated.
Normal version number only suggests softforks, which is usually not a concern for SPV clients. An unknown hardfork is a completely different story as the values of the forks are completely unknown.
Yes, they should.

@_date: 2016-02-09 04:37:36
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] On Hardforks in the Context of SegWit 
Thanks for this proposal. Just some quick response:
1. The segwit hardfork (BIP HF) could be deployed with BIP141 (segwit
softfork). BIP141 doesn't need grace period. BIP HF will have around 1 year
of grace period.
2. Threshold is 95%. Using 4 versoin bits: a) BIP 141; b) BIP HF; c) BIP 141
if BIP HF has already got 95%; d) BIP HF if BIP141 has already got 95%.
Voting a and c (or b and d) at the same time is invalid. BIP 141 is
activated if a>95% or (a+c>95% and b+d>95%). BIP HF is activated if b>95% or
(a+c>95% and b+d>95%).
3. Fix time warp attack: this may break some SPV implementation
4. Limiting non-segwit inputs may make some existing signed tx invalid. My
proposal is: a) count the number of non-segwit sigop in a tx, including
those in unexecuted branch (sigop); b) measure the tx size without scripgSig
(size); c) a new rule is SUM(sigop*size) < some_value . This allows
calculation without actually running the script.
-----Original Message-----
[mailto:bitcoin-dev-bounces at lists.linuxfoundation.org] On Behalf Of Matt
Corallo via bitcoin-dev
Sent: Tuesday, 9 February, 2016 03:27
Hi all,
I believe we, today, have a unique opportunity to begin to close the book on
the short-term scaling debate.
First a little background. The scaling debate that has been gripping the
Bitcoin community for the past half year has taken an interesting turn in
2016. Until recently, there have been two distinct camps - one proposing a
significant change to the consensus-enforced block size limit to allow for
more on-blockchain transactions and the other opposing such a change,
suggesting instead that scaling be obtained by adding more flexible systems
on top of the blockchain. At this point, however, the entire Bitcoin
community seems to have unified around a single vision - roughly 2MB of
transactions per block, whether via Segregated Witness or via a hard fork,
is something that can be both technically supported and which adds more
headroom before second-layer technologies must be in place. Additionally, it
seems that the vast majority of the community agrees that segregated witness
should be implemented in the near future and that hard forks will be a
necessity at some point, and I don't believe it should be controversial
that, as we have never done a hard fork before, gaining experience by
working towards a hard fork now is a good idea.
With the apparent agreement in the community, it is incredibly disheartening
that there is still so much strife, creating a toxic environment in which
developers are not able to work, companies are worried about their future
ability to easily move Bitcoins, and investors are losing confidence. The
way I see it, this broad unification of visions across all parts of the
community places the burden of selecting the most technically-sound way to
achieve that vision squarely on the development community.
Sadly, the strife is furthered by the huge risks involved in a hard fork in
the presence of strife, creating a toxic cycle which prevents a safe hard
fork. While there has been talk of doing an "emergency hardfork" as an
option, and while I do believe this is possible, it is not something that
will be easy, especially for something as controversial as rising fees.
Given that we have never done a hard fork before, being very careful and
deliberate in doing so is critical, and the technical community working
together to plan for all of the things that might go wrong is key to not
destroying significant value.
As such, I'd like to ask everyone involved to take this opportunity to
"reset", forgive past aggressions, and return the technical debates to
technical forums (ie here, IRC, etc).
As what a hard fork should look like in the context of segwit has never
(!) been discussed in any serious sense, I'd like to kick off such a
discussion with a (somewhat) specific proposal.
First some design notes:
* I think a key design feature should be taking this opportunity to add
small increases in decentralization pressure, where possible.
* Due to the several non-linear validation time issues in transaction
validation which are fixed by SegWit's signature-hashing changes, I strongly
believe any hard fork proposal which changes the block size should rely on
SegWit's existence.
* As with any hard fork proposal, its easy to end up pulling in hundreds of
small fixes for any number of protocol annoyances. In order to avoid doing
this, we should try hard to stick with a few simple changes.
Here is a proposed outline (to activate only after SegWit and with the
currently-proposed version of SegWit):
1) The segregated witness discount is changed from 75% to 50%. The block
size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
maximum block size of 3MB and a "network-upgraded" block size of roughly
2.1MB. This still significantly discounts script data which is kept out of
the UTXO set, while keeping the maximum-sized block limited.
2) In order to prevent significant blowups in the cost to validate
pessimistic blocks, we must place additional limits on the size of many
non-segwit transactions. scriptPubKeys are now limited to 100 bytes in size
and may not contain OP_CODESEPARATOR, scriptSigs must be push-only (ie no
non-push opcodes), and transactions are only allowed to contain up to 20
non-segwit inputs. Together these limits limit total-bytes-hashed in block
validation to under 200MB without any possibility of making existing outputs
unspendable and without adding additional per-block limits which make
transaction-selection-for-mining difficult in the face of attacks or
non-standard transactions. Though 200MB of hashing (roughly 2 seconds of
hash-time on my high-end
workstation) is pretty strongly centralizing, limiting transactions to fewer
than 20 inputs seems arbitrarily low.
Along similar lines, we may wish to switch MAX_BLOCK_SIGOPS from
1-per-50-bytes across the entire block to a per-transaction limit which is
slightly looser (though not too much looser - even with libsecp256k1
1-per-50-bytes represents 2 seconds of single-threaded validation in just
sigops on my high-end workstation).
3) Move SegWit's generic commitments from an OP_RETURN output to a second
branch in the merkle tree. Depending on the timeline this may be something
to skip - once there is tooling for dealing with the extra OP_RETURN output
as a generic commitment, the small efficiency gain for applications checking
the witness of only one transaction or checking a non-segwit commitment may
not be worth it.
4) Instead of requiring the first four bytes of the previous block hash
field be 0s, we allow them to contain any value. This allows Bitcoin mining
hardware to reduce the required logic, making it easier to produce
competitive hardware [1].
I'll deliberately leave discussion of activation method out of this
proposal. Both jl2012 and Luke-Jr recently begun some discussions about
methods for activation on this list, and I'd love to see those continue.
If folks think a hard fork should go ahead without SPV clients having a say,
we could table  or activate  a year or two after 1-3 activate.
[1] Simpler here may not be entirely true. There is potential for
optimization if you brute force the SHA256 midstate, but if nothing else,
this will prevent there being a strong incentive to use the version field as
nonce space. This may need more investigation, as we may wish to just set
the minimum difficulty higher so that we can add more than 4 nonce-bytes.
Obviously we cannot reasonably move forward with a hard fork as long as the
contention in the community continues. Still, I'm confident continuing to
work towards SegWit as a 2MB-ish soft-fork in the short term with some plans
on what a hard fork should look like if we can form broad consensus can go a
long way to resolving much of the contention we've seen.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2016-02-09 22:16:15
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A roadmap to a better header format and bigger block 
I would like to present a 2-3 year roadmap to a better header format and
bigger block size
1. Multistage rule changes to make sure everyone will have enough time to
2. Make mining easier, without breaking existing mining hardware and the
Stratum protocol
3. Make future hardfork less disruptive (with Luke-Jr's proposal)
Stage 1 is Segregated Witness (BIP141), which will not break any existing
full or light nodes. This may happen in Q2-Q3 2016
Stage 2 is fixes that will break existing full nodes, but not light nodes:
a. Increase the MAX_BLOCK_SIZE (the exact value is not suggested in this
roadmap), potentially change the witness discount
b. Anti-DoS rules for the O(n^2) validation of non-segwit scripts
c. (optional) Move segwit's commitments to the header Merkle tree. This is
optional at this stage as it will be fixed in Stage 3 anyway
This may happen in Q1-Q2 2017
Stage 3 is fixes that will break all existing full nodes and light nodes:
a. Full nodes upgraded to Stage 2 will not need to upgrade again, as the
rules and activation logic should be included already
b. Change the header format to Luke-Jr's proposal, and move all commitments
(tx, witness, etc) to the new structure. All existing mining hardware with
Stratum protocol should work.
c. Reclaiming unused bits in header for mining. All existing mining chips
should still work. Newly designed chips should be ready for the new rule.
d. Fix the time warp attack
This may happen in 2018 to 2019
a. Light nodes (usually less tech-savvy users) will have longer time to
b. The stage 2 is opt-in for full nodes.
c. The stage 3 is opt-in for light nodes.
a. The stage 2 is not opt-in for light nodes. They will blindly follow the
longest chain which they might actually don't want to
b. Non-upgraded full nodes will follow the old chain at Stage 2, which is
likely to have lower value.
c. Non-upgraded light nodes will follow the old chain at Stage 3, which is
likely to have lower value. (However, this is not a concern as no one should
be mining on the old chain at that time)
In terms of safety, the second proposal is better. In terms of disruption,
the first proposal is less disruptive
I would also like to emphasize that it is miners' responsibility, not the
devs', to confirm that the supermajority of the community accept changes in
Stage 2 and 3.
Matt Corallo's proposal:
Luke-Jr's proposal:

@_date: 2016-02-10 12:26:03
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] A roadmap to a better header format and bigger 
I am actually suggesting 1 hardfork, not 2. However, different rules are
activated at different time to enhance safety and reduce disruption. The
advantage is people are required to upgrade once, not twice. Any clients
designed for stage 2 should also be ready for stage 3.
-----Original Message-----
Sent: Wednesday, 10 February, 2016 06:15
As for your stages idea, I generally like the idea (and mentioned it may be
a good idea in my proposal), but am worried about scheduling two hard-forks
at once....Lets do our first hard-fork first with the things we think we
will need anytime in the visible future that we have reasonable designs for
now, and talk about a second one after we've seen what did/didnt blow up
with the first one.
Anyway, this generally seems reasonable - it looks like most of this matches
up with what I said more specifically in my mail yesterday, with the
addition of timewarp fixes, which we should probably add, and Luke's header
changes, which I need to spend some more time thinking about.

@_date: 2016-02-12 13:02:37
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP CPRKV: Check private key verify 
Seems it could be done without any new opcode:
Bob is trading b Bitcoins for a altcoins.
1. Bob Pays D Bitcoins to
 CLTV DROP  CHECKSIG
HASH160  EQUALVERIFY  CHECKSIG
2. Alice pays a altcoins to
HASH160  EQUALVERIFY  CHECKSIG
HASH160  EQUALVERIFY  CHECKSIG
3. Bob pays b Bitcoins to
 CLTV DROP  CHECKSIG
HASH160  EQUALVERIFY  CHECKSIG
4. Alice claims output from step 3 and reveals secret A
5. Bob claims output from step 2
6. Bob claims output from step 1 and reveals secret B
Sent: Friday, 12 February, 2016 04:05
There was some discussion on the bitcointalk forums about using CLTV for cross chain transfers.
Many altcoins don't support CLTV, so transfers to those coins cannot be made secure.  I created a protocol.  It uses on cut and choose to allow commitments to publish private keys, but it is clunky and not entirely secure.
I created a BIP draft for an opcode which would allow outputs to be locked unless a private key was published that matches a given public key.
This email has been sent from a virus-free computer protected by Avast.

@_date: 2016-01-05 00:43:41
@_author: jl2012 
@_subject: [bitcoin-dev] Segregated Witness BIPs 
A new BIP, as part of the SW softfork, is pending BIP number assignment:
This proposal defines a new transaction digest algorithm for signature verification in version 0 and version 1 witness program, in order to minimize redundant data hashing in verification (solving the O(n^2) issue), and to cover the input value by the signature (a frequently requested feature for cold wallet).
jl2012 via bitcoin-dev ? 2015-12-24 09:22 ??:

@_date: 2016-07-03 03:20:42
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Code Review: The Consensus Critical Parts of 
It is trivial to softfork a new rule to require the witness must not be empty for a witness output. However, does it really make the code simpler?
Actually I would like to see even shorter hash and pubkey to be used. Storing 1 BTC for a few months does not really require the security level of P2PKH.
In BIP143 sig, first 4 bytes is nVersion, and the next 32 bytes (hashPrevouts) is a hash of all prevouts. (in the case of ANYONECANPAY, the bytes following would be zero, as you proposed)
In the original sig, first 4 bytes in nVersion, next 4 bytes is number of inputs, and the next 32 bytes is a txid.
For a signature to be valid for both schemes, the last 28 bytes of the hashPrevouts must also be the first 28 bytes of a valid txid. This is already impossible. And this is just one of the many collisions required. In such case SHA256 would be insecure and adding a zero after the nVersion as you suggest would not be helpful at all.

@_date: 2016-07-30 22:03:11
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP114 MAST updated 
I have published a new version for BIP114 MAST. It's a bit more complicated with some new features:
1. It allows different parties in a contract not to expose their scripts to each other until redemption.
2. It includes a field to indicate the script language version so new opcodes could be added without touching the version byte nor the witness program.
You can find the updated BIP and code at:
The old version:

@_date: 2016-06-08 13:57:36
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP141 segwit consensus rule update: extension of 
Please note that the segregated witness (BIP141) consensus rule is updated. Originally, a witness program is a scriptPubKey or redeemScript that consists of a 1-byte push opcode (OP_0 to OP_16) followed by a data push between 2 and 32 bytes. The definition is now extended to 2 to 40 bytes:
 BIP141 defines only version 0 witness program: 20 bytes program for P2WPKH and 32 bytes program for P2WSH. Versions 1 to 16 are not defined, and are considered as anyone-can-spend scripts, reserved for future extension (e.g. the proposed BIP114). BIP141 also requires that only a witness program input may have witness data. Therefore, before this update, an 1-byte push opcode followed by a 33 bytes data push was not considered to be a witness program, and no witness data is allowed for that.
This may be over-restrictive for a future witness program softfork. When 32-byte program is used, this leaves only 16 versions for upgrade, and any ?sub-version? metadata must be recorded in the witness field. This may not be compatible with some novel hashing functions we are exploring.
By extending the maximum length by 8 bytes, it allows up to 16 * 2 ^ 64 versions for future upgrades, which is enough for any foreseeable use.
Why not make it even bigger, e.g. 75 bytes?
A 40 bytes witness program allows a 32-byte hash with 8-byte metadata. For any scripts that are larger than 32 bytes, they should be recorded in the witness field, like P2WSH in BIP141, to reduce the transaction cost and impact on UTXO set. Since SHA256 is already used everywhere, it is very unlikely that we would require a larger witness program (e.g. SHA512) without also a major revamp of the bitcoin protocol.
In any case, since scripts with a 1-byte push followed by a push of >40 bytes remain anyone-can-spend, we always have the option to redefine them with a softfork.
What are affected?
As defined in BIP141, a version 0 witness program is valid only with 20 bytes (P2WPKH) or 32 bytes (P2WSH). Before this update, an OP_0 followed by a data push of 33-40 bytes was not a witness program and considered as anyone-can-spend. Now, such a script will fail due to incorrect witness program length.
Before this update, no witness data was allowed for a script with a 1-byte push followed by a data push of 33-40 bytes. This is now allowed.
Actions to take:
If you are running a segnet node, or a testnet node with segwit code, please upgrade to the latest version at If you have an alternative implementation, please make sure your consensus code is updated accordingly, or your node may fork off the network.

@_date: 2016-06-08 16:23:51
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP141 segwit consensus rule update: extension of 
This is exactly why I proposed to extend the definition. My initial proposal was extending it to 33 bytes to effectively allow 16*256 new script versions, assuming we will keep using 32 bytes program hash.
If someday 32 bytes hash is deemed to be unsafe, the txid would also be unsafe and a hard fork might be needed. Therefore, I don?t see how a witness program larger than 40 bytes would be useful in any case (as it is more expensive and takes more UTXO space). I think Pieter doesn?t want to make it unnecessarily lenient.

@_date: 2016-06-29 00:22:45
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Code Review: The Consensus Critical Parts of 
Thanks for Peter Todd?s detailed report:
I have the following response.
Please note that unlimited space has been reserved after the witness commitment:
  block.vtx[0].vout[o].scriptPubKey.size() >= 38
 Which means anything after 38 bytes has no consensus meaning. Any new consensus critical commitments/metadata could be put there. Anyway, there is no efficient way to add a new commitment with softfork.
I don?t see any reason to have such check. We simply leave unknown witness program as any-one-can-spend without looking at the witness, as described in BIP141.
Since ~90% of current transactions are P2PKH, we expect many people will keep using this type of transaction in the future. P2WPKH gives the same level of security as P2PKH, and smaller scriptPubKey.
This is actually discussed on the mailing list. P2WSH with multi-sig is subject to birthday attack, and therefore 256-bit is used to provide 128-bit security. P2WPKH is used as single sig and therefore 160-bit is enough.
Something wrong here? In P2WPKH, the witness is  520 is the original limit. BIP141 tries to mimic the existing behaviour as much as possible. Anyway, normally nothing in the current scripts should use a push with more than 75 bytes
How could that be? That?d be a hash collision.

@_date: 2016-03-22 14:48:49
@_author: jl2012@xbt.hk 
@_subject: [bitcoin-dev] BIP147 minor error 
Do you mean BIP141?
Your example is an error by BIP141:
1*4 + 79999*1 = 80003 > 80000
Sent: Monday, 21 March, 2016 10:51
The BIP147 reads:
Sigop cost is defined. The cost of a sigop in traditional script is 4, while the cost of a sigop in witness program is 1. The new rule is total sigop cost ? 80,000. But the code implements:
if (nSigOps + (nWitSigOps + 3) / 4 > MAX_BLOCK_SIGOPS)
 ... error....
Which is not the same.
For example:
nSigOps = 1
nWitSigOps =79999
Is not an error by BIP definition but it's an error by the implemented code.
Regards, Sergio.

@_date: 2016-05-20 11:46:32
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
How is this compared to my earlier proposal:   ?
In my proposal, only the (pruned) UTXO set, and 32 bytes per archived block, are required for mining. But it is probably more difficult for people to spend an archived output. They need to know the status of other archived outputs from the same block. A full re-scan of the blockchain may be needed to generate the proof but this could be done by a third party archival node.

@_date: 2016-05-20 13:34:03
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP: OP_PRANDOM 
Using the hash of multiple blocks does not make it any safer. The miner of the last block always determines the results, by knowing the hashes of all previous blocks.

@_date: 2016-11-03 11:35:02
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Implementing Covenants with 
Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a different way from the Elements. Instead of hashing the data on stack, I directly put the 32 byte hash to the stack. This should be more flexible as not every system are using double-SHA256

@_date: 2016-11-17 23:40:05
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
Everything you said after this point is irrelevant.
Having hash collision is **by definition** a consensus failure, or a hardfork. You could replace the already-on-chain tx with the collision and create 2 different versions of UTXOs (if the colliding tx is valid), or make some nodes to accept a fork with less PoW (if the colliding tx is invalid, or making the block invalid, such as being to big). To put it simply, the Bitcoin protocol is broken. So with no doubt, Bitcoin Core and any implementation of the Bitcoin protocol should assume SHA256 collision is unquestionably **impossible**. If some refuse to make such assumption, they should have introduced an alternative hash algorithm and somehow run it in parallel with SHA256 to prevent the consensus failure.

@_date: 2016-11-18 01:22:56
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
I?m not sure if you really understand what you and I am talking. It has nothing to do with BIP30, 34, nor any other BIPs.
Say tx1 is confirmed 3 years ago in block X. An attacker finds a valid tx2 which (tx1 != tx2) and (SHA256(tx1) == SHA256(tx2)). Now he could replace tx1 with tx2 in block X and the block is still perfectly valid. Anyone trying to download the blockchain from the beginning may end up with a different ledger. The consensus is irrevocably broken as soon as tx1 or tx2 is spent.
Or, alternatively, an attacker finds an invalid tx3 which (tx1 != tx3) and (SHA256(tx1) == SHA256(tx3)). Now he could replace tx1 with tx3 in block X. Anyone trying to download the blockchain from the beginning will permanently reject the hash of block X. They will instead accept a fork built on top of block X-1. The chain will be permanently forked.

@_date: 2016-11-18 02:08:09
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
The fact that some implementations ban an invalid block hash and some do not, suggests that it?s not a pure p2p protocol issue. A pure p2p split should be unified by a bridge node. However, a bridge node is not helpful in this case. Banning an invalid block hash is an implicit ?first seen? consensus rule.

@_date: 2016-11-18 22:43:03
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP30 and BIP34 interaction (was Re: [BIP 
In this case I don?t understand how your implementation won?t be DoS-ed. An attacker could keep sending you inv for the same block / transaction. Since you don?t assume the hash is unique, each time you have to download the block/tx again before you could tell if that is the same one you have already known. Otherwise, you are implementing the ?first seen? rule.
Also, you can?t ban a peer just because you get an invalid tx from him, because he might be referring to a hash-colliding UTXO that you don?t know. In that case you need to request for the parent tx to verify. I wonder if you are really doing that.

@_date: 2016-10-06 12:27:38
@_author: Johnson Lau 
@_subject: [bitcoin-dev] To-be-softfork policy in 0.13.1 
Coupled with the release of segwit in 0.13.1, there are 3 default relay and mining policy rules that may become softfork proposals in the near future.
Generally, users must not assume that a script spendable in pre-segregated witness system would also be spendable as a P2WPKH or P2WSH script. Before large-scale deployment in the production network, developers should test the scripts on testnet with the default relay policy turned on, and with a small amount of money after BIP141 is activated on mainnet.
The rules include:
1. Only compressed public keys are accepted in P2WPKH and P2WSH (See BIP143 for details)
2. The argument of OP_IF/NOTIF in P2WSH must be minimal (see 3. Signature(s) must be null vector(s) if an OP_CHECKSIG or OP_CHECKMULTISIG is failed (for both pre-segregated witness script and P2WSH. See BIP146)
The BIP141 and 143 are updated with the aforementioned rules:

@_date: 2016-10-17 00:57:46
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
This is completely wrong. SPV wallets will work as normal without upgrade. Full nodes will only provide transactions to SPV in a format they understand, and SPV will accept the transaction since they are not doing any validation anyway. The only reason an end user may want to upgrade is for lower transaction fee when they are sending transaction. If they don't upgrade, that means the fee is too low for them to care, which is a good news
 ---- On Mon, 17 Oct 2016 00:42:26 +0800 Tom Zander via bitcoin-dev  wrote ----   > > On Sun, Oct 16, 2016 at 10:58 AM, Tom Zander via bitcoin-dev <  > >    > > > The fallow period sounds waaaay to short. I suggest 2 months at minimum  > > > since anyone that wants to be safe needs to upgrade.  > >   > > I asked a lot of businesses and individuals how long it would take them to  > > upgrade to a new release over the last year or two.  > >   > > Nobody said it would take them more than two weeks.  >   > The question you asked them was likely about the block size. The main   > difference is that SPV users do not need to update after BIP109, but they do   > need to have a new wallet when SegWit transactions are being sent to them.  >   > This upgrade affects also end users, not just businesses etc.  >   > Personally, I'd say that 2 months is even too fast.  >    > --   > Tom Zander  > Blog:   > Vlog:   > _______________________________________________  > bitcoin-dev mailing list  > bitcoin-dev at lists.linuxfoundation.org  >   >

@_date: 2016-10-17 03:11:23
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
---- On Mon, 17 Oct 2016 02:54:04 +0800 Tom Zander via bitcoin-dev  wrote ----  > Honestly, if the reason for the too-short-for-safety timespan is that you   > want to use BIP9, then please take a step back and realize that SegWit is a   > contriversial soft-fork that needs to be deployed in a way that is extra   > safe because you can't roll the feature back a week after deployment.  > All transactions that were made in the mean time turn into everyone-can-  > spent transactions. No one should use, nor anyone is advised to use, segwit transactions before it is fully activated. Having 2 months or 2 weeks of grace period makes totally no difference in this regard. If anyone tried to use segwit tx during your proposed 2 months grace period, all those txs were still everyone-can-spent.
All you are advocating is just stalling the process with no improvement in security.
 >   > I stand by the minimum of 2 months. There is no reason to use BIP9 as it was   > coded in an older client. That is an excuse that I don't buy.  > --   > Tom Zander  > Blog:   > Vlog:   > _______________________________________________  > bitcoin-dev mailing list  > bitcoin-dev at lists.linuxfoundation.org  >   >

@_date: 2016-10-17 11:46:11
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
---- On Mon, 17 Oct 2016 04:08:29 +0800 Tom Zander via bitcoin-dev  wrote ----   > > > Honestly, if the reason for the too-short-for-safety timespan is that  > > > you  > > > want to use BIP9, then please take a step back and realize that SegWit  > > > is a contriversial soft-fork that needs to be deployed in a way that is  > > > extra safe because you can't roll the feature back a week after  > > > deployment. All transactions that were made in the mean time turn into  > > > everyone-can- spent transactions.  > >   > > No one should use, nor anyone is advised to use, segwit transactions  > > before it is fully activated.   >   > Naturally, I fully agree.  >   > It seems I choose the wrong words, let me rephrase;  >   > You can't roll the SegWit back a week after people are allowed to send   > segwit transactions (lock-in + fallow period). All transactions that were   > made in the mean time turn into everyone-can- spent transactions.  >  > Because the network as a whole and any implementation is unable to roll back   > in an environment where SegWit is a contriversial soft-fork, it is super   > important to make sure that it is properly supported by all miners. This   > takes time and the risk you take by pushing this is that actual real people   > loose actual real money because of the issue I outlined inthe previous   > paragraph. It would only happen if a large proportion of miners are false-signalling, like how BU did with BIP109 and forked your Classic away on testnet
But this is a egg-and-chicken problem and extending the grace period would not have any improvement. Until the rules are fully activated, it is totally impossible to tell if some miners are false signalling. The only method to prevent it, as usual, is the majority of miners will orphan the blocks of malicious miners. Like in the last year, some miners did not correctly implement BIP66 and got punished by losing many blocks.
If your are suggesting >51% of miners may false-signal (like in the BIP109 case), we already have a much bigger problem.
If people are really worrying about that, I would advise them not to use segwit extensively at the beginning, and wait for at least a week to see any sign of false signalling (which will be shown as invalid orphaned blocks). If the grace period was 2 weeks, they need to wait for 3 weeks; if the grace period was 2 months, they need to wait for 2 months and a week. Pre-activation consensus-imposed grace period could never replace post-activation self-imposed observation period

@_date: 2016-10-25 01:37:12
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
Some comments and questions
1. In the BIP you mentioned scriptSig 3 times, but I don't think you are really talking about scriptSig. Especially, segwit has aborted the use of scriptSig to fix malleability. From the context I guess you mean redeemScript (see BIP141)
2. It seems that 51% of miners may steal all money from the peg, right? But I think this is unavoidable for all 2-way-peg proposals. To make it safer you still need notaries.
3. Instead of using a OP_NOPx, I suggest you using an unused code such as 0xba. OP_NOPx should be reserved for some simple "VERIFY"-type codes that does not write to the stack.
4. I don't think you should simply replace "(witversion == 0)" with "((witversion == 0) || (witversion == 1))". There are only 16 available versions. It'd be exhausted very soon if we use a version for every new opcode. As a testing prototype this is fine, but the actual softfork should not waste a witversion this way. We need a better way to coordinate the use of new witness version. BIP114 suggests an additional field in the witness to indicate the script version (
5. It seems this is the first BIP in markdown format, not mediawiki (but this is allowed by BIP1)
6. The coinbase space is limited to 100 bytes and is already overloaded by many different purposes. I think any additional consensus critical message should go to a dummy scriptPubKey like the witness commitment. You may consider to  have a new OP_RETURN output like BIP141, with different magic bytes. However, please don't make this output mandatory (cf. witness commitment output is optional if the block does not have witness tx)
6a. "..........due to lack of space to include the proper ack tag in a block": this shouldn't happen if you use a OP_RETURN output
7.  "It can be the case that two different secondary blockchains specify the same transaction candidate, but **at least** one of them will clearly be unauthentic."
8. Question: is an ack-poll valid only for 1 transaction? When the transaction is confirmed, could full nodes prune the corresponding ack-poll data? (I think it has to be prunable after spending because ack-poll data is effectively UTXO data) 9. No matter how you design a softfork, "Zero risk of invalidating a block" couldn't be true for any softfork. For example, even if a miner does not include any txs with OP_COUNT_ACKS, he may still build on top of blocks with invalid OP_COUNT_ACKS operations.
 ---- On Sun, 02 Oct 2016 23:49:08 +0800 Sergio Demian Lerner via bitcoin-dev  wrote ----  > Since ScalingBitcoin is close, I think this is a good moment to publish our proposal on drivechains. This BIP proposed the drivechain we'd like to use in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it implemented in Bitcoin. Until that happens, we're using a federated approach.  > I'm sure that adding risk-less Bitcoin extensibility through sidechains/drivechains is what we all want, but it's of maximum importance to decide which technology will leads us there.
 > We hope this work can also be the base of all other new 2-way-pegged blockchains that can take Bitcoin the currency to new niches and test new use cases, but cannot yet be realized because of current limitations/protections.
 >  > The full BIP plus a reference implementation can be found here:
 >  > BIP (draft):
 >  >  > Code & Test cases:
 >  > (Note: Code is still unaudited)
 >  > As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked opcode that counts acks and nacks tags in coinbase fields, and push the resulting totals in the script stack.
 >  > The system was designed with the following properties in mind:
 >  > 1. Interoperability with scripting system  > 2. Zero risk of invalidating a block
 > 3. No additional computation during blockchain management and re-organization
 > 4. No change in Bitcoin security model
 > 5. Bounded computation of poll results
 > 6. Strong protection from DoS attacks
 > 7. Minimum block space consumption
 > 8. Zero risk of cross-secondary chain invalidation
 >  > Please see the BIP draft for a more-detailed explanation on how we achieve these goals.
 >  > I'll be in ScalingBitcoin in less than a week and I'll be available to discuss the design rationale, improvements, changes and ideas any of you may have.
 >  > Truly yours,  > Sergio Demian Lerner
 > Bitcoiner and RSK co-founder
 >   >  _______________________________________________  > bitcoin-dev mailing list  > bitcoin-dev at lists.linuxfoundation.org  >   >

@_date: 2016-10-26 01:45:20
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Drivechain proposal using OP_COUNT_ACKS 
>  3. Instead of using a OP_NOPx, I suggest you using an unused code such as 0xba. OP_NOPx should be reserved for some simple "VERIFY"-type codes that does not write to the stack.
 >  > Ok. I'm not sure, but if everyone agrees to it, I will. Also Segwit versioning allows to create new opcode multiplexing opcodes, so I was thinking about adding an "opcode index" to a more generic OP_OPERATE. But that prevents using all NOP space, but prevents easily counting OP_ACK_COUNT for checksig block limit.
The other reason not to touch NOPx is they are shared by SIGVERSION_BASE and SIGVERSION_WITNESS_V0. If we later decide to introduce new opcodes to legacy versions, we may still use this space.
And yes, I think you should keep OP_ACT_COUNT easily countable for block sigop limit.
 >   >  4. I don't think you should simply replace "(witversion == 0)" with "((witversion == 0) || (witversion == 1))". There are only 16 available versions. It'd be exhausted very soon if we use a version for every new opcode. As a testing prototype this is fine, but the actual softfork should not waste a witversion this way. We need a better way to coordinate the use of new witness version. BIP114 suggests an additional field in the witness to indicate the script version (
 >   > Good. But currently that version is not enforced, so this BIP cannot make use of it. I can use (witversion == 1) but add the BIP114 version field so that the next BIP can make use of it.
Probably BIP114 would never be deployed. I don't know. But I think we should try to move the script version to witness, as it is cheaper. The major witness version could be reserved for some fundamental changes in language.
 >  6. The coinbase space is limited to 100 bytes and is already overloaded by many different purposes. I think any additional consensus critical message should go to a dummy scriptPubKey like the witness commitment. You may consider to  have a new OP_RETURN output like BIP141, with different magic bytes. However, please don't make this output mandatory (cf. witness commitment output is optional if the block does not have witness tx)
 >   >  6a. "..........due to lack of space to include the proper ack tag in a block": this shouldn't happen if you use a OP_RETURN output
 >   > I'm not sure about this. The fact that the space for acknowledge and proposal is short has been seen by other developers a benefit and not a drawback. It prevent hundreds of sidechains to be offered, which might hurt solo miners. 70 bytes allows for approximately 10 active polls.
That's 1 active poll per minute on average, which sounds very small if it ever gets really popular. Have you made any forecast? I could foresee people have to bid for the coinbase space for their ack-poll, and they will yell at the devs asking for more poll space (well.....)
We used an OP_RETURN output for segwit as some miners wanted to retain the coinbase space for other purpose like advertisement.  Even if you want to set an artificial limit, you could still use an OP_RETURN output. It just means you will need a OP_COUNT_ACKS2 softfork when you want to expand the space.  Since polls are not fixed size, if an artificial limit is desired, maybe it makes more sense to limit the number of polls, instead of number of bytes.
 >  8. Question: is an ack-poll valid only for 1 transaction? When the transaction is confirmed, could full nodes prune the corresponding ack-poll data? (I think it has to be prunable after spending because ack-poll data is effectively UTXO data)
 >   > Yes, there is no ack-poll data stored except for the coinbase field cache, which periodically cleans itself by using a FIFO approach.
If the target tx of a ack-poll is never confirmed on the blockchain, I guess you need to keep the data of the poll forever?  It's like creating an unspendable and unprunable UTXO (just want you to clarify. People are spamming the UTXO already so your proposal won't make it worse anyway)
 >   9. No matter how you design a softfork, "Zero risk of invalidating a block" couldn't be true for any softfork. For example, even if a miner does not include any txs with OP_COUNT_ACKS, he may still build on top of blocks with invalid OP_COUNT_ACKS operations.
 >   > I'm not sure. I assumed that transactions redeeming a script using OP_COUNT_ACKS  would be non-standard, so the the problem you point out would only happen if the block including the transaction would be mined specifically for the purpose to defeat subsequent miners. A honest pre-fork miner would never include a redeemScript that that verifies an OP_COUNT_ACKS, since that transaction would never be received by the p2p protocol (it could if the miner accepts non-standard txs by a different channnel).    >  > But I must check this in the BIP source code if OP_COUNT_ACKS is really non-standard as I designed it to be.
It must be non-standard because witversion != 0 are non-standard already. I mean, you proposal is probably as safe as OP_CSV, but no one sold OP_CSV as "zero risk".

@_date: 2016-09-01 07:29:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Attack by modifying non-segwit transactions after 
Thank you so much for taking time to actually review the codes. I hope you will keep raising questions when you feel something might be wrong. This is how things supposed to work and we should not be affected by some forum discussions.

@_date: 2016-09-01 07:39:51
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
Restriction for segwit OP_IF argument as a policy has got a few concept ACK. I would like to have more people to ACK or NACK, especially the real users of OP_IF. I think Lightning network would use that at lot.
Pull request: more related discussion could be found at It does have impact if your script uses the combination of "OP_SIZE OP_IF" or "OP_DEPTH OP_IF". With this policy/softfork, you need to use  "OP_SIZE OP_0NOTEQUAL OP_IF" or "OP_DEPTH OP_0NOTEQUAL OP_IF", or reconstruct your scripts.

@_date: 2016-09-02 00:40:58
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with dummy stack element malleability 
This document specifies proposed changes to the Bitcoin transaction validity rules to fix the malleability of extra stack element for OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY.
The original plan was to do the LOW_S and NULLDUMMY (BIP146) together with segwit in 0.13.1. However, as we discovered some undocumented behavior in LOW_S, we may want to deploy the LOW_S softfork in a later release. I will edit the BIP146 later.
  BIP: ?
  Title: Dealing with dummy stack element malleability
  Author: Johnson Lau   Status: Draft
  Type: Standards Track
  Created: 2016-09-02
This document specifies proposed changes to the Bitcoin transaction validity rules to fix the malleability of extra stack element for OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY.
Signature malleability refers to the ability of any relay node on the network to transform the signature in transactions, with no access to the relevant private keys required. For non-segregated witness transactions, signature malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).
A design flaw in OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY makes them consuming an extra stack element ("dummy element") after signature validation. The dummy element is not inspected in any manner, and could be replaced by any value without invalidating the script. This document specifies a new rule to fix this signature malleability.
To fix the dummy element malleability, a new consensus rule ("NULLDUMMY") is deployed to require that the dummy element MUST be the empty byte array. Anything else makes the script evaluate to false immediately. The NULLDUMMY rule applies to OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY in pre-segregated scripts, and also pay-to-witness-script-hash scripts described in BIP141.
This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.
For Bitcoin mainnet, the BIP9 starttime is midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout is midnight TBD UTC (Epoch timestamp TBD).
For Bitcoin testnet, the BIP9 starttime is midnight 1 May 2016 UTC (Epoch timestamp 1462060800) and BIP9 timeout is midnight 1 May 2017 UTC (Epoch timestamp 1493596800).
The reference client has produced compatible signatures from the beginning, and the NULLDUMMY rule has been enforced as relay policy by the reference client since v0.10.0. There has been no transactions violating the requirement being added to the chain since at least August 2015. In addition, every non-compliant signature can trivially be converted into a compliant one, so there is no loss of functionality by this requirement.
An implementation for the reference client is available at This document is extracted from the previous BIP62 proposal, which was composed by Pieter Wuille and had input from various people.
This document is placed in the public domain.

@_date: 2016-09-02 04:28:14
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Low S values signatures 
The BIP146 is revised the second time:
1. NULLDUMMY is removed from BIP146 and becomes another softfork that will implement at the same time as segwit?
2. A new rule, namely NULLFAIL, is added to require empty signature(s) when a CHECK(MULTI)SIG returns a FALSE
3. NULLFAIL will be implemented as a policy rule in 0.13.1. However, the softfork won't be deployed in 0.13.1.
As we discovered some undocumented behavior in LOW_S, we may want to deploy the LOW_S softfork in a later release.?The newly added NULLFAIL rules should cover all the special cases.?
BIP: 146
  Title: Dealing with signature encoding malleability
  Author: Johnson Lau           Pieter Wuille   Status: Draft
  Type: Standards Track
  Created: 2016-08-16
This document specifies proposed changes to the Bitcoin transaction validity rules to fix signature malleability related to ECDSA signature encoding.
Signature malleability refers to the ability of any relay node on the network to transform the signature in transactions, with no access to the relevant private keys required. For non-segregated witness transactions, signature malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).
Since the enforcement of Strict DER signatures (BIP66), there are 2 remaining known sources of malleability in ECDSA signatures:
Inherent ECDSA signature malleability: ECDSA signatures are inherently malleable as taking the negative of the number S inside (modulo the curve order) does not invalidate it.
Malleability of failing signature: If a signature failed to validate in OP_CHECKSIG or OP_CHECKMULTISIG, a FALSE would be returned to the stack and the script evaluation would continue. The failing signature may take any value, as long as it follows all the rules described in BIP66.
This document specifies new rules to fix the aforesaid signature malleability.
To fix signature malleability, the following new rules are applied:
We require that the S value inside ECDSA signatures is at most the curve order divided by 2 (essentially restricting this value to its lower half range). Every signature passed to OP_CHECKSIG[1], OP_CHECKSIGVERIFY, OP_CHECKMULTISIG, or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied, MUST use a S value between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive) with strict DER encoding (see BIP66).
If a signature passing to ECDSA verification does not pass the Low S value check and is not an empty byte array, the entire script evaluates to false immediately.
A high S value in signature could be trivially replaced by S' = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141 - S.
If an OP_CHECKSIG is trying to return a FALSE value to the stack, we require that the relevant signature must be an empty byte array.
If an OP_CHECKMULTISIG is trying to return a FALSE value to the stack, we require that all signatures passing to this OP_CHECKMULTISIG must be empty byte arrays, even the processing of some signatures might have been skipped due to early termination of the signature verification.
Otherwise, the entire script evaluates to false immediately.
The following examples combine the LOW_S and NULLFAIL rules.
  CO       : curve order = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141
  HCO      : half curve order = CO / 2 = 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0
  P1, P2   : valid, serialized, public keys
  S1L, S2L : valid low S value signatures using respective keys P1 and P2 (1 ? S ? HCO)
  S1H, S2H : signatures with high S value (otherwise valid) using respective keys P1 and P2 (HCO < S < CO)
  F        : any BIP66-compliant non-empty byte array but not a valid signature
These scripts will return a TRUE to the stack as before:
  S1L P1 CHECKSIG
  0 S1L S2L 2 P1 P2 2 CHECKMULTISIG
These scripts will return a FALSE to the stack as before:
  0 P1 CHECKSIG
  0 0 0 2 P1 P2 2 CHECKMULTISIG
These previously TRUE scripts will fail immediately under the new rules:
  S1H P1 CHECKSIG
  0 S1H S2L 2 P1 P2 2 CHECKMULTISIG
  0 S1L S2H 2 P1 P2 2 CHECKMULTISIG
  0 S1H S2H 2 P1 P2 2 CHECKMULTISIG
These previously FALSE scripts will fail immediately under the new rules:
  F P1 CHECKSIG
  0 S2L S1L 2 P1 P2 2 CHECKMULTISIG
  0 S1L F   2 P1 P2 2 CHECKMULTISIG
  0 F   S2L 2 P1 P2 2 CHECKMULTISIG
  0 S1L 0   2 P1 P2 2 CHECKMULTISIG
  0 0   S2L 2 P1 P2 2 CHECKMULTISIG
  0 F   0   2 P1 P2 2 CHECKMULTISIG
  0 0   F   2 P1 P2 2 CHECKMULTISIG
This BIP will be deployed by "version bits" BIP9. Details TBD.
For Bitcoin mainnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).
For Bitcoin testnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).
The reference client has produced LOW_S compatible signatures since v0.9.0, and the LOW_S rule has been enforced as relay policy by the reference client since v0.11.1. As of August 2016, very few transactions violating the requirement are being added to the chain. For all scriptPubKey types in actual use, non-compliant signatures can trivially be converted into compliant ones, so there is no loss of functionality by these requirements.
Scripts with failing OP_CHECKSIG or OP_CHECKMULTISIG rarely happen on the chain. The NULLFAIL rule has been enforced as relay policy by the reference client since v0.13.1.
Users MUST pay extra attention to these new rules when designing exotic scripts.
Implementations for the reference client is available at:
^ Including pay-to-witness-public-key-hash (P2WPKH) described in BIP141
This document is extracted from the previous BIP62 proposal which had input from various people.
This document is placed in the public domain.

@_date: 2016-09-04 08:29:37
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Dealing with dummy stack element 
Although it is technically possible to bundle 2 independent softforks in one release, it increases the burden of testing and maintenance. We need to test and prepare for 4 scenarios: both not activated, only NULLDUMMY activated, only SEGWIT activated, and both activated.
Also, as we learnt from BIP66, softfork activation could be risky. It is evident that today a non-negligible percentage of miners are hard-coding the block version number. This increases the risks of softfork transition as miners may not enforce what they are signaling (btw this is also happening on testnet) Making 2 independently softforks would double the risks, and I believe NULLDUMMY alone is not worth the risks.

@_date: 2016-09-10 17:41:21
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
Concept ACK.
For the details of executing the plan, I think the following is less disruptive:
1. Send a message with (max sequence - 1), notifying all nodes that the key will be retired on or before a date. People with systems relying on this key should either upgrade or ignore the revocation message. We don't know the actual date because the key is shared by many people.
With the max - 1 sequence, no message except the max sequence revocation message may override this message. 2. Send the revocation message at the pre-announced time, if no one have done that before
3. After a few months or so, publish the private key.
 >   > One of the facilities in the alert system is that you can send a  > maximum sequence alert which cannot be overridden and displays only a  > static key compromise text message and blocks all other alerts. I plan  > to send a triggering alert in the not-distant future (exact time to be  > announced well in advance) feedback on timing would be welcome.  >   > There are likely a few production systems that automatically shut down  > when there is an alert, so this risks some small one-time disruption  > of those services-- but none worse than if an alert were sent to  > advise about a new system upgrade.  >   > At some point after that, I would then plan to disclose this private  > key in public, eliminating any further potential of reputation attacks  > and diminishing the risk of misunderstanding the key as some special  > trusted source of authority.  >   > Cheers,  > _______________________________________________  > bitcoin-dev mailing list  > bitcoin-dev at lists.linuxfoundation.org  >   >

@_date: 2016-09-10 17:29:50
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Proposed segwit related consensus and policy rules in 
There are several opening pull requests for segwit related consensus and policy rules. This email summarize and explain the rationale.
As a general warning, people must not assume that a script spendable in pre-segwit system would also be spendable as a segwit script. They share much similarity but there are also notable differences, such as BIP143 and those proposals listed below. In any case, test your segwit system on testnet with the standard rules turned on, and a small amount of money after segwit is activated on mainnet.
Script Malleability fixes: Segwit (BIP141) fixes the most nasty malleability in Bitcoin: transaction ID malleability. However, due to the flexibility of scripting system, it is still possible for a relay node to insert arbitrary data to the witness without invalidating the transaction. Although segwit makes such attacks much harmless, this could still be annoying as people may write data to the blockchain at others costs.
NULLDUMMY, MINIMALIF, NULLFAIL are fixing this type of problem. NULLDUMMY has been implemented as a policy for more than a year and a softfork is proposed in the upcoming 0.13.1. MINIMALIF and NULLFAIL are both new policy proposed for 0.13.1, and may become softforks in the future. Script designers must pay attention to these potential softforks to avoid creation of unspendable scripts.
BIP147 "NULLDUMMY" softfork (for both segwit and pre-segwit scripts)
PR: Related discussion: "MINIMALIF" Minimal OP_IF/NOTIF argument (segwit scripts only)
PR: Related discussion: "NULLFAIL" Null signature for failed CHECK(MULTI)SIG (for both segwit and pre-segwit scripts)
PR: Related discussion: Policy: Resources limit for P2WSH
PR: For P2WSH, a policy limit is proposed with witnessScript <= 3600 bytes, witness stack item size <= 80 bytes, and witness stack items <= 100
3600 bytes witnessScript and 100 stack items are adequate for a n-of-100 multisig using 100 OP_CHECKSIG, 99 OP_ADD, and 1 OP_EQUAL. Before segwit, the biggest standard mutlisig is n-of-15 with P2SH.
The max size for ECDSA signature is 73 bytes and nothing (except hashing opcodes) should use more than that with the current scripting language.
This is to prevent abuse of witness space, and reduce the risks of DoS attack with some unknown special and big scripts.
The consensus limits described in BIP141 are not changed, as witnessScript <= 10000 bytes and  witness stack item size <= 520 bytes. (There is also an implied limit for witness stack items of 412, see the inline comments in Policy: Public key must be compressed (segwit only)
PR: It is proposed that only compressed keys (33 bytes starting with 0x02 or 0x03) are allowed in segwit scripts.
This is a policy only and non-compressed keys are still valid in a block. A softfork based on this may be proposed with further risks and benefits analysis
We can't have such policy or softfork in non-segwit scripts since there are many UTXOs being stored that way. Since segwit is a completely new script system, there is no strong reasons to support non-compressed keys.
Wallet developers must pay attention to this policy and must not assume that existing P2PKH hashes or P2SH scripts are spendable in segwit.
The RPC command addwitnessaddress will refuse to return a segwit address if the given key/multi-sig is unknown or is not compressed.
createwitnessaddress will return an address for whatever scripts given, without checking the validity at all. (even an OP_RETURN is provided, it will still return a P2WSH address). We may need to give a warning, or simply remove this command.
DoS protection: Banning peers for sending certain types of consensus invalid witness
PR: Peers sending certain types of invalid witness will be banned before fee and SigOp policy are checked. Those are all based on explicit or implicit consensus rules, and will protect P2WPKH and canonical multisigs against the DoS issues described in  The rest of P2WSH scripts will be covered by  by not storing witness txs in rejection cache.
DoS protection:  Mandatory softfork flags for segwit txs
PR: Since all segwit-aware nodes must be aware of all existing softforks, including BIP66, 65, 112, 141, and 143, the verification flags for these BIPs will be mandatory for transactions with non-empty witness.  Wallets relaying witness transactions violating these rules will be banned (even if the violation happens in a non-segwit input).

@_date: 2016-09-10 22:57:35
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
We need to make sure the revocation message is widely distributed before making the private key public
 ---- On Sat, 10 Sep 2016 21:23:37 +0800 Andrew C  wrote ----   > > 3. After a few months or so, publish the private key.  > Why wait a few months? Why not just publish the key a few days after the  > final alert?  >

@_date: 2016-09-30 18:57:53
@_author: Johnson Lau 
@_subject: [bitcoin-dev] New BIP: Limiting excessive SignatureHash operation 
============================== START ==============================
 A new BIP is proposed to prevent excessive O(n^2) SignatureHash operation.
 (Tight estimation)
 (Loose estimation)
Two methods of sighash size estimation are proposed, with different tradeoff. The tight estimation is more permissive (disabling less txs) but require disabling of OP_CODESEPARATOR, FindAndDelete, and unusual nHashType. The loose estimation is less permissive (may disable more big and strange txs) but does not depend on further policy/consensus rules. With either type of estimation, normal standard txs (<100kB, P2PK, P2PKH, canonical bare or P2SH multisig) are totally unaffected by this BIP.
  BIP: ?
  Title: Limiting excessive SignatureHash operation
  Author: Johnson Lau   Status: Draft
  Type: Standards Track
  Created: 2016-09-21
This proposal defines a new type of block-level resources limit, with several (optional) script restrictions, to prevent excessive SignatureHash operation.
There are 4 ECDSA signature verification codes in the Bitcoin script system: CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG, CHECKMULTISIGVERIFY (?sigops?). According to the SIGHASH type, a transaction digest (sighash) is generated with a double SHA256 of a serialized subset of the transaction, with a function called SignatureHash, and the signature is verified against this sighash with a given public key. Due to a design weakness, the amount of data hashing in SignatureHash is proportional to the size of the transaction. Therefore, data hashing grows in O(n2) as the number of sigops in a transaction increases. While a 1 MB block would normally take 2 seconds to verify with an average computer in 2015, a 1 MB transaction with 5569 sigops may take 25 seconds to verify. [1][2][3]
BIP143 fixes this problem by introducing a new SignatureHash algorithm in segregated witness transactions. However, it would not be able and is not intended to fix the problem of pre-segregated witness transactions. This document proposes a new type of block-level resources limit to prevent excessive SignatureHash operation. However, the calculation of sighash is a complicated process involving several consensus-critical procedures, including the use of OP_CODESEPARATOR, the FindAndDelete function, and the interpretation of nHashType. A correct limitation should be made based on the effects of these procedures.
Transaction hashable size
Transaction hashable size (TxHashableSize) is defined as the size of a transaction, which:
is serialized without witness data (BIP144), and
has scriptSig in all inputs replaced by zero-size script
TxHashableSize is an estimation of the amount of data hashed with a SIGHASH_ALL. Without counting the size of scriptCode and nHashType, it always underestimates the size. However, the difference is negligible since it grows linearly with the number of sigops.
int64_t GetTransactionHashableSize(const CTransaction& tx)
    int64_t size = ::GetSerializeSize(tx, SER_NETWORK, PROTOCOL_VERSION | SERIALIZE_TRANSACTION_NO_WITNESS);
    for (unsigned int i = 0; i < tx.vin.size(); i++) {
        int64_t scriptSigSize = tx.vin[i].scriptSig.size();
        size -= scriptSigSize;
        // If the scriptSig size is larger than 252, 2 bytes compactSize encoding is deducted.
        if (scriptSigSize > 252)
            size -= 2;
        /*
         * Theoretically, 4 bytes should be deducted if the scriptSig is larger than 65535 bytes,
         * and 8 bytes should be deducted if it is larger than 4294967295 bytes.
         * However, scriptSig larger than 10000 bytes is invalid so it is not needed.
         */
    }
    return size;
SignatureHash equivalent operation
SignatureHash equivalent operation (SigHashOp) is defined as the maximum possible number of times which a sigop would perform SignatureHash at the TxHashableSize. Depends on whether some extra restrictions in script use are enforced, there are 2 ways to estimate the SigHashOp:
Loose estimation: A loose SigHashOp estimation does not depend on any extra script restrictions. It assumes that SignatureHash is performed not more than once for every ECDSA signature passing to a sigop. It looks for all sigops in a transaction (even in an unexecuted conditional branch), in scriptSig, in scriptPubKey of the outputs being spent, and in redeemScript of P2SH transactions. Each OP_CHECKSIG or CHECKSIGVERIFY is counted as 1 SigHashOp. Each OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is counted as 20 SigHashOp, unless all the following conditions are satisfied:
The OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is immediately preceded by a value push as OP_n (1 ? n ? 16), denoting the number of public keys (n).
The n opcodes preceding the OP_n must be push type (i.e. 0x00 ? opcode ? 0x60), as the public key(s)
The opcodes preceding the public key(s) is a OP_m, with 1 ? m ? 16 and m ? n, denoting the number of signatures (m)
If all these conditions are met, the OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is counted as m SigHashOp.
SigHashOp of a transaction is the sum of SigHashOp of each of its inputs.
unsigned int CScript::GetSigHashOpCount() const
    unsigned int n = 0;
    const_iterator pc = begin();
    std::vector pushOpcodes;
    while (pc < end())
    {
        opcodetype opcode;
        if (!GetOp(pc, opcode))
            break; // The script is invalid
        if (opcode == OP_CHECKSIG || opcode == OP_CHECKSIGVERIFY)
            n++;
        else if (opcode == OP_CHECKMULTISIG || opcode == OP_CHECKMULTISIGVERIFY) {
            unsigned int nKey = 0;
            unsigned int nSig = 0;
            // We assume a CHECKMULTISIG will hash the transaction for 20 times, unless it is in some canonical form.
            n += 20;
            // The number of keys must be k = 1 to 16 denoted by OP_k
            if (pushOpcodes.size() >= 3 && pushOpcodes.back() >= OP_1 && pushOpcodes.back() <= OP_16) {
                nKey = DecodeOP_N(pushOpcodes.back());
                // All the k + 2 opcodes before the CHECKMULTISIG must be push only
                if (pushOpcodes.size() >= nKey + 2) {
                    opcodetype nSigCode = pushOpcodes.at(pushOpcodes.size() - nKey - 2);
                    // The number of signatures must be k = 1 to 16 denoted by OP_k, and not larger than number of keys
                    if (nSigCode >= OP_1 && nSigCode <= OP_16) {
                        nSig = DecodeOP_N(nSigCode);
                        if (nSig <= nKey)
                            // We use the number of signatures as the SigOpCount of this CHECKMULTISIG
                            n = n - 20 + nSig;
                    }
                }
            }
        }
        if (opcode <= OP_16)
            pushOpcodes.push_back(opcode);
        else
            pushOpcodes.clear();
    }
    return n;
Tight estimation: An tight SigHashOp estimation depends on these extra consensus rules for pre-segregated witness scripts:
nHashType is confined to only 6 types: 0x01 for SIGHASH_ALL, 0x02 for SIGHASH_NONE, 0x03 for SIGHASH_SINGLE, 0x81 for SIGHASH_ALL|SIGHASH_ANYONECANPAY, 0x82 for SIGHASH_NONE|SIGHASH_ANYONECANPAY, and 0x83 for SIGHASH_SINGLE|SIGHASH_ANYONECANPAY. A signature with other nHashType is invalid.
Script with OP_CODESEPARATOR, even in an unexecuted conditional branch, is invalid.
Script that involves non-zero FindAndDelete results is invalid.
It also assumes that SignatureHash is performed not more than once in each script for each nHashType.
SigHashOp is counted in the same way as the loose estimation. However, if the SigHashOp for a script is found to be larger than 3, it is counted as only 3 SigHashOp.[4] The SigHashOp of a transaction is the sum of SigHashOp of each of its inputs.
SigHashOp of the generating transaction is defined to be 0.
SignatureHash size
SignatureHash size (SigHashSize) of a transaction is the product of TxHashableSize and SigHashOp.
SigHashSize of a block is the sum of SigHashSize of all transactions in the block.
Consensus and policy limits for SigHashSize
A new consensus rule is enforced to require that SigHashSize of a block MUST NOT be larger than 500,000,000 (500MB). Consequently, SigHashSize of a valid transaction MUST NOT be larger than 500MB.
A new relay and mempool policy is recommended to reject any unconfirmed transaction that has a SigHashSize to Transaction weight ratio larger than 90. This policy limit is equivalent to 36MB SigHashSize for a 100kB non-segregated witness transaction, or 360MB for a full block of such transactions.
Static analysis
This proposal employs a static analysis approach to estimate SigHashSize of transactions and blocks. This allows early rejection of violating transactions and blocks without executing the scripts at all. Despite that the size of scriptCode and nHashType are not considered in the estimation, the difference is negligible comparing with size of the main transaction body, since the overheads grows linearly with the number of sigops, which is mostly restricted by the 80,000 sigop limit (BIP141). [5]
Loose SigHashOp estimation
The loose SigHashOp estimation assumes that SignatureHash is performed not more than once for every ECDSA signature passing to a sigop. This assumption is obviously correct for OP_CHECKSIG and CHECKSIGVERIFY since they would never perform SignatureHash more than once, while no SignatureHash would be performed if they happen in an unexecuted conditional branch, or if the signature is an empty vector. This assumption is also correct for OP_CHECKMULTISIG and CHECKMULTISIGVERIFY with appropriate code refactoring. While a signature may be verified against multiple public keys, the sighash for this signature must remain unchanged across the whole operation and therefore could be reused. Therefore, SigHashOp of a OP_CHECKMULTISIG and CHECKMULTISIGVERIFY is equal to the number of signatures.
Tight SigHashOp estimation with extra script restrictions
The tight SigHashOp estimation is based on more assumptions. It assumes that the scriptCode serialized within SignatureHash is a constant value for all sigops in an transaction input. For this assumption to be true, we must disable any process that may modify the scriptCode, which are OP_CODESEPARATOR and FindAndDelete. Transactions involving OP_CODESEPARATOR and FindAndDelete are extremely rare in the main network, and arguably all of those were performed for testing purpose. Removal of these operations would have next to no functional loss, significantly simply the consensus-critical logic, and reduce the risks of unintentional consensus forks. [6]
The tight SigHashOp estimation also assumes that only 6 nHashType are allowed. This is a relay policy in reference implementation since v0.?, and transactions with violating signatures are extremely rare in the main network. However, at consensus level, SigHashOp could be any value from 0 to 255, and a SIGHASH type could be encoded in multiple ways. For example, there are 116 ways to denote SIGHASH_ALL. Since nHashType is serialized inside SignatureHash, the sighash produced by different nHashType are not the same, even if all of them were SIGHASH_ALL.
With all these extra consensus rules implemented, we could be assured that SignatureHash is performed not more than once in each script for each nHashType, due to the invariability of scriptCode. The 6 nHashType limitation further guarantees that each script, with whatever number of sigops, would never perform SignatureHash for more than approximately 3 times of TxHashableSize (excluding some linearly growing overhead), as shown below:
TxHashableSize could be divided into 3 parts: size of inputs, size of outputs, and size of overhead including nVersion, nLockTime, and maybe some CompactSize encoding. The size of overhead grows linearly with the number of sigops, and is negligible.
SIGHASH_ALL would hash all inputs and outputs of the transaction.
SIGHASH_NONE would hash all inputs of the transaction, but no output is hashed.
SIGHASH_SINGLE would hash all inputs of the transaction. It also hashes the scriptPubKey of output with matching index, and all outputs with lower indexes with empty scriptPubKey. With the famous Gauss summation formula, it could be shown that if a transaction has the same number of inputs and outputs, and all inputs use a SIGHASH_SINGLE, the worst case would be hashing approximately 50% of all outputs of the transaction.
nHashType with SIGHASH_ANYONECANPAY would hash only one input, which scales linearly and is negligible. Therefore,
SIGHASH_ALL|SIGHASH_ANYONECANPAY would hash all outputs.
SIGHASH_NONE|SIGHASH_ANYONECANPAY is negligible.
SIGHASH_SINGLE|SIGHASH_ANYONECANPAY would hash approximately 50% of all outputs in the worst case.
By adding up the effects of 6 nHashType, it could be shown that the total amount of data hashed would be equal to 3 times of input size and 3 times of output size. Therefore, in the worst case, a script may perform SignatureHash for up to approximately 3 times of TxHashableSize.
SigHashSize policy limit
A policy limit for SigHashSize to Transaction weight ratio is recommended as 90, which is equivalent to 36MB SigHashSize for a 100kB non-segregated witness transaction. This limit is chosen based on the concept of normal transaction.
A transaction is normal if each SigHashOp consumes at least 70 bytes of space in scriptSig on average. According to BIP66, the maximum size of an ECDSA signature is 73 bytes, which should consume 74 bytes of scriptSig space including the push opcode. Using the low S value, the maximum signature size becomes 72 bytes. It could be shown that with 99.6% of chance, a randomly generated low S signature would be at least 69 bytes (consuming 70 bytes of space in scriptSig).
In actual use, the scriptSig size associated to a SigHashOp is often much more than 70 bytes. For example, pay-to-public-key-hash transactions and OP_CHECKMULTISIG inside P2SH would consume extra scriptSig space with their public keys. In such cases, more than 100 bytes of scriptSig would be consumed by a SigHashOp.
If a transaction is performing many SigHashOp with disproportionately small scriptSig, very likely it employed some strange scripts, such as using OP_DUP to copy a signature.
The size of scriptSig is important in determining the SigHashSize limit, since it is deducted from the transaction size for the TxHashableSize. Comparing 2 transactions of the same size, the one with more SigHashOp may have smaller SigHashSize due to the smaller TxHashableSize. With the 100kB standard transaction size limit, it could be shown that the maximum SigHashSize happens when there are 714 SigHashOp, consuming at least 714 * 70 = 49.98kB of scriptSig. With the resulting TxHashableSize = 100 - 49.98 = 50.02kB, the SigHashSize is 50.02kB * 714 = 35.7MB, which is just below the recommended policy limit.
Despite the limit is determined based on normal transactions, abnormal transactions may still be accepted as long as they are not too big. For example, if the transaction size is 10kB, a transaction may remain standard with the recommended policy limit even if each SigHashOp is associated with only 7 bytes of scriptSig.
SigHashSize consensus limit
The consensus limit of 500MB SigHashSize per block is based on the policy limit of SigHashSize to Transaction weight ratio. It is set above the policy limit, to make sure that a miner enforcing the policy limit would never produce a block violating the consensus rules. The 500MB limit is compromise between avoiding loss of functionality (as it may disable some very big transactions) and the harm of intentional or unintentional sighash attack initiated by a miner.
This is a softfork to be deployed with BIP9.
Backward compatibility
Impact of the recommended policy limit
No matter the loose or the tight SigHashOp estimation is employed, this softfork with recommended policy limit should be completely transparent to users of normal standard transactions, including pay-to-public-key, pay-to-public-key-hash, and P2SH m-of-n OP_CHECKMULTISIG with 1 ? m ? n ? 15. A complete scan up to block 430368 showed that the transaction 7b587808a7f6b135ef91011be9b42fcbb0892da50963822e47a5827ced8653ce was the normal standard transaction with highest SigHashSize to weight ratio. With a ratio of 80.1, it is still well below the policy limit of 90. Should this policy had been deployed since genesis block, all normal standard transactions should still have been accepted.
If the loose estimation had been employed, a few abnormal standard transactions would have been rejected by policy, but were still valid by consensus. This is a full list of the affected transactions:
    bea1c2b87fee95a203c5b5d9f3e5d0f472385c34cb5af02d0560aab973169683
    24b16a13c972522241b65fbb83d09d4bc02ceb33487f41d1f2f620b047307179
    53666009e036171b1aee099bc9cd3cb551969a53315410d13ad5390b8b4f3bd0
    ffc178be118bc2f9eaf016d1c942aec18441a6c5ec17c9d92d1da7962f0479f6
    2f1654561297114e434c4aea5ca715e4e3f10be0be8c1c9db2b6f68ea76dae09
    62fc8d091a7c597783981f00b889d72d24ad5e3e224dbe1c2a317aabef89217e
    d939315b180d3d73b5e316eb57a18f8137a3f5943aef21a811660d25f1080a3f
    8a6bfaa78828a81147e4848372d491aa4e9048631982a670ad3a61402a4ec327
    02cc78789cc070125817189ec378daa750355c8b22bbce982ed96aa549facb1f
    b97a16ae2e8ae2a804ed7965373b42055f811653f4628e4bef999145d4b593bc
    c51ffaf08188859669571f897f119b6d39ea48a9334212f554bf4927401b71f3
    324456fe9ec97a380effba0a0205a226e380790b93e7366d39f2a416a44d2a34
These transactions all used a large number of sigops, and obviously were made for testing purpose. However, they would have gone through if the tight estimation had been used.
Impact of the block-level consensus limit
With the block-level consensus limit of 500MB SigHashSize, transactions with SigHashSize above 500MB would also become invalid. Up to block 430368, 49 transactions would have become invalid with this limit (with either loose or tight estimation):
    Transaction ID                                                       SigHashSize
    9c667c64fcbb484b44dcce638f69130bbf1a4dd0fbb4423f58ceff92af4219ec	 2,215,084,200
    9fdbcf0ef9d8d00f66e47917f67cc5d78aec1ac786e2abb8d2facb4e4790aad6	 2,215,076,850
    5d8875ed1707cfee2221741b3144e575aec4e0d6412eeffe1e0fa07335f61311	 1,271,892,772
    cb550c9a1c63498f7ecb7bafc6f915318f16bb54069ff6257b4e069b97b367c8	 1,271,892,772
    14dd70e399f1d88efdb1c1ed799da731e3250d318bfdadc18073092aa7fd02c2	 1,271,892,772
    a684223716324923178a55737db81383c28f055b844d8196c988c70ee7075a9a	 1,271,892,772
    bb41a757f405890fb0f5856228e23b715702d714d59bf2b1feb70d8b2b4e3e08	 1,271,820,375
    5b0a05f12f33d2dc1507e5c18ceea6bb368afc51f00890965efcc3cb4025997d	 1,091,954,040
    bb75a8d10cfbe88bb6aba7b28be497ea83f41767f4ee26217e311c615ea0132f	 1,025,295,000
    5e640a7861695fa660343abde52cfe10b5a97dd8fc6ad3c5e4b2b4bb1c8c3dd9	 1,025,295,000
    dd49dc50b54b4bc1232e4b68cfdd3d349e49d3d7fe817d1041fff6dd583a6eaf	 1,025,230,000
    3d724f03e8bcc9e2e3ea79ebe4c6cffca86d85e510742cd6d3ac29d420787a34	 1,025,210,000
    8bcf8e8d8265922956bda9b651d2a0e993072c9dca306f3a132dcdb95c7cee6e	 1,025,210,000
    54bf51be42ff45cdf8217b07bb233466e18d23fd66483b12449cd9b99c3a0545       995,042,075
    6bb39576292c69016d0e0c1fe7871640aab12dd95874d67c46cf3424822f8dfd	   988,589,147
    d38417fcc27d3422fe05f76f6e658202d7fa394d0c9f5b419fef97610c3c49f1	   923,884,836
    66b614e736c884c1a064f7b0d6a9b0abd97e7bb73ac7e4b1b92b493d558a0711	   902,501,490
    d985c42bcd704aac88b9152aede1cca9bbb6baee55c8577f84c42d600cfec8e4	   898,372,800
    e32477636e47e1da5fb49090a3a87a3b8ff637d069a70cd5b41595da225e65b4	   893,548,487
    bf40393fedc45a1b347957124ef9bb8ae6a44feecee10ef2cc78064fabf8125f	   891,859,369
    1d93bfe18bc05b13169837b6bc868a92da3c87938531d6f3b58eee4b8822ecbf	   888,420,676
    79e30d460594694231f163dd79a69808904819e2f39bf3e31b7ddc4baa030a04	   877,542,875
    4eba5deb2bbf3abf067f524484763287911e8d68fb54fa09e1287cf6cd6d1276	   874,353,609
    c3f2c2df5388b79949c01d66e83d8bc3b9ccd4f85dbd91465a16fb8e21bf8e1b	   869,060,209
    446c0a1d563c93285e93f085192340a82c9aef7a543d41a86b65e215794845ef	   833,655,283
    e0c5e2dc3a39e733cf1bdb1a55bbcb3c2469f283becf2f99a0de771ec48f6278	   802,433,929
    2e7c454cfc348aa220f53b5ba21a55efa3d36353265f085e34053c4efa575fda	   789,067,716
    01d23d32bccc04b8ca5a934be16da08ae6a760ccaad2f62dc2f337eee7643517	   785,833,449
    9f8cc4496cff3216608c2f2177ab360bd2d4f58cae6490d5bc23312cf30e72e0	   775,457,104
    1e700d8ce85b17d713cad1a8cae932d26740e7c8ab09d2201ddfe9d1acb4706c	   757,230,231
    9db4e0838c55ef20c5eff271fc3bf09a404fff68f9cdad7df8eae732500b983d	   756,319,396
    763e13f873afa5f24cd33fc570a178c65e0a79c05c88c147335834fc9e8f837b	   734,988,489
    b8ba939da1babf863746175b59cbfb3b967354f04db41bd13cb11da58e43d2a8	   732,906,849
    f62f2c6a16b5da61eaae36d30d43bb8dd8932cd89b40d83623fa185b671c67f9	   723,659,859
    6e278c0ca05bf8e0317f991dae8a9efa141b5a310a4c18838b4e082e356ef649	   703,394,401
    e3de81a5817a3c825cf44fbf8185e15d446393615568966a6e3fc22cba609c7d	   697,632,336
    b5ca68205e6d55e87bd6163b28467da737227c6cbcc91cb9f6dc7b400163a12b	   665,208,049
    9c972a02db30f9ee91cc02b30733d70d4e2d759b5d3c73b240e5026a8a2640c4	   653,370,601
    02313ac62ca8f03930cdc5d2e437fabc05aea60a31ace18a39678c90b45d32bd	   622,323,625
    e245f6c3c6b02dc81ea1b6694735565cc535f603708783be027d0e6a94ac3bd5	   609,926,656
    1cf52f9ef89fa43bb4f042cbd4f80e9f090061e466cbe14c6b7ba525df0e572e	   607,214,327
    461308024d89ea4231911df4ef24e65e60af2a9204c8282a6b67f4214c1714e7	   606,137,296
    fa5a58f787f569f5b8fab9dadb2447161fac45b36fb6c2c0f548ed0209b60663	   589,853,184
    905df97982a2904d6d1b3dfc272435a24d705f4c7e1fc4052798b9904ad5e597	   546,737,250
    d85ce71f583095a76fb17b5bb2a1cbf369e2a2867ca38103aa310cbb2aaf2921	   546,737,250
    1b604a075075197c82d33555ea48ae27e3d2724bc4c3f31650eff79692971fb7	   531,511,200
    ba31c8833b7417fec9a84536f32fcb52d432acb66d99b9be6f3899686a269b2b	   531,511,200
    92f217ec13ab309240adc0798804b3418666344a5cbfff73fb7be8192dad5261	   509,443,536
    22e861ee83c3d23a4823a3786460119425d8183783068f7ec519646592fac8c2	   506,268,969
Extra consensus rules required by tight SigHashOp estimation
Transactions in the main network, up to block 430368, that would have been affected by the extra consensus rules are listed below:
Transactions with OP_CODESEPARATOR:
    eb3b82c0884e3efa6d8b0be55b4915eb20be124c9766245bcc7f34fdac32bccb
    055707ce7fea7b9776fdc70413f65ceec413d46344424ab01acd5138767db137
    6d36bc17e947ce00bb6f12f8e7a56a1585c5a36188ffa2b05e10b4743273a74b
    bc179baab547b7d7c1d5d8d6f8b0cc6318eaa4b0dd0a093ad6ac7f5a1cb6b3ba
    4d932e00d5e20e31211136651f1665309a11908e438bb4c30799154d26812491
    0157f2eec7bf856d66714856182a146998910dc6fa576bec200a9fa8039459e7
    ddd070541bf2fddaa5e08a9d93126f73211fe15291beb897c762908949420ad9
    d4a27d10404d87ee0b8a05fb700e55f9f83f80a59ebf87af2fbf87e5c9546177
    492cdb3c95c1fe0c597d8dc847adb5459d403ea083f4b5e706300d437c84748f
    b3e977a2c48145255d84e1c82d4ea07522528991d50ead1cf3a783559d9733e3
Transactions with non-zero FindAndDelete results:
    5df1375ffe61ac35ca178ebb0cab9ea26dedbd0e96005dfcee7e379fa513232f
    ded7ff51d89a4e1ec48162aee5a96447214d93dfb3837946af2301a28f65dbea
    307b173ef009b970c1a0dd67166a8ce3e91fc5551b8950d2d17f1fe0eaa07358
Transactions with abnormal nHashType:
    c99c49da4c38af669dea436d3e73780dfdb6c1ecf9958baa52960e8baee30e73
    0ad07700151caa994c0bc3087ad79821adf071978b34b8b3f0838582e45ef305
    7c451f68e15303ab3e28450405cfa70f2c2cc9fa29e92cb2d8ed6ca6edb13645
    a6c116351836d9cc223321ba4b38d68c8f0db53661f8c2229acabbc269c1b2c8
    f5efee46ccfa4191ccd9d9f645e2f5d09bbe195f95ef5608e992d6794cd653cd
    904bda3a7d3e3b8402793334a75fb1ce5a6ff5cf1c2d3bcbd7bd25872d0e8c1e
    8ac76995ce4ac10dd02aa819e7e6535854a2271e44f908570f71bc418ffe3f02
    e218970e8f810be99d60aa66262a1d382bc4b1a26a69af07ac47d622885db1a7
    ba4f9786bb34571bd147448ab3c303ae4228b9c22c89e58cc50e26ff7538bf80
    38df010716e13254fb5fc16065c1cf62ee2aeaed2fad79973f8a76ba91da36da
Reference Implementation
Policy only:
 (Tight estimation)
 (Loose estimation)
^ CVE-2013-2292
^ New Bitcoin vulnerability: A transaction that takes at least 3 minutes to verify
^ The Megatransaction: Why Does It Take 25 Seconds?
^ It should be noted that since sigops may exist in both scriptSig (non-standard and extremely rare) and scriptPubKey, in theory an input may have up to 6 SigHashOp
^ Not totally, since it does not count the sigops inside the scriptPubKey of the outputs being spent, while inappropriately counting sigops in scriptPubKey of the current transaction.
^ This document is placed in the public domain.

@_date: 2017-04-03 04:13:23
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP draft: Extended block header hardfork 
This is the first of a series of BIPs describing my ?spoonnet? experimental hardfork. Recently many bitcoin businesses expressed their requirements for supporting a hardfork proposal. While it is proven to be extremely difficult to obtain community-wide consensus, spoonnet fulfills all the commonly requested features, including deterministic activation logic, strong and simple 2-way replay protection, wipe-out protection, and predictable resources use. A few more BIPs are coming to describe these features.
The activation is purely based on flag day. Since it is very difficult to measure community consensus on-chain, this may only be done off-chain, and everyone switch to the new software when the vast majority agree. This is more a social issue than a technical one.
Reference implementation for consensus codes could be found at:  . This does not include mempool, wallet, and mining support. Mempool and wallet support are more tricky due to replay attack protection.
BIP: ? Layer: Consensus (hard fork) Title: Extended block header hardfork Author: Johnson Lau  Comments-Summary: No comments yet. Comments-URI: Status: Draft Type: Standards Track Created: 2017-03-31 License: BSD-2-Clause
This BIP proposes a flexible and upgradable extended block header format thorough a hardfork.
In the current Bitcoin protocol, the block header is fixed at 80 bytes with no space reserved for additional data. The coinbase transaction becomes the only practical location for new consensus-critical data, such as those proposed by BIP100 and BIP141. Although this preserves maximal backward compatibility for full nodes, it is not ideal for light clients because the size of coinbase transaction and depth of Merkle tree are indefinite.
This BIP proposes an extended block header format with the following objectives:
The following rules are activated when the median timestamp of the past 11 blocks is equal to or greater than a to-be-determined time and after activation of BIP65.
A special extheader softfork is defined, with the following BIP9 parameters:
Until the extheader softfork is activated, the following extra rules are enforced:
Activation of the special extheader softfork is independent to the activation time of the hardfork. If the special softfork is activated before the hardfork, the aforementioned extra rules will not be enforced when the hardfork is activated. Nodes that are not aware of the new rules should consider extheader softfork as an unknown upgrade and issue warnings to users accordingly.
This hardfork employs a simple flag day deployment based on the median timestamp of previous blocks. Beyond this point, supporting nodes will not accept blocks with original rules. This ensures a deterministic and permanent departure with the original rules.
The witness field of the coinbase input is used as a convenient unused space to store the extended header. For any other purposes the extended header is not considered as part of the coinbase transaction (it is removed when the wtxid is calculated) This design minimizes the changes in the peer-to-peer protocol between full nodes, as no new message type is required to transmit the extended header. However, a new protocol message is still needed for serving light nodes.
Committing to the block height allows determining the value before all parental headers are obtained.
By fixing the bytes 4 to 5 as 0x0000, in the worst case an unupgraded light node may consider the block has only one transaction with no input and output, and will not see any real transactions.
The 48 byte extra data field is reserved for miners for any purposes and designed to be compatible with the Stratum mining protocol. Miners are expected to use 4 to 16 bytes as extra nonce, and 32 to 44 bytes for merge mining. This requires a hardfork for all AuxPOW blockchains, while significantly reduces the size of AuxPOW block headers.
hashMerkleRoot is relocated to the extended header, followed by hashWitnessRoot. The new structure allows hashWitnessRoot committing to the wtxid of coinbase transaction with extended header removed.
Committing to the number of transactions allows light nodes to determine the Merkle tree structure easily.
Committing to the transaction fees and block weight provides information for fees estimation.
The reserved space (16 bytes until the extheader softfork is activated) MUST NOT be used without community consensus. It should be primarily used for consensus critical commitments and network status data for light nodes. Other arbitrary data should use the extra data field in extended header or the scriptSig of the coinbase transaction.
The special extheader softfork allows future protocol upgrades to increase the size of the extended header and redefine the calculation of block weight in a backward compatible way.
Other proposed hardfork changes are described in other BIPs.
This is a hard forking change, which breaking compatibility with old full node and light node. It should not be deployed without widespread consensus.
Old full nodes will consider the block header nVersion as invalid and refuse the follow the hardfork chain.
Depending on the design of light nodes, they may consider the hardfork chain as the best chain if it has the most total proof-of-work. However, they will not see any transactions in the chain and cease to properly function until either upgrading to the new rules, or rejecting the new rules with the negative block header nVersion.
Reference implementation
This BIP is licensed under the 2-clause BSD license.

@_date: 2017-04-03 11:36:13
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP draft: Extended block header hardfork 
Yes, it?d look much better. But I?m thinking of a different approach: instead of using a hash of 0000?.0000, we use the hash of previous block for the coinbase input. With some new SIGHASH design, this allows people to pay to a child of a particular block. This is actually implemented in my spoonnet2 branch. I?ll describe it with a BIP soon
However, what I?m trying to do in the extended block header is independent to the design of coinbase tx. Here I?m trying to let people knowing the height just by a header and extended header (<300 bytes), without requiring all headers in the history.
Also I forgot to post the link of the BIP:

@_date: 2017-04-05 02:35:01
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Extension block proposal by Jeffrey et al 
I feel particularly disappointed that while this BIP is 80% similar to my proposal made 2 months ago (  ), Matt Corallo was only the person replied me. Also, this BIP seems ignored the txid malleability of the resolution tx, as my major technical critique of xblock design.
But anyway, here I?m only making comments on the design. As I said in my earlier post, I consider this more as an academic topic than something really ready for production use.
Softforks by definition tighten consensus rules
so the authors don?t consider segwit as a consensus-layer solution to increase transaction throughput, or not think segwit is safe? But logically speaking if segwit is not safe, this BIP could only be worse. OTOH, segwit also obviously increases tx throughput, although it may not be as much as some people wish to have.
The 2013 one is outdated. As the authors are not quoting it, not sure if they read my January proposal
I think extension block in the proposed form actually breaks BIP141. It may say it activates segregated witness as a general idea, but not a specific proposal like BIP141
It needs to be more specific here. How are they exactly arranged? I suggest it uses a root of all txids, and a root of all wtxids, and combine them as the commitment. The reason is to allow people to prune the witness data, yet still able to serve the pruned tx to light wallets. If it makes txid and wtxid as pairs, after witness pruning it still needs to store all the wtxids or it can?t reconstruct the tree
This hits the biggest question I asked in my January post: do you want to allow direct exit payment to legacy addresses? As a block reorg will almost guarantee changing txid of the resolution tx, that will permanently invalidate all the child txs based on the resolution tx. This is a significant change to the current tx model. To fix this, you need to make exit outputs unspendable for up to 100 blocks. Doing this, however, will make legacy wallet users very confused as they do not anticipate funding being locked up for a long period of time. So you can?t let the money sent back to a legacy address directly, but sent to a new format address that only recognized by new wallet, which understands the lock up requirement. This way, however, introduces friction and some fungibility issues, and I?d expect people using cross chain atomic swap to exchange bitcoin and xbitcoin
To summarise, my questions are:
1. Is it acceptable to have massive txid malleability and transaction chain invalidation for every natural happening reorg?  Yes: the current spec is ok; No: next question (I?d say no)
2. Is locking up exit outputs the best way to deal with the problem? (I tried really hard to find a better solution but failed)
3. How long the lock-up period should be? Answer could be anywhere from 1 to 100
4. With a lock-up period, should it allow direct exit to legacy address? (I think it?s ok if the lock-up is short, like 1-2 block. But is that safe enough?)
5. Due to the fungibility issues, it may need a new name for the tokens in the ext-block
I suggest to only allow push-only and OP_RETURN scriptPubKey in xblock. Especially, you don?t want to replicate the sighash bug to xblock. Also, requires scriptSig to be always empty
Why 7? There are 16 unused witness program versions
There is a flaw here: witness script with no sigop will be counted as 0 and have a lot free space
so 72 bytes is 1 point or 0 point? Maybe it should just scale everything up by 64 or 128, and make 1 witness byte = 1 point . So it won?t provide any ?free space? in the block.
I?d suggest to have at least 16 points for each witness v0 output, so it will make it always more expensive to create than spend UTXO. It may even provide extra ?discount? if a tx has more input than output. The overall objective is to limit the UTXO growth. The ext block should be mainly for making transactions, not store of value (I?ll explain later)
In general I think it?s ok, but I?d suggest a higher threshold like 5000 satoshi. It may also combine the threshold with the output witness version, so unknown version may have a lower or no threshold. Alternatively, it may start with a high threshold and leave a backdoor softfork to reduce it.
It is a double-edged sword. While it is good for us to be able to discard an unused chain, it may create really bad user experience and people may even lose money. For example, people may have opened Lightning channels and they will find it not possible to close the channel. So you need to make sure people are not making time-locked tx for years, and require people to refresh their channel regularly. And have big red warning when the deactivation SF is locked in. Generally, xblock with deactivation should never be used as long-term storage of value.
some general comments:
1. This BIP in current form is not compatible with BIP141. Since most nodes are already upgraded to BIP141, this BIP must not be activated unless BIP141 failed to activate. However, if the community really endorse the idea of ext block, I see no reason why we couldn?t activate BIP141 first (which could be done in 2 weeks), then work together to make ext block possible. Ext block is more complicated than segwit. If it took dozens of developers a whole year to release segwit, I don?t see how ext block could become ready for production with less time and efforts.
2. Another reason to make this BIP compatible with BIP141 is we also need malleability fix in the main chain. As the xblock has a deactivation mechanism, it can?t be used for longterm value storage.
3. I think the size and cost limit of the xblock should be lower at the beginning, and increases as we find it works smoothly. It could be a predefined growth curve like BIP103, or a backdoor softfork. With the current design, it leaves a massive space for miners to fill up with non-tx garbage. Also, I?d also like to see a complete SPV fraud-proof solution before the size grows bigger.

@_date: 2017-04-05 18:28:07
@_author: Johnson Lau 
@_subject: [bitcoin-dev] A different approach to define and understand 
Softforks and hardforks are usually defined in terms of block validity (BIP99): making valid blocks invalid is a softfork, making invalid blocks valid is a hardfork, and SFs are usually considered as less disruptive as it is considered to be ?opt-in?. However, as shown below this technical definition could be very misleading. Here I?m trying to redefine the terminology in terms of software upgrade necessity and difficulty.
Softforks are defined as consensus rule changes that non-upgraded software will be able to function exactly as usual, as if the rule changes have never happened
Hardforks are defined as consensus rule changes that non-upgraded software will cease to function or be severely handicapped
SFs and HFs under this definitions is a continuum, which I call it ?hardfork-ness?. A pure softfork has no hardfork-ness.
*Mining node
Under this definitions, for miners, any trivial consensus rule changes is somewhat a hardfork, as miners can?t reliably use non-upgraded software to create blocks. However, there is still 3 levels of ?hardfork-ness?, for example:
1. Those with lower hardfork-ness would be the SFs that miners do not need to upgrade their software at all. Instead, the minimum requirement is to setup a boarder node with latest rules to make sure they won?t mine on top of an invalid block. Examples include CSV and Segwit
2. Some SFs have higher hardfork-ness, for example BIP65 and BIP66. The minimum actions needed include setting up a boarder node and change the block version. BIP34 has even higher hardfork-ness as more actions are needed to follow the new consensus.
3. Anything else, ranging from simple HFs like BIP102 to complete HFs like spoonnet, or soft-hardfork like forcenet, have the highest hardfork-ness. In these cases, boarder nodes are completely useless. Miners have to upgrade their servers in order to stay with the consensus.
*Non-mining full node
Similarly, in terms of non-mining full node, as the main function is to fully-validate all applicable rules on the network, any consensus change is a hardfork for this particular function. However, a technical SF would have much lower hardfork-ness than a HF, as a border node is everything needed in a SF. Just consider a company has some difficult-to-upgrade software that depends on Bitcoin Core 0.8. Using a 0.13.1+ boarder node will make sure they will always follow the latest rules. In case of a HF, they have no choice but to upgrade the backend system.
So we may use the costs of running a boarder node to further define the hardfork-ness of SFs, and it comes to the additional resources needed:
1. Things like BIP34, 65, 66, and CSV involves trivial resources use so they have lowest hardfork-ness.
2. Segwit is higher because of increased block size.
3. Extension block has very high hardfork-ness as people may not have enough resources to run a boarder node.
* Fully validating wallets
In terms of the wallet function in full node, without considering the issues of validation, the hardfork-ness could be ranked as below:
1. BIP34, 65, 66, CSV, segwit all have no hardfork-ness for wallets. Non-upgraded wallets will work exactly in the same way as before. Users won?t notice any change at all. (In some cases they may not see a new tx until it has 1 confirmation, but this is a mild issue and 0-conf is unsafe anyway)
2. Extension block, as presented in my January post (   ), has higher hardfork-ness, as users of legacy wallets may find it difficult to receive payments from upgraded wallet. However, once they got paid, the user experience is same as before
3. Another extension block proposal (   ) has very high hardfork-ness for wallets, as legacy wallets will frequently and suddenly find that incoming and outgoing txs becoming invalid, and need to sign the invalidated txs again, even no one is trying to double spend.
4. Hardfork rule changes have highest hardfork-ness for full node wallets
I?ll explain the issues with extension block in a separate post in details
* Real SPV wallet
The SPV wallets as proposed by Satoshi should have the ability to fully validate the rules when needed, so they could be somehow seen as fully validating wallets. So far, real SPV wallet is just vapourware.
* Fake SPV wallet, aka light wallet
All the so-called SPV wallets we have today are fake SPV according to whitepaper definition. Since they validate nothing, the hardfork-ness profile is very different:
1. BIP34, 65, 66, CSV, segwit has no hardfork-ness for light wallets. Block size HF proposals (BIP10x) and Bitcoin Unlimited also have no hardfork-ness (superficially, but not philosophically). Along the same line, even an inflation hardfork has no hardfork-ness for light wallets.
2. Extension block has the same kind of hardfork-ness issue as I mentioned.
3. HFs that deliberately breaks light wallets, such as spoonnet, is a complete hardfork.
While some people try to leverage weakness of light wallets, the inability to validate any important rules like block size, double spending, and inflation is a serious vulnerability.
Before I finish, I?d also like to analyse some other interesting cases.
1. Soft-hardfork: which requires miners to mine empty blocks with 0 reward, and put the tx merkle tree in the legacy coinbase (e.g.   ). This allows most hardfork-ing changes including block size and inflation. In terms of block validity this is a softfork. But with the definition I presented, soft-hardforks are clearly hardforks for every practical purposes.
2. On-chain KYC, blacklist, account freezing: technically softforks, but all are very disruptive hardforks in terms of user experience.
3. Lightning network and side chains are not consensus rule changes, and they could provide new features without any hardfork-ness.

@_date: 2017-04-06 01:04:10
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Extension block proposal by Jeffrey et al 
Why so? It seems you are describing it as a softfork. With hardfork or extension block, a new rule could simply grant extra space when the tagged UTXO is spent. So if the usual block size limit is 1MB, when the special UTXO is made, the block size limit decreases to 1MB-700 byte, and the user has to pay for that 700 byte. When it is spent, the block size will become 1MB+700 byte.
But miners or even users may abuse this system: they may try to claim all the unused space when the blocks are not congested, or when they are mining empty block, and sell those tagged UTXO later. So I think we need to limit the reservable space in each block, and deduct more space than it is reserved. For example, if 700 bytes are reserved, the deduction has to be 1400 byte.
With BIP68, there are 8 unused bits in nSequence. We may use a few bits to let users to fine tune the space they want to reserve. Maybe 1 = 256 bytes
I think this is an interesting idea to explorer and I?d like to include this in my hardfork proposal.

@_date: 2017-04-09 03:23:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
I don?t fully understand your storage engine. So the following deduction is just based on common sense.
a) It is possible to make unlimited number of 1-in-100-out txs
b) The maximum number of 100-in-1-out txs is limited by the number of previous 1-in-100-out txs
c) Since bitcrust performs not good with 100-in-1-out txs, for anti-DoS purpose you should limit the number of previous 1-in-100-out txs. d) Limit 1-in-100-out txs == Limit UTXO growth
I?m not surprised that you find an model more efficient than Core. But I don?t believe one could find a model that doesn?t become more efficient with UTXO growth limitation.
Maybe you could try an experiment with regtest? Make a lot 1-in-100-out txs with many blocks, then spend all the UTXOs with 100-in-1-out txs. Compare the performance of bitcrust with core. Then repeat with 1-in-1-out chained txs (so the UTXO set is always almost empty)
One more question: what is the absolute minimum disk and memory usage in bitcrust, compared with the pruning mode in Core?

@_date: 2017-04-09 04:21:04
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
pre-synced means already in mempool and verified? Then it sounds like we just need some mempool optimisation? The tx order in a block is not important, unless they are dependent
Please no conspiracy theory like stepping on someone?s toes. I believe it?s always nice to challenge the established model. However, as I?m trying to make some hardfork design, I intend to have a stricter UTXO growth limit. As you said "protocol addressing the UTXO growth, might not be worth considering protocol improvements*, it sounds like UTXO growth limit wouldn?t be very helpful for your model, which I doubt.

@_date: 2017-04-10 18:14:36
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Extension block proposal by Jeffrey et al 
This won?t solve the problem. Think about the following conversation:
Alice (not upgraded): Please pay 1 BTC to my address 1ALicExyz
Bob (upgraded): ok, paid, please check
10 minutes later
Alice: received and confirmed, thanks!
5 minutes later:
Carol (not upgraded): Please pay 0.5BTC to my address 3CaroLXXX
Alice: paid, please check
1 hour later:
Carol: it?s not confirmed. Have you paid enough fees?
Alice: ok, I?ll RBF/CPFP it
2 hours later:
Carol: it?s still not confirmed.
Alice: I have already paid double fees. Maybe the network is congested and I need to pay more?..
Repeat until the lock up period ends.
So this so-called ?softfork? actually made non-upgraded wallet totally unusable. If failed to meet the very important requirement of a softfork: backward compatibility
More discussion:
 This is also unacceptable.
When someone says "Please pay 1 BTC to my address 1ALicExyz?, no one anticipates being paid by a coinbase output. Some exchanges like btc-e explicitly reject coinbase payment.
Such deterioration in user experience is unacceptable. It basically forces everyone to upgrade, i.e. a hardfork with soft fork?s skin
As I explained above, no legacy wallet would anticipate a lock up. If you want to make a softfork, all burden of incompatibility must be taken by the upgraded system. Only allow exit to a new address guarantees that only upgraded wallet will see the locked-up tx:
 I think it?s unacceptable if malleability is not fixed in main chain, for 3 reasons: 1. a solution is *already* available and tested for > 1 year.
2. the deactivation design (which I think is an interesting idea) makes the ext block unsuitable for long-term storage of value.
3. LN over main chain allows instant exchange of main coin and xcoin without going through the ugly 2-way-peg process.

@_date: 2017-04-27 03:31:38
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Segwit v2 
I prefer not to do anything that requires pools software upgrade or wallet upgrade. So I prefer to keep the dummy marker, and not change the commitment structure as suggested by another post.
For your second suggestion, I think we should keep scriptSig empty as that should be obsoleted. If you want to put something in scriptSig, you should put it in witness instead.
Maybe we could restrict witness to IsPushOnly() scriptPubKey, so miners can?t put garbage to legacy txs. But I think relaxing the witness program size to 73 bytes is enough for any purpose.

@_date: 2017-04-27 04:09:34
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Segwit v2 
You can do this with witness too, which is also cheaper. Just need to make sure the signature covers a special part of the witness. I will make a proposal to Litecoin soon, which allows signing and executing extra scripts in witness. Useful for things like OP_PUSHBLOCKHASH
Witness is cheaper and bigger

@_date: 2017-04-28 04:10:03
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Trustless Segwit activation bounty protocol (aka. 
As other explained, your scheme is broken.
Unless we have a softfork first (OP_CHECKBIP9VERIFY: payment is valid only if a BIP9 proposal is active), it is not possible to create a softfork bounty in a decentralised way
On the other hand, hardfork bounty is very simple. You just need to make sure your tx violates existing rules

@_date: 2017-08-29 11:30:07
@_author: Johnson Lau 
@_subject: [bitcoin-dev] P2WPKH Scripts, P2PKH Addresses, 
Yes it is allowed in TxOuts. And yes it is designed to save space. But the problem is Bob can?t assume Alice understands the new TxOuts format. If Bob really wants to save space this way, he should first ask for a new BIP173 address from Alice. Never try to convert a P2PKH address to a P2SH or BIP173 address without the consent of the recipient.

@_date: 2017-02-06 20:39:21
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Spoonnet: another experimental hardfork 
Finally got some time over the Chinese New Year holiday to code and write this up. This is not the same as my previous forcenet (  ). It is much simpler. Trying to activate it on testnet will get you banned. Trying to activate it on mainnet before consensus is reached will make you lose money.
This proposal includes the following features:
1. A fixed starting time. Not dependent on miner signalling. However, it requires at least 51% of miners to actually build the new block format in order to get activated.
2. It has no mechanism to prevent a split. If 49% of miners insist on the original chain, they could keep going. Split prevention is a social problem, not a technical one.
3. It is compatible with existing Stratum mining protocol. Only pool software upgrade is needed
4. A new extended and flexible header is located at the witness field of the coinbase transaction
5. It is backward compatible with existing light wallets
6. Dedicated space for miners to put anything they want, which bitcoin users could completely ignore. Merge-mining friendly.
7. Small header space for miners to include non-consensus enforced bitcoin related data, useful for fee estimation etc.
8. A new transaction weight formula to encourage responsible use of UTXO
9. A linear growth of actual block size until certain limit
10. Sighash O(n^2) protection for legacy (non-segwit) outputs
11. Optional anti-transaction replay
12. A new optional coinbase tx format that allows additional inputs, including spending of immature previous coinbase outputs
Specification [Rationales]:
* A "hardfork signalling block" is a block with the sign bit of header nVersion is set [Clearly invalid for old nodes; easy opt-out for light wallets]
* If the median-time-past of the past 11 blocks is smaller than the HardForkTime (exact time to be determined), a hardfork signalling block is invalid.
* Child of a hardfork signalling block MUST also be a hardfork signalling block
* Initial hardfork signalling is optional, even if the HardForkTime has past [requires at least 51% of miners to actually build the new block format]
* HardForkTime is determined by a broad consensus of the Bitcoin community. This is the only way to prevent a split.
Extended header:
* Main header refers to the original 80 bytes bitcoin block header
* A hardfork signalling block MUST have a additional extended header
* The extended header is placed at the witness field of the coinbase transaction [There are 2 major advantages: 1. coinbase witness is otherwise useless; 2. Significantly simply the implementation with its stack structure]
* There must be exactly 3 witness items (Header1; Header2 ; Header3)
**Header1 must be exactly 32 bytes of the original transaction hash Merkle root.
**Header2 is the secondary header. It must be 36-80 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets]
**Header3 is the miner dedicated space. It must not be larger than 252 bytes. Anything put here has no consensus meaning [space for merge mining; non-full nodes could completely ignore data in this space; 252 is the maximum size allowed for signal byte CompactSize]
* The main header commitment is H(Header1|H(H(Header2)|H(Header3)))  H() = dSHA256() [The hardfork is transparent to light wallets, except one more 32-byte hash is needed to connect a transaction to the root]
* To place the ext header, segwit becomes mandatory after hardfork
A ?backdoor? softfork the relax the size limit of Header 2 and Header 3:
* A special BIP9 softfork is defined with bit-15. If this softfork is activated, full nodes will not enforce the size limit for Header 2 and Header 3. [To allow header expansion without a hardfork. Avoid miner abuse while providing flexibility. Expansion might be needed for new commitments like fraud proof commitments]
* Hardfork network version bit is 0x02000000. A tx is invalid if the highest nVersion byte is not zero, and the network version bit is not set.
* Masked tx version is nVersion with the highest byte masked. If masked version is 3 or above, sighash for OP_CHECKSIG alike is calculated using BIP143, except 0x02000000 is added to the nHashType (the nHashType in signature is still a 1-byte value) [ensure a clean split of signatures; optionally fix the O(n^2) problem]
* Pre-hardfork policy change: nVersion is determined by the masked tx version for policy purpose. Setting of Pre-hardfork network version bit 0x01000000 is allowed.
* Details: Sighash limitation:
* Sighash impact is estimated by ?Loose estimation? in * Only txs with masked version below 3 are counted. [because they are fixed by the BIP-143 like signature]
* Each SigHashSize is defined as 1 tx weight (defined later).
* SIGHASH_SCALE_FACTOR is 90 (see the BIP above)
New tx weight definition:
* Weight of a transaction is the maximum of the 4 following metrics:
** The total serialised size * 2 * SIGHASH_SCALE_FACTOR  (size defined by the witness tx format in BIP144)
** The adjusted size = (Transaction weight by BIP141 - (number of inputs - number of non-OP_RETURN outputs) * 41) * SIGHASH_SCALE_FACTOR
** nSigOps * 50 * SIGHASH_SCALE_FACTOR. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.
** SigHashSize defined in the last section
Translating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.
See rationales in this post: Block weight growing by time:
* Numbers for example only. Exact number to be determined.
* Block weight at HardForkTime is (5,000,000 * SIGHASH_SCALE_FACTOR)
* By every 16 seconds growth of the median-time-past, the weight is increased by (1 * SIGHASH_SCALE_FACTOR)
* The growth stops at (16,000,000 * SIGHASH_SCALE_FACTOR)
* The growth does not dependent on the actual hardfork time. It?s only based on median-time-past [using median-time-past so miners have no incentive to use a fake timestamp]
* The limit for serialized size is 2.5 to 8MB in about 8 years. [again, numbers for example only]
New coinbase transaction format:
* Existing coinbase format is allowed, except the new extended header in the coinbase witness. No OP_RETURN witness commitment is needed.
* A new coinbase format is defined. The tx may have 1 or more inputs. The outpoint of the first input MUST have an n value of 0xffffffff, and use the previous block hash as the outpoint hash [This allows paying to the child of a particular block by signing the block hash]
* ScriptSig of the first (coinbase) input is not executed. The size limit increased from 100 to 252 (same for old coinbase format)
* Additional inputs MUST provide a valid scriptSig and/or witness for spending
* Additional inputs may come from premature previous coinbase outputs [this allows previous blocks paying subsequent blocks to encourage confirmations]
Witness merkle root:
* If the coinbase is in old format, the witness merkle root is same as BIP141 by setting the witness hash of the coinbase tx as 0 (without the 32 byte witness reserved value)
* If the coinbase is in new format, the witness hash of the coinbase tx is calculated by first removing the extended header
* The witness merkle root is put in the extended header 2, not as an OP_RETURN output in coinbase tx.
* The witness merkle root becomes mandatory. (It was optional in BIP141)
Other consensus changes:
* BIP9 will ignore the sign bit. [Setting the sign bit now is invalid so this has no real consensus impact]
An experimental implementation of the above spec could be found at Not the same as my previous effort on the ?forcenet?, the ?spoonnet? is a full hardfork that will get you banned on the existing network.
Haven?t got the time to test the codes yet, not independently reviewed. But it passes all existing tests in Bitcoin Core. No one should use this in production, but I *think* it works fine on testnet like a normal bitcoind (as long as it is not activated)
Things not implemented yet:
1. Automated testing
2. Post-hardfork support for old light wallets
3. Wallet support, especially anti-tx-replay
4. New p2p message to transmit secondary header (lower priority)
5. Full mining and mempool support (not my priority)
Potential second stage change:
Relative to the actual activation time, there could be a second stage with more drastic changes to fix one or both of the following problems:
1. SHA256 shortcut like ASICBoost. All fixes to ASICBoost are not very elegant. But the question is, is it acceptable to have bitcoin-specific patent in the consensus protocol? Still, I believe the best way to solve this problem is the patent holder(s) to kindly somehow release the right to the community. 2. Providing more nonce space in the 80-byte main header. However, this depends on ASICBoost being a free technology.
3. Block withholding attack. There are pros and cons, but I generally agree with the analysis by Peter Todd at  . One point he didn?t mention is that only small really needs pool mining, for the purpose of variance reduction. Big miners using pools are just lazy, and they work well without pool. That means only big solo miners are able to attack pools (i.e. small miners), while pools cannot do any counterattack. This obviously shows why fixing this is pro-small-miners. Also, with same hash rate, block withholding attack is more effective against a smaller pool than a big pool.
All of these changes involve a header change and require light wallets to upgrade. They also require firmware upgrade for all existing miners (change 2 doesn?t). I think these shouldn?t happen at least 2 years after the actual activation of the hardfork so people will have enough time to upgrade.

@_date: 2017-02-07 02:06:22
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Spoonnet: another experimental hardfork 
I fixed a flaw in my original design. It allowed a miner to create a fake transaction to cheat light wallets.
Also, with a second thought, I removed the backward compatibility with light wallets. Everyone, light or full, should opt-in to a hardfork.
The extended header is amended as follow
* There must be exactly 2 witness items in coinbase witness (Header1; Header2)
**Header1 is the miner dedicated space. Nothing here has no consensus meaning and is used for extranonce and merge mining. However, if the size is larger than 14 bytes, the first 6 bytes must be 0. For a legacy light node, this will look like a version 0 transaction with zero input and output. [No need for 14 bytes or below: even with the most broken design, a wallet can?t misinterpret data with this small size as a valid tx]
**Header2 is the secondary header. It must be 70-128 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 2 bytes must be 0. The next 32 bytes must be the transaction Merkle root. The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets; the structure of the first 6 bytes make it looks like an empty tx]
* The main header commitment is H(H(Header1)|H(Header2))  H() = dSHA256() [legacy light wallets are broken and they must upgrade to join the new network]
The implementation is updated accordingly

@_date: 2017-01-03 11:39:19
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Script Abuse Potential? 
No, there could only have not more than 201 opcodes in a script. So you may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB
For OP_CAT, just check if the returned item is within the 520 bytes limit.

@_date: 2017-01-15 05:14:55
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
I created a second version of forcenet with more experimental features and stopped my forcenet1 node.
1. It has a new header format: Height (4), BIP9 signalling field (4), hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4), nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches leading to header C (compactSize + 32 bit hashes)
2. Anti-tx-replay. If, after masking the highest byte, the tx nVersion is >=3, the sighash for both segwit and non-segwit outputs is calculated with BIP143, except 0x2000000 is added to the nHashType. Such signatures are invalid for legacy nodes. But since they are non-std due the nVersion, they won?t be relayed nor validated by legacy nodes. This also removes the O(n^2) sighash problem when spending non-segwit outputs. (anti-replay is a long story and I will discuss in a separate post/BIP)
3. Block sighashlimit ( ). Due to point 2, SigHashSize is counted only for legacy non-segwit inputs (with masked tx nVersion < 3). We have to support legacy signature to make sure time-locked txs made before the hard fork are still valid.
4. A totally new way to define tx weight. Tx weight is the maximum of the following metrics:
a. SigHashSize (see the bip in point 3)
b. Witness serialised size * 2 * 90
c. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of non-OP_RETURN outputs - number of inputs) * 41 * 4
d. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.
90 is the scaling factor for SigHashSize, to maintain the 1:90 ratio (see the BIP in point 3)
50 is the scaling factor for nSigOps, maintaining the 1:50 ratio in BIP141
Rationale for adjusted size: 4 is witness scaling factor. 41 is the minimum size for an input (32 hash + 4 index + 4 nSequence + 1 scriptSig). This requires people to pre-pay majority of the fee of spending an UTXO. It makes creation of UTXO more expensive, while spending of UTXO cheaper, creates a strong incentive to limit the growth of UTXO set.
Rationale for taking the maximum of different metrics: this indirectly set an upper block resources for _every_ metrics, while making the tx fee estimation a linear function. Currently, there are 2 block resources limits: block weight and nSigOp cost (BIP141). However, since users do not know what the other txs are included in the next block, it is difficult to determine whether tx weight of nSigOp cost is a more important factor in determining the tx fee. (This is not a real problem now, because weight is more important in most cases). With an unified definition of tx weight, the fee estimation becomes a linear problem.
Translating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.
Any new block-level limit metrics could be added to tx weight using soft forks.
5. Smooth halving: the reward of the last 2016 blocks in a halving cycle will be reduced by 25%, which is contributed to the first 2016 blocks of the new halving cycle. (different parameters for forcenet) This makes a more graceful transition but we will lose some fun around halving.
6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more than 1 input. The prevout hash of first input must be the hash of previous block, and index must be 0xffffffff. The other inputs (if any) must come from UTXOs with valid signatures. Spending of previous coinbase outputs in a coinbase tx is exempted from the 100 block maturity requirement. Therefore, miners of an earlier block may pay other miners to convince them to confirm their blocks.
7. Merkle sum tree: it allows generating of fraud-proof for fee and weight. A special softfork (bit 15) is defined. When this softfork is activated, the full node will not validate the sum tree. This is needed because when the definition of tx weight is changed through a softfork (e.g. a new script version introducing new sigop), olds nodes won?t know the new rules and will find the sum tree invalid. Disabling the sum tree validation won?t degrade the security of a full node by more than an usual softfork, because the full node would still validate all other known rules.
However, it is still not possible to create fraud proof for spending of non-existing UTXO. This requires commitment of the block height of inputs, and the tx index in the block. I?m not quire sure how this could be implemented because a re-org may change such info (I think validation is easy but mining is more tricky)
How to join: codes at   , start with "bitcoind ?forcenet" .
Connection: I?m running a node at 8333.info  with default port (39901)
Mining: there is only basic internal mining support. To use the internal miner, writeup a shell script to repeatedly call ?bitcoin-cli ?forcenet generate 1?

@_date: 2017-01-24 22:33:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
This is a pre-BIP. Just need some formatting to make it a formal BIP
In general, hardforks are consensus rule changes that make currently invalid transactions / blocks valid. It requires a very high degree of consensus and all economic active users migrate to the new rules at the same time. If a significant amount of users refuse to follow, a permanent ledger split may happen, as demonstrated by Ethereum (?DAO hardfork"). In the design of DAO hardfork, a permanent split was not anticipated and no precaution has been taken to protect against transaction replay attack, which led to significant financial loss for some users.
A replay attack is an attempt to replay a transaction of one network on another network. It is normally impossible, for example between Bitcoin and Litecoin, as different networks have completely different ledgers. The txid as SHA256 hash guarantees that replay across network is impossible. In a blockchain split, however, since both forks share the same historical ledger, replay attack would be possible, unless some precautions are taken.
Unfortunately, fixing problems in bitcoin is like repairing a flying plane. Preventing replay attack is constrained by the requirement of backward compatibility. This proposal has the following objectives:
A. For users on both existing and new fork, anti-replay is an option, not mandatory.
B. For transactions created before this proposal is made, they are not protected from anti-replay. The new fork has to accept these transactions, as there is no guarantee that the existing fork would survive nor maintain any value. People made time-locked transactions in anticipation that they would be accepted later. In order to maximise the value of such transactions, the only way is to make them accepted by any potential hardforks.
C. It doesn?t require any consensus changes in the existing network to avoid unnecessary debate.
D. As a beneficial side effect, the O(n^2) signature checking bug could be fixed for non-segregated witness inputs, optionally.
?Network characteristic byte? is the most significant byte of the nVersion field of a transaction. It is interpreted as a bit vector, and denotes up to 8 networks sharing a common history.
?Masked version? is the transaction nVersion with the network characteristic byte masked.
?Existing network? is the Bitcoin network with existing rules, before a hardfork. ?New network? is the Bitcoin network with hardfork rules. (In the case of DAO hardfork, Ethereum Classic is the existing network, and the now called Ethereum is the new network)
?Existing network characteristic bit? is the lowest bit of network characteristic byte
?New network characteristic bit? is the second lowest bit of network characteristic byte
Rules in new network:
1. If the network characteristic byte is non-zero, and the new network characteristic bit is not set, this transaction is invalid in the new network. (softfork)
2. If the network characteristic byte is zero, go to 4
3. If the network characteristic byte is non-zero, and the new network characteristic bit is set, go to 4, regardless of the status of the other bits.
4. If the masked version is 2 or below, the new network must verify the transaction with the existing script rules. (no change)
5. If the masked version is 3 or above, the new network must verify the signatures with a new SignatureHash algorithm (hardfork). Segwit and non-segwit txs will use the same algorithm. It is same as BIP143, except that 0x2000000 is added to the nHashType before the hash is calculated.
Rules in the existing network:
6. No consensus rule changes is made in the existing network.
7. If the network characteristic byte is non-zero, and the existing network characteristic bit is not set, this transaction is not relayed nor mined by default (no change)
8. If the network characteristic byte is zero, no change
9. If the network characteristic byte is non-zero, and the existing network characteristic bit is set, the masked version is used to determine whether a transaction should be mined or relayed (policy change)
10. Wallet may provide an option for setting the existing network characteristic bit.
Rationales (by rule number):
1. This makes sure transactions with only existing network characteristic bit set is invalid in the new network (opt-in anti-replay for existing network transactions on the new network, objective A)
2+4. This makes sure time-locked transactions made before this proposals are valid in the new network (objective B)
2+5. This makes sure transactions made specifically for the new network are invalid in the existing network (anti-replay for new network transactions on the old network); also fixing the O(n^2) bug (objectives A and D)
3. This is to prepare for the next hardfork from the new network (objective A)
6, 7, 8. These minimise the change to the existing network (objective C)
9, 10. These are not strictly needed until a hardfork is really anticipated. Without a significant portion of the network and miners implement this policy, however, no one should create such transactions. (objective A)
* It is not possible to protect transactions made before the proposal. To avoid a replay of such transactions, users should first spend at least a relevant UTXO on the new network so the replay transaction would be invalidated.
* It is up to the designer of a hardfork to decide whether this proposal is respected. As the DAO hardfork has shown how harmful replay attack could be, all hardfork proposals (except trivial and totally uncontroversial ones) should take this into account
* The size of network characteristic byte is limited to 8 bits. However, if we are sure that some of the networks are completely abandoned, the bits might be reused.
Reference implementation:
A demo is available in my forcenet2 branch:

@_date: 2017-01-25 12:03:40
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
Yes, it?s similar. I?ll quote your design if/when I formalise my BIP. But it seems they are not the same: yours is opt-out, while mine is opt-in.
However, my proposal in nowhere depends on standardness for the protection. It depends on the new network enforcing a new SignatureHash for txs with an nVersion not used in the existing network. This itself is a hardfork and the existing network would never accept such txs.
This is to avoid requiring any consensus changes to the existing network, as there is no guarantee that such softfork would be accepted by the existing network. If the new network wants to protect their users, it?d be trivial for them to include a SignatureHash hardfork like this, along with other other hardfork changes. Further hardforks will only require changing the network characteristic bit, but not the SignatureHash.
If the hardfork designers don?t like the fix of BIP143, there are many other options. The simplest one would be a trivial change to Satoshi?s SignatureHash, such as adding an extra value at the end of the algorithm. I just don?t see any technical reasons not to fix the O(n^2) problem altogether, if it is trivial (but not that trivial if the hardfork is not based on segwit)

@_date: 2017-01-25 15:05:59
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
What you describe is not a fix of replay attack. By confirming the same tx in both network, the tx has been already replayed. Their child txs do not matter.

@_date: 2017-01-25 15:21:57
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
Assuming Alice is paying Bob with an old style time-locked tx. Under your proposal, after the hardfork, Bob is still able to confirm the time-locked tx on both networks. To fulfil your new rules he just needs to send the outputs to himself again (with different tx format). But as Bob gets all the money on both forks, it is already a successful replay

@_date: 2017-01-25 15:42:13
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
If Alice still has full control, she is already protected by my proposal, which does not require any protecting child transaction.
But in many cases she may not have full control. Make it clearer, consider that?s actually a 2-of-2 multisig of Alice and Bob, and the time locked tx is sending to Bob. If the time locked tx is unprotected in the first place, Bob will get all the money from both forks anyway, as there is no reason for him to renegotiate with Alice.

@_date: 2017-01-26 15:14:52
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
I don?t think this is how the blockchain consensus works. If there is a split, it becomes 2 incompatible ledgers. Bitcoin is not a trademark, and you don?t need a permission to hardfork it. And what you suggest is also technically infeasible, as the miners on the new chain may not have a consensus only what?s happening in the old chain.

@_date: 2017-01-26 17:20:54
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
BIP50 was an accident, and my proposal is just for a planned hardfork. You can?t anti-replay if you don?t even know a hardfork might happen. And I think your hypothesis (replay reduces the incentive of split) is not supported by the ETC/ETH split.
Aside the philosophical argument, your proposal is not technically feasible. In my understanding, you require the new chain to replay all the txs in the old chain. But this is not possible because there could be orphaning in the old chain, and different miners of the new chain may see a different history of the old chain. Not to mention that mining is a random process, and the hashing power is going up and down. Just by chance, 10 blocks might be generated in the old chain while no block is generated in the new chain. This is also unfair to the new chain miners, as they may not satisfied with the fees paid while they are forced to include those txs from the old chain (remember that people may just pay the old chain miners out of band, and pay no fee in the transaction)
I don?t think these technical issues are solvable when both forks are decentralised mining. If time machines, for example, are not technically feasible, there is not much point to talk about the benefits of time machines.

@_date: 2017-01-26 17:39:43
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Extension block softfork proposal 
This is a pre-BIP which allows extra block space through a soft-fork. It is completely transparent to existing wallets (both send and receive), but new wallets taking advantage of the extra block space will have a very different user experience.
I?m sure this is controversial but I think it?s an interesting academic topic. If we?d ever have any fully consensus enforced 2-way-peg side chain design, that?d be something like this.
1. Provide more block space through a soft forks
2. Completely transparent to existing wallets
3. Not breaking any current security assumptions
Specification and Terminology:
Main block / block: the current bitcoin block (with witness if BIP141 is activated)
Main transaction / tx: txs in the current bitcoin network (with witness)
Main UTXO / UTXO: the normal UTXO
Extension transaction / xtx: transactions with a format same as the witness tx format described in BIP141, without scriptSig field, and the ?flag? as 0x02. Only witness program are allowed for scriptPubKey of xtx
Extension block / xblock: xblock is a collection of xtx. Each block may have 0 or 1 xblock when this softfork is activated.
Extension UTXO / xUTXO: the UTXO set for of the extension block.
Bridging witness program: A new type of witness program is defined. The witness script version is OP_2. The program length could be 4 to 40. The first byte ("direction flag?[note 1]) must be 0x00 (indicating block->xblock) or 0x01 (indicating xblock->block). Like P2WPKH and P2WSH, the bridging program could be wrapped by P2SH. There are 2 ways to spend this program type on the main block:
  1) Spend it like a usual witness program with a tx. For example, if the bridging program is OP_2 <0x000014{20 bytes}>, it could be spent like a version-0 20bytes programme, i.e. P2WPKH. Nothing special would happen in this case
  2) Spend it like a usual witness program with a special xtx, the genesis xtx. In this case, the miner including this xtx will need to do more as described below.
Integrating UTXO: a special UTXO with a value >= the total value of all existing xUTXO and scriptPubKey is OP_1. (to make the spec easier to read, here we assume that now we have a zero value UTXO with its outpoint hardcoded as the initial integrating UTXO. In practice we may have the first miner making xblock to create the initial integrating UTXO)
Integrating transaction: if a block has an xblock, the second transaction in the block must be the integrating transaction. The inputs include the spent UTXO of all the genesis xtx in this xblock. If it is a bare witness program, the witness must be empty. If it is a P2SH witness program, the scriptSig must be the bridging witness program and the witness must be empty. The last input must be the original integrating UTXO, with empty witness and scriptSig. If no one is trying to send money back from the xblock to the main block, the only output is the updated integrating UTXO, which the value must be >= the total value of all xUTXO
Up to now, I have described how we could send bitcoins from the main UTXO to the xUTXO. Simply speaking, people send money to a new form of witness programme. They have the flexibility to spend it in the main block or xblock. Nothing special would happen if they send to the main block. If they send to the xblock, the value of such UTXO will be collected by the integrating UTXO.
After people sent money to xblock, they could trade inside the xblock just like in the main block. Since xblock is invisible to the pre-softfork users, we could have whatever size limit for the xblock, which is not a topic of this proposal.
The tricky part is sending from xblock to main block.
Returning transaction: returning transaction is a special xtx, sending money to a bridging witness program, with a direction flag of 0x01. These bridging witness program won?t be recorded in the xUTXO set. Instead, an output is added to the integrating tx, with the bridging witness program and corresponding value, called the ?returning UTXO?. The returning UTXOs are not spendable until confirmed by 100 blocks. The updated integrating UTXO is the last output, and is not restricted by the 100-block requirement
Fees collection in xblock: Same as normal tx, people pay fee in xblock by making output value < input value. Since the value of the integrating UTXO is >= the total value of all existing xUTXO, if fees are paid in the xblock, that will reduce the value of the integrating UTXO, and miners are paid through the usual coinbase tx as fee.
xblock commitment: 2 xblock merkle root, with and without witness, are placed exactly after the witness commitment in the coinbase tx.(maybe we could use the coinbase reserved witness value, details TBD). If there is no xblock commitment, xblock must be empty and integrating tx is not allowed.
Same as any 2-way-peg proposal, sending money from the side chain to the main chain is always the most tricky part. Different from other side chain proposals like Rootstock, extension block is fully consensus enforced, and has the same security level as existing bitcoin transactions. To ensure this, an 100-block maturity is needed for the returning UTXO, as the TXID of the integrating transaction is *very* likely to change after a reorg, which will break the transaction chains coming from it. The 100-block maturity requirement bring us back to the usual assumption that txs become permanent after 100 confirmations.
Please note that this drastically changes the user experience, as no current users (expect miners) would expect such 100-block freezing. That?s why I don?t allow the returning UTXO to have an arbitrary scriptPubKey, as users of current wallet would never expect such freezing. Using a special output scriptPubKey guarantees that the recipient must understand the implications. Users of the new wallet should be warned that despite they may enjoy lower fees in the xblock, it may be difficult for them to send money to legacy wallets. This is a huge limitation.
Maybe we could have some decentralised market (using simple hash-time-locked txs) allowing people to exchange value between block and xblock, bypassing the 100 block requirement. This is actually cheaper, because a full returning is a 2-step process, while p2p exchange is only 1-step.
1. Is it possible to simplify the design, without compromising security?
2. Is it acceptable to do it without the 100-block maturity requirement, thus breaking some long-held assumptions? (This would vastly improve the usability, until a reorg happens)
3. Even with maturity requirement, is 100-block an overkill? We never had a fork over maybe 20 blocks. Also, breaking of transaction chain due to reorg is already possible, as people may double spend during a reorg.
[note 1] the direction flag is needed to make sure a recipient won?t be paid with a returning transaction, unless explicitly requested. It might be combined with the serialised witness version to save one byte.

@_date: 2017-01-26 20:57:32
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Changing the transaction version number to be 
Yes, because:
a) what you are talking is a hardfork, because existing nodes will not be able to deserialise the transaction. They will forever interpret the first 4 bytes as nVersion.
b) it is not a ?lie? to use non-version 1 txs. It is permitted since v0.1. And version 2 txs is already used due to BIP68.
c) if you are talking about changing the tx serialisation just for network transfer, it?s just a p2p protocol upgrade, not softfork nor hardfork
There are 3 ways to introduce new tx formats:
1. through a softfork, and make the old clients blind to the new format. That?s the segwit approach
2. through a hardfork. Forget the old clients and require new clients to understand the new format. That?s the FlexTran approach (in my understanding)
3. p2p only, which won?t affect consensus. No one could stop you if you try to copy a block by writing in your native language and pass to your peer.
In either way, one could introduce whatever new format one wants.

@_date: 2017-01-27 12:21:21
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
I can?t recommend your first 2 proposals. But I only have the time to talk about the first one for now.
There are 2 different views on this topic:
1. ?The block size is too small and people can?t buy a coffee with an on-chain transaction. Let?s just remove the limit?
2. ?The block size is too big and people can?t run full nodes or do initial blockchain download (IBD). Let?s just reduce the limit?
For me, both approaches just show the lack of creativity, and lack of responsibility. Both just try to solve one problem, disregarding all the other consequences.
The 1MB is here, no matter you like it or not, it?s the current consensus. Any attempts to change this limit (up or down) require wide consensus of the whole community, which might be difficult.
Yes, I agree with you that the current 1MB block size is already too big for many people to run a full node. That?s bad, but it doesn?t mean we have no options other than reducing the block size. Just to cite some:
1. Blockchain pruning is already available, so the storage of blockchain is already an O(1) problem. The block size is not that important for this part
2. UTXO size is an O(n) problem, but we could limit its growth without limit the block size, by charging more for UTXO creation, and offer incentive for UTXO spending  **
3. For non-mining full node, latency is not critical. 1MB per 10 minutes is not a problem unless with mobile network. But I don?t think mobile network is ever considered as a suitable way for running a full node
4. For mining nodes, we already have compact block and xthin block, and FIBRE
5. For IBD, reducing the size won?t help much as it is already too big for many people. The right way to solve the IBD issue is to implement long latency UTXO commitment. Nodes will calculate a UTXO commitment every 1000 block, and commit to the UTXO status of the previous 1000 block (e.g. block 11000 will commit to the UTXO of block 10000). This is a background process and the overhead is negligible. When such commitments are confirmed for sufficiently long (e.g. 1 year), people will assume it is correct, and start IBD from that point by downloading UTXO from some untrusted sources. That will drastically reduce the time for IBD
6. No matter we change the block size limit or not, we need to implement a fraud-proof system to allow probabilistic validation by SPV nodes. So even a smartphone may validate 0.1% of the blockchain, and with many people using phone wallet, it will only be a net gain to the network security For points 2 and 6 above, I have some idea implemented in my experimental hardfork.

@_date: 2017-01-28 04:36:03
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Consensus critical limits in Bitcoin protocol and 
There are many consensus critical limits scattered all over the Bitcoin protocol. The first part of this post is to analyse what the current limits are. These limits could be arranged into different categories:
1. Script level limit. Some limits are restricted to scripts, including size (10000 bytes), nOpCount (201), stack plus alt-stack size (1000), and stack push size (520). If these limits are passed, they won?t have any effects on the limits of the other levels.
2. Output value limit: any single output value must be >=0 and <= 21 million bitcoin
3. Transaction level limit: The only transaction level limit we have currently, is the total output value must be equal to or smaller than the total input value for non-coinbase tx.
4. Block level limit: there are several block level limits:
a. The total output value of all txs must be equal to or smaller than the total input value with block reward.
b. The serialised size including block header and transactions must not be over 1MB. (or 4,000,000 in terms of tx weight with segwit)
c. The total nSigOpCount must not be over 20,000 (or 80,000 nSigOpCost with segwit)
There is an unavoidable layer violation in terms of the block level total output value. However, all the other limits are restricted to its level. Particularly, the counting of nSigOp does not require execution of scripts. BIP109 (now withdrawn) tried to change this by implementing a block level SigatureHash limit and SigOp limit by counting the accurate value through running the scripts.
So currently, we have 2 somewhat independent block resources limits: weight and SigOp. A valid block must not exceed any of these limits. However, for miners trying to maximise the fees under these limits, they need to solve a non-linear equation. It?s even worse for wallets trying to estimate fees, as they have no idea what txs are miners trying to include. In reality, everyone just ignore SigOp for fee estimation, as the size/weight is almost always the dominant factor.
In order to not introduce further non-linearity with segwit, after examining different alternatives, we decided that the block weight limit should be a simple linear function:  3*base size + total size, which allows bigger block size and provides incentives to limit UTXO growth. With normal use, this allows up to 2MB of block size, and even more if multi-sig becomes more popular. A side effect is that allows a theoretical way to fill up the block to 4MB with mostly non-transaction data, but that?d only happen if a miner decide to do it due to non-standardness. (and this is actually not too bad, as witness could be pruned in the future)
Some also criticised that the weight accounting would make a ?simple 2MB hardfork? more dangerous, as the theoretical limits will be 8MB which is too much. This is a complete straw man argument, as with a hardfork, one could introduce any rules at will, including revolutionising the calculation of block resources, as shown below.
Proposal: a new block resources limit accounting
Objectives: 1. linear fee estimation
2. a single, unified, block level limit for everything we want to limit
3. do not require expensive script evaluation
1. the maximum base block size is about 1MB (for a hardfork with bigger block, it just needs to upscale the value)
2. a hardfork is done (despite some of these could also be done with a softfork)
Version 1: without segwit
The tx weight is the maximum of the following values:
? Serialised size in byte
? accurate nSigOpCount * 50 (statical counting of SigOp in scriptSig, redeemScript, and previous scriptPubKey, but not the new scriptPubKey)
The block level limit is 1,000,000
Although this looks similar to the existing approach, this actually makes the fee estimation a linear problem. Wallets may now calculate both values for a tx and take the maximum, and compare with other txs on the same basis. On the other hand, the total size and SigOpCount of a block may never go above the existing limits (1MB and 20000) no matter how the txs look like. (In some edge cases, the max block size might be smaller than 1MB, if the weight of some transactions is dominated by the SigOpCount)
Version 2: extending version 1 with segwit
The tx weight is the maximum of the following values:
? Serialised size in byte * 2
? Base size * 3 + total size
? accurate SigOpCount * 50 (as a hardfork, segwit and non-segwit SigOp could be counted in the same way and no need to scale) The block level limit is 4,000,000
For similar reasons the fee estimation is also a linear problem. An interesting difference between this and BIP141 is this will limit the total block size under 2MB, as 4,000,000 / 2 (the 2 as the scaling factor for the serialised size). If the witness inflation really happens (which I highly doubt as it?s a miner initiated attack), we could introduce a similar limit just with a softfork.
Version 3: extending version 2 to limit UTXO growth:
The tx weight is the maximum of the following values:
? Serialised size in byte * 2
? Adjusted size = Base size * 3 + total size + (number of non-OP_RETURN outputs - number of inputs) * 4 * 41
? accurate SigOpCount * 50
I have explained the rationale for the adjusted size in an earlier post but just repeat here. ?4? in the formula is the witness scale factor, and ?41? is the minimum size of transaction input (32 hash + 4 index + 4 sequence + 1 for empty scriptSig). This requires everyone to pay a significant portion of the spending fee when they create a UTXO, so they pay less when it is spent. For transactions with 1:1 input and output ratios, the effect is cancelled out and won?t actually affect the weight estimation. When spending becomes cheaper, even UTXOs with lower value might become economical to spend, which helps cleaning up the UTXO. Since UTXO is the most expensive aspect, I strongly believe that any block size increase proposal must somehow discourage further growth of the set.
Version 4: including a sighash limit
This is what I actually implemented in my experimental hardfork network:  I?m not repeating here, but it shows how further limits might be added on top of the old ones through a softfork. Basically, you just add more metrics, and always take to maximum one.

@_date: 2017-01-28 04:47:22
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Anti-transaction replay in a hardfork 
I think there are some misunderstanding. You?d better read my source code if my explanation is not clear.
From my understanding our proposals are the same, just with a bitwise not (~) before the network characteristic byte. So you set a bit to opt-out a network, while I set a bit to opt-in a network (and opt-out any other)

@_date: 2017-01-28 15:28:14
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Forcenet: an experimental network with a new 
I think the following features are necessary for a hardfork. The rest are optional:
1. A secondary header
2. Anti-replay
3. SigHash limit for old scripts
4. New tx weight accounting
1. New coinbase format is nice but not strictly needed. But this can?t be reintroduced later with softfork due to the 100 block maturity requirement
2. Smooth halving: could be a less elegant softfork
3. Mekle sum tree: definitely could be a softfork
Without nBits in the header, the checking of PoW become contextual and I think that may involve too much change. The saving of these 4 bytes, if it is really desired, might be done on a p2p level Regarding the header format, a big question we never came into consensus is the format of the hardfork. Although I designed forcenet to be a soft-hardfork, I am now more inclined to suggest a simple hardfork, given that the warning system is properly fixed (at the minimum:  )
Assuming a simple hardfork is made, the next question is whether we want to keep existing light wallets functioning without upgrade, cheating them by hiding the hash of the new header somewhere in the transaction merkle tree.
We also need to think about the Stratum protocol. Ideally we should not require firmware upgrade.
For the primary 80 bytes header, I think it will always be a fixed size. But for the secondary header, I?m not quite sure. Actually, one may argue that we already have a secondary header (i.e. coinbase tx), and it is not fixed size.
The max() is at transaction level, not block level. Unless your wallet is full of different types of UTXOs, coin selection would not be more difficult than current.
Among the 4 limits, the SigHash limit is mostly a safety limit that will never be hit by a tx smaller than 100kB. As part of the replay attack fix, a linear SigHash may be optionally used. So wallets may just ignore this limit in coin selection
Similarly, the SigOp limit is also unlikely to be hit, unless you are using a very big multi-sig. Again, this is very uncommon and wallets primarily dealing with signal sig may safely ignore this
Finally, an important principle here is to encourage spending of UTXO, and limiting creation of UTXO. This might be a bit difficult to fully optimise for this, but I think this is necessary evil.
More discussion at:  Yes, but maybe we just don?t need this at all. This could also be done with a softfork using OP_CSV, just a bit ugly.
This allows people to sign an input, to be part of a coinbase tx, but limited to a particular previous block hash. This is currently not possible, but through a later softfork we could introduce a new SigHash function that allows something between SIGHASH_ALL and SIGHASH_ANYONECANPAY, so people may sign its own input and another input, while ignoring the rests of input. (in other words: change the name SIGHASH_ANYONECANPAY to SIGHASH_SINGLE_INPUT, and we introduce SIGHASH_DUAL_INPUT. But we don?t need to do this in this hardfork)
This is just a demo, and I agree this could be added through a softfork later. But even if we add this as a softfork, we have to have the ability to disable it through a special softfork. I think I have explained the reason but let me try again.
Here, when I talking about ?tx weight?, it?s the ?tx weight? defined in point 4, which covers not only size, but also other limits like SigOp. For a fraud proof to be really useful, it has to cover every type of block level limits. One feature of segwit is the script versioning, which allows introduction of new scripts. In the process, we will change the definition of SigOp: previous 0 SigOp scripts now carries some amount of SigOp. This is by itself a softfork (we did this type of softfork twice already: P2SH and segwit). However, if we have a merkle sum root covering the SigOp, old nodes won?t count these new SigOps, and they will fail to validate the sum root.
Without a backdoor to disable the sum tree validation in old nodes, the only way would be keeping the original sum tree untouched, while create another sum tree, every time we have a new script version. This is not acceptable at all.
But even such backdoor would not be harmful to the security of full nodes because they will still fully verify the tx and witness merkle root.
I?d argue that any fraud proof related commitment: sum tree, delayed UTXO commitment etc will require such a backdoor to disable. Maybe we should just remove this from here and make this a new topic. We could even do this as a softfork today.

@_date: 2017-03-29 01:34:23
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
You are probably not the first one nor last one with such idea. Actually, Luke wrote up a BIP with similar idea in mind:
 Instead of just lifting the block size limit, he also suggested to remove many other rules. I think he has given up this idea because it?s just too complicated.
If we really want to prepare for a hardfork, we probably want to do more than simply increasing the size limit. For example, my spoonnet proposal:
 In a HF, we may want to relocate the witness commitment to a better place. We may also want to fix Satoshi's sighash bug. These are much more than simple size increase.
So if we really want to get prepared for a potential HF with unknown parameters, I?d suggest to set a time bomb in the client, which will stop processing of transactions with big warning in GUI. The user may still have an option to continue with old rules at their own risks.
Or, instead of increasing the block size, we make a softfork to decrease the block size to 1kB and block reward to 0, activating far in the future. This is similar to the difficulty bomb in ETH, which will freeze the network.

@_date: 2017-03-29 12:21:33
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Just take something like FlexTran as example. How you could get prepared for that without first finalising the spec?
Or changing the block interval from 10 minutes to some other value?
Also, fixing the sighash bug for legacy scripts?
There are many other ideas that require a HF:

@_date: 2017-03-29 23:34:41
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
4 * 144 * 30 = 17.3GB per month, or 207GB per year. Full node initialisation will become prohibitive for most users until a shortcut is made (e.g. witness pruning and UTXO commitment but these are not trust-free)
Also as the co-author of the selfish mining paper, you should know all these technology assume big miners being benevolent.

@_date: 2017-05-09 23:45:04
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
As we could change any parameter in a hardfork, I don?t think this has any relation with the current BIP141 proposal. We could just use 75% in a softfork, and change that to a different value (or completely redefine the definition of weight) with a hardfork later.

@_date: 2017-05-10 00:27:40
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
No, changing from 50% to 75% is a hardfork. (75 -> 50 is a softfork). Unless you make it pre-scheduled, or leave a special ?backdoor? softfork to change the discount.
And that would certainly reduce the max tx/s with 50% discount, also reduce the incentive to spend witness UTXO.

@_date: 2017-05-10 01:56:28
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Extension block proposal by Jeffrey et al 
To make it completely transparent to unupgraded wallets, the return outputs have to be sent to something that is non-standard today, i.e. not P2PK, P2PKH, P2SH, bare multi-sig, and (with BIP141) v0 P2WPKH and v0 P2WSH.
Mainchain segwit is particularly important here, as that allows atomic swap between the bitcoin and xbitcoin. Only services with high liquidity (exchanges, payment processors) would need to occasionally settle between the chains.

@_date: 2017-11-16 02:02:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Making OP_CODESEPARATOR and FindAndDelete in 
In   I propose to make OP_CODESEPARATOR and FindAndDelete in non-segwit scripts non-standard
I think FindAndDelete() is one of the most useless and complicated functions in the script language. It is omitted from segwit (BIP143), but we still need to support it in non-segwit scripts. Actually, FindAndDelete() would only be triggered in some weird edge cases like using out-of-range SIGHASH_SINGLE.
Non-segwit scripts also use a FindAndDelete()-like function to remove OP_CODESEPARATOR from scriptCode. Note that in BIP143, only executed OP_CODESEPARATOR are removed so it doesn?t have the FindAndDelete()-like function. OP_CODESEPARATOR in segwit scripts are useful for Tumblebit so it is not disabled in this proposal
By disabling both, it guarantees that scriptCode serialized inside SignatureHash() must be constant
If we use a softfork to remove FindAndDelete() and OP_CODESEPARATOR from non-segwit scripts, we could completely remove FindAndDelete() from the consensus code later by whitelisting all blocks before the softfork block. The first step is to make them non-standard in the next release.

@_date: 2017-11-21 01:45:18
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Why SegWit Anyway? 
We can?t ?just compute the Transaction ID the same way the hash for signing the transaction is computed? because with different SIGHASH flags, there are 6 (actually 256) ways to hash a transaction.
Also, changing the definition of TxID is a hardfork change, i.e. everyone are required to upgrade or a chain split will happen.
It is possible to use ?normalised TxID? (BIP140) to fix malleability issue. As a softfork, BIP140 doesn?t change the definition of TxID. Instead, the normalised txid (i.e. txid with scriptSig removed) is used when making signature. Comparing with segwit (BIP141), BIP140 does not have the side-effect of block size increase, and doesn?t provide any incentive to control the size of UTXO set. Also, BIP140 makes the UTXO set permanently bigger, as the database needs to store both txid and normalised txid

@_date: 2017-11-21 03:58:57
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Why SegWit Anyway? 
Not really. BIP140 might be easier to implement, but in longterm the UTXO overhead is significant and unnecessary. There are also other benefits of segwit written in BIP141. Some of those are applicable even if you are making a new coin.

@_date: 2017-10-02 05:32:56
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
So there are 3 proposals with similar goal but different designs. I try to summarise some questions below:
1. How do we allow further upgrade within v1 witness? Here are some options:
a. Minor version in witness. (Johnson / Luke) I prefer this way, but we may end up with many minor versions.
b. OP_RETURNTRUE (Luke). I proposed this in an earlier version of BIP114 but now I think it doesn?t interact well with signature aggregation, and I worry that it would have some other unexpected effects.
c. Generalised NOP method: user has to provide the returned value, so even VERIFY-type code could do anything
2. Do we want to allow signature-time commitment of extra scripts?
I think all proposals allow this, just with different way
a. Tail-call semantics with CHECKSIGFROMSTACK (Mark). I think this is too rigid as it works only with specially designed scriptPubKey
b. scriptWitCode: extra scripts are put in some fixed location in witness (Johnson). This makes sure static analysability.
c. Extra-data as script in OP_CHECKSIG (Luke)
3. Do we want to allow static analysis of sigop?
BIP114 and the related proposals are specifically designed to allow static analysis of sigop. I think this was one of the main reason of OP_EVAL not being accepted. This was also the main reason of Ethereum failing to do a DAO hacker softfork, leading to the ETH/ETC split. I?m not sure if we really want to give up this property. Once we do it, we have to support it forever.

@_date: 2017-09-08 17:21:22
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
Some comments with the tail-call execution semantics BIP:
Tail-call execution semantics require ?unclean stake?, i.e. final stake with more than one item. However, ?unclean stake? is invalid (not just non-standard) in BIP141, so you could only use it with legacy P2SH (which is totally pointless?.). A different design like OP_EVAL might be needed, or you need a new witness script version.
I think you have also missed the sigOp counting of the executed script. As you can?t count it without executing the script, the current static analysability is lost. This was one of the reasons for OP_EVAL being rejected. Since sigOp is a per-block limit, any OP_EVAL-like operation means block validity will depend on the precise outcome of script execution (instead of just pass or fail), which is a layer violation.
(An alternative is to make sigOp a per-input limit instead of per-block limit, just like the 201 nOp limit. But this is a very different security model)
Witness script versioning is by design fully compatible with P2SH and BIP173, so there will be no hurdle for existing wallets to pay to BIP114. Actually it should be completely transparent to them.
For code complexity, the minimal BIP114 could be really simple, like <30 lines of code? It looks complex now because it does much more than simply hiding scripts in a hash.

@_date: 2017-09-08 17:49:46
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP114 Merklized Script update and 5 BIPs for new 
I have rewritten and simplified BIP114, and renamed it to ?Merklized Script?, as a more accurate description after consulting the original proposers of MAST. It could be considered as a special case of MAST, but has basically the same functions and scaling properties of MAST.
Compared with Friedenbach?s latest tail-call execution semantics proposal, I think the most notable difference is BIP114 focuses on maintaining the static analysability, which was a reason of OP_EVAL (BIP12) being rejected. Currently we could count the number of sigOp without executing the script, and this remains true with BIP114. Since sigOp is a block-level limit, any OP_EVAL-like operation means block validity will depend on the precise outcome of script execution (instead of just pass or fail), which is a layer violation.
Link to the revised BIP114: On top of BIP114, new script functions are defined with 5 BIPs:
VVV: Pay-to-witness-public-key: WWW: String and Bitwise Operations in Merklized Script Version 0: XXX: Numeric Operations in Merklized Script Version 0: YYY: ECDSA signature operations in Merklized Script Version 0: ZZZ: OP_PUSHTXDATA: As a summary, these BIPs have the following major features:
1. Merklized Script: a special case of MAST, allows users to hide unexecuted branches in their scripts (BIP114)
2. Delegation: key holder(s) may delegate the right of spending to other keys (scripts), with or without additional conditions such as locktime. (BIP114, VVV)
3. Enabling all OP codes disabled by Satoshi (based on Elements project with modification. BIPWWW and XXX)
4. New SIGHASH definition with very high flexibility (BIPYYY)
5. Covenant (BIPZZZ)
6. OP_CHECKSIGFROMSTACK, modified from Elements project (BIPYYY)
7. Replace ~72 byte DER sig with fixed size 64 byte compact sig. (BIPYYY)
All of these features are modular and no need to be deployed at once. The very basic BIP114 (merklized script only, no delegation) could be done quite easily. BIP114 has its own versioning system which makes introducing new functions very easy.
Things I?d like to have:
1. BIP114 now uses SHA256, but I?m open to other hash design
2. Using Schnorr or similar signature scheme, instead of ECDSA, in BIPYYY.
Reference implementation:

@_date: 2017-09-12 16:55:59
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
This is ugly and actually broken, as different script path may require different number of stack items, so you don?t know how many OP_TOALTSTACK do you need. Easier to just use a new witness version
I like the idea to have an unified global limit and suggested a way to do it ( But I think this is off-topic here.
In any case, I think maintaining static analysability for global limit(s) is very important. Ethereum had to give up their DAO softfork plan at the last minute, exactly due to the lack of this: Otherwise, one could attack relay and mining nodes by sending many small size txs with many sigops, forcing them to validate, and discard due to insufficient fees.
Technically it might be ok if we commit the total validation cost (sigop + hashop + whatever) as the first witness stack item, but that?d take more space and I?m not sure if it is desirable. Anyway, giving up static analysability for scripts is a fundamental change to our existing model.
Without the limit I think we would be DoS-ed to dead
You can find it here: But this does more than your proposal as it allows users adding extra scripts when spending a coin. The rationale is described in the revised BIP114:
So to make it functionally comparable with your proposal, the IsMSV0Stack() function is not needed. The new 249-254 lines in interpreter.cpp could be removed. The new 1480-1519 lines could be replaced by a few lines copied from the existing P2WSH code. I can make a minimal version if you want to see how it looks like

@_date: 2017-09-12 19:44:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Fast Merkle Trees 
I think you overestimated the difficulty. Consider this MAST branch (an example in BIP114)
"Timestamp" CHECKLOCKTIMEVERIFY  CHECKSIGVERIFY
This requires just a few bytes of collision.

@_date: 2017-09-20 13:13:04
@_author: Johnson Lau 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
I have implemented OP_RETURNTRUE in an earlier version of MAST (BIP114) but have given up the idea, for 2 reasons:
1. I?ve updated BIP114 to allow inclusion of scripts in witness, and require them to be signed. In this way users could add additional conditions for the validity of a signature. For example, with OP_CHECKBLOCKHASH, it is possible to make the transaction valid only in the specified chain. (More discussion in   )
2. OP_RETURNTRUE does not work well with signature aggregation. Signature aggregation will collect (pubkey, message) pairs in a tx, combine them, and verify with one signature. However, consider the following case:
OP_RETURNTRUE OP_IF  OP_CHECKSIGVERIFY OP_ENDIF OP_TRUE
For old nodes, the script terminates at OP_RETURNTRUE, and it will not collect the (pubkey, message) pair.
If we use a softfork to transform OP_RETURNTRUE into OP_17 (pushing the number 17 to the stack), new nodes will collect the (pubkey, message) pair and try to aggregate with other pairs. This becomes a hardfork.
Technically, we could create ANY op code with an OP_NOP. For example, if we want OP_MUL, we could have OP_MULVERIFY, which verifies if the 3rd stack item is the product of the top 2 stack items. Therefore, OP_MULVERIFY OP_2DROP is functionally same as OP_MUL, which removes the top 2 items and returns the product. The problem is it takes more witness space.
If we don?t want this ugliness, we could use a new script version for every new op code we add. In the new BIP114 (see link above), I suggest to move the script version to the witness, which is cheaper.

@_date: 2017-09-21 11:58:05
@_author: Johnson Lau 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
This is exactly what I suggest with BIP114. Using v1, 32-byte to define the basic structure of Merklized Script, and define the script version inside the witness

@_date: 2017-09-21 16:02:42
@_author: Johnson Lau 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
I think it?s possible only if you spend more witness space to store the (pubkey, message) pairs, so that old clients could understand the aggregation produced by new clients. But this completely defeats the purpose of doing aggregation.
We use different skills to save space. For example, we use 1-byte SIGHASH flag to imply the 32-byte message. For maximal space saving, sig aggregation will also rely on such skills. However, the assumption is that all signatures aggregated must follow exactly the same set of rules.
Sounds interesting but I don?t get it. For example, how could you make a OP_MUL out of OP_NOP?

@_date: 2017-09-22 01:38:01
@_author: Johnson Lau 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
I think the current idea of sigagg is something like this: the new OP_CHECKSIG still has 2 arguments: top stack must be a 33-byte public key, and the 2nd top stack item is signature. Depends on the sig size, it returns different value:
If sig size is 0, it returns a 0 to the top stack
If sig size is 1, it is treated as a SIGHASH flag, and the SignatureHash() ?message? is calculated. It sends the (pubkey, message) pair to the aggregator, and always returns a 1 to the top stack
If sig size is >1, it is treated as the aggregated signature. The last byte is SIGHASH flag. It sends the (pubkey, message) pair and the aggregated signature to the aggregator, and always returns a 1 to the top stack.
If all scripts pass, the aggregator will combine all pairs to obtain the aggkey and aggmsg, and verify against aggsig. A tx may have at most 1 aggsig.
(The version I presented above is somewhat simplified but should be enough to illustrate my point)
So if we have this script:
OP_1 OP_RETURNTRUE  OP_CHECKSIG
Old clients would stop at the OP_RETURNTRUE, and will not send the pubkey to the aggregator
If we softfork OP_RETURNTRUE to something else, even as OP_NOP11, new clients will send the (key, msg) pair to the aggregator. Therefore, the aggregator of old and new clients will see different data, leading to a hardfork.
OTOH, OP_NOP based softfork would not have this problem because it won?t terminate script and return true.
I don?t think it?s worth the code complexity, just to save a few bytes of data sent over wire; and to be a soft fork, it still takes the block space.
Maybe we could create many OP_DROPs and OP_2DROPs, so new VERIFY operations could pop the stack. This saves 1 byte and also looks cleaner.
Another approach is to use a new script version for every new non-verify type operation. Problem is we will end up with many versions. Also, signatures from different versions can?t be aggregated. (We may have multiple aggregators in a transaction)

@_date: 2018-08-24 17:35:11
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Getting around to fixing the timewarp attack. 
To determine the new difficulty, it is supposed to compare the timestamps of block (2016n - 1) with block (2016n - 2017). However, an off-by-one bug makes it compares with block (2016n - 2016) instead.
A naive but perfect fix is to require every block (2016x) to have a timestamp not smaller than that of its parent block. However, a chain-split would happen even without any attack, unless super-majority of miners are enforcing the new rules. This also involves mandatory upgrade of pool software (cf. pool software upgrade is not mandatory for segwit). The best way is to do it with something like BIP34, which also requires new pool software. We could have a weaker version of this, to require the timestamp of block (2016x) not smaller than its parent block by t-seconds, with 0 <= t <= infinity. With a bigger t, the fix is less effective but also less likely to cause intentional/unintentional split. Status quo is t = infinity.
Reducing the value of t is a softfork. The aim is to find a t which is small-enough-to-prohibit-time-wrap-attack but also big-enough-to-avoid-split. With t=86400 (one day), a time-wrap attacker may bring down the difficulty by about 1/14 = 7.1% per round. Unless new blocks were coming incredibly slow, the attacker needs to manipulate the MTP for at least 24 hours, or try to rewrite 24 hours of history. Such scale of 51% attack is already above the 100-block coinbase maturity safety theshold and we are facing a much bigger problem.
With t=86400, a non-majority, opportunistic attacker may split the chain only if we have no new block for at least 24 - 2 = 22 hours (2-hours is the protocol limit for using a future timestamp) at the exact moment of retarget. That means no retarget is possible in the next 2016 blocks. Doing a time-wrap attack at this point is not quite interesting as the coin is probably already worthless. Again, this is a much bigger problem than the potential chain spilt. People will yell for a difficulty (and time wrap fix, maybe) hardfork to resuscitate the chain.

@_date: 2018-08-31 04:44:25
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Testnet3 Reest 
A public testnet is still useful so in articles people could make references to these transactions.
Maybe we could have 2 testnets at the same time, with one having a smaller block size?

@_date: 2018-08-31 04:38:06
@_author: Johnson Lau 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
After gathering some feedbacks I substantially revised the proposal. This version focus on improving security, and reduces the number of optional features.
Formatted BIP and sample code at:
The major new features compared with BIP143:
1. If signing all inputs, also sign all input value. BIP143 signature only covers the value of the same input. In some cases this may not be adequate for hardware wallet to determine the right amount of fees. Signing all input values will secure any possible case.
2. Sign both scriptCode and previous scriptPubKey. In the original bitcoin design, previous scriptPubKey is signed as the scriptCode. However, this is not the case with P2SH and segwit. Explicitly committing to the scriptPubKey allows hardware wallet to confirm what it is actually signing (e.g. P2SH-segwit vs. Native-segwit).
3. SIGHASH2_NOINPUT: basically same as BIP118, but the signature commits to both scriptCode and scriptPubKey. This prevents signature replay if the same public key is used in different scripts.
4. SIGHASH2_MATCHOUTPUT (previously SIGHASH_SINGLE) disallows out-of-range case.
5. SIGHASH2_LASTOUTPUT: signs only the highest index output.
6. SIGHASH2_DUALOUTPUT: signs the matched output and the highest index output. Described by gmaxwell at  7. Signing the amount of fees (optional, yes by default). In case of not signing all inputs or outputs, users may still want to commit to a specific fee amount.
8. Signing the witness size (optional, yes by default). While segwit fixed txid malleability, it is not a fix of script malleability. It may cause some trouble if an attacker could bloat the witness and reduce the fee priority of a transaction. Although the witness size is not malleable for most simple scripts, this is not guaranteed for more complex ones. Such kind of size malleability could be avoided if signatures commit to the size of witness.
Any suggestions are welcomed. But I have the following questions:
1. Should NOINPUT commit to scriptCode and/or scriptPubKey? I think it should, because that helps avoid signature replay in some cases, and also lets hardware wallets know what they are signing. I am asking this because BIP118 proposes the opposite. I want to make sure I?m not missing something here.
2. Do we want to add LASTOUTPUT and DUALOUTPUT? Suggested by gmaxwell, an example use case is kickstarter, where individual supporters send money to the last output for a kickstarter project, and send change to the matched output. However, I doubt if this would be actually used this way, because the kickstarter organiser could always take the money before the target is met, by making up the difference with his own input. This is an inherent problem for any anonymous kickstarter scheme. If these new SIGHASHs are not useful in other applications, I am not sure if we should add them.
3. Instead of these restrictive MATCH/LAST/DUALOUTPUT, do we want a fully flexible way to sign a subset of outputs? The indexes of signed outputs are put at the end of the signature, and the signature won?t commit to these index values. Therefore, a third party could collect all transactions of this kind and merge them into one transaction. To limit the sighash cost, number of signed outputs might not be more than 2 or 3. Some potential problems: a) code complexity; b) 1-byte or 2-byte index: 1-byte will limit the number of outputs to 256. 2-byte will use more space even for smaller txs; c) highly variable signature size makes witness size estimation more difficult
4. Should we sign the exact witness size (as proposed), or an upper size limit? Signing an upper limit will take up more space, as the limit has to be explicitly shown in the witness. The overhead could be avoided by showing the limit only if the actual witness size is not equal to the committed limit. However, I tend to keep it simple and sign the exact value. If in a multi-sig setup some signers are unable to accurately estimate the witness size, they should leave this responsibility to the last signer who should know the exact size.

@_date: 2018-08-31 15:42:07
@_author: Johnson Lau 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
Great, I?ll revise it.
Follow-up questions:
1. Is there any useful case which one would like to use NOINPUT with scriptCode and/or scriptPubKey committed? (Note that with taproot/MAST, scriptCode and scriptPubKey are not interchangeable. scriptPubKey commits to all branches, and scriptCode is just one script branch). If yes, we could make this configurable.
2. Which of the following approaches is better?
A) sign scriptPubKey in every cases except NOINPUT
B) sign the type (P2SH-segwit vs. Native-segwit) of scriptPubKey in every cases, including NOINPUT
C) all of the above
D) none of the above
Option B is very easy to implement as SignatureHash() could distinguish the type by the size of scriptSig in TxTo. Option A is more complicated as GenericTransactionSignatureChecker needs to know the scriptPubKey.
If the only reason for doing this is to allow hardware wallet to distinguish the segwit type, option B is probably enough. This is also compatible with eltoo.
Option A is useful when a hardware wallet reuses the same public key in different scripts, but it couldn?t be applied to NOINPUT
3. Is the proposed DUALOUTPUT somehow useful for eltoo? Eltoo use NOINPUT|SINGLE to allow fee pumping, since it is an one-input-one-output tx. This is not possible with the existing LN as the tx is one-input-two-output. If we had DUALOUTPUT which signs the matched and last output, fee-pumping would be possible in the existing LN.

@_date: 2018-12-10 03:13:34
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
The current proposal is that a 64-byte signature will be used for the default ?signing all? sighash, and 65-byte for other sighash types. The space saved will allow a few more txs in a block, so I think it worths doing. However, this also makes witness weight estimation more difficult in multisig cases.
This idea of signing witness weight has been brought up before. I think the concern is the difficulty to estimate the witness weight for complex scripts, which need this feature most. So it will work when it is not needed, and will not work when it is needed.
Is there any script example that witness size malleability is unavoidable?

@_date: 2018-12-13 03:53:38
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I think the root cause of witness weight malleability is some opcodes accept variable size input (without affecting the output), and that input is provided by the puzzle solver. Going through the opcode list, I think such opcodes include IF, NOTIF, VERIFY, DROP, 2DROP, NIP, DEPTH, and all arithmetic opcode that accepts CScriptNum (including CHECKMULTISIG)
VERIFY, DROP, 2DROP, NIP are not real problem, since they should not be the first opcode to interact with data directly provided by the puzzle solver.
CHECKMULTISIG is fixed by BIP147. For the key number and sig number, they should be part of the script, so not malleable.
DEPTH is a problem only if its inputs are not later examined by other opcodes. Again, this is pointless.
The liberally example should be protected by the MINIMAL_IF policy, which requires the input of OP_IF be minimal. As you note, OP_IF could be replaced by taproot in many cases
Non-minimal CScriptNum is also banned as BIP62 policy.
For the purpose of preventing malicious third party witness bloating, all we need is the miners to enforce the policy. There is no reason for miners to accept size malleated txs, as that will reduce the usable block space. If they hate a tx, they would simply drop it, instead of wasting the block space.

@_date: 2018-12-13 04:00:50
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Because a hardware wallet may want to know what exact script it is signing?
Masked script has reduced security, but this is a tradeoff with functionality (e.g. eltoo can?t work without masking part of the script). So when you don?t need that extra functionality, you go back to better security
However, I?m not sure if there is any useful NOINPUT case with unmasked script.
It makes sure that your signature is applicable to a specific script branch, not others (assuming you use the same pubkey in many branches, which is avoidable)

@_date: 2018-12-13 20:32:44
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don?t see any hope to change this norm any time soon, if possible at all.
As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, ?smart? and ?dumb? are non-technical judgement. The DAO contract has always been faithfully executed. It?s dumb only for those invested in the project. For me, it was just a comedy show.
So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.
The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)
The proposed solution is that an output must be ?tagged? for it to be spendable with NOINPUT, and the ?tag? must be made explicitly by the payer. There are 2 possible ways to do the tagging:
1. A certain bit in the tx version must be set
2. A certain bit in the scriptPubKey must be set
I will analyse the pros and cons later.
Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.
In payer?s perspective, tagging means ?I believe this address is for one-time-use only? Since we can?t control how other people manage their addresses, we should never do tagging when paying to other people.
I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.
A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become ?risky? for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.
Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.
On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.
There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.
While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?

@_date: 2018-12-14 21:55:43
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I don?t think this has been mentioned: without signing the script or masked script, OP_CODESEPARATOR becomes unusable or insecure with NOINPUT.
In the new sighash proposal, we will sign the hash of the full script (or masked script), without any truncation. To make OP_CODESEPARATOR works like before, we will commit to the position of the last executed OP_CODESEPARATOR. If NOINPUT doesn?t commit to the masked script, it will just blindly committing to a random OP_CODESEPARATOR position, which a wallet couldn?t know what codes are actually being executed.

@_date: 2018-12-18 03:08:26
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Every security measures are overkill, until someone get burnt. If these security measures are really effective, no one will get burnt. The inevitable conclusion is: every effective security measures are overkill.
Assuming an user holds a private key exclusively and securely, currently there are only 2 ways to lose funds by private key reuse: 1. reusing the same signature nonce; 2. signing the hash ?one?, for the SIGHASH_SINGLE consensus bug.
People lost money for the first reason. Since this is a feature of the signature schemes we use, unavoidably that will happen again from time to time. The second one has been fixed in segwit (though incompletely), and could be completely fixed with a simple softfork.
Overall speaking, while private key reuse hurts fungibility and privacy, it is not terribly insecure, as long as you use rfc6979 and are not foolish enough to sign hash ?one?. This actually thanks to the fact that signatures always committed to the previous txid. It makes sure that a signature is never valid for more than one UTXO. Unfortunately, the guarantee of non-replayability incentified the practice of key-reuse, since the day-one of bitcoin. While NOINPUT fundamentally changes this security assumption, it won?t change this long-established culture of key reuse.
So you argument seems just begging the question. Without NOINPUT, it is just impossible to lose money by key reuse, and this is exactly the topic we are debating.
Sorry that I?m not familiar with the implementation details of your wallet. But as you don?t have code to parse scripts, I assume your wallet can?t handle OP_CODESEPARATOR? However, this is exactly what you should do: only implement what you actually need, and ignore those unrelated details.
Also, trying to faithfully and completely reproduce the consensus code in a wallet (even if you don?t need that at all) could be extremely dangerous. Such wallet might be tricked, for example, to sign the hash ?one? and get all money stolen (I was told someone really did that, but I don?t know the details)
If you didn?t implement OP_CODESEPARATOR because you didn?t use it, there is no reason for you to fully implement OP_MASKEDPUSH nor script parsing. In existing signature schemes (e.g. BIP143), signatures always commit to the script being executed (the ?scriptCode?). I assume that all wallets would re-construct the scriptCode at signing time, based on the limited set of script templates they support. If a wallet had a function called GetScriptCodeForMultiSig() for this purpose, all they need now is a GetMaskedScriptCodeForMultiSig() that returns the masked template, or a new option in the existing GetScriptCodeForMultiSig(). It does not need to be something like GetMaskedScript(GetScriptCodeForMultiSig()). After all, only a very small number of script templates really need NOINPUT. A GetMaskedScript() in a wallet is just an overkill (and a vulnerability if mis-implemented) It is a 3-way tradeoff of security, complexity, and functionality. While not everyone might appreciate this, security seems to always be the dominent factor in bitcoin protocol development. It was the reason why most core contributors were hesitant towards BIP148, despite they all love the functionality of segwit.
It?s also about functionality here: as I mentioned in another reply, OP_CODESEPARATOR couldn?t function properly with NOINPUT but without OP_MASKEDPUSH
This debate happens because NOINPUT introduces the third way to lose fund with key reuse. And once it is deployed, we have to support it forever, and is not something that we could softfork it away.

@_date: 2018-12-18 04:08:55
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Yes, but the ?tagging? emphasises more on the payer?s side: if the payer cannot guarantee that the payee would never reuse the key, the payer could avoid any NOINPUT-related trouble by tagging properly.
For the design considerations I mentioned above, the tags must be explicit and configurable by the payer. So it couldn?t be hidden in taproot.
If you don?t care about fungibility, you can always tag your setup output, and makes it ready for NOINPUT spending. Every update will need 2 signatures: a NOINPUT to spend the setup output or an earlier update output, and a NOINPUT to settle the latest update output.
If you care about fungibility, you can?t tag your setup output. Every update will need 3 signatures: a SINGLEINPUT (aka ANYONECANPAY) to spend the setup output, a NOINPUT to spend an earlier update output, and a NOINPUT to settle the latest update output.
(Actually, as soon as you made the first update tx with SINGLEINPUT, you don?t strictly need to make any SINGLEINPUT signatures in the later updates again, as the first update tx (or any update with a SINGLEINPUT signature) could be effectively the trigger tx. While it makes the settlement more expensive, it also means accidentally missing a SINGLEINPUT signature will not lead to any fund loss. So security-wise it?s same as the always-tagging scenario.)
The most interesting observation is: you never have the need to use NOINPUT on an already confirmed UTXO, since nothing about a confirmed UTXO is mutable. And every smart contract must anchor to a confirmed UTXO, or the whole contract is double-spendable. So the ability to NOINPUT-spend a setup output should not be strictly needed. In some (but not all) case it might make the protocol simpler, though.
So the philosophy behind output tagging is ?avoid NOINPUT at all cost, until it is truly unavoidable"

@_date: 2018-12-18 04:16:12
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
I proposed the same in BIP114. I wish Satoshi had designed that way. But I?m not sure if that would do more harm than good. For example, people might lose money by copying an existing script template. But they might also lose money in the same way as CHECKMULTISIG is disabled. So I?m not sure.
Another related thing I?d like to bikeshed is to pop the stack after OP_CLTV and OP_CSV. The same pros and cons apply.

@_date: 2018-12-18 18:00:59
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Schnorr and taproot (etc) upgrade 
I think you mean   CHECKSIGVERIFY  CLTV, but this works only for simple script. Most likely you need a DROP if you use IF or CODESEPARATOR.
However, if we change the rule from ?one true stack item? to ?empty stack?, CLTV/CSV popping stack will make more sense. So I think either we change all, or change nothing.
The ?true stack item? and CLTV/CSV as NOP are tech debt. Fixing them in new script version makes script creation easier and sometimes cheaper, but the fix itself creates further tech debts in the code. So I don?t have strong opinion on this topic.

@_date: 2018-12-18 18:48:40
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
After thinking more carefully, I believe output tagging could have no adverse effect on eltoo.
Consider a system without tagging, where you could always spend an output with NOINPUT. Under taproot, state update could be made in 2 ways:
a) Making 2 sigs for each update. One sig is a ?script path? locktime NOINPUT spending of the setup output or an earlier update output. One sig is a ?key path? relative-locktime NOINPUT spending of the new update output. In taproot terminology, ?key path? means direct spending with the scriptPubKey, and ?script path? means revealing the script hidden in taproot. Key path spending is always cheaper.
b) Making 3 sigs for each update. One sig is a key path SINGLEINPUT (aka ANYONECANPAY) or NOINPUT spending of the setup output, without any locktime. One sig is a script path locktime NOINPUT spending of an earlier update output (if this is not the first update). One sig is a key path relative-locktime NOINPUT spending of the new update output
Note that in b), the first signature could be either SINGLEINPUT or NOINPUT, and they just work as fine. So SINGLEINPUT should be used to avoid unnecessary replayability.
In the case of uncooperative channel closing, b) is always cheaper than a), since this first broadcast signature will be a key path signature. Also, b) has better privacy if no one is cheating (only the last update is broadcast). The only information leaked in b) is the use of SINGLEINPUT and the subsequent relative-locktime NOINPUT. However, the script path signature in a) will leak the state number, which is the maximum number of updates made in this channel.
In conclusion, b) is cheaper and more private, but it is more complex by requiring 3 sigs per update rather than 2. I think it is an acceptable tradeoff. (And as I mentioned in my last mail, missing some SINGLEINPUT sigs is not the end of the world. As long as you find one SINGLEINPUT sig in your backup, it safely falls back to the trigger tx model)
What if we require output tagging? For privacy reason you shouldn?t tag your setup tx, so the setup output could not be spent with NOINPUT. Option a) doesn?t work, but b) only requires SINGLEINPUT and has no problem. So in a fee-minimising and privacy-maximising eltoo design, output tagging should have no adverse effect.

@_date: 2018-12-20 19:00:53
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Correct me if I?m wrong.
For the sake of simplicity, in the following I assume BIP118, 143, and 141-P2WSH are used (i.e. no taproot). Also, I skipped all the possible optimisations.
1. A and B are going to setup a channel.
2. They create one setup tx, with a setup output of the following script:  CLTV DROP 2 Au Bu 2 CHECKMULTISIG. Do not sign
3. They create the update tx 0, spending the setup output with NOINPUT and locktime = s+1, to the update-0 output with the script:
IF 2 As0 Bs0 2 CHECKMULTISIG ELSE  CLTV DROP 2 Au Bu 2 CHECKMULTISIG ENDIF
4. They create the settlement tx 0, spending the update-0 output with As0 and Bs0 using BIP68 relative-locktime, with 2 settlement outputs
5. They sign the setup tx and let it confirm
6. To update, they create the update tx 1, spending the setup output with NOINPUT and locktime = s+2, to the update-1 output with the script:
IF 2 As1 Bs1 2 CHECKMULTISIG ELSE  CLTV DROP 2 Au Bu 2 CHECKMULTISIG ENDIF
and create the settlement tx 1, spending the update-1 output with As1 and Bs1 using relative-locktime, with 2 settlement outputs
7. To close the channel, broadcast update tx 1. Wait for several confirmations. And broadcast settlement-tx-1

@_date: 2018-12-21 02:04:37
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
CLTV is absolute locktime. Only CSV will have the ?time ticking? issue, but that?s not used here. The required locktime  is many years in the past. To collaboratively close, you just need to sign with SIGHASH_ALL, with a locktime s+1.
I think the use of OP_CSV (BIP112) is not needed here (although it doesn?t really harm except taking a few more bytes). All you need is to sign the settlement tx with a BIP68 relative locktime. Since this is a 2-of-2 branch, both parties need to agree with the relative locktime, so it is not necessary to restrict it through OP_CSV
I believe you confused OP_CSV (BIP112) with BIP68. Relative locktime is enforced by BIP68 (i.e. setting the nSequence). OP_CSV indirectly enforces relative-locktime by checking the value of nSequence. BIP68 could work standalone without OP_CSV, while OP_CSV is dependant on BIP68. In the case of n-of-n eltoo state update, OP_CSV is not needed because all n parties need to agree with the same nSequence value of the settlement tx. This is enough to make sure the settlement tx has delayed settlement.
Sure. This is obvious.
Collaborative close is always simple as I explained in the beginning
If no one is cheating (i.e. only the last update is broadcast), you always need only 3 txs. Think about this: every update tx could be a trigger tx, and you can settle directly on a trigger tx, so effectively you eliminate trigger tx.

@_date: 2018-12-21 03:34:38
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Because it could make scripts more compact in some cases?
This is an example:  But this is probably not a good example for taproot, as it could be more efficient by making the 2 branches as different script merkle leaves.
Let me elaborate more. Currently, scriptCode is truncated at the last executed CODESEPARATOR. If we have a very big script with many CODESEPARATORs and CHECKSIGs, there will be a lot of hashing to do.
To fix this problem, it is proposed that the new sighash will always commit to the same H(script), instead of the truncated scriptCode. So we only need to do the H(script) once, even if the script is very big
In the case of NOINPUT with MASKEDSCRIPT, it will commit to the H(masked_script) instead of H(script).
To make CODESEPARATOR works as before, the sighash will also commit to the position of the last executed CODESEPARATOR. So the semantics doesn?t change. For scripts without CODESEPARATOR, the committed value is a constant.
IF NOINPUT does not commit to H(masked_script), technically it could still commit to the position of the last executed CODESEPARATOR. But since the wallet is not aware of the actual content of the script, it has to guess the meaning of such committed positions, like ?with the HD key path m/x/y/z, I assume the script template is blah blah blah because I never use this path for another script template, and the meaning of signing the 3rd CODESEPARATOR is blah blah blah?. It still works if the assumptions hold, but sounds quite unreliable to me.

@_date: 2018-12-21 23:37:05
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
Generally speaking, I think walletless protocol is needed only when you want to rely a third party to open a offchain smart contract. It could be coinswap, eltoo, or anything similar.
However, since NOINPUT still commits to the input value, if the third party paid an unexpected value, even off by 1 satoshi, the smart contract is toast. It is not uncommon as some exchanges would deduct fees from withdrawal amount. Since we don?t have a social norm to require the payer to always pay the exact requested amount, the exchange might not be liable for the loss.
It is of course possible to have a NOINPUT_NOAMOUNT, but I can?t see any chance for this being accepted.
So, unless the payer is liable for paying a wrong amount, walletless contract opening is unreliable.

@_date: 2018-12-22 00:21:42
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
With taproot, this actually saves a lot more than a few bytes. For each update, you will make 3 signatures. One is a SIGHASH_ALL spending the setup TXO with no locktime. One is a NOINPUT spending a previous update TXO with absolute locktime. One is a NOINPUT spending the latest update TXO with relative locktime. For the first and third signatures, you will just sign directly with the scriptPubKey, without revealing the hidden taproot script. The second signature will reveal the taproot script, but it is needed only when someone published an outdated update tx.

@_date: 2018-12-22 02:54:42
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
The question I would like to ask is: is OP_CODESEPARATOR useful under taproot? Generally speaking, CODESEPARATOR is useful only with conditional opcodes (OP_IF etc), and conditional opcodes are mostly replaced by merklized scripts. I am not sure how much usability is left with CODESEPARATOR
If no one needs CODESEPARATOR, we might just disable it, and makes the validation code a bit simpler
If CODESEPARATOR is useful, then we should find a way to make it works with NOINPUT. With H(masked_script) committed, the meaning of the CODESEPARATOR position is very clear. Without H(masked_script), the meaning of the position totally relies on the assumption that ?this public key is only used in this script template?.
Ignore CODESEPARATOR and more generally, I agree with you that script masking does not help in the case of address (scriptPubKey) reuse, which is the commonest type of reuse. However, it prevents replayability when the same public key is reused in different scripts

@_date: 2018-12-23 00:56:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
If the users are expected to manually operate a hardware wallet to fund the channel, they might do stupid things like using 2 wallets to make 2 txs, thinking that they could combine the values this way; or ?refilling? the offchain wallet with the address, as you suggested. While I appreciate the goal to separate the coin-selecting wallet with the offchain wallet, I am not sure if we should rely on users to do critical steps like entering the right value or not reusing the address. Especially, the setup address should be hidden from user?s view, so only a very few ?intelligent advanced users" could try to refill the channel.
If we don?t rely on the user as the bridge between the hardware wallet and the offchain wallet, we need a communication protocol between them. With such protocol, there is no need to spend the setup TXO with NOINPUT.

@_date: 2018-12-24 00:33:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I don?t think CODESEPARATOR is useful without conditionals. By useful I mean making a script more compact
You can and should always use a different in different branch. If this best practice is always followed, committing to masked script is not necessary
Yes, I don?t think it needs Alice signature in S1 at all. So the original example doesn?t even need CODESEPARATOR at all. Could anyone propose a better use case of CODESEPARATOR?

@_date: 2018-12-25 05:23:44
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I find another proposed use of CODESEPARATOR here:   OP_CHECKSIG
 OP_CSV OP_DROP
OP_CODESEPARATOR It is actually 2 scripts:
S1:  OP_CHECKSIGVERIFY  OP_CHECKSIG
S2:  OP_CSV OP_DROP  OP_CHECKSIG
Under taproot, we could make Q = P + H(P||S2)G, where P = MuSig(KeyA, KeyB)
S1 becomes a direct spending with Q, and there is no need to use OP_IF or CODESEPARATOR in S2 at all.
If it is only to force R reuse, there is no need to use CODESEPARATOR:
Input:     Script: 2DUP EQUAL NOT VERIFY 2 PICK SWAP CAT  DUP TOALTSTACK CHECKSIGVERIFY CAT FROMALTSTACK CHECKSIG
But using CODESEPARATOR will save 3 bytes
Input:       Script:  OVER SWAP CAT  DUP TOALTSTACK CHECKSIGVERIFY CODESEPARATOR SWAP CAT FROMALTSTACK CHECKSIG
However, a much better way would be:
Input:  Script:  SWAP CAT  CHECKSIG
The discrete log of R could be a shared secret between A and B. If the purpose is to publish the private key to the whole world, R = G could be used.

@_date: 2018-02-16 17:49:17
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Alternative way to count sigops 
Short history
Satoshi introduced sigops counting as a softfork to limit the number of signature operation in a block. He statically counted all OP_CHECK(MULTI)SIG(VERIFY) in both scriptSig and scriptPubKey, assumed a OP_CHECKMULTISIG is equivalent to 20 OP_CHECKSIG, and enforced a block limit of 20000 sigop. The counting is not contextual, i.e. one doesn?t need the UTXO set to determine the number of sigop. The counting was also static so one doesn?t need to execute a script in order to count sigop. However, this is completely wrong for few reasons: a) opcodes in scriptPubKey are not executed; b) scriptPubKey of spent UTXO, which are actually executed, are not counted at all; c) it greatly overestimate the cost of multi-sig; d) it counts sigop in unexecuted branch.
As P2SH was introduced, sigop counting also covered the sigop redeemScript. This is good because redeemScript is what being executed. It also improved the counting of OP_CHECKMULTISIG. If it is in certain canonical form, it would count the number of public keys instead of assuming it as 20. On the other hand, counting sigop becomes not possible without the UTXO set, since one needs UTXO to identify P2SH inputs. Also, the canonical OP_CHECKMULTISIG counting is not quite elegant and created more special cases in the code.
Segwit (BIP141) scaled the legacy sigop limit by 4x. So every legacy sigop becomes 4 new sigop, with a block limit of 80000 new sigop. P2WPKH is counted as 1 new sigop, and P2WSH is counted in the same way as P2SH.
We now have multiple 2nd generation script proposals, such as BIP114, BIP117, taproot, etc. BIP114 and taproot allows static sigop counting, but not BIP117 as it requires execution to determine what would be run as script (like OP_EVAL). As we want to allow more complicated script functions, static sigop counting might not be easy. However, we still want to have a limit to avoid unexpected DoS attack.
Since we have a block weight limit of 4,000,000 and sigop limit of 80,000, each sigop could not use more than 50 weight unit on average. For new script proposals we could count the actual number of sigop at execution (i.e. skip unexecuted sigop, skip 0-size signature, count the actual checksig operations in multi-sig), and make sure the number of executed sigop * 50 is not greater than the size of the input.
The minimal size of each input is 32 (prevout.hash) + 4 (prevout.n) + 4 (nSequence) + 1 (empty scriptSig) = 41 bytes or 164 weight unit. So the new rule would require that (164 + input witness size) >= (actual_sigop * 50). This is a per-input limit, as script validation is parallel.
Since a compressed key is 33 bytes and a normal compact signature is 64 bytes, the 1:50 ratio should be more than enough to allow any normal use of CHECKSIG, unless people are doing weird things like 2DUP 2DUP 2DUP??..CHECKSIG CHECKSIG CHECKSIG CHECKSIG , which would have many sigop with a relatively small witness. Interactive per-tx signature aggregation allows 64bytes/tx signature, and per-block non-interatcitve signature aggregation allows 32bytes/signature ( ). In such cases, the 1:50 ratio might not be enough if many signatures are aggregated. Depends on the number of sigop we could tolerate, the 1:50 ratio might be reduced to 1:32 or lower to make sure legitimate use would never hit the limit. I think 32 is reasonable as it is about the size of a public key, which would guarantee each pubkey must get a sigop slot.
So a relay node could be certain that a tx won?t spend excessive CPU power by just looking at its size. If it spends too much, it is invalid and script execution could be terminated early.

@_date: 2018-06-02 01:03:05
@_author: Johnson Lau 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
I think it?s just a tradition to use double SHA256. One reason we might want to keep dSHA256 is a blind signature might be done by giving only the single SHA256 hash to the signer. At the same time, a non-Bitcoin signature scheme might use SHA512-SHA256. So a blind signer could distinguish the message type without learning the message.
sigversion is a response to Peter Todd?s comments on BIP143:  I make it a 0x01000000 at the end of the message because the last 4 bytes has been the nHashType in the legacy/BIP143 protocol. Since the maximum legacy nHashType is 0xff, no collision could ever occur.
Putting a 64-byte constant at the beginning should also work, since a collision means SHA256 is no longer preimage resistance. I don?t know much about SHA256 optimisation. How good it is as we put a 64-byte constant at the beginning, while we also make the message 64-byte longer?
For CHECKSIGFROMSTACK (CSFS), I think the question is whether we want to make it as a separate opcode, or combine that with CHECKSIG. If it is a separate opcode, I think it should be a separate BIP. If it is combined with CHECKSIG, we could do something like this: If the bit 10 of SIGHASH2 is set, CHECKSIG will pop one more item from stack, and serialize its content with the transaction digest. Any thought?

@_date: 2018-06-02 02:45:57
@_author: Johnson Lau 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
Agreed. This is one of the reasons I think we should remove CHECKMULTISIG in the new script system

@_date: 2018-03-05 10:28:20
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP 117 Feedback 
Altstack in v0 P2WSH should be left untouched. If anyone is already using altstack, BIP117 would very likely confiscate those UTXOs because the altstack would unlikely be executable.
Even in v1 witness, I think altstack should remain be a temporary data storage.
The ?(many scripts) concatinated together in reverse order to form a serialized script? in BIP117 is exactly the same security hole of Satoshi?s scriptSig + OP_CODESAPARATOR + scriptPubKey . That means it is possible to skip execution of scriptPubKey by using a scriptSig with an invalid push operation, so the whole concatenated script becomes a simple push.
For SigOp limit, I think it?d become more and more difficult to maintain the current statical analyzability model as we try to introduce more functions. I think we should just migrate to a model of limiting sigop per weight, and count the actual number of sigop during execution.  (  ) Actually, this approach is cheaper to analyse, as you only need to look at the witness size, and don?t need to look at the script at all.

@_date: 2018-05-10 01:56:46
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
You should make a ?0 fee tx with exactly one OP_TRUE output? standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won?t pollute the UTXO set
Instead, would you consider to use ANYONECANPAY to sign the tx, so it is possible add more inputs for fees? The total tx size is bigger than the OP_TRUE approach, but you don?t need to ask for any protocol change.
In long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.

@_date: 2018-05-10 04:19:31
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
My only concern is UTXO pollution. There could be a ?CPFP anchor? softfork that outputs with empty scriptPubKey and 0 value are spendable only in the same block. If not spent immediately, they become invalid and are removed from UTXO. But I still think the best solution is a more flexible SIGHASH system, which doesn?t need CPFP at all.

@_date: 2018-05-25 17:46:29
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
While you have rescind your concern, I?d like to point out that it?s strictly a problem of SIGHASH_NOINPUT, not graftroot (or script delegation in general).
For example, we could modify graftroot. Instead of signing the (script), we require it to sign (outpoint | script). That means a graftroot signature would never be valid for more than one UTXO.

@_date: 2018-05-25 18:14:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Actually, we could introduce graftroot-like delegation to all existing and new P2PK and P2PKH UTXOs with a softfork:
1. Define SIGHASH_GRAFTROOT = 0x40. New rules apply if (nHashType & SIGHASH_GRAFTROOT)
2. The third stack item MUST be at least 64 bytes, with 32-byte R and 32-byte S, followed by a script of arbitrary size. It MUST be a valid signature for the script with the original public key.
3. The remaining stack items MUST solve the script
Conceptually this could be extended to arbitrary output types, not just P2PK and P2PKH. But it might be too complicated to describe here.
(We can?t do this in P2WPKH and P2WSH due to the implicit CLEANSTACK rules. But nothing could stop us to do it by introducing another witness structure (which is invisible to current nodes) and store the extra graftroot signatures and scripts)
A graftroot design like this is a strict subset of existing signature checking rules. If this is dangerous, the existing signature checking rules must be dangerous. This also doesn?t have any problem with blind signature, since the first signature will always sign the outpoint, with or without ANYONECANPAY. (As I pointed out in my reply to Andrew, his concern applies only to SIGHASH_NOINPUT, not graftroot)
With the example above, I believe there is no reason to make graftroot optional, at the expense of block space and/or reduced privacy. Any potential problem (e.g. interaction with blind signature) could be fixed by a proper rules design.

@_date: 2018-06-01 02:35:41
@_author: Johnson Lau 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
Since 2016, I have made a number of proposals for the next generation of script. Since then, there has been a lot of exciting development on this topic. The most notable ones are Taproot and Graftroot proposed by Maxwell. It seems the most logical way is to implement MAST and other new script functions inside Taproot and/or Graftroot. Therefore, I substantially simplified my earlier proposal on SIGHASH2. It is a superset of the existing SIGHASH and the BIP118 SIGHASH_NOINPUT, with further flexibility but not being too complicated. It also fixes some minor problems that we found in the late stage of BIP143 review. For example, the theoretical (but not statistical) possibility of having same SignatureHash() results for a legacy and a witness transaction. This is fixed by padding a constant at the end of the message so collision would not be possible.
A formatted version and example code could be found here:
BIP: YYY
  Layer: Consensus (soft fork)
  Title: Signature checking operations in version 1 witness program
  Author: Johnson Lau   Comments-Summary: No comments yet.
  Comments-URI:   Status: Draft
  Type: Standards Track
  Created: 2017-07-19
  License: BSD-3-Clause
This BIP defines signature checking operations in version 1 witness program.
Use of compact signatures to save space.
More SIGHASH options, more flexibility
The following specification is applicable to OP_CHECKSIG and OP_CHECKSIGVERIFY in version 1 witness program.
**Public Key Format
The pubic key MUST be exactly 33 bytes.
If the first byte of the public key is a 0x02 or 0x03, it MUST be a compressed public key. The signature is a Schnorr signature (To be defined separately)
If the first byte of the public key is neither 0x02 nor 0x03, the signature is assumed valid. This is for future upgrade.
**Signature Format
The following rules apply only if the first byte of the public key is a 0x02 or 0x03.
If the signature size is 64 to 66 byte, it MUST be a valid Schnorr signature or the script execution MUST fail (cf. BIP146 NULLFAIL). The first 32-byte is the R value in big-endian. The next 32-byte is the S value in big-endian. The remaining data, if any, denotes the hashtype in little-endian (0 to 0xffff).
hashtype MUST be minimally encoded. Any trailing zero MUST be removed.
If the signature size is zero, it is accepted as the "valid failing" signature for OP_CHECKSIG to return a FALSE value to the stack. (cf. BIP66)
The script execution MUST fail with a signature size not 0, 64, 65, or 66-byte.
**New hashtype definitions
hashtype and the SignatureHash function are re-defined:
  Double SHA256 of the serialization of:
     1. nVersion (4-byte little endian)
     2. hashPrevouts (32-byte hash)
     3. hashSequence (32-byte hash)
     4. outpoint (32-byte hash + 4-byte little endian)
     5. scriptCode (serialized as scripts inside CTxOuts)
     6. nAmount (8-byte little endian)
     7. nSequence (4-byte little endian)
     8. hashOutputs (32-byte hash)
     9. nLocktime (4-byte little endian)
    10. nInputIndex (4-byte little endian)
    11. nFees (8-byte little endian)
    12. hashtype (4-byte little endian)
    13. sigversion (4-byte little endian for the fixed value 0x01000000)
The bit 0 to 3 of hashtype denotes a value between 0 and 15:
The bit 4 and 5 of hashtype denotes a value between 0 and 3:
If bit 6 is set (SIGHASH2_NOFEE), nFees is 0x0000000000000000. Otherwise, it is the fee paid by the transaction.
If bit 7 is set (SIGHASH2_NOLOCKTIME), nLockTime is 0x00000000. Otherwise, it is the transaction nLockTime.
If bit 8 is set (SIGHASH2_NOVERSION), nVersion is 0x00000000. Otherwise, it is the transaction nVersion.
If bit 9 is set (SIGHASH2_NOSCRIPTCODE), scriptCode is an empty script. Otherwise, it is same as described in BIP143.
Bits 10 to 15 are reserved and ignored, but the signature still commits to their value as hashtype.
hashtype of 0 is also known as SIGHASH2_ALL, which covers all the available options. In this case the singnature MUST be exactly 64-byte.
hashtype of 0x3ff is also known as SIGHASH2_NONE, which covers nothing and is effectively forfeiting the right related to this public key to anyone.
**Signature Format
The current DER format is a complete waste of block space. The new format saves ~8 bytes per signature.
**New hashtype definitions
The default and most commonly used case is SIGHASH2_ALL. Making it zero size to save space. As a result, the bit flags are defined in a negative way (e.g. NOLOCKTIME)
Why decouple INPUT and SEQUENCE? Maybe you want NOINPUT but still have a relative lock-time?
Why some combinations are missing? To save some bits for useless flags. If you sign all inputs, you must know its index and value. If you sign only this input, you must know its value, but probably don't know its index in the input vector.
Why only allow signing all SEQUENCE if all INPUT are signed? It doesn't make much sense if you care about their sequence without even knowing what they are.
Why signing INPUTINDEX? Legacy and BIP143 SINGLE|ANYONECANPAY behaves differently for input index. Better make it explicit and optional.
Why signing FEE? Sometimes you don't sign all inputs / outputs but still want to make sure the fees amount is correct.
Putting NOVERSION and NOSCRIPTCODE in the second byte makes most signatures below 66 bytes:
Reserved bits: These bits are ignored but should normally be unset. Users MUST NOT set these bits until they are defined by a future proposal, or they might lose money.
Why sigversion? Make sure the message digest won't collide with SIGHASH schemes in the past (legacy and BIP143) and future (which will use a different sigversion).
Equivalent SIGHASH2 value for other SIGHASH schemes:
Legacy/BIP143 ALL: 0 (commit to everything)
Legacy/BIP143 SINGLE with matching output: 0x62 (all input, one sequence, one output, no fee)
Legacy SINGLE without matching output: 0x3ff (Not exactly. Both signatures commit to nothing, but the legacy one is valid only without a matched output. Practically, they are both "wildcard" signatures that allow anyone to spend any related UTXO)
Legacy/BIP143 NONE: 0x72 (all input, one sequence, no output, no fee)
Legacy/BIP143 ANYONECANPAY|ALL: 0x46 (one input without index, one sequence, all output, no fee)
Legacy ANYONECANPAY|SINGLE with matching output: 0x64 (one input with index, one sequence, one output, no fee)
Legacy/BIP143 ANYONECANPAY|NONE: 0x76 (one input without index, one sequence, no output, no fee)
BIP143 SINGLE without matching output: 0x62 (all input, one sequence, no output, no fee)
BIP143 ANYONECANPAY|SINGLE with matching output: 0x66 (one input without index, one sequence, one output, no fee)
BIP143 ANYONECANPAY|SINGLE without matching output: 0x66 (one input without index, one sequence, no output, no fee)
BIP118 NOINPUT: 0x14b (no input but with value, no index, no sequence, no fee, no scriptcode)
1. In legacy and BIP143 SIGHASH, only ALL but not other types implicitly commits to the fee paid.
2. Legacy SIGHASH always implicitly commits to the input value. BIP143 and BIP118 commits to that explicitly.
3. Legacy and BIP143 SIGHASH behaves differently in the case of SINGLE without matching output. In legacy SIGHASH it is a true "wildcard signature" that allows anyone to spend any related UTXO. In BIP143 such signature applies only to a specific UTXO.
4. BIP143 ANYONECANPAY never commits to the input index. Legacy ANYONECANPAY|SINGLE implicitly commits to the input index.
*Backward compatibility
This is a soft-fork.
Exact details TBD.
*Reference Implementation
 (To be updated)
This document is licensed as BSD 3-clause.

@_date: 2018-06-01 02:53:01
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Disallow insecure use of SIGHASH_SINGLE 
I?ve made a PR to add a new policy to disallow using SIGHASH_SINGLE without matched output:
Signature of this form is insecure, as it commits to no output while users might think it commits to one. It is even worse in non-segwit scripts, which is effectively SIGHASH_NOINPUT|SIGHASH_NONE, so any UTXO of the same key could be stolen. (It?s restricted to only one UTXO in segwit, but it?s still like a SIGHASH_NONE.)
This is one of the earliest unintended consensus behavior. Since these signatures are inherently unsafe, I think it does no harm to disable this unintended ?feature? with a softfork. But since these signatures are currently allowed, the first step is to make them non-standard.

@_date: 2018-11-22 01:55:22
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
If we sign the txids of all inputs, we should also explicitly commit to their values. Only this could fully eliminate any possible way to lie about input value to hardware wallets
SIGHASH_NONE should be kept. ANYONECANPAY|NONE allows donation of dust UTXOs to miners
We might refuse to sign weird combinations like NOFEE|ALLINPUT|ALLOUTPUT. But to keep the consensus logic simple, we should just validate it as usual.
Yes, it looks complicated to me, and it improves security only in some avoidable edge cases in SIGHASH_NOINPUT:
The common case: the exact masked script or address is reused. OP_MASK can?t prevent signature replay since the masked script is the same.
The avoidable case: the same public key is reused in different script templates. OP_MASK may prevent signature replay is the masked script is not the same.
The latter case is totally avoidable since one could and should use a different public key for different script.
It could be made much simpler as NOINPUT with/without SCRIPT. This again is only helpful in the avoidable case above, but it doesn?t bring too much complexity.
OP_MASK is designed to preserve the hackiness, while provide some sort of replay protection (only in avoidable cases). However, I?m not sure who would actually need NOINPUT with KNOWNSCRIPT

@_date: 2018-11-22 22:28:35
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
With MAST in taproot, OP_IF etc become mostly redundant, with worse privacy. To maximise fungibility, we should encourage people to use MAST, instead of improve the functionality of OP_IF and further complicate the protocol.

@_date: 2018-11-23 04:52:54
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Assuming a script size of 128 bytes (including SHA256 padding), 2^20 scripts is 134MB. Double it to 268MB for the merkle branch hashes. With roughly 100MB/s, this should take 2.5s (or 42min for 30 levels). However, memory use is not considered.
I?m not sure if this is correct. Actually, CTransactionSignatureSerializer() scans every script for OP_CODESEPARATOR. Scripts with and without OP_CODESEPARATOR should take exactly the same O(script-size) time (see Also, this is no longer a concern under segwit (BIP143), which CTransactionSignatureSerializer() is not used. Actually, OP_CODESEPARATOR under segwit is way simpler than the proposed OP_MASK. If one finds OP_MASK acceptable, there should be no reason to reject OP_CODESEPARATOR.
If I have to choose among OP_CODESEPARATOR and ?flow operator counting?, I?d rather choose OP_CODESEPARATOR. At least we don?t need to add more lines to the consensus code, just for something that is mostly archivable with MAST.

@_date: 2018-11-23 18:47:10
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
In the existing sighash (i.e. legacy and BIP143), there are 6 canonical SIGHASH types: 1, 2, 3, 0x81, 0x82, 0x83. In consensus, however, all 256 types are valid and distinct. An adversarial miner could use non-standard sighash types to nullify any attempt to cache sighash values (i.e. you have to compute a new tx digest for every OP_CHECKSIG, even without using OP_CODESEPARATOR).
The only way to prevent this is reject OP_CODESEPARATOR, FindAndDelete(), and non-standard SIGHASH with a softfork. However, this doesn?t work in the next-generation SIGHASH, as tens of standard sighash types will exist. And, more importantly, sighash cache is no longer necessary in segwit, with the legacy O(n^2) hash bug being fixed.
In summary, sighash cache is not necessary nor efficient in the next-generation SIGHASH, and is not a sufficient reason to remove OP_CODESEPARATOR, especially when people find OP_CODESEPARATOR useful in some way.
But just to be clear, I think OP_CODESEPARATOR should be deprecated in legacy scripts. There is a general negative sentiment against OP_CODESEPARATOR but I think we need to evaluate case by case.

@_date: 2018-11-24 16:13:46
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I think we just make it as simple as this: Always commit to sequence of the same input. Commit to hashSequence if and only if all inputs and all outputs are signed.
The next-generation SIGHASH will introduce not only NOINPUT, but also signing of fees, previous scriptPubKey, and all input values, etc. So it won?t be a simple hack over BIP143. BIP118 might be better changed to be an informational BIP, focus on the rationale and examples of NOINPUT, and be cross-referenced with the consensus BIP.

@_date: 2018-11-28 16:31:48
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I suggest to use the place of OP_RESERVED (0x50) as OP_MASKEDPUSH. The reason is 0x50 is not counted towards the 201 opcode limit, so people could mask as many pushes as needed.
In a new script version, of course, we could make any opcode not being counted. But that would just create another special case in the EvalScript() code.
(Or, maybe we should limit the use of OP_MASKEDPUSH? I think this is open for discussion.)
Do you want to make it 1 hash or 3 hashes? With 3 hashes, it could share hashPrevouts and hashSequence with BIP143. Making everything 1 hash will only result in redundent hashing for each input
Starting from this sighash version, I think we should forbid the use of SINGLE without a matching output. Also, the undefined output type should also be invalid.
I think we should just use the scriptPubKey, since sPK is fixed size (23-byte for p2sh and 35-byte for native segwit).
In order to distinguish p2sh and native segwit for MASKED NOINPUT, you also need to commit to an additional 1-bit value
For direct key-spending (i.e. not taprooted script), I suggest to set the H(scriptCode) to zero, for the following reasons:
1) Since we have already committed to sPK, which is already a *direct* hash of scriptCode, it is redundant to do it again.
2) This could save one SHA256 compression for direct key-spending, which is probably 90% of all cases
3) This allows hardware wallet to tell whether they are using direct-spending path or taproot script path
Since we may want 3) anyway, we don?t need to commit to another 1-bit value if we simply set H(scriptCode) to zero
We should also ban MASKED NOINPUT for direct-spending, which doesn?t make sense. And it is not safe since both H(scriptCode) and sPK are empty.
This proposal will only use 4 out of the 8 sighash bits. Do we want to make those 4 unused bits invalid, or ignored? Leaving at least 1 bit valid but ignored (?bit-x"), and 1 bit invalid (?bit-y?), will allow opt-in/out hardfork replay-protection, for example:
* default signatures are those with both bit-x and bit-y unset.
* If we want to make default signatures replayable across chains, the new fork should reject signatures with bit-x, and accept sigs with or without bit-y. In this case, defaults sigs are valid for both chains. Sigs with bit-x is valid only for original chain, and sigs with bit-y is valid only for new chain.
* If we want to make default signatures non-replayabble, the new fork should reject all default sigs, but accept sigs with either bit-x or bit-y set. In this case, default sig is valid only for original chain. Sigs with bit-x is valid for both chains, and sigs with bit-y is valid only for new chain.
Replayability is sometimes desirable, for example, an LN opened before a fork should be able to be settled on both chains

@_date: 2018-11-28 16:40:34
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
This is incompatible with bip-schnorr, which intentionally disallow such use by always committing to the public key: With the recent fake Satoshi signature drama, and other potential ways to misuse and abuse, I think this is a better way to go, which unfortunately might disallow some legitimate applications.
Covenants could be made using OP_CHECKSIGFROMSTACK ( or OP_PUSHTXDATA ( I think this is the next step following the taproot soft fork

@_date: 2018-09-27 03:45:49
@_author: Johnson Lau 
@_subject: [bitcoin-dev] BIP sighash_noinput 
In BIP143, the nSequence of the same input is always signed, with any hashtype. Why do you need to sign the sequence of other inputs?

@_date: 2019-02-09 18:15:17
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
This is really interesting. If I get it correctly, I think the fungibility hit could be avoided, just by making one more signature, and not affecting the blockchain space usage.
Just some terminology first. In a 3-party channel, ?main channel? means the one requires all parties to update, and ?branch channel? requires only 2 parties to update.
By what you describe, I think the most realistic scenario is ?C is going to offline soon, and may or may not return. So the group wants to keep the main channel open, and create a branch channel for A and B, during the absence of C?. I guess this is what you mean by being able to "predict in advance who will become absent?
I call this process as ?semi-cooperative channel closing? (SCCC). During a SCCC, the settlement tx will have 2 outputs: one as (A & B), one as (C). Therefore, a branch channel could be opened with the (A & B) output. The channel opening must use NOINPUT signature, since we don?t know the txid of the settlement tx. With the output tagging requirement, (A & B) must be tagged, and lead to the fungibility loss you described.
However, it is possible to make 2 settlement txs during SCCC. Outputs of the settlement tx X are tagged(A&B) and C. Outputs of the settlement tx Y are untagged(A&B) and C. Both X and Y are BIP68 relative-time-locked, but Y has a longer time lock.
The branch channel is opened on top of the tagged output of tx X. If A and B want to close the channel without C, they need to publish the last update tx of the main channel. Once the update tx is confirmed, its txid becomes permanent, so are the txids of X and Y. If A and B decide to close the channel cooperatively, they could do it on top of the untagged output of tx Y, without using NOINPUT. There won?t be any fungibility loss. Other people will only see the uncooperative closing of the main channel, and couldn?t even tell the number of parties in the main channel. Unfortunately, the unusual long lock time of Y might still tell something.
If anything goes wrong, A or B could publish X before the lock time of Y, and settle it through the usual eltoo style. Since this is an uncooperative closing anyway, the extra fungibility loss due to tagging is next to nothing. However, it may suggest that the main channel was a multi-party one.
For C, the last update tx of the main channel and the settlement tx Y are the only things he needs to get the money back. C has to sign tx X, but he shouldn?t get the complete tx X. Otherwise, C might have an incentive to publish X in order to get the money back earlier, at the cost of fungibility loss of the branch channel.
To minimise the fungibility loss, we?d better make it a social norm: if you sign your tx with NOINPUT, always try to make all outputs tagged to be NOINPUT-spendable. (NOTE: you can still spend tagged outputs with normal signatures, so this won?t permanently taint your coins as NOINPUT-spendable) It makes sense because the use of NOINPUT signature strongly suggests that you don?t know the txid of the parent tx, so you may most likely want your outputs to be NOINPUT-spendable as well. I thought of making this a policy or consensus rule, but may be it?s just overkill.

@_date: 2019-02-10 00:48:40
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
In a 3 parties channel, let?s say the balance for A, B, C is 2, 3, 6BTC respectively, there are few ways they could make the settlement tx.
The first type we may call it ?simple settlement?, which has 3 outputs with A=2, B=3, C=6.
The second type we may call it ?fully combinatorial settlement?, which has 3 outputs with (A & B), (B & C), and (A & C). The value distribution is flexible, but never exceed the total balance of the involved parties. For example, (A & B) may have any value between 0 and 5BTC. For the following example, I will use (A & B) = 3; (B & C) = 6; (A & C) = 2, but there are infinitely many valid combinations.
The third type we may call it ?partially combinatorial settlement?. It may have 2 multi-sig outputs, for example, (A & B) = 4 and (B & C) = 7; or 1 multi-sig output and 1 single-sig output, for example, (A & B) = 5 and C=6 (known as "semi-cooperative channel closing? SCCC in my last post)
I?ll just focus on the fully combinatorial settlement. The partial type works in the same way, with benefits and limitations.
In a combinatorial settlement, the multi-sig outputs are actually eltoo-style scripts. Therefore, A and B will further distribute the value of (A & B) by a 2-party eltoo channel (?branch channels"). Again, there are infinitely many valid ways to distribute the values. If the AB branch channel is distributed as A=1 and B=2, then the BC channel must be B=1 and C=5, and the AC channel must be A=1 and C=1.
A clear benefit of this model is that any 2 parties could trade with each other, in the absence of any other party(s), as long as there is enough liquidity in their branch channel. There is also no way to ?fork? the state, because liquidity is restricted to each branch channel. In some way, this is like the existing lightning network where the 3 parties have direct channel with each other. However, this is superior to lightning network, because when the 3 parties are online simultaneously, they could re-distribute the channel capacities without closing any channels. They could even change it to a partially combinatorial settlement. If they find that A and C rarely trade with each other, they could remove the (A & C) output, and improve the capacities of the remaining channels. If C is going offline for a week, they could make it (A & B), C, (aka. SCCC) which will maximise the capacity of the AB branch channel, and minimise the cost in case C is not coming back.
A problem with combinatorial settlement is the increased costs of uncooperative settlement. It is more expensive, as more parties are missing. Simple settlement has the same settlement cost for any number of missing party. However, even if one party is missing, a simple settled channel will cease to function and force an immediate on-chain settlement. In combinatorial settlement, the surviving parties may keep trading, may or may not with reduced capacity depending on the exact settlement model, and in the meantime hope that the missing parties may return.
It requires 6 outputs for 4 parties doing fully combinatorial settlement, 10 outputs for 5 parties, 15 outputs for 6 parties, etc. However, in a many parties situation, not every parties might want to trade with all the other parties, and those branch channels might be omitted to improve the capacities of the other channels. If some pairs want to trade without a direct branch channel, they might try to find a third (internal) party to forward the tx. When the next time all parties are online, they could rearrange the branch channel capacities at no cost.
The combinatorial settlement model could be generalised to a hierarchical settlement model, where we might have 4 settlement outputs (A&B&C), (A&B&D), (A&C&D), (B&C&D) for a 4-party channel, and each settlement output will have 3 branch channels. If A is missing, for example, we will still have one BC branch channel, one BD branch channel, one CD branch channel, and one BCD 3-party branch channel. The benefit of having a BCD 3-party branch channel is the 3 parties could rearrange the channel capacities without involving A. Let?s say D is going for vacation, he could do a SCCC in the BCD branch channel to maximise the capacity of its BC channel. Without the involvement of A, however, the capacities of the other BC, BD, and CD branch channels are not modifiable, and B and C?s balance in the BD/CD channels are frozen during the absence of D.
As the number of parties increase, the number of settlement txs will grow factorially in a fully hierarchical settlement model, and will soon be out-of-control. The result could be catastrophic if many parties are gone. So the group needs to continuously evaluate the risks of each party being missing, and modify the settlement model accordingly.

@_date: 2019-02-10 01:43:50
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
And this scheme could be generalised to the combinatorial settlement model in my earlier post.
Let?s say the settlement tx has 3 outputs: (A&B),(B&C),(A&C). There will be 4 versions of this tx:
tx-X: all 3 outputs are tagged, signed by all 3 parties
tx-Y-AB: output (A&B) is untagged, the other 2 outputs are tagged. Signed only by C
tx-Y-AC: output (A&C) is untagged, the other 2 outputs are tagged. Signed only by B
tx-Y-BC: ???
All 4 txs will have the same relative-lock-time
If C is missing at the time of settlement, A and B will settle upon tx-Y-AB with a simple signature
If B and C are missing, A will settle upon tx-X
However, I think this is just an overkill, and hardly improves fungibility. It is very clear that this is an uncooperative eltoo closing (due to the update tx of the main channel), and this is a multi-party channel (due to multiple settlement outputs). There is little doubt that the remaining parties would like to continue trading. So there is actually no secret to hide, and it might be easier to just tag all outputs
Nonetheless, this example shows that the fungibility impact of output tagging is quite manageable. Most likely you just need to prepare more versions of intermediate txs, and only use the tagged one when things go against you.

@_date: 2019-02-20 03:22:07
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
This only depends on the contract between the payer and payee. If the contract says address reuse is unacceptable, it?s unacceptable. It has nothing to do with how the payee spends the coin. We can?t ban address reuse at protocol level (unless we never prune the chain), so address reuse could only be prevented at social level.
Using NOINPUT is also a very weak excuse: NOINPUT always commit to the value. If the payer reused an address but for different amount, the payee can?t claim the coin is lost due to previous NOINPUT use. A much stronger way is to publish the key after a coin is well confirmed.

@_date: 2019-02-20 04:36:51
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safer NOINPUT with output tagging 
This is totally unrelated to NOINPUT. You can make a wallet like this today already, and tell your payer not to reuse address.
It sounds like you actually want to tag such outputs as scriptPubKey, so you could encode this requirement in the address?
If we allow NOINPUT unconditionally (i.e. all v1 addresses are spendable with NOINPUT), you may only create a different proposal to indicate such special requirements

@_date: 2019-03-21 16:37:54
@_author: Johnson Lau 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
It means either.
If you use  CHECKSIGVERIFY  CHECKSIG style, A and B will exchange the NOINPUT sig, and they will add the required non-NOINPUT sig when needed.
If you use  CHECKVERIFY  CHECKSIG, A and B will co-sign the muSig(A,B) with NOINPUT. They will also share the private key of Q, so they could produce a non-NOINPUT sig when needed.
The first style is slightly easier as it doesn?t need muSig. But with 3 or more parties, the second style is more efficient.
However, if you use watchtower, you have to use the second style. That means you need to share the private key for Q with the watchtower, That also means the watchtower will have the ability to reply the NOINPU muSig. But it is still strictly better than anyone-can-replay.

@_date: 2019-03-22 12:23:28
@_author: Johnson Lau 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
OP_CSV (BIP112) is not needed. Only BIP68 relative-time is needed.
With this script:
 OP_CHECKLOCKTIMEVERIFY OP_DROP  OP_CHECKSIGVERIFY  OP_CHECKSIG
For update purpose, A and B will co-sign the muSig with nLockTime = t, not committing to the scriptCode, and no BIP68 lock time
For settlement purpose, A and B will co-sign the muSig with nLockTime = t, committing to the scriptCode, and with an agreed BIP68 locktime
Without committing to the scriptCode and BIP68 lock time, the update sig could be bind to any previous update tx immediately.
OTOH, the settlement sig will only bind to a specific update tx (thought scriptCode), and only after the relative locktime is passed.
The eltoo paper is wrong about using OP_CSV. That?s a common mistake even for experienced bitcoin developer. OP_CSV is needed only if one party could single handedly decide the relative-lock-time. However, this is not the case here as it is a muSig.
(With some risks of distracting the discussion, please note that even this script:  OP_CHECKLOCKTIMEVERIFY OP_DROP  OP_CHECKSIGVERIFY  OP_CHECKSIG doesn?t need OP_CSV, despite not using muSig. It is because the 2 sigs must use the same relative locktime, or the tx is invalid.)

@_date: 2019-05-10 00:56:57
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Taproot proposal 
This is not possible since the whole annex is signed. It is possible if the signed ?script? does not require further input, like per-input lock-time, relative lock-time, check block hash

@_date: 2019-05-24 04:54:01
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safety of committing only to transaction outputs 
This is a meta-discussion for any approach that allows the witness committing to only transaction outputs, but not inputs.
We can already do the following things with the existing bitcoin script system:
* commit to both inputs and outputs: SIGHASH_ALL or SIGHASH_SINGLE, with optional SIGHASH_ANYONECANPAY
* commit to only inputs but not outputs: SIGHASH_NONE with optional SIGHASH_ANYONECANPAY
* not commit to any input nor output: not using any sigop; using a trivial private key; using the SIGHASH_SINGLE bug in legacy script
The last one is clearly unsafe as any relay/mining node may redirect the payment to any output it chooses. The witness/scriptSig is also replayable, so any future payment to this script will likely be swept immediately
SIGHASH_NONE with ANYONECANPAY also allows redirection of payment, but the signature is not replayable
But it?s quite obvious that not committing to outputs are inherently insecure
The existing system doesn?t allow committing only to outputs, and we now have 3 active proposals for this function:
1. CAT and CHECKSIGFROMSTACK (CSFS):  2. ANYPREVOUT (aka NOINPUT):  3. CHECKOUTPUTSHASHVERIFY (COHV):  With outputs committed, redirecting payment is not possible. On the other hand, not committing to any input means the witness is replayable without the consent of address owner. Whether replayability is acceptable is subject to controversy, but the ANYPREVOUT proposal fixes this by requiring a chaperone signature that commits to input. However, if the rationale for chaperone signature stands, it should be applicable to all proposals listed above.
A more generic approach is to always require a ?safe" signature that commits to at least one input. However, this interacts poorly with the "unknown public key type? upgrade path described in bip-tapscript ( ), since it?d be a hardfork to turn an ?unknown type sig? into a ?safe sig?. But we could still use a new ?leaf version? every time we introduce new sighash types, so we could have a new definition for ?safe sig?. I expect this would be a rare event and it won?t consume more than a couple leaf versions. By the way, customised sighash policies could be done with CAT/CSFS.

@_date: 2019-05-24 16:15:45
@_author: Johnson Lau 
@_subject: [bitcoin-dev] OP_DIFFICULTY to enable difficulty hedges (bets) 
A gamble like this, decentralised or not, is easy to manipulate since difficulty is determined entirely by the last block in a cycle

@_date: 2019-05-25 03:12:32
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Congestion Control via OP_CHECKOUTPUTSHASHVERIFY 
Functionally, COHV is a proper subset of ANYPREVOUT (NOINPUT). The only justification to do both is better space efficiency when making covenant.
With eltoo as a clear usecase of ANYPREVOUT, I?m not sure if we really want a very restricted opcode like COHV. But these are my comments, anyway:
1. The ?one input? rule could be relaxed to ?first input? rule. This allows adding more inputs as fees, as an alternative to CPFP. In case the value is insufficient to pay the required outputs, it is also possible to rescue the UTXO by adding more inputs.
2. While there is no reason to use non-minimal push, there is neither a reason to require minimal push. Since minimal push is never a consensus rule, COHV shouldn?t be a special case.
3. As I suggested in a different post ( ), the argument for requiring a prevout binding signature may also be applicable to COHV

@_date: 2019-05-25 15:53:34
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Safety of committing only to transaction outputs 
The salt will be published when it is first spent. Salting won?t help if the address is reused.
It is circular dependent: the script has to commit to the txid, and the txid is a function of script
This restricts replayability to input with same value, but is still replay-able, just like ANYPREVOUT committing to the input value

@_date: 2019-05-26 22:33:06
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Two questions about segwit implementation 
Witness is not script. There is no op_pushdata or any other opcodes.
Witness is a stack. For each input, the witness starts with a CCompactSize for the number of stack elements for this input. Each stack element in turns starts with a CCompactSize for the size of this element, followed by the actual data
A ?00? element means the size of this element is zero. Since it?s zero size, no data is followed. This will create an empty element on the stack. It?s effectively same as OP_0 (Again, witness is not script)
A ?0100? element means the element size is one, and the data for this element is ?00?. So it will leave an 1-byte element on the stack.
The ?00? here means "this input has no witness stack element?. You need this even for non segwit inputs, because there is no way to tell whether an input is segwit-enabled or not, until you look up the UTXO, which might not be always available. Transaction serialization couldn?t rely on contextual information.
However, if all inputs have no stack element, the spec requires you to always use the non-segwit serialization.

@_date: 2019-05-27 00:28:57
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Two questions about segwit implementation 
This is not how it works. While the transaction creator may know which inputs are segwit, the validators have no way to tell until they look up the UTXO set.
In a transaction, all information about an input the validators have is the 36-byte outpoint (txid + index). Just by looking at the outpoint, there is no way to tell whether it is segwit-enabled or not. So there needs to be a way to tell the validator that ?the witness for this input is empty?, and it is the ?00?.

@_date: 2019-05-27 01:24:13
@_author: Johnson Lau 
@_subject: [bitcoin-dev] Two questions about segwit implementation 
Empty scriptSig doesn?t imply segwit input: if the previous scriptPubKey is OP_1 (which does not allow witness), it could still be spent with an empty scriptSig
Similarly, a scriptSig looking like a spend of P2SH-segwit doesn?t imply segwit input: if the previous scriptPubKey is empty, it could be spent with a push of any non-zero value.
