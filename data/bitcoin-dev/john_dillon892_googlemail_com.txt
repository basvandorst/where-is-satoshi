
@_date: 2013-04-18 06:07:23
@_author: John Dillon 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Hash: SHA1
Please don't say Gavin agrees with you. This reminds me of discussing
security in the early days of the internet when the general assumption
that everyone played nice was still correct.
We're seeing huge, expensive, DoS attacks against mining pools,
exchanges, information sites, stores etc. Bitcoin has enemies. Peter
Todd is 100% correct, tx replacement is another form of zero
confirmation transaction and all that has to happen is some subset of
mining power start doing replace by tx fee for it to have no security
while with your proposed implementation opening up a DoS attack
You also see the DoS attack vector as unimportant and suggest to
handle it as a prioritization problem. "real world experience
indicates that people don't pointlessly mount attacks over and over
again if there's nothing to be gained by doing so." <- of course there
is something to be gained, shutting down a service dependent on tx
replacement, as seen by all the DoS attacks we are seeing. If I were
deciding if my service should use tx replacement and I understood that
it could be trivially shut down, I sure wouldn't be happy I could
"just warn users not to take advantage of the feature whilst the flood
is in progress"
Gavin do you actually agree with Mike on this stuff like he implies?
Because if you do, I think people should know. Myself I wouldn't want
to be contributing to your salary as a foundation member if you don't
take Bitcoin security seriously.
The rapidly-adjusted payments stuff on the Contracts page of the wiki
is broken in multiple ways:
1. (known) Requires DoS vulnerable infrastructure.
2. (known) TX mutability
3. (unknown?) Just doesn't work. Step 5 is to check that T2 is signed
correctly by the access point, and if so, sign T1 and T2. But the
signature of T2 includes the txid of T1 and that isn't known until T1
is fully signed.
That  has not been noticed before shows that for all this hot air
no-one has ever bothered making an implementation of the idea. So
Mike, why are you happy to make testnet vulnerable to an unusually
easy DoS attack for an idea you haven't even tried on your own private
testnet with replacement enabled?
Anyway, with Peter Todd's much saner tx-replacement-by-fee the
following can be done:
1. Create a new public key PK1
2. Request a public key PK2 from the access point.
2. Create TX1 with two inputs and two outputs. Both parties sign it
and broadcast it.
access point input -> 2 PK1 PK2 checkmultisig, value = input  - fee
you input -> 2 PK1 PK2 checkmultisig, value == input  - fee
3. Create TX2.
TX1  -> pay to access point PK2
TX1  -> pay to yourself PK1 (change)
Set TX2 nLockTime to some time in the future.
4. Set the initial value's of TX2 out  and out  to the value the
access point and you committed in TX1. Both parties sign with
SIGHASH_SINGLE. (which means both parties are signing for both inputs)
5. Update TX2 as required and sign both inputs. The access point
doesn't need to sign TX2 or give the updated copy of TX2 to the other
party. The TX is not broadcast when updated (like the earlier contract
proposal) although doing so harms no-one.
When the session ends with both parties online, do the following:
1. You sign a version of TX2 with the final output values and nLockTime=0
2. If the final output values are acceptable to the access point, they
sign the other half of the 2-of-2 inputs and broadcast. (with whatever
fees required)
If the buyer quits the session abruptly:
1. Access point signs the last (most funds) version of TX2 given to
them, waits until the nLockTime expires, and broadcasts. This also
gets their TX1 input back.
If the access point quits abruptly they can do the above when they go
back online. The buyer has the first, signed, version of TX2 and at
worst can broadcast it eventually to get their deposit back.
After TX1 is signed and broadcast both parties are in on the contract
together, so the funds can't move without the consent of the other.
Both parties can block the movement of the other's deposit, but they
lose their deposit too. With tx mutability there is a small window of
time for a technical mistake, but that should be very, very rare.
You can broadcast an earlier version of the transaction where you pay
less than you were supposed too. However if you do this, the access
point can broadcast a new version of the transaction, splicing
together your signature on the correct output value, with your
signature on an earlier version of the access point's output, thus
paying miners a higher fee than the transaction you broadcast.
Rational miners will chose the latter version rather than your one.
This bidding process can continue until you are out the full amount
you were supposed to pay, with the whole payment going to fees, so why
bother? With nLockTime you don't have a better chance of mining the
transaction than any other miner.
I apologize if the above has already been discussed. I only looked at
the wiki and source code and don't waste too much time reading the
endless bitcointalk forums. The wiki should be updated with these
ideas as they are developed by people and vetted.
Strict replacement by fee should be written so it can be tested
properly and people in the Bitcoin ecosystem use proper security
practices with regard to unconfirmed transactions. I'm willing to
pledge $500USD to anyone who implements it. That is write the core
functionality that does replacement by fee, and a simple 'undo' RPC
command. I would do it myself but my programming is rusty.

@_date: 2013-04-19 04:38:35
@_author: John Dillon 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
I understand that Gavin has spent effort on security efforts against
small-scale attackers. It's the fact that he is so dismissive of the
threat that large attackers play that is what bothers me. But if I am
being divisive I understand.
I posted a clarification of what the reward is for exactly on the
forums:

@_date: 2013-04-23 12:40:13
@_author: John Dillon 
@_subject: [Bitcoin-development] Anti DoS for tx replacement 
Sorry I don't have time to reply more in depth, but I wanted to say to
Jeremy (especially) and Peter I'm very impressed to see such a good
design be created so fast that does not depend on replacement at all.
This is a great example of how often the right approach to a problem
is to accept that the easy solution will not work, and find a way to
overcome the issue, rather than trying to paper over the easy
solution's problems with insecure design. I'm reminded of Peter's work
on fidelity bonded banking to overcome Bitcoin's scalability problem,
although that needs to become real, and soon, so we can find all the
flaws in it that will only become apparent when the idea is
implemented for real.
Jeremy: There does not seem to be a PGP key listed for your email
address. Is that correct?
On Sat, Apr 20, 2013 at 8:51 PM, Jeremy Spilman

@_date: 2013-04-29 02:57:53
@_author: John Dillon 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
Hash: SHA256
Have we considered just leaving that problem to a different protocol such as
BitTorrent? Offering up a few GB of storage capacity is a nice idea but it
means we would soon have to add structure to the network to allow nodes to find
each other to actually get that data. BitTorrent already has that issue thought
through carefully with it's DHT support.
What are the logistics of either integrating a DHT capable BitTorrent client,
or just calling out to some library? We could still use the Bitcoin network to
bootstrap the BitTorrent DHT.

@_date: 2013-04-29 03:48:18
@_author: John Dillon 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
Hash: SHA256
Unfortunate. What makes them not work out? DHT torrents seem pretty popular.
Now don't get me wrong, I'm not proposing we do this if it requires additional
steps or other software. I only mean if it is possible in an easy way to
integrate the BitTorrent technology into Bitcoin in an automatic fashion. Yes
part of that may have to be finding a way to re-use the existing port for
Sure I guess my concern is more how do you find the specific part of the chian
you need without some structure to the network? Although I guess it may be
enough to just add that structure or depend on just walking the nodes
advertising themselves until you find what you want.
We can build this stuff incrementally I'll agree. It won't be the case that one
in a thousand nodes serve up the part of the chain you need overnight. So many
I am over engineering the solution with BitTorrent.
Good point. Sadly one that may apply to the Tor network too in the future.

@_date: 2013-08-05 05:29:00
@_author: John Dillon 
@_subject: [Bitcoin-development] Preparing for the Cryptopocalypse 
Hash: SHA256
Is NTRU still an option? My understanding is that NTRUsign, the algorithm to
produce signatures as opposed to encryption, was broken last year:
Having said that my understanding is also that the break requires a few
thousand signatures, so perhaps for Bitcoin it would still be acceptable given
that we can, and should, never create more than one signature for any given key
anyway. You would be betting that improving the attack from a few thousand
signatures to one is not possible however.
In any case, worst comes to worst there are always lamport signatures. If they
are broken hash functions are broken and Bitcoin is fundementally broken
anyway, though it would be nice to have alternatives that are similar is pubkey
and signature size to ECC.

@_date: 2013-08-19 02:53:32
@_author: John Dillon 
@_subject: [Bitcoin-development] Bloom io attack effectiveness 
Hash: SHA256
On Mon, Aug 19, 2013 at 12:59 AM, Gavin Andresen
So tell us how is your "vision" of 10,000 big beefy full nodes with SPV peers
any different from the Electrum model? These days Electrum clients have block
headers and verify that transactions have merkle paths to the block headers.
The only difference I see is that SPV uses bloom filtering and Electrum can
query by transaction. But Mike wants to add querying by transaction to full
nodes anyway, and one of the purported advantages of this UTXO proof stuff is
that you can query servers for UTXO's by address, so I see no difference at
all. A patch to do bloom filtering on Electrum would be amusing to me.
Here you have Peter talking about clever ways to actually get decentralization
by having SPV peers donate back to the network with spare bandwidth, like
relaying blocks, not to mention his partial UTXO set ideas, and you completely
ignore that. But I guess that would raise ugly questions when people realize
they can't now contribute back to Bitcoin, because the blocksize is a gigabyte
of microtransactions... It may also raise ugly questions with regulators that
may find the idea of "full node == data chokepoint == regulatory chokepoint" an
attractive notion. Why are there not any competent people other than Peter who
really have the guts to bring up these proposals? I've little luck getting
proof-of-concepts built for money anyway. Maybe we just have a darth of smart
competent people in this space.
You do a good job of signaling your priorities Gavin. The payment protocol
includes no notion that you may want to pay anyone but a SSL certified
merchant. Yes I know the crypto can be upgraded, but it says volumes that you
pushed for that first, without even the slightest token effort to allow
individuals to participate in any way. Sad given you have made things *less*
secure because there is no safe way to get money *into* my wallet with the
payment protocol, but could have been.
Tell me, when my decentralization pull-req is voted on, which way are you
planning on voting?

@_date: 2013-08-19 03:09:07
@_author: John Dillon 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Excellent, and makes a mockery of zero-confirmation transactions to boot.
Can be prevented by passing along txin proofs, but they require the full
transaction, so the effective UTXO set size would go up greatly post-pruning. I
am sure Mike would love to demand that full nodes do this for their peers
though, at least until UTXO commitments are greated, at great cost to full
On the other hand, a tx with some txin proofs can be safely relayed by SPV
nodes, an interesting concept. Do the UTXO commitment people have keeping proof
size small in mind?
That is good too.
I'll bounty 2.5BTC to implement the first attack, and 0.5BTC for the second.
Should be easy to do as a patch to satoshi bitcoin I think. The implementation
must include a RFC3514 compliant service bit to let peers know of the operators
intentions. Along those lines I'll donate 3BTC to adding service bit selection
to DNS seeds.
We should clearly show people the limitations of SPV before they depend too
much on it. Nothing wakes users up like a 21 million BTC transaction in their

@_date: 2013-08-19 05:34:07
@_author: John Dillon 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
My apologies, that was for Peter
On Mon, Aug 19, 2013 at 5:00 AM, John Dillon

@_date: 2013-07-14 19:05:26
@_author: John Dillon 
@_subject: [Bitcoin-development] Reward for P2SH IsStandard() patch. 
Hash: SHA256
As you all know keeping the size of the UTXO set small is critical, and more
recently we've also had problems with distasteful data being added to the UTXO
set. ( Gregory Maxwell
has an excellent solution to the distasteful data problem in the form of P2SH^2
( and Peter Todd
pointed out how we can implement it with the existing P2SH form. We're also
going to be implementing some kind of OP_RETURN  soon which handles the
timestamping and similar use-cases, again without UTXO impact.
Right now the only scriptPubKey form with any significant use is the
checksighash. Bare pubkey gets used by the odd miner, and by Deepbit due to
their ancient codebase. The former isn't an issue as the miner mines the txout
themselves, and the latter shouldn't find updating to be a big deal.
OP_CHECKMULTISIG is used by Peter Todd's timestamper, but that can be changed
to OP_RETURN without difficulty. However all that will (hopefully!) soon change
as hardware wallets and the payment protocol make hardware wallets worthwhile,
and we should make sure these protocols take the extra step of using P2SH
before we get locked into a bunch OP_CHECKMULTISIG implementations.
We also have the problem that the IsStandard() code accepts up to 120 bytes of
junk data as a pubkey, allowing injection of 240 bytes of *spendable* data into
the UTXO set with bare OP_CHECKMULTISIG. This capability has to be stopped.
Thus I'm offering a reward of 1BTC for whomever creates a patch to change
IsStandard() to accept only P2SH and pubkeyhash in a raw scriptSig, allowing
other forms only when used with P2SH. I'm offering a further 1BTC to whomever
gets such a patch accepted into mainline. It's a pretty easy patch, so I'm
asking that all core-developers (that includes you Peter) hold off for one week
to give less experienced developers a crack at it. If for some reason you want
to remain anonymous that is ok by me as well provided you assign copyright to
me. I do expect unittests. Should be about half a day to a days work.
Long-term we should be using P2SH with an inner OP_CHECKSIG for most addresses
as it's a 1 byte savings. Change addresses can have this done first, although
bitcoinj support will help so that satoshidice and similar sites can pay to
P2SH change. As for multisig's P2SH overhead for a 1-of-2 and 2-of-2 and
3-of-3, is 10%, 8.6% and 6.2% respectively, all pretty minor, especially if you
assume the blocksize limit will be raised.

@_date: 2013-07-14 19:22:10
@_author: John Dillon 
@_subject: [Bitcoin-development] libzerocoin released, 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Seems that Peter is describing a system that requires no changes at all to the
Bitcoin codebase and thus there are no costs whatsoever.
Peter: I'm a bit confused by this concept of "bi-directional sacrifice" though,
I assume there exists only a sacrifice in one direction right? Wouldn't selling
a zerocoin be just a matter of giving zerocoin a rule so that the zerocoin tx
moving it to the new owner only happens if a specific form of bitcoin tx
happens too?
Merge mining is very much mining a coin for free. Ask not what the total reward
is, ask that the marginal cost of merge mining an additional coin is. The issue
is that unless there is a cost to mining a *invalid* block the merge mined coin
has little protection from miners who mine invalid blocks, either maliciously
or through negligence. If the coin isn't worth much, either because it's market
value is low or the worth is negative to the malicious miner, your theories of
value have nothing to do with the issue.
Gregory Maxwell has written about this issue before on the  IRC
channel and on bitcointalk as well if memory serves. I advise you to look up
his description of the problem, almost everything he writes on the topic of
crypto-coin theory is spot-on correct.

@_date: 2013-07-14 19:40:21
@_author: John Dillon 
@_subject: [Bitcoin-development] Reward for P2SH IsStandard() patch. 
Hash: SHA256
By "impact" I am referring to the impact on transaction size and thus
blockchain space and fees, not UTXO size as stored by nodes themselves.
Specifically take the size of the txout and txin and compare the version using
P2SH to the equivalent version not using it to get my numbers.
Anyway, given how much uncompressed keys are still used obviously fee pressure
isn't even close to getting people to create efficient transactions.

@_date: 2013-07-14 19:48:45
@_author: John Dillon 
@_subject: [Bitcoin-development] libzerocoin released, 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Again, you forget that there may exist miners for which the value of the coin
is negative.
Never mind that in practice you want there to exist a cost to encourage miners
to actually pay attention to what they mind and to encourage them to update
software when required and participate.
Validating clients, not SPV clients.
All those things simply change the amount of alt-coin the miner gets, which to
the miner may have no reward. You also have the issue that we may be talking
about a non-currency chain where reward is more nebulous.
In any case, regarding a zerocoin chain, Peter's observation that
proof-of-sacrifice allows a strong 51% attck defense is very clever and IMO is
significantly stronger than proof-of-work mining, merged or not, would provide.
It's essentially the ability to conjur up mining capacity on demand, but only
by those who have a stake in the crypto-coin. It does depend on the existance
of a proof-of-work chain, but we have a perfectly good one handy.
PS: good to see you signing you email!

@_date: 2013-07-14 19:52:56
@_author: John Dillon 
@_subject: [Bitcoin-development] libzerocoin released, 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
You said it better than I did.
Essentially I am worried about the chain being strangled at birth, merge-mining
makes doing so cost nothing for the attacker. With zerocoin this is a
particularly dangerous possibility due to those in the Bitcoin community who
would like to see Bitcoin continue to have poor privacy properties.

@_date: 2013-07-14 22:12:00
@_author: John Dillon 
@_subject: [Bitcoin-development] Protecting Bitcoin against network-wide DoS 
Hash: SHA256
It's been pointed out recently how a fairly cheap attack on the Bitcoin network
would be to take advantage of the fact that we limit the number of incoming
connections, but don't require anything of those connections. This means an
attacker can simply repeatedly query the the DNS seeds for new addresses and
make enough incoming connections that those nodes can not accept further
clients. nMaxConnections defaults to 125, and beyond that there is the limit on
file descriptors, as well as possible limits by stateful firewalls. (how much
memory/cpu does an incoming connection require?) The DNS seeds themselves crawl
the network on your behalf, and let you direct the attack starting at the nodes
new SPV clients are most likely to connect too.
The cost to the attacker is minimal, 1 INV message per transaction and block,
and some gossiped peer addresses.  Currently that should be on the order of 30
bytes a second. The attacker can do even better by pretending to be an SPV
client, thus reducing their incoming bandwidth consumption to nearly nothing,
yet increasing resource usage on the node.
Peter estimated you would need just 200 or so well distributed IP addresses to
make it impossible to use an SPV client. In fact as far as I can tell for
incoming connections we don't force incoming connections to be well
distributed, so the attack could be done by simply one server with enough
amount of bandwidth. Estimates of the total number of nodes out there on
mainnet are in the tens of thousands, let's say 25,000 for arguments sake. 125
connections to every one of those nodes would only cost the attacker 94MB/s of
incoming bandwidth, easily attainable by a few cheap EC2 nodes, and on EC2
incoming bandwidth is free. The SPV version of the attack would let the
attacker spend as little as they wished.
Obviously if we want to make it possible for SPV nodes to reliably connect to
the network we need to give them a way to prove they have sacrificed some
limited resource to allow nodes to distinguish legit users from attackers.
Failing that, we need to make attacks sufficiently expensive to discourage
bored script-kiddies, much the same way flooding the network with transactions
is sufficiently expensive due to fees that such attacks are impractical.
Now something to keep in mind is whatever we ask SPV nodes to sacrifice must
not be reusable. For instance proof-of-stake *doesn't* work without consensus
because an attacker can reuse the proof for multiple connections. Similarly IP
addresses don't work, requring incoming connections to be "well distributed" in
IP space isn't a bad idea, but it doesn't buy much DoS resistance. Fees paid by
confirmed transactions do work, but only if something links the transaction to
the specific connection.
We also want whatever the nodes to sacrifice to be something not much more
costly to the client than to the attacker. Bandwidth isn't reusable, but an
attacker with EC2 or a botnet has vastly lower costs for bandwidth than a user
with an Android wallet on a phone.
For a non-SPV-mode client we can easily do anti-DoS by requiring the peer to do
"useful work". As the incoming connections slots get used up, simply kick off
the incoming peers who have relayed the least fee-paying transactions and valid
blocks, keeping the peers who have relayed the most. We can continue to use the
usual, randomized, logic for outgoing peers to attempt to preserve the
randomized structure of the bitcoin network. Without an ongoing attack nodes
making new connections are unaffected, and during an attack new connections are
made somewhat easier by the increased numbers of incoming slots made available
as the attackers connections timeout.
Yes an attacker can simply relay some high-fee transactions to keep their nodes
from being kicked off, but in that case are they really an attacker? I reject
the argument that we are letting them de-randomize the structure of the network
because as I've shown they can already do that with little expenditure.
For SPV nodes again in the absense of an attack such anti-DoS code has no
effect. When an attack is launched the SPV client can simply create some
high-fee transactions with their own coins to get connection priority. SPV
nodes already have serious privacy issues, so I don't see the creation of
transactions as a big deal. Re-use is an issue, but nodes can take into account
how long it takes for another nodes to advertise the transactions when dealing
with SPV peers. Better systems can be implemented later, such as micropayment
channels and coinbase probabalistic payments, that don't result in blockchain
transactions just for the sake of anti-DoS.
A demo of the attack against would be useful. Pieter Wuille's bitcoin-seeder
code could probably be re-used as it already has the required functionality of
making large numbers of connections. In fact, simply running multiple instances
of it could do the trick.

@_date: 2013-07-28 18:21:44
@_author: John Dillon 
@_subject: [Bitcoin-development] Linux packaging letter 
My signature:
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Linux distribution packaging and Bitcoin
This note summarises the dangers inherent in the Linux distribution
packaging model for Bitcoin, and forms a request from upstream
maintainers to not distribute Bitcoin node software as part of
distribution package repositories without understanding the special
requirements of Bitcoin.
Distributors typically unbundle internal libraries and apply other
patches for a variety of generally good reasons, including ensuring
that security-critical fixes can be applied once, rather than multiple
times for many different packages. In most cases, the common
distribution packaging policy has many advantages.
However, Bitcoin nodes are an unusual category of software: they
implement a complex group consensus in which every client verifies the
behaviour of every other exactly. Even an exceptionally subtle change -
including apparently harmless bugfixes - can cause a failure to reach
consensus. A consensus failure of one client is a security risk to the
user of that client. A significant number of nodes failing to reach
consensus - as happened in March 2013 due to a change in database
libraries[1] - is a critical problem that threatens the functionality
and security of the system for all users.
For this reason, it is _vital_ that as much of the network as possible
uses _unmodified_ implementations that have been carefully audited and
tested, including dependencies. For instance, if the included copy
of LevelDB in bitcoind is replaced by a system-wide shared library,
_any_ change to that shared library requires auditing and testing,
a requirement generally not met by standard distributor packaging
Because distributed global consensus is a new area of computer science
research, the undersigned request that distributors refrain from
packaging Bitcoin node software (including bitcoind and Bitcoin-Qt)
and direct users to the upstream-provided binaries instead _until they
understand the unique testing procedures and other requirements to
achieve consensus_. Beyond being globally consistent, upstream binaries
are produced using a reproducible build system[2], ensuring that they
can be audited for backdoors.
1. 2.

@_date: 2013-07-28 18:42:26
@_author: John Dillon 
@_subject: [Bitcoin-development] Distributing low POW headers 
Hash: SHA256
As Peter said, "much" should be quantified.
Remember that there is a statistical distribution here, what is the probability
of how many seconds per headers?
Sounds like you are changing economics and requiring miners to have even better
network connections. This is not a thing to do lightly and it probably a bad
I understand Pieter Wuille is working on letting Bitcoin propagate and make use
of pure block headers, a step towards SPV and partial UTXO mode.
Orphan measurement would be very useful for a lot of reasons, how about you
think about that first? It wouldn't have the potential data rate issues either
and should be a very simple change. Just set some threshold relative to the
height of the best block where you will not further propagate and orphan
block(header) and prior to that limit do so freely. I believe the change would
be 100% compatible with the P2P protocol as it is based on inventories.

@_date: 2013-07-28 19:11:42
@_author: John Dillon 
@_subject: [Bitcoin-development] Two factor wallet with one-time-passwords 
Hash: SHA256
You missed a 'CAT' opcode here.
I think you should disclose whether or not you have any ties to the pulp and
paper business... By my calculations the production of a single OTP table would
consume roughly half of all the forest biomass on this planet.
Your idea is better than you realize, you are just too paranoid for your own
good. The thing is the attacker isn't going to be someone paying you funds over
your minimum spending limit, which means the size of the table deriving which
H(nonce) is selected for a given txid:vout can be significantly smaller. For
instance if you want to have 256 total payments before a 50:50 chance of any
pair using the same nonce, you only need a table with ~2^16 elements or with 20
byte hashes just a megabyte of data. It is the 16 level merkle proofs that are
the problem, 16*21=336 bytes of data in the scriptSig. Then again, that's only
4.5x the size of a single signature, not unreasonable.
Also your nested IF statements, while a lovely and hilarious use of MAST, can
be replaced by simply creating the merkle tree over the tuples [i,H(nonce_i)]
and proving that the nonce_i you provided matched the precommitted tree. Now
you only need to provide one merkle proof, not two.
But don't let me discourage you, rarely do I see elaborate jokes that also meet
the criteria to be a least publishable unit. :)

@_date: 2013-07-28 19:39:08
@_author: John Dillon 
@_subject: [Bitcoin-development] Opcode whitelist for P2SH? 
Hash: SHA256
Peter Todd recently came up with two related, and IMO very good, uses for
non-standard transactions to implement both oracles and one-time-password
protection of wallet funds. While the wallet fund case could be implemented as
only a single standard type, at the cost of generality, the oracle case would
be most useful with more arbitrary rules. More generally it is also useful to
be able to have scriptPubKeys like the following:
    n ... m CHECKMULTISIG  CHECKSIG BOOLOR
and many other similar constructions.
What are your thoughts on creating a whitelist for specific opcodes that would
apply to scripts serialized using P2SH, retaining the existing standard
whitelist for scriptPubKeys? (I would still recommend dropping pay-to-pubkey
and pay-to-multisig due to their potential for dumping data in the UTXO set)
I'm thinking it should contain the following opcodes, picked for either being
already used, or having simple semantics:
0 to 75 byte pushdata
OP 1 to OP16 (numbers are allowed through pushdata anyway)
FROMALTSTACK (the alt-stack makes stack manipulation in complex ways possible)
Note how this list allows for complex logic, but does not allow for arithmetic,
thus not exposing us to a source of problems in the past.

@_date: 2013-06-04 14:55:36
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
100% miner fee is not a proof of anything because the miner could have created
that transaction for themselves. You must have proof that all miners had an
equal opportunity at collecting the fee, and the only way to do that is by
Peter's announce-commit protocol, or his unspendable until after n blocks
Also the idea of a zero-output transaction is silly. In almost all cases you
are making the sarifice to link that act to an identity, and linking that act
to arbitrary data is far more flexible than any scheme relying on the pubkeys
that paid for the transaction. With a arbitrary data you can slice up the
sacrifice for instance with a merkle-sum-tree, as well as hide what the
sacrifice was for to preserve anonymity. The extra cost in size of the provably
unspendable OP_RETURN scriptPubKey is minimal for the rare time when it isn't

@_date: 2013-06-10 04:09:26
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit with 
Hash: SHA256
It has been suggested that we leave the decision of what the blocksize to be
entirely up to miners. However this leaves a parameter that affects every
Bitcoin participant in the control of a small minority. Of course we can not
force miners to increase the blocksize if they choose to decrease it, because
the contents of the blocks they make are their decision and their decision
only. However proposals to leave the maximum size unlimited to allow miners to
force us to accept arbitrarily large blocks even if the will of the majority of
Bitcoin participants is that they wish to remain able to validate the
What we need is a way to balance this asymetrical power relationship.
Proof-of-stake voting gives us a way of achieving that balance. Essentially for
a miner to prove that the majority will of the poeple is to accept a larger
blocksize they must prove that the majority has in fact voted for that
increase. The upper limit on the blocksize is then determined by the median of
all votes, where each txout in the UTXO set is one vote, weighted by txout
value. A txout without a corresponding vote is considered to be a vote for the
status quo. To allow the voting process to continue even if coins are "lost"
votes, including default votes, are weighted inversely according to their age
in years after 1 year. IE a vote with weight 1BTC that is 1.5 years old will be
recorded the same as a <1 year old vote weighted as 0.67BTC, and a 1 day old
and 6 months old UTXO are treated equivalently. The 1 year minimum is simply to
make voting required no more than once per year. (of course, a real
implementation should do all of these figures by block height, IE after 52,560
blocks instead of after 1 year)
A vote will consist of a txout with a scriptPubKey of the following form:
    OP_RETURN magic vote_id txid vout vote scriptSig
Where scriptSig is a valid signature for a transaction with nLockTime
500,000,000-1 spending txid:vout to scriptPubKey:
    OP_HASH160 H(OP_RETURN magic vote_id txid vout vote) OP_EQUAL
vote_id is the ID of the specific vote being made, and magic is included to
allow UTXO proof implementations a as yet unspecified way of identifying votes
and including the weighted median as part of the UTXO tree sums. (it also
allows SPV clients to verify the vote if the UTXO set is a Patricia tree of
scriptPubKeys) vote is just the numerical vote itself. The vote must compute
the median, rather than the mean, so as to not allow someone to skew the vote
by simply setting their value extremely high. Someone who still remembers their
statistics classes should chime in on the right way to compute a median in a
The slightly unusual construction of votes makes implementation by wallet
software as simple as possible within existing code-paths. Votes could still be
constructed even in wallets lacking specific voting capability provided the
wallet software does have the ability to set nLockTime.
Of course in the future the voting mechanism can be used for additional votes
with an additional vote_id. For instance the Bitcoin community could vote to
increase the inflation subsidy, another example of a situation where the wishes
of miners may conflict with the wishes of the broader community.
Users may of course actually create these specially encoded txouts themselves
and get them into the blockchain.  However doing so is not needed as a given
vote is only required to actually be in the chain by a miner wishing to
increase the blocksize. Thus we should extend the P2P protocol with a mechanism
by which votes can be broadcast independently of transactions. To prevent DoS
attacks only votes with known vote_id's will be accepted, and only for
txid:vout's already in the blockchain, and a record of txouts for whom votes
have already broadcast will be kept. (this record need not be authoritative as
its purpose is only to prevent DoS attacks) Miners wishing to increase the
blocksize can record these votes and include them in the blocks they mine as
required. To reduce the cost of including votes in blocks 5% of every block
should be assigned to voting only. (this can be implemented by a soft-fork)
For any given block actual limit in effect is then the rolling median of the
blocks in the last year. At the beginning of every year the value considered to
be the status quo resets to the mean of the limit at the beginning and end of
the interval.  (again, by "year" we really mean 52,560 blocks) The rolling
median and periodic reset process ensures that the limit changes gradually and
is not influenced by temporary events such as hacks to large exchanges or
malicious wallet software.  The rolling median also ensures that for a miner
the act of including a vote is never wasted due to the txout later being spent.
Implementing the voting system can happen prior to an actual hard-fork allowing
for an increase and can be an important part of determining if the hard-fork is
required at all.
Coercion and vote buying is of course possible in this system. A miner could
say that they will only accept transactions accompanied by a vote for a given
limit. However in a decentralized system completely preventing vote buying is
of course impossble, and the design of Bitcoin itself has a fundemental
assumption that a majority of miners will behave in a specific kind of "honest"
A voting process ensures that any increase to the blocksize genuinely
represents the desires of the Bitcoin community, and the process described
above ensures that any changes happen at a rate that gives all participants
time to react. The process also gives a mechanism for the community to vote to
decrease the limit if it turns out that the new one was in fact too high. (note
how the way the status quo is set ensures the default action is for the limit
to gradually decrease even if everyone stops voting)
As many of you know I have been quite vocal that the 1MB limit should stay. But
I would be happy to support the outcome of a vote done properly, whatever that
outcome may be.

@_date: 2013-06-10 04:59:45
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
Hash: SHA256
The default should *not* be set by wallets at all in fact. The default is that
by not voting, you accept the status quo, which is defined as the mean of the
old and new limits in the past year.
So lets say the limit is 1MB, and through voting it ends up at 2MB in one year.
Until that time by not voting you are in effect voting for the limit to be 1MB,
but after the next interval you not voting is equivalent to voting for a 1.5MB
limit. A subtle issue is then txout age, and at that point a 1.5 year old txout
should be like voting for the 1MB limit still, albeit weighted less. What you
don't want is your lack of vote to suddenly turn into a 1.5MB vote. This makes
sure that at all levels the increases are gradual rather than abrupt, although
the rate of increase may still be quite fast if the community votes that way.
(first derivative of the limit is a close approximation to a continuous

@_date: 2013-06-10 08:26:39
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
Hash: SHA256
On Mon, Jun 10, 2013 at 8:14 AM, Melvin Carvalho
Indeed it was. Which is why as GPU's came onto the scene Satoshi was strongly
against them. I have to wonder what he thinks of ASICs where just a handful of
companies control the supply of Bitcoin hashing power.
Satoshi also never forsaw pools, which are why just 2 or 3 people control the
majority of Bitcoin hashing power.
That's why I'm very clear that doing nothing is a vote for the status quo. Of
course wallet authors can do what they want to try to get users to vote
according to their wishes, or for that matter simply steal your vote, but we
already must put a lot of faith into wallets to not steal our funds.
People are proposing we put control of the blocksize entirely into the hands of
miners, yet we all have an interest in auditing the blocks miners produce.
There must be balance.

@_date: 2013-06-15 18:28:23
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Rather than "OP_VOTE" all you really need is the "spending tx matches a
template" functionality that has been proposed for many other things.

@_date: 2013-06-28 10:09:16
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
Hash: SHA256
Tor does not act as a particularly effective man in the middle for nodes
that support connections to hidden services because while your
connections to standard Bitcoin nodes go through your exit node, the
routing path for each hidden service peer is independent. Having said
that we should offer modes that send your self-generated transactions
out via Tor, while still maintaining non-Tor connections.
Anyway Sybil attacks aren't all that interesting if you are the one
sending the funds, and receivers are reasonably well protected simply
because generating false confirmations is extremely expensive and very
difficult to do quickly. After all, you always make the assumption that
nearly all hashing power in existence is honest when you talk about
replace-by-fee among other things, and that assumption naturally leads
to the conclusion that generating false confirmations with a sybil
attack would take more than long enough that the user would be
suspicious that something was wrong long before being defrauded.
I'd be surprised if anyone has ever bothered with a false confirmation
sybil attack. I wouldn't be the slightest bit surprised if the NSA is
recording all the Bitcoin traffic they can for future analysis to find
true transaction origins. Which reminds me, again, we need node-to-node
connections to be encrypted to at least protect against network-wide
passive sniffiing.
Regarding usage I would be interested to hear from those running Bitcoin
nodes advertising themselves as hidden services.
For what it is worth I ran a double-spend generator a month or so ago
against the replace-by-fee node that Peter setup and I found that a
small number of the double-spends did in fact appear to be mined under
replace-by-fee rules.
Specifically the generator would create a transaction from confirmed
inputs, wait 60-180 seconds (randomized) to allow for full propagation,
and then create a double-spend if the transaction hadn't already been
mined. The transactions were randomized to look like normal traffic,
including occasional bets to Satoshidice and similar for fun. (for the
record the script had no way of knowing if a bet won and would happily
attempt to double-spend wins) Fees for the replacement were power-law
distributed IIRC, with some occasionally set to be quite hefty.
Though possibly just an artifact of unusually slow transaction
propagation it appeared that about 0.25% of hashing power was following
replace-by-fee rules. (not including transactions involving gambling, I
know Eligius and perhaps others block such transactions from their
mempools making double-spends easy to accomplish by including
Satoshidice outputs)
I'm actually surprised by that figure myself given Peter Todd and I
haven't made a serious attempt yet to get miners to use replace-by-fee
rules. An interesting experiment would be to advertise that money is
being given away by such a tx generator in the mining forum, although I
would prefer to see solid mempool support for the "scorched-earth"
double-spend countermeasure first; Peter sounds like he has some great
ideas there, although as usual I am seeing very little in the way of
code. :)

@_date: 2013-06-28 10:25:28
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
Hash: SHA256
I disagree entirely. Your example of "straw polls" for bug fixes and
features is precisely what the current method of rough consensus and
running code, an IETF expression, handles just fine.
What the method does not handle effectively are issues that are
fundementally political rather than technical in nature. Blocksize is
precisely the latter because while the tradeoffs are technical in
nature the fundemental issue at hand is what do we want Bitcoin to be?
Who are we going to allow to participate?

@_date: 2013-06-28 10:32:38
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
Hash: SHA256
Possibly, but it is a rather short window of opportunity and the mining node
would have to be connected directly, to Peter's replace-by-fee node. I also
took care to ensure transactions were only ever broadcast once. (I disabled the
wallet rebroadcast mechanism)

@_date: 2013-06-28 10:59:32
@_author: John Dillon 
@_subject: [Bitcoin-development] Proposal: MultiBit as default desktop 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Possible non-validation data that can be usefully propagated:
1) Block headers.
2) *Confirmed* transactions linked to an aformentioned blockheader.
3) Proof-of-work/sacrifice limited P2P messages, for instance to
co-ordinate trust-free-mixes or act as a communication channel for
micropayment channels.
4) With UTXO existance proof support propagate transactions
accompanied by proofs that all inputs exist. This would also allow for
implementation of Peter's low-bandwidth decentralized P2Pool proposal.
5) UTXO fraud proofs. (one day)
Strictly speaking  doesn't even need the protocol to be changed
actually as it can be handled entirely within the existing INV/getdata
mechanism. Sure someone could throw away a lot of hashing power and
get an invalid block propagated, but really so what? SPV nodes should
always take confirmations with a grain of salt anyway.

@_date: 2013-05-04 18:07:42
@_author: John Dillon 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I think you too should ask yourself why you are putting so much effort into
optimizing a centralized service, the DNS seeds, rather than putting effort
into optimizing the P2P peer discovery instead. DNS seeds are a necessary evil,
one that shouldn't be promoted with additional features beyond simply obtaining
your initial set of peers.
After all Peter, just like you have implemented alternate block header
distribution over twitter, in the future we should have many different means of
peer discovery. Right now we have DNS seeds, a fixed list, and IRC discovery
that does not work because the servers it was pointed too no longer exist. Not
a good place to be.
Some random ideas:
search engines - search for "bitcoin seed address" or something and try IP's
found (twitter is similar)
ipv4 scanning - not exactly friendly, but the density of bitcoin nodes is
probably getting to the point where a brute force search is feasible
anycast peers - would work best with UDP probably, who has the resources to set
this up?
It is probably not worth the effort implementing the above immediately, but it
is worth the effort to ensure that we don't make the DNS seed system so complex
and sophisticated that we depend on it.

@_date: 2013-05-05 13:12:15
@_author: John Dillon 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Sorry I should have used the word bootstrapping there rather than discovery.
But again I think that shows my point clearly. Centralized methods like DNS
should be used for as little as possible, just simple initial bootstrapping,
and focus the development efforts towards the non-centralized peer discovery

@_date: 2013-05-09 00:57:42
@_author: John Dillon 
@_subject: [Bitcoin-development] Discovery/addr packets (was: Service bits 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Unless the government told them too.
The employer example actually shows something important: between a worker and
an employer double-spending already irrelevant. People get paid after they work
their two weeks not before, so the double-spend is already irrelevant.
However when your employer pays you on the blockchain until the transaction
confirms for someone else to accept funds from that payment they not only have
to trust you, but also the employer. Sure they could take it as "you said you
would apy me so it is your responsibility to make that happen" but that brings
a whole new level of complexity.
A scheme where you vouch for your payments with your identity can benifit from
being able to follow that chain all the way back to the last confirmed
transaction, although actually implementing this may be too complex to be
worthwhile, especially initially.
Yes. But the issue is how are you going to optmize it? By adding yet more
restrictions and limitations on those who chose to run a node or mining
operation, or by actually fixing the trust issue? We know you can do the
latter, so do not sacrifice Bitcoin's core layer in silly attempts to make
double-spends harder. Fundementally Bitcoin has exactly one way of achieving
consensus, and that is the blockchain.
It must be your right to chose what transactins you chose to mine and chose to
relay. End of story. Bitcoin is not about imposing regulation on those who
choose to use it.
Indeed. Especially for the most popular use of Bitcoin as a payment system:
buying things PayPal won't let you. In that circumstance the only leverage you
have is the protections of the blockchain and the damage you can do to the
other (often anonymous) parties reputation.

@_date: 2013-05-09 01:00:05
@_author: John Dillon 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
Hash: SHA256
Perhaps Satoshi did this delibrately, knowing that at some point a hard-fork
would be a good idea, so that we all would have a good excuse to do one?

@_date: 2013-05-09 01:27:33
@_author: John Dillon 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Doesn't most mining hardware at the ASCI level start with a SHA256 midstate
given that the nonce is at the end?  Adding further information to the block
should be possible at the beginning of the block without major changes to the
mining hardware.
I feel somewhat uncomfortable about the "after-the-fact" auditing possible in
this scenario. Besides the timestamping provided by the block headers appears
to be useful in some payment protocols, not to mention in general.

@_date: 2013-05-09 02:33:11
@_author: John Dillon 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
I actually just meant how Pieter Wuille was talking about a blocktime accurate
to only within 18 hours. :) But it is a nice writeup!
In any case, for many things simple relative ordering is enough rather than
absolute time.
Nope. The attacker can make the timestamp on the block they mine as little as
the minimum from GetMedianTimePast(), and adding two hours to that number could
easily be well before true time.
What you probably need to do is some sort of median time calculation for the
blocks around your timestamp. The proof becomes probabalistic based on the % of
hashing power the attacker controls and in turn depends on if the time they
created their timestamp was of their own choosing.
IE, if you just want to create an inaccurate timestamp, but don't care when,
you can just mine blocks and wait until you get lucky. If you need to create an
inaccurate timestamp *now* the problem is much harder.
But all this analysis can be developed later, and data timestamped now. :)
Oh, right, yes, that is a much more simple idea and far less prone to bugs.
Many SPV clients wouldn't even need upgrades if they don't acturally validate
the blocks they receive and just look for the biggest PoW.
Anyway, you are being distracted from what we were talking about before, get
back to work!

@_date: 2013-05-09 09:58:50
@_author: John Dillon 
@_subject: [Bitcoin-development] An initial replace-by-fee implementation is 
Hash: SHA256
After some consultation with affected sites by myself and Peter we have decided
to release an initial replace-by-fee implementation and setup a server using
those rules on testnet. This implementation does not include recursive fee
evaluation, and is therefore vulnerable to DoS attack, so hopefully that will
continue to allow adoption to proceed gradually. We can-not recommend mining on
mainnet with it. It does not include an "undo" RPC command or an adjust fees,
and Peter says he has not implemented one yet.  Patches are welcome.
Specifically there were requests from vulnerable parties, which interestingly
included a site that knew they had bugs related to replacement but not
financial vulnerabilities, to put up a server on testnet to check wallet code.
The vulnerable requested to remain undisclosed. An additional consideration was
the upcoming anti-dust rules which are yet another example of why zero-conf is
so much more dangerous to accept than single-conf. Two of the people contacting
us brought up that issue in fact.
The code is on github:
    and a replace-by-fee server operating on testnet is available at
testnet-replace-by-fee.bitcoin.petertodd.org To test you will need to use the
raw transaction API and manually create the replacement transaction. Do note
that your wallet will retain the existing one and no mechanism yet exists to
delete the old transaction from your wallet. Again, a certain amount of
"cludgyness" to this is intentional to discourage premature non-testing use.
Regarding the reward, I've decided Peter will collect the full amount even
though the work is not %100 complete (the mempool aspect) due to his concern
about staging an implementation properly, working with vulnerable sites, and
overall genuine interest in the actual issues at hand rather than the reward.

@_date: 2013-05-13 07:31:21
@_author: John Dillon 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
No you didn't. :)
What is special about what Peter is proposing is that it is *not* merge-mining.
You see, merge-mining is essentially where you use one PoW for two purposes,
two different blockchains. So you are getting more value from just one unit of
But Peter's coinbase hashcash protocol carefully ensures that the act of mining
the hashcash is guaranteed to cost the miner at least some well-defined amount,
and that amount can be easily calculated by considering the probability that a
block could have been found with the effort required to generate the proof of
work, and the amount of value the miner would have then given away in a
"anyone-can-spend" output. (you may not realize this, but a scriptPubKey with a
single pushdata opcode is always evaluated as true, which means it can be
respent by anyone)
Don't feel bad though, I had to ask him to explain it to me too. :)
I think you are misremembering. I just checked the paper and PayWord is based
on chains of hashes and you give the receiver a digest and if after n repeated
hashes it is considered to have been worth n*k It is not a probabalistic
Incedentally while it is an obvious enough idea, though I didn't see a
reference to it, PayWords can be easily extended with a time-bandwidth
trade-off by using a structure similar to a merkle tree. The roots could be
created from some fixed nonce K and a increaing integer, H(K | n) Then you
would provide a merkle path to the previously agreed upon final digest. So
proof size for your payment would be log(n), and time to check the proof
log(n). Unfortunately setting up the scheme is still 2*n however that only
needs to be done once.
I have to respect a man who after all these years is still thinking
about anti-spam for email. :)
Peter actually made a blockchain headers over DNS system, and a blockchain
headers over twitter system as an April fools joke. See
Blockchain header data may very well be one of the most widely distributed
single data sets in the history of mankind, and most of its closest cousins are
definitions such as the ASCII table or near definitions like the DNS root
servers. Not something with new data every 10 minutes.

@_date: 2013-05-13 08:19:03
@_author: John Dillon 
@_subject: [Bitcoin-development] P2P non-blockchain message proposal 
Hash: SHA256
Peter's "Coinbase TxOut Hashcash" scheme mentions in passing anti-DoS
protection on the P2P flood-fill network for non-transaction messages, and an
application to use those messages, trust-free mixing. I did some review of the
source code and I think we can create a generalized P2P flood-fill message
system without difficulty.
First of all like Mike Hearn suggested in the tx-replacement (satoshi's
version) thread we can easily prioritize, or I should say deprioritize
non-essentially traffic with a simple "allocated bandwidth" scheme where nodes
set how many KB/second of bandwidth they want to give to P2P network messages.
Next out of that allocation use a priority scheme where higher priority
messages get put ahead of lower priority ones in the queue for retransmission.
Messages of too low a priority are dropped, and in general they basically just
don't get good network propagation, much like transactions with dust outputs
will increasingly face.
How do you determine priority? Why simply a genuine bitcoin sacrifice!
Previously that would have been some big bulky fidelity bond scheme, but now we
can use Peter's Coinbase TxOut Hashcash, or PowPos (proof-of-work
proof-of-sacrifice) as he mentioned in the forums. (and privately) I'll jump
the gun a bit and call it PowPos, it's a nice name as you say. :) We already
relay transactions based on 1mBTC/KB, so a similar per KB message cost is
Peter suggested using PayWords to amortize the cost of transmitting the PowPos
initial proof, which of course needs to be paid for. Storing original proofs
should be pretty cheap, so lets make the default to store them on disk, like
the UTXO set, with per-node settings for just how many of these proofs we are
willing to store. We can treat them kinda like accounts, and old enough
accounts simply get deleted, with a per-node configuration on how old is too
old. It's best effort, not permanent like the UTXO set. Nodes should be able to
ask their peers for the actual proof corresponding to a payment attempt if they
need it. (maybe a general delta compression system could do this and other
Nitty gritty: define either NODE_P2PMSG for "supports P2P messages" or a more
general NODE_SELECTIVE_RELAY to advertise general limitations on what a node
relays. (min txout value, replace-by-fee, p2p message etc?)
Define MSG_FLOOD_MSG inventory type. The logic is that flood-fill messages
*are* something you can have in your "inventory" of data, even though they are
still things that expire. Again nodes can set message expiration and prune them
in some sane way, a days worth of messages is probably fine for the vast
majority of nodes.
Each flood fill message needs a header with the sacrifice proof, either a full
PowPos, or a PayWord linked to a PowPos, followed by a type disambiguator
(64-bit uuid?) followed by the data payload. Limiting the payload to 100K by
default seems fine to me, same limit as on transactions.
The PowPos initial proof could also be it's own message type, MSG_POWPOS, and
again it's something you would have in your "inventory"
I haven't written up a formal trust-free-mix message proposal. Peter do you
have one? Seems to me negotiating transaction fees could be a bit tricky as you
want people to somehow at least say that yes they are willing to pay
transaction fees of X for the outputs they want, without revealing what those
inputs are. At the same time opportunistic mixing is cool, where you say "make
these coins available to be mixed if it doesn't cost me anything" and duplicate
other's txout values on the fly to make determining what is what as difficult
as possible, as well as occasionally broadcasting requests for random, or even
*common* txout values. (don't forget about zipfs law!)
Anti-anti-propagation DoS prevent is going to be hard too. You can rank nodes
based on the sum of genuine Bitcoin, or in this case genuine sacrifice activity
they are broadcasting to you. Tricky though because PayWord sacrifices aren't
stored anywhere, so a peer could reuse the same PowPos if they control all your
peers that are using the protocol.

@_date: 2013-05-14 02:30:18
@_author: John Dillon 
@_subject: [Bitcoin-development] merged mining hashcash & bitcoin (Re: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Sorry I don't have time for a full reply due to some other commitments, but you
remind me of an idea bouncing around to use a Merkle Sum tree as a way to split
one sacrifice among an arbitrarily large set of users. Credit goes to Gregory
Maxwell (according to the wiki) and the idea is to have the roots of the tree
be account "numbers" (pubkeys here) and account amounts. He proposed it for
off-chain transaction account ledgers, but the idea works equally well here to
split some initial sacrifice into lots of little bits. For instance a on-chain
sacrifice to an anyone-can-pay output could be split into enough parts to make
it useful even when tx fees become large.
Incidentally all this stuff about rivest paywords is probably silly, why not
just commit your sacrifice to a pubkey and make signatures saying what your new
balance is for each message and how much you intended to spend? This allows for
easy fraud proof creation, and gives you a choice of either lying to some
nodes, and getting poor propagation, or being honest and spending the amount
you should have.
For DoS protection it seems to me that mostly trusting nodes to give accurate
balances, enforced with a fraud proof system to halt double-spending, is
perfectly adequate. But no sense implementing so much complexity right at the
start of the effort! Just a thought for where things can go in the future.

@_date: 2013-11-13 20:01:27
@_author: John Dillon 
@_subject: [Bitcoin-development] Even simpler minimum fee calculation 
Hash: SHA256
Peter claims on IRC that he is writing a paper of some kind on this topic. I
suggest he submit it to that crypto-currency thing the foundation is
sponsoring. Given the Nov 24th deadline, I also suggest at least making part of
it public ASAP so some peer review can be done. It would be a shame for a
simple math error to cause embarassment later.
Are you sure about that? You are assuming linearity where none may exist.
Are those stats accurate? Have any pool operators at least confirmed that the
orphaned blocks that blockchain.info reports match their own records?
My gut feeling is to relay all orphaned blocks. We know that with a high
investment and sybil attack as blockchain.info has done you can have better
awareness of orphaned blocks than someone without those resources. If having
that awareness is ever a profitable thing we have both created an incentive to
sybil attack the network and we have linked profitability to high up-front
capital investments.
On those grounds alone I will argue that we should relay all orphans to even
the playing field. If there is a circumstance where we do not want the attacker
to have that knowledge we have failed anyway, as blockchain.info's sybil attack
on the network clearly shows.
With relayed orphans you could even have P2Pool enforce an optimal tx inclusion
policy based on a statistical model by including proof of those orphans into
the P2Pool share chain. P2Pool needs to take fees into account soon, but simply
asking for blocks with the highest total fees or even highest fee/kb appears to
be incomplete according to what your and Peter's analysis is suggesting.

@_date: 2013-11-13 20:13:40
@_author: John Dillon 
@_subject: [Bitcoin-development] [ANN] High-speed Bitcoin Relay Network 
Hash: SHA256
You should split the block-only and block+tx not only by port number, but also
by DNS address. DoS attack by flooding blocks is fundamentally more difficult
than DoS attack by flooding transctions, so doing the split by IP address
ensures that in the event of an attack the more important block relaying
functionality is less likely to be damaged. In the meantime point both DNS
addresses to the same IP until it becomes an issue.

@_date: 2013-11-13 20:27:52
@_author: John Dillon 
@_subject: [Bitcoin-development] 1. Re: On the optimal block size and why 
Hash: SHA256
On Fri, Nov 8, 2013 at 4:21 PM, Goss, Brian C., M.D.
The propagation time you're thinking of is from the pool to the miner, and even
now that is significant for pools that do not pay for stale shares. I remember
an Australian pool mentioning that problem on their website as a reason for the
pools existence.
I would expect selfish mining, as well as orphans becoming more important in
general, to centralize the physical location of hashing power too. If the 100ms
delay to your pool impacts profits you'll have an incentive to locate your
mining equipment physically closer to the pool. The next step is pools wanting
to physically locate themselves closer to other pools.
It would not be good if all Bitcoin mining was done in Iceland...
Not ignorant at all IMO.

@_date: 2013-10-28 05:58:13
@_author: John Dillon 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
Hash: SHA256
I think this is a great idea and wish to see it done. Here is 1BTC for you,
redeemable when you finish this task. I trust either Jeff Garzik or Peter Todd
to evaluate your finished product, or possibly someone elses:
redeemScript : "5241045f4bba15dbfe94a45f362aa13bbaef8bbf21ff84fec1be5b27fa628f4b3acca1a2e5711503c8b8fe2e228229b8b8814f9e33e0f7a314a089d7140269ffd51fe44104d34775baab521d7ba2bd43997312d5f663633484ae1a4d84246866b7088297715a049e2288ae16f168809d36e2da1162f03412bf23aa5f949f235eb2e71417834104f005d39733ec09a1efa0cf8dcf3df50691e22c2374ff9a96d1d9ecb98a1e866c9f558a9fa1ba8ef0bbbad01f396768c0cb2dda9924dc0aaee1481604a8bd9ce453ae"

@_date: 2013-10-28 07:17:50
@_author: John Dillon 
@_subject: [Bitcoin-development] Making fee estimation better 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
On Sat, Oct 26, 2013 at 12:25 AM, Gavin Andresen
This discussion seems to be a lot of hot air over a simple observation that
estimates are imperfect and always will be. I do not understand you vehement
opposition the notion that a backup is a good thing except in the context that
replacement to change fees is halfway to profit-seeking replacement by fee.
Peter Todd:
You did a fair bit of leg work for replace-by-fee. Seems to me that
replace-for-fee will help prep infrastructure to eventual replace-by-fee usage,
while avoiding some of the politics around zero-conf transactions.
Go dust off your code and make it happen. I want to see a mempool
implementation similar to what you did for me on replace-for-fee, and I
understand much of the code is written in any case. This time I also want to
see a increasetxfee RPC command, and erasewallettx RPC command to deal with
duplicates. (I know touching the wallet code is scary) Having all will enable
usage, and I can imagine getting pools to use this will be easy enough.
Here is your 4BTC bounty. In the event I am not around Gregory Maxwell can also
adjudicate. If both you and him feel someone else deserves it, by all means
send them the funds
bitcoind decodescript
    "asm" : "2 02d527466a144aac2030cd16d8be3d91231af26a95c2f8fc345a0ea0e8d53ac391
3 OP_CHECKMULTISIG",
    "reqSigs" : 2,
    "type" : "multisig",
    "addresses" : [
        "1L9p6QiWs2nfinyF4CnbqysWijMvvcsnxe",
        "1FCYd7j4CThTMzts78rh6iQJLBRGPW9fWv",
        "1GMaxweLLbo8mdXvnnC19Wt2wigiYUKgEB"
    ],
    "p2sh" : "3BST1dPxvgMGL3d9GPCHvTyZNsJ7YKTVPo"
(I realized right after my Tor payment protocol bounty that I would need some
bit of uniqueness like a bounty-specific pubkey to disambiguate multiple such
