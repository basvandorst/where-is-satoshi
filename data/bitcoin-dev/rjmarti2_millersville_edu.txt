
@_date: 2017-12-01 07:58:01
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] BIP Idea: Marginal Pricing 
Interesting thoughts William, however much of what you describe already exists:
"I predict that this scheme would result in two markets: a backlog market and a real-time market. Users targeting the backlog market would match the price of the largest backlog section in order to be included in the next backlog block."
This happens with users deciding to pay (essentially) a fee smaller or larger than the 1999th tx in the mempool. If user is willing to pay more than the 1999th highest fee tx in mempool (or whatever tx byte 1,000,000 in a mempool ranked by tx fee) then they will get to be on the next block. This is a simplification but you get the idea.
Further, Greg Maxwell's one reply covers alot of the biggest pitfalls well. Especially that much of this already happens and that behavior in response to such a change could be hairy to predict. So I'm not sure that---what is basically--- a multi-unit Vickrey auction is the best way to go... but it is an interesting idea worth further examination.
Second, minimizing invididual user tx fees and maximizing total tx fees are essentially incompatible within the current development structure. Even if the block limit is increased to 3mb the same problem will eventually occur. While I agree it would be ideal to maximize social benefit (i.e. maximize both consumer and producer surplus) through more simple protocol changes---at least in the medium term, that isn't on the table so it is better to move on.
  This is not an easy optimization problem to solve though. The difficulty is trying to model a market like bitcoin; the only thing that comes close is electricity markets. With 2,000 tx in the pool, users aren't willing to pay shit to send a tx; there is always a miner to process it. So at 2,000tx per 10 min (3.333 txps, 1mb/10min,etc) and below users price elasticity is flat. Once the pool has >2,000 tx in it, especially for any extended amount of time, users price elasticity is about as elastic as a brick and goes near vertical. This creates a situation where miners are always better off when there is a significant backlog (this can be seen in miners revenue from tx fees anytime there is an ongoing backlog).
Simply put, it would take some very large blocks to have total fees/block exceed total fees/block for constrained size blocks given the near vertical price elasticity users face when there is a backlog.
So I suspect that the multi-unit Vickrey would potentially do some to help this, but likely not much:
Users willingness to pay is what they pay---no surplus. Miner elasticity is more or less flat but we can call their willingness to accept whatever the lowest fee tx is in a block. Say it is 180 sat/b (NL), and the highest tx fee in a block (NH)is 320 sat/b. If you subtract each tx in the block (N) greater than NL
and sum the result you get surplus to miners:  ?(N1,2...- NL)
So, yes, the multi-unit Vickrey simply shifts the surplus to users. However, a case could be made that since---as mentioned in an earlier reply, the optimal strategy for miners is to accept zero fees (given their flat elasticity), and therefore all fees are surplus benefit to miners---shifting this surplus over to consumers could create some good effects. Primarily pushing users' price elasticity away from near vertical inelasticity as it would take some of the upward pressure off rapidly increasing fees in a backlog scenario.
It would be interesting to see a simulation of how this would play out, but without that this is too risky to implement.
Ryan J. Martin (tunafizz / ohituna)
Sent: Wednesday, November 29, 2017 7:47 PM
Long term, tx fees must support hash power by themselves. The following is an economic approach to maximize total fee collection, and therefore hashpower.
Maximize total transaction fees
Reduce pending transaction time
Reduce individual transaction fees
Validators must agree on the maximum block size, else miners can cheat and include extra transactions.
Allowing too many transactions per block will increase the cost of the mining without collecting much income for the network.
In the transaction market, users are the demand curve, because they will transact less when fees are higher, and prefer altcoins. The block size is the supply curve, because it represents miners' willingness to accept transactions.
Currently, the supply curve is inelastic:
?Increasing the block size will not affect the inelasticity for any fixed block size. The downsides of a fixed block size limit are well-known:
- Unpredictable transaction settlement time
- Variable transaction fees depending on network congestion
- Frequent overpay
1. Miners implicitly choose the market sat/byte rate with the cheapest-fee transaction included in their block. Excess transaction fees are refunded to the inputs.
2. Remove the block size limit, which is no longer necessary.
- Dynamic block size limit regulated by profit motive
- Transaction fees maximized for every block
- No overpay; all fees are fair
?Miners individually will make decisions to maximize their block-reward profit.
Miners are incentivized to ignore low-fee transactions because they would shave the profits of their other transactions and increase their hash time.
Users and services are free to bid higher transaction fees in order to reach the next block, since their excess bid will be refunded.
The block size limit was added as a spam-prevention measure, but in order for an attacker to spam the network with low-fee transactions, they would have to offset the marginal cost of reducing the price with their own transaction fees. Anti-spam is thus built into the marginal system without the need for an explicit limit.
Rarely, sections of the backlog would become large enough to be profitable. This means every so many blocks, lower-fee transactions would be included en masse after having been ignored long enough. Low-fee transactions thus gain a liveness property not previously enjoyed: low-fee transactions will eventually confirm. Miners targeting these transactions would be at a noteworthy disadvantage because they would be hashing a larger block. I predict that this scheme would result in two markets: a backlog market and a real-time market. Users targeting the backlog market would match the price of the largest backlog section in order to be included in the next backlog block.
Scenario 1
Sat/byte        Bytes   Reward
400     500000  200000000
300     700000  210000000
200     1000000 200000000
100     1500000 150000000
50      5000000 250000000
20      10000000        200000000
A miner would create a 5MB block and receive 0.25 BTC
Scenario 2
Sat/byte        Bytes   Reward
400     600000  240000000
300     700000  210000000
200     1000000 200000000
100     1800000 180000000
50      4000000 200000000
20      10000000        200000000
A miner would create a 600KB block and receive 0.24 BTC
William Morriss

@_date: 2017-02-10 04:10:15
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] A Modified Version of Luke-jr's Block Size BIP 
"10% say literally never.  That seems like a significant disenfranchisement
and lack of consensus."
Certainly the poll results should be taken with a grain of salt and not a definitive answer or measure . However if we agree the poll has some worth (or even if not, then lets use it as hyptothetical): If we split it into two groups: those okay with a hardfork at some point > now, and those never okay with hardfork, that means there is 90% that agree a hardfork is acceptable in the future. That said, what threshold defines consensus then? 98%? 100%?       Personally I think pursuing paths that maximize net social benefit in terms of cost surplus/burden is the best way to go since consensus is such an impossible to define, variable, case-by-case thing that doesn't always lead to the best choice.
-Ryan J. MArtin
Sent: Thursday, February 09, 2017 7:00 AM
Send bitcoin-dev mailing list submissions to
        bitcoin-dev at lists.linuxfoundation.org
To subscribe or unsubscribe via the World Wide Web, visit
        or, via email, send a message with subject or body 'help' to
        bitcoin-dev-request at lists.linuxfoundation.org
You can reach the person managing the list at
        bitcoin-dev-owner at lists.linuxfoundation.org
When replying, please edit your Subject line so it is more specific
than "Re: Contents of bitcoin-dev digest..."
Today's Topics:
   1. Re: A Modified Version of Luke-jr's Block Size BIP (alp alp)

@_date: 2017-01-10 04:14:55
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] BIP - Block75 - Historical and future projections 
The adaptive/automatic block size notion has been around for a while--- others would be better able to speak to why it hasn't gotten traction. However my concern with something like that is that it doesn't regard the optimal economic equilibrium for tx fees/size---not that the current limit does either but the concern with an auto-adjusting size limit that ignores this  would be the potential to create unforeseen externalities for miners/users. Miners may decide it is more profitable to mine very small blocks to constrict supply and increase marginal fees and with how centralized mining is, where a dozen pools have 85% hashrate, a couple of pools could do this. Then on the other side, maybe the prisoner's dilemma would hold and all miners would have minrelaytxfee set at zero and users would push the blocks to larger and larger sizes causing higher and higher latency and network issues.     Perhaps something like this could work (I can only speak to the economic side anyway) but it would have to have some solid code that has a social benefit model built in to adjust to an equilibrium that is able to optimize---as in maximizes benefit/minimize cost for both sides via a Marshallian surplus model--- for each size point.      To be clear, I'm not saying an auto-adjusting limit is unworkable (again only from an economic standpoint), just that it would need to have these considerations built in. -Ryan J. Martin
Message: 2
Using daily average block size over the past year (source:
), here's how Block75 would have altered max block sizes:
[image: Inline image 1]
As of today, the max block size would be 1,135KB.
Looking forward and using the last year's growth rate as a model:
[image: Inline image 2]
This shows the max block size one year from now would be 2,064KB, if
Block75 activated today.
Of course, this is just an estimate, but even accounting for a substantial
increase in transactions in the last quarter of 2017 and changes brought
about by SegWit (hopefully) activating, Block75 alters the max size in such
a way that allows for growth, keeps blocks as small as possible, *and*
maintains transaction fees at a level similar to May/June 2016.
If anyone has an alternate way to model future behavior, please run it
through the Block75 algorithm.
Every 2016 blocks, do this:
new max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))
TARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full
AVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a
x = current max block size
- t.k.

@_date: 2017-06-20 19:49:57
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
I concur with Mark's reply. Just to underscore this: Miners arent going to bother to signaling or changing a setting unless they have to. Anything that requires time--especially if requiring a restart/any time not mining or risks a crash---reduces income. So why would they change any settings unless they have to?
-Ryan J. Martin
I think it is very na?ve to assume that any shift would be temporary.
We have a hard enough time getting miners to proactively upgrade to
recent versions of the reference bitcoin daemon. If miners interpret
the situation as being forced to run non-reference software in order
to prevent a chain split because a lack of support from Bitcoin Core,
that could be a one-way street.
On Tue, Jun 20, 2017 at 9:49 AM, Hampus Sj?berg via bitcoin-dev
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-03-30 05:23:31
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
There is alot going on in this thread so I'll reply more broadly.
     The original post and the assorted limit proposals---lead me to something I think is worth reiterating: assuming Bitcoin adoption continues to grow at similar or accelerating rates, then eventually the mempool is going to be filled with thousands of txs at all times whether block limits are 1MB or 16MB. This isn't to say that increasing the limit isn't a worthwhile change, but rather, that if we are going to change the block limit then it should be done with the intent to achieve a fee rate that maximize surplus (and minimize burden) to both users and miners. Even with implementation of a a payment channels system, the pool will likely be faced with having a mountain of txs. Thus the block limit should be optimized in such that social welfare is optimized.
    Optimized is likely not keeping the limit at 1MB; this maximizes benefit to miners (producers) while minimizing users' surplus (consumer). 'Unlimited' blocks are purely the reverse; maximizing user surplus while minimizing miners' (with the added bonus of creating blocks that will put technical/hardware strain on the network). So perhaps pursue something in-between that actually optimizes based on a social welfare formula---not just an arbitrary auto-adjusting limit like the other proposals I've seen. Feel free to poke holes in this or e-mail me if curious.
     Finally, with respect to getting node counts up, didn't luke-jr or someone come up with an idea of paying nodes a reward by scraping dust and pooling it into a fund of sorts? Was this not possible/feasible? Perhaps at least in the near and medium term something outside of protocol changes could be done to pay a reward to nodes. Even if this is done via voluntary donation system, it may be useful for the purposes of seeing how people respond to incentives and working out an elasticity measure of sorts for running a node.
Ryan J. Martin
rjmarti2 at millersville.edu
(on freenode: tunafizz )
Sent: Wednesday, March 29, 2017 6:33 PM
I have heard such theory before, it's a complete mistake to think that others would run full nodes to protect their business and then yours, unless it is proven that they are decentralized and independent
Running a full node is trivial and not expensive for people who know how to do it, even with much bigger blocks, assuming that the full nodes are still decentralized and that they don't have to fight against big nodes who would attract the traffic first
I have posted many times here a small proposal, that exactly describes what is going on now, yes miners are nodes too... it's disturbing to see that despite of Tera bytes of BIPs, papers, etc the current situation is happening and that all the supposed decentralized system is biased by centralization
Do we know what majority controls the 6000 full nodes?
Le 29/03/2017 ? 22:32, Jared Lee Richardson via bitcoin-dev a ?crit :
That's very poor logic, sorry.  Restricted-space SSD's are not a cost-effective hardware option for running a node.  Keeping blocksizes small has significant other costs for everyone.  Comparing the cost of running a node under arbitrary conditons A, B, or C when there are far more efficient options than any of those is a very bad way to think about the costs of running a node.  You basically have to ignore the significant consequences of keeping blocks small.
If node operational costs rose to the point where an entire wide swath of users that we do actually need for security purposes could not justify running a node, that's something important for consideration.  For me, that translates to modern hardware that's relatively well aligned with the needs of running a node - perhaps budget hardware, but still modern - and above-average bandwidth caps.
You're free to disagree, but your example only makes sense to me if blocksize caps didn't have serious consequences.  Even if those consequences are just the threat of a contentious fork by people who are mislead about the real consequences, that threat is still a consequence itself.
Perhaps you are fortunate to have a home computer that has more than a single 512GB SSD. Lots of consumer hardware has that little storage. Throw on top of it standard consumer usage, and you're often left with less than 200 GB of free space. Bitcoin consumes more than half of that, which feels very expensive, especially if it motivates you to buy another drive.
I have talked to several people who cite this as the primary reason that they are reluctant to join the full node club.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
Zcash wallets made simple: Bitcoin wallets made simple: Get the torrent dynamic blocklist: Check the 10 M passwords list: Anti-spies and private torrents, dynamic blocklist: Peersm : torrent-live: node-Tor : GitHub :

@_date: 2018-02-19 05:10:31
@_author: Ryan J Martin 
@_subject: [bitcoin-dev] Some thoughts on removing timestamps in PoW 
To be frank, this kind of thing would be better off attempted as a fork to a new coin. Changing the max number of coins, the block reward, the difficulty algo, mining policy and protocol is going to be a non-starter. Also, what are the proposed quantifiavle benefits from removing timestamps? How would this be done at the protocol level? Are these other changes related to removing timestamps/rationale for other supply changes?
Ryan J. Martin
Copied from: # Blockchain Timestamps Unnecessary In Proof-of-Work?
*Author: Greg Slepak ([ at mastodon.social](
The Bitcoin blockchain has a 10-minute target blocktime that is achieved by a difficulty adjustment algorithm.
I assert, or rather, pose the hypothesis, that the use of timestamps in Bitcoin's blockchain may be unnecessary, and that Bitcoin can operate with the same security guarantees without it (except as noted in [Risks and Mitigations]( and therefore does not need miners to maintain global clock synchronization.
The alternative difficulty adjustment algorithm would work according to the following principles:
- The incentive for miners is and always has been to maximize profit.
- The block reward algorithm is now modified to issue coins into perpetuity (no maximum). Any given block can issue _up to_ `X` number of coins per block.
- The number of coins issued per block is now tied directly to the difficulty of the block, and the concept of "epocs" or "block reward halving" is removed.
- The chain selection rule remains "chain with most proof of work"
- The difficulty can be modified by miners in an arbitrary direction (up or down), but is limited in magnitude by some maximum percentage (e.g. no more than 20% deviation from the previous block), we call this `Y%`.
 Observations
- Miners are free to mine blocks of whatever difficulty they choose, up to a maximum deviation
- The blockchain may at times produce blocks very quickly, and at other times produce blocks more slowly
- Powerful miners are incentivized to raise the difficulty to remove competitors (as is true today)
- Whether miners choose to produce blocks quickly or slowly is entirely up to them. If they produce blocks quickly, each block has a lower reward, but there are more of them. If they produce blocks slowly, each block has a higher reward, but there are fewer of them. So an equilibrium will be naturally reached to produce blocks at a rate that should minimize orphans.
A timestamp may still be included in blocks, but it no longer needs to be used for anything, or represent anything significant other than metadata about when the miner claims to have produced the block.
 Risks and Mitigations
Such a system may introduce risks that require further modification of the protocol to mitigate.
The most straightforward risk comes from the potential increase in total transaction throughput that such a change would introduce (these are the same concerns that exist with respect to raising the blocksize). The removal of timestamps would allow a cartel of miners to produce high-difficulty blocks at a fast rate, potentially resulting in additional centralization pressures not only on miners but also on full nodes who then would have greater difficulty keeping up with the additional bandwidth and storage demands.
Two equally straightforward mitigations exist to address this if we are given the liberty of modifying the protocol as we wish:
1. Introducing state checkpoints into the chain itself could make it possible for full nodes to skip verification of large sections of historical data when booting up.
2. A sharded protocol, where each shard uses a "sufficiently different" PoW algorithm, would create an exit for users should the primary blockchain become captured by a cartel providing poor quality-of-service.
Please do not email me anything that you are not comfortable also sharing with the NSA.
