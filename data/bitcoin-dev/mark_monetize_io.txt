
@_date: 2012-12-03 09:02:13
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
My only comment is that it should be called escheatment, not demurrage ;)
It's relation to demurrage is only that it might be desirable to garbage
collect decayed bit-dust. We looked at it early-on in the Freicoin
development, but rejected it as a possibility due to reasons others have
mentioned, even though we were starting from a hard-fork position.

@_date: 2012-12-04 10:57:40
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Roadmap to getting users onto SPV clients 
Alan's UTxO meta-chain proposal becomes vastly easier to do now that
ultraprune is merged. That would allow the Satoshi client to know it's
wallet balance and operate with a >=SPV level of security during the
initial block download, and keep them on the path of becoming a full node.
If users can see their balances, send and receive transactions, and
otherwise go about their business (except for mining) during the initial
block download, would that not address your concerns?
IMHO the only time bitcoin.org should recommend a SPV-only client is when
it is dynamically when it is being accessed from a mobile device, but
that's a separate issue.

@_date: 2012-12-22 18:33:22
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
I hope that this input does not come too late; I haven't had time to review
the proposal until now.
For alt-chains that have time-varying value (Freicoin[1], currently), it is
necessary in some applications to include a "reference height" in the
invoice. Since the bitcoin protocol does not assume a universally
agreed-upon time source, Freicoin (and presumably other
yet-to-be-implemented time-varying chains) uses blocktime as the clock for
time-value calculations: outputs lose 2**-20 of their value with each
passing block. The reference height for an invoice is the blocktime at
which amount values are specified and the reference point for time-varying
calculations. As a concrete example, an invoice for payment of 50 frc today
could be satisfied by 49.99313402 frc tomorrow.
To implement this, we would require an optional "uint64 refheight" field in
the invoice structure. "refheight" or "nRefHeight" is what we call this
value internally, but "blocktime" or "blockheight" would work as well.
Github is currently down, so I apologize if a suitable field has already
been added.
Mark Friedenbach
[1]  "Freicoin: a P2P digital currency delivering freedom
from usury."

@_date: 2012-07-06 09:56:16
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP 34: Block v2, Height in Coinbase 
But those issues are solvable through other, non-backwards incompatible
means. For example, mandate that a  refers
to the first such pair that is not already spent. No?

@_date: 2012-07-13 13:41:40
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Gitian builds on all platforms using 
I had some difficulty setting up and maintaining my own gitian build
system. So in the time-honored tradition of scratching my own itch, I
recorded the necessary steps into a makefile so that doing a gitian build
is a simple as "cd contrib/vagrant && make", on any platform provided that
you have the necessary dependencies installed.
I hope that others find this useful. I have submitted these scripts as
pull-request  on github.
Mark Friedenbach

@_date: 2012-06-17 18:27:39
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed new P2P command and response: 
Sorry for the duplication Amir, I meant to send this to everyone:
BitTorrent might be an example to look to here. It's a peer-to-peer network
that has undergone many significant protocol upgrades over the years while
maintaining compatibility. More recent clients have had the ability to
expose the capabilities of connected peers and modify behavior accordingly,
and overall it has worked very well.
Capability-based systems do work, and provide an excellent means of trying
out new algorithms, adding new features for upgraded clients, and when
necessary reverting protocol changes (by depreciating or removing
The problem with OpenGL was and continues to be that the two superpowers of
that industry develop and maintain competing proposals for similar
functionality, which are thrust upon developers which must support both if
they want access to the latest and greatest features, until such time that
the ARB arbitrarily choses one to standardize upon (in the process creating
yet another extension of the form ARB_* that may be different and must be
explicitly supported by developers).
I think the BitTorrent example shows that a loosely organized, open-source
community *can* maintain a capability-based extension system without
falling into capability-hell.

@_date: 2012-06-19 11:18:07
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Ultimate Blockchain Compression w/ 
Then use a 2-3-4 tree (aka self-balancing B-tree of order 4), which is a
generalization of RB-trees that doesn't allow for implementation choices in
balancing (assuming ordered insertion and deletion).
As gmaxwell points out, this is an trivially fixable 'problem'. Choose a
standard, mandate it, and write test cases.
If we were to use a raw trie structure, then we'd have all the above
No, a trie of any sort is dependent upon distribution of input data for
balancing. As Peter Todd points out, a malicious actor could construct
transaction or address hashes in such a way as to grow some segment of the
trie in an unbalanced fashion. It's not much of an attack, but in principle
exploitable under particular timing-sensitive circumstances.
Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer from
this problem.

@_date: 2012-09-26 09:06:41
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bitcoin Testing Project 
Running a concurrent Mantis tracker would be confusing and fragment the
development pathway. We have an issue tracker; it's on github.
What's being talked about here are two separate things. Jenkins is a
continuous integration system. It can be configured to run the suite of
unit tests, regression tests, and any kind of automated functional tests
for every commit on github and every pull request.
Github is our issue tracker. Github, and only github, is where new issues
should be reported (unless it's security related, in which case an email
should be sent to the core devs directly).
Certainly developers should be responsible for making sure that regression
tests for bugs they fix make it either into the unit tests or Matt's
functional test repository. QA should hold them accountable for that
(re-opening tickets for bugs that have been fixed but without regression
The other thing we're talking about is coordinated release testing--getting
release candidates in the hands of actual users and making sure that issues
are reported. This is something that can't be automated as the point really
is to pick up on things that the testing suite missed. You sound more
qualified than me for coming up with a process, but in the end discovered
issues should be reported to github, the final repository of issues that
hold up Gavin from doing a release.
Just my 0.002BTC

@_date: 2013-08-18 22:11:40
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Gavin's post-0.9 TODO list... 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
keeping proof
More than a kilobyte, probably less than a few tens of kilobytes. It
depends on parameters (branching factor, script vs hash(script)) that
are tweakable with time/space and long-term/short-term tradeoffs.

@_date: 2013-08-23 17:14:02
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Freimarkets: a proposal for user assets, 
Hash: SHA1
Off-and-on for the past couple years, Jorge Tim?n and I have been
developing an extension of the Bitcoin and (pre-OpenCoin) Ripple
distributed protocols which enable user-specified bearer instruments,
distributed peer-to-peer exchange, off-chain accounting, auctions,
derivatives and transitive transactions, and the multitude of financial
contracts having such primitives would make possible. The specification
is now reasonably complete enough that we would like to receive input
from the community. The PDF is available for viewing here:
We're looking for public comments about this or related approaches. In
particular we've spent a fair chunk of time working out how to handle
coordination of private accounting servers with the public chain and
derivatives contracts, both of which are basically cryptographic
protocols expressed as bitcoin scripts. Input from any of the resident
cryptographers would be very appreciated.
Happy hacking,
Mark Friedenbach

@_date: 2013-12-08 13:46:18
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Dedicated server for bitcoin.org, 
I too would be against the foundation taking control of hosting or the
domain. I have no reason at this time not to trust them, by checks and
balances are a good thing.

@_date: 2013-12-13 11:19:05
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Merge avoidance and P2P connection 
Hash: SHA1
Or alternatively, the user-signed payment request without iteration
count is enclosed within a payr.com-signed envelope that contains the
iteration count. Having fields completely unsigned by anybody leaves
me a little nervous.

@_date: 2013-12-17 14:48:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] RFC: MERGE transaction/script/process for 
Hash: SHA1
Transactions != blocks. There is no need for a "merge" block.
You are free to trade transactions off-line, so long as you are
certain the other parties are not secretly double-spending coins they
send you on the block chain.
When connection to the bitcoin network is re-established, you simply
transmit the transactions and in the regular course of things they
make their way into one of the next blocks.
Any transactions which derive from the double-spent one are invalid.
But that's your problem, not the miners - chase after Bob and get him
to give you the money he owes.
Rapidly troubleshoot problems before they affect your business. Most IT

@_date: 2013-12-19 17:47:52
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Hash: SHA1
Hello fellow bitcoin developers. Included below is the first draft of
a BIP for a new Merkle-compressed data structure. The need for this
data structure arose out of the misnamed "Ultimate blockchain
compression" project, but it has since been recognized to have many
other applications.
In addition to this BIP I am preparing three additional BIPs
describing the use of this data structure in stateless validation &
mining, the UBC address index for "SPV+" operating modes, document
timestamping and merged mining.
A Python implementation of this data structure is available here:
A C++ implementation is being worked on.
As per the BIP-1 procedure, I am submitting this rough draft to the
community for discussion. I welcome all comments and criticisms of
both form and content.
- -Mark
This BIP describes a [ Merkle
hash tree] variant of the [
prefix-tree data structure], ideally suited for encoding key-value
indices which support memory-efficient proofs.
There are a number of applications which would benefit from having a
data structure with the following properties:
* '''Arbitrary mapping of keys to values.''' A ''key'' can be any
bytestring, and its ''value'' any other bytestring.
* '''Duplicate keys disallowed.''' Every key has one, and only one
value associated with it. Some applications demand assurance that no
key value is reused, and that this constraint can be checked without
requiring access to the entire data structure.
* '''Efficient look-up by key.''' The data structure should support
sub-linear lookup operations with respect to the number of keys in the
mapping. Logarithmic time or linear with respect to the length of the
key should be achievable and would be sufficient for realistic
* '''Merkle compression of mapping structure.''' It should be possible
to produce a reduced description of the tree consisting of a single
root hash value which is deterministically calculated from the mapping
* '''Efficient proofs of inclusion.''' It should be possible to
extract a proof of key/value mapping which is limited in size and
verification time by the length of the key in the worst case.
* '''Computation of updates using local information.''' Given a set of
inclusion proofs, it should be possible to calculate adjustments to
the local mapping structure (update or deletion of included mappings,
or insertion between two included mappings which are adjacent in the
global structure).
Such applications include committed validation indices which enable
stateless mining nodes, committed wallet indices which enable
trust-less querying of the unspent transaction output set by
scriptPubKey, efficient document time-stamping, and
secure & efficient merged mining. This BIP describes an authenticated
prefix tree which has the above properties, but leaves the myriad
applications to be formalized in future BIPs.
==Data structure==
This BIP defines a binary prefix tree. Such a structure provides a
mapping of bitstrings (the ''keys'') to bytestrings (the ''values'').
It is an acyclic binary tree which implicitly encodes keys within the
traversal path -- a "left" branch is a 0, and a "right" branch is a 1.
Each node is reachable by only one unique path, and reading off the
branches taken (0 for each left, 1 for each right) as one follows the
path from root to target yields the node's key.
The particular binary prefix tree defined by this BIP is a hybrid
PATRICIA / de la Brandais tree structure.
[ PATRICIA trees] compress a
long sequence of non-branching nodes into a single interior node with
a per-branch ''skip prefix''. This achieves significant savings in
storage space, root hash calculation, and traversal time.
A de la Brandais trie achieves compression by only storing branches
actually taken in a node. The space savings are minimal for a binary
tree, but place the serialized size of a non-branching interior node
under the SHA-256 block size, thereby reducing the number of hash
operations required to perform updates and validate proofs.
This BIP describes the authenticated prefix tree and its many
variations in terms of its serialized representation. Additional BIPs
describe the application of authenticated prefix trees to such
applications as committed indices, document time-stamping, and merged
==Serialization format==
As a hierarchical structure, the serialization of an entire tree is
the serialization of its root node. A serialized node is the
concatenation of five structures:
    node := flags || VARCHAR(extra) || value || left || right
The flags is a single byte field whose composite values
determine the bytes that follow.
    flags = (left_flags  << 0) |
            (right_flags << 2) |
            (has_value   << 4) |
            (prune_left  << 5) |
            (prune_right << 6) |
            (prune_value << 7)
The left_flags and right_flags are special
2-bit enumeration fields. A value of 0 indicates that the node does
not branch in this direction, and the corresponding left
or right branch is missing (replaced with the empty
string in the node serialization). A value of 1 indicates a single bit
key prefix for this branch, implicitly 0 for left and 1
for right. A 2 indicates up to 7 bits of additional skip
prefix (beyond the implicit first bit, making 8 bits total) are stored
in a compact single-byte format. A 3 indicates a skip prefix with
greater than 7 additional bits, stored length-prefix encoded.
The single bit has_value indicates whether the node
stores a data bytestring, the value associated with its key prefix.
Since keys may be any value or length, including one key being a
prefix of another, it is possible for interior nodes in addition to
leaf nodes to have values associated with them, and therefore an
explicit value-existence bit is required.
The remaining three bits are used for proof extraction, and are masked
away prior to hash operations. prune_left indicates that
the entire left branch has been pruned. prune_right has
similar meaning for the right branch. If has_value is
set, prune_value may be set to exclude the node's value
from encoded proof. This is necessary field for interior nodes, since
it is possible that their values may be pruned while their children
are not.
The value field is only present if the bit
flags.has_value is set, in which case it is a
VARCHAR bytestring:
    switch flags.has_value:
      case 0:
        value := ?
      case 1:
        value := VARCHAR(node.value)
The extra field is always present, and takes on a
bytestring value defined by the particular application. Use of the
extra field is application dependent, and will not be
covered in this specification. It can be set to the empty bytestring
(serialized as a single zero byte) if the application has no use for
the extra field.
    value := VARCHAR(calculate_extra(node))
The left and right non-terminals are only
present if the corresponding flags.left_flags or
flags.right_flags are non-zero. The format depends on the
value of this flags setting:
    switch branch_flags:
      case 0:
        branch := ?
      case 1:
        branch := branch_node_or_hash
      case 2:
        prefix  = prefix >> 1
        branch := int_to_byte(1 << len(prefix) | bits_to_int(prefix)) ||
                  branch_node_or_hash
      case 3:
        prefix  = prefix >> 1
        branch := VARINT(len(prefix) - 9) ||
                  bits_to_string(prefix) ||
                  branch_node_or_hash
branch_flags is a stand-in meant to describe either
left_flags or right_flags, and likewise
everywhere else in the above pseudocode branch can be
replaced with either left or right.
prefix is the key bits between the current node and the
next branching, terminal, and/or leaf node, including the implicit
leading bit for the branch (0 for the left branch, 1 for the right
branch). In the above code, len(prefix) returns the
number of bits in the bitstring, and prefix >> 1 drops
the first bit reducing the size of the bitstring by one and
renumbering the indices accordingly.
The function int_to_byte takes an integer in the range
[0, 255] and returns the octet representing that value. This is a NOP
in many languages, but present in this pseudocode so as to be explicit
about what is going on.
The function bits_to_int interprets a sequence of bits as
a little-endian integer value. This is analogous to the following
    def bits_to_int(bits):
        result = 0
        for idx in 1..len(bits):
            if bits[idx] == 1:
                result |= 1<bits_to_string serializes a sequence of bits
into a binary string. It uses little-endian bit and byte order, as
demonstrated by the following pseudocode:
    def bits_to_string(bits):
        bytes = [0] * ceil(len(bits) / 8)
        for idx in 1..len(bits):
            if bits[idx] == 1:
                bytes[idx / 8] |= 1 << idx % 8
        return map(int_to_byte, bytes)
branch_node_or_hash is either the serialized child node
or its SHA-256 hash and associated meta-data. Context determines which
value to use: during digest calculations, disk/database serialization,
and when the branch is pruned the hash value is used and serialized in
the same way as other SHA-256 values in the bitcoin protocol (note
however that it is single-SHA-256, not the double-SHA-256 more
commonly used in bitcoin). The number of terminal (value-containing)
nodes and the serialized size in bytes of the fully unpruned branch
are suffixed to the branch hash. When serializing a proof or
snapshotting tree state and the branch is not pruned, the serialized
child node is included directly and the count and size are omitted as
they can be derived from the serialization.
    if branch_pruned or SER_HASH:
        branch_node_or_hash := SHA-256(branch) ||
                               count(branch) ||
                               size(branch)
    else:
        branch_node_or_hash := serialize(branch)
As an example, here is the serialization of a prefix tree mapping the
names men and women of science to the year of their greatest publication:
    >>> dict = AuthTree()
    >>> dict['Curie'] = VARINT(1898)
    >>> dict('Einstein') = VARINT(1905)
    >>> dict['Fleming'] = VARINT(1928)
    >>> dict['??'] = VARINT(2009)
    >>> dict.serialize()
    # An bytestring, broken out into parts:
    # . Root node:
    0x0e # left_flags: 2, right_flags: 3, has_value: 1
    0x00 # extra: ?
    # .l Inner node: 0b01000
    0x11 # 0b01000
    0x07 # left_flags: 3, right_flags: 1
    0x00 # extra: ?
    # .l.l Inner node: 0b01000011 0b01110101 0b01110010 0b01101001
    #                  'C'        'u'        'r'        'i'
    #                  0b01100101
    #                  'e'
    0x1abb3a599a02 # 0b01101110101011100100110100101100101
    0x10           # has_value: 1
    0x00           # extra: ?
    0x03fd6a07     # value: VARINT(1911)
    # .l.r Inner node: 0b010001
    0x0f # left_flags: 3, right_flags: 3
    0x00 # extra: ?
    # .l.r.l Inner node: 0b01000101 0b01101001 0b01101110 0b01110011
    #                    'E'        'i'        'n'        's'
    #                    0b01110100 0b01100101 0b01101001 0b01101110
    #                    't'        'e'        'i'        'n'
    0x312ded9c5d4c2ded00 # 0b1011010010110111
                         # 0b0011100110111010
                         # 0b0011001010110100
                         # 0b101101110
    0x10                 # has_value: 1
    0x00                 # extra: ?
    0x03fd7107           # value: VARINT(1905)
    # .l.r.r Inner node: 0b01000110 0b01101100 0b01100101 0b01101101
    #                    'F'        'l'        'e'        'm'
    #                    0b01101001 0b01101110 0b01100111
    #                    'i'        'n'        'g'
    0x296c4c6d2dedcc01 # 0b0011011000110010
                       # 0b1011011010110100
                       # 0b10110111001100111
    0x10               # has_value: 1
    0x00               # extra: ?
    0x03fd8807         # value: VARINT(1928)
    # .r Inner node: 0b11100100 0b10111000 0b10101101
    #                '?'
    #                0b11100110 0b10011100 0b10101100
    #                '?'
    0x27938edab39c1a # 0b1100100101110001
                     # 0b0101101111001101
                     # 0b001110010101100
    0x10             # has_value: 1
    0x00             # extra: ?
    0x03fdd907       # value: VARINT(2009)
There are two variations of the authenticated prefix tree presented in
this draft BIP. They differ only in the way in which hash values of a
node and its left/right branches are constructed. The variations,
discussed below, tradeoff computational resources for the ability to
compose operational proofs. Whether the performance hit is
significant, and whether or not the added features are worth the
tradeoff depends very much on the application.
===Variation 1: Level-compressed hashing===
In this variation the referenced child node's hash is used in
construction of an interior node's hash digest. The interior node is
serialized just as described (using the child node's digest instead of
inline serialization), the resulting bytestring is passed through one
round of SHA-256, and the digest that comes out of that is the hash
value of the node. This is very efficient to calculate, requiring the
absolute minimum number of SHA-256 hash operations, and achieving
level-compression of computational resources in addition to reduction
of space usage.
For example:
    >>> dict = AuthTree()
    >>> dict['a'] = 0xff
    >>> dict.serialize()
    0x0200c3100001ff
    >>> dict.root
    AuthTreeNode(
        left_prefix = 0b01100001,
        left_hash   =
        left_count  = 1,
        left_size   = 4)
    >>> dict.hash
    0xb4837376022a7c9ddaa7d685ad183bcbd5d16c362b81fa293a7b9e911766cf3c
Assuming uniform distribution of key values, level-compressed hashing
has time-complexity logarithmic with respect to the number of keys in
the prefix tree. The disadvantage is that it is not possible in
general to "rebase" an operational proof on top of a sibling,
particularly if that sibling deletes branches that result in
reorganization and level compression of internal nodes used by the
rebased proof.
===Variation 2: Proof-updatable hashing===
In this variation, level-compressed branches are expanded into a
series of chained single-branch internal nodes, each including the
hash of its direct child. For a brach with a prefix N bits in length,
this requires N chained hashes. Thanks to node-compression (excluding
empty branches from the serialization), it is possible for each hash
operation + padding to fit within a single SHA-256 block.
Note that the serialization semantics are unchanged! The variation
only changes the procedure for calculating the hash values of interior
nodes. The serialization format remains the same (modulo differing
hash values standing in for pruned branches).
Using the above example, calling dict.hash causes the
following internal nodes to be constructed:
    >>> node1 = AuthTreeNode(
        right_prefix = 0b1,
        right_hash   =
        right_count  = 1,
        right_size   = 4)
    >>> node2 = AuthTreeNode( left_prefix=0b0,  left_hash=node1.hash,
 left_count=1,  left_size=4)
    >>> node3 = AuthTreeNode( left_prefix=0b0,  left_hash=node2.hash,
 left_count=1,  left_size=4)
    >>> node4 = AuthTreeNode( left_prefix=0b0,  left_hash=node3.hash,
 left_count=1,  left_size=4)
    >>> node5 = AuthTreeNode( left_prefix=0b0,  left_hash=node4.hash,
 left_count=1,  left_size=4)
    >>> node6 = AuthTreeNode(right_prefix=0b1, right_hash=node5.hash,
right_count=1, right_size=4)
    >>> node7 = AuthTreeNode(right_prefix=0b1, right_hash=node6.hash,
right_count=1, right_size=4)
    >>> node8 = AuthTreeNode( left_prefix=0b0,  left_hash=node7.hash,
 left_count=1,  left_size=4,
                              value=0xff)
    >>> dict.hash == node8.hash
    True
    >>> dict.hash
    0xc3a9328eff06662ed9ff8e82aa9cc094d05f70f0953828ea8c643c4679213895
The advantage of proof-updatable hashing is that any operational proof
may be "rebased" onto the tree resulting from a sibling proof, using
only the information locally available in the proofs, even in the
presence of deletion operations that result in level-compression of
the serialized form. The disadvantage is performance: validating an
updatable proof requires a number of hash operations lower-bounded by
the length of the key in bits.
==Inclusion proofs==
An inclusion proof is a prefix tree pruned to contain a subset of its
keys. The serialization of an inclusion proof takes the following form:
    inclusion_proof := variant || root_hash || root_node || checksum
Where variant is a single-byte value indicating the
presence of level-compression (0 for proof-updatable hashing, 1 for
level-compressed hashing). root_hash is the Merkle
compression hash of the tree, the 32-byte SHA-256 hash of the root
node. tree is the possibly pruned, serialized
representation of the tree. And finally, checksum is the
first 4 bytes of the SHA-256 checksum of variant,
root_hash, and root_node.
For ease of transport, the standard envelope for display of an
inclusion proof is internet-standard base64 encoding in the following

@_date: 2013-12-19 23:54:31
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] bitcoin-qt 
Hash: SHA1
JSON-RPC is a huge security risk. It's perfectly reasonable that
enabling it requires some technical mumbo-jumbo.
Are there specific configuration settings that you would like to see
exposed by the GUI?

@_date: 2013-12-20 03:21:38
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Hash: SHA1
Hi Jeremy, Let's give a preview of the application-oriented BIPs I
Stateless validation and mining involves prefixing transaction and
block messages with proofs of their UTxO state changes. These are the
"operational proofs" I describe in the draft, and they work on prefix
trees whose root hashes committed to the coinbase in a soft-fork
upgrade of the validation rules.
"Ultimate blockchain compression" involves consensus over an address
index, which can be queried over the p2p network by lightweight nodes.
The structure of the index is an authenticated prefix tree, and the
results of such a query is an an inclusion proof.
Document time-stamping and this new method of merged mining use the
same structure: a prefix tree whose root hash value is committed to a
pruneable output of the coinbase transaction. Document timestamp
proofs and merged mining proof-of-works are inclusion proofs over this
I hope that shows how the BIP directly affects interoperability of the
bitcoin protocol and clients which use these applications. I released
this BIP first to get some feedback on the structure itself, which
will be used by all of the application-specific BIPs which follow.
Stepping back and speaking generically, the purpose of a BIP as I see
it is to standardize details which affect interoperability between
clients. In fact, at a cursory glance only about half of the BIPs deal
with protocol issues directly - the rest deal with local /
user-interface issues like key derivation or JSON-RPC APIs. Even if
none of the applications involved protocol changes, I still think BIPs
like this would be of value in that they serve to standardize things
which are or will seek to become commonly used and widely implemented.

@_date: 2013-12-20 10:41:31
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
(Sorry Peter, this was meant for the whole list:)
This is incorrect. If the slower proof-updatable hashes are used, then
mining only requires what I've called "operational proofs" to be
attached to received transactions and blocks.
Access to the UTXO set is required to make new transactions, at least
for the outputs of the transaction, but I do not believe this is as
significant a problem as you do. It is a service that can be
outsourced for a minimal fee - include an explicit output of the
necessary amount to a scriptPubKey specified by the archival node, and
they will make sure the proper proofs are attached.
You might have to explain this to me, but it is not clear to me how
the validation index could be twisted into providing a Namecoin-like
system. Or the address index either, which I presume is what you are
referring to. Namecoin works by assigning domains to outputs, and then
tracking ownership and configuration of that domain through chains of
outputs. But the UTXO set doesn't contain connecting information. At
best all it would be is a glorified, and expensive time-stamper,
unattractive because there are already better solutions.
At the cost of having the supposedly lightweight client query for each
of its coins on every single block, to construct a negative
I don't think this is true, or at least you are not considering
available optimizations. You certainly don't need to store multiple
copies of the UTXO set.
I'm a little confused as to the exact situation you are describing.
When a key is loaded into a wallet, or a wallet comes online after a
significant absence, it looks for coins in the current UTXO set. If
any coins are found, their attached transaction record has a block
height field, so the confirmation count can be derived from that. As
blocks go by that count is naturally increased. I'm not sure how this
is different from the current situation.

@_date: 2013-12-20 14:04:23
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Hash: SHA1
I got the inputs from IRC, but thank you for posting to the list so
that others can see and review.
A length-prefixed string, using the shortest representation VARINT for
the length. Same as how scripts are serialized in transactions.
Yes I considered midstate compression which is why the branch hashes
come last, but "extra" was an oversight. In every application I've
considered it's either not used (and therefore a single byte), or
updated whenever the node or its children updates.
Honestly I don't expect midestate compression to offer much since in
the nodes that are updated frequently it is unlikely that there will
be enough static data at the front to fill even a 512 bit block of the
smaller hash function.
But it doesn't hurt to prepare just in case. I'll move it to the end.
Yes, this is a great suggestion. Moving to SHA-512/256 will let most
inner nodes fit inside a single block, so long as the "extra" field is
not too long. Also apparently SHA-512 is faster on 64-bit CPUs, which
is a nice advantage. I didn't know that.
I'm concerned about speed but I did not go with a faster hash function
because users are more likely to have hardware acceleration for the
SHA-2 family.
The serialization format encodes lengths in such a way that you cannot
extend the data structure merely by appending bits. You would have to
change the prior, already hashed bits as well. I believe this makes it
immune to length extension attacks.
Well.. the UTXO tree is big. Let's assume 5,000 transactions per
block, with an average of 3 inputs/outputs per transaction. This is
close to the worst-case scenario with the current block size. That's
15,000 insert, update, or delete operations.
The number of hashes required when level-compression is used is log2
the number of items in the tree, which for bitcoin is currently about
2.5 million transactions. So that's about ~21 hashes per input/ouput,
or 315,000 hash operations. A CPU is able to do about 100,000 hashes
per second per core, that'll probably take about a second on a modern
4- or 8-core machine.
For updatable proofs, the number of hash operations is equal to the
number of bits in the key, which for the validation index is always
256. That means 3.84 million hashes, or about 10 seconds on a 4-core
The numbers for the wallet index are worse, as it scales with the
number of outputs, which is necessarily larger, and the keys are longer.
This is not an insignificant cost in the near term, although it is the
type of operation that could be easily offloaded to a GPU or FPGA.
This is something I know less about, and I welcome constructive input.
There is *no* reason that the hash serialization needs to have fancy
space-saving features. You could even make the SIG_HASH node
serialization into fixed-size, word-aligned data structures.
But this is absolutely not my field, and I may need some hand-holding.
Do the fields need to be at fixed offsets? With fixed widths? Should I
put variable-length stuff like the level-compressed prefixes and value
data at the end (midstate be damned) to keep fixed offsets? What's
expected word alignment, 32-bit or 64-bit?
I believe what you mean by "compact update proofs" is what I call
"updatable proofs", where level compression is only used in the disk
and network serialization. These are what I propose to use for the
validation and wallet indexes, if the computational costs can be
brought under control, because it allows composable proofs.
Unlike a time-ordered index, it does require that someone, somewhere
has random access to the entire UTXO set since you can't predict in
advance what your txid will be. But this is a matter of tradeoffs and
I don't believe at this time that there is a clearcut advantage of one
approach over the other. I'm pursuing a txid-indexed UTXO set because
it most closely matches the current operational model.
That said, you still want level-compression within the serialization
format itself, if for no other reason than to keep proof sizes small.

@_date: 2013-12-22 19:22:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bitcoin difficulty sanity check suggestion 
Hash: SHA1
Ryan, these sort of adjustments introduce security risks. If you were
isolated from the main chain by a low-hashpower attacker, how would
you know? They'd need just three days without you noticing that
network block generation has stalled - maybe they wait for a long
weekend - then after that the block rate is normal but completely
controlled by the attacker (and isolated from mainnet).
There are fast acting alternative difficulty adjustment algorithms
being explored by some alts, such as the 9-block interval, 144-block
window, Parks-McClellan FIR filter used by Freicoin to recover from
just such a mining bubble. If it were to happen to bitcoin, there
would be sophisticated alternative to turn to, and enough time to make
the change.

@_date: 2013-07-23 12:36:55
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Anyone who wants HTTP authentication or TLS can wrap it with nginx, or
something similar. In the process they could put appropriate
restrictions in place on incoming requests, and the onus would be on
them, not us to keep it secure.

@_date: 2013-06-03 17:22:43
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposal: soft-fork to make 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I would suggest the more general 'push depth onto stack'. You can then
use the usual math/relational operators which otherwise have seen little
Assuming it's even a good idea to go down this route at all.

@_date: 2013-06-06 11:18:26
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Revocability with known trusted escrow 
Hash: SHA1
If a technical solution could be found, I don't doubt that it will
quickly become the only legal way to do transfers in the U.S.
Peter, you are Executive Director of the Bitcoin Foundation. I would
like to know that your efforts are focused on fighting this archaic
world view, not bending over backwards to comply with it.
large corporations taking money from consumers over fraudulent
reversals. Actually, I won't, I just said it.
without any protocol change, as an opt-in?
promises for signing. If it doesn't receive a cancel message, it will
sign at the end of the time.
registry, so you could see if you were going to have a delay period when
you saw a transaction go out.
trusted escrow service, and is vulnerable to griefing, but I thought I'd
see if some of the brighter minds than me can come up with a layer-on
approach here.
my coins in a one day reversible system, because I would have warning if
someone wanted to try and spend them, and could do something about it.
I'm not sure if it gets me anything over a standard escrow arrangement,
SKYPE: vessenes

@_date: 2013-06-10 09:46:16
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
Hash: SHA1
What you are recommending is a drastic change that the conservative
bitcoin developers probably wouldn't get behind (but let's see). However
proof-of-stake voting on protocol soft-forks has vast implications even
beyond the block size limit. Within Freicoin, we have looked at is as a
possibility for determining how to distribute the demurrage, a proposal
we are calling 'Republicoin' due to the fact that with proxy voting we
expect a system to emerge similar to the government budgeting in
parliamentary republics. Distributed, non-coersive government by
protocol, if you will.
So anyway, even if you get shot down, please continue to pursue this
proposal. It very likely has uses that you haven't thought of yet.
can not
miners to
majority of
Essentially for
median of
for the
their age
old will be
day old
simply to
after 52,560
included to
identifying votes
tree of
the vote
remembers their
median in a
still be
provided the
additional votes
vote to
the wishes
prevent DoS
authoritative as
mine as
of the
considered to
end of
gradually and
being spent.
hard-fork is
a given
buying is
vote to
high. (note
the limit
stay. But
whatever that

@_date: 2013-03-11 11:59:14
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Blocking uneconomical UTXO creation 
If you think demurrage has a bad rep, wait until you see the response to
escheatment (which is what's really being proposed here).
UTXO growth over time is worst-case linear, while computational resources
increase exponentially. Mike nailed it on the head: all of this is a
solution in search of a problem.

@_date: 2013-03-13 10:41:29
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] 0.8.1 ideas 
I'm not sure I understand the need for hard forks. We can get through this
crisis by mining pool collusion to prevent forking blocks until there is
widespread adoption of patched clients.
1) Patch the pre-0.8 branches to support an increased lock count, whatever
number is required to make sure that this problem never shows up again at
the current block size (I defer to Luke-Jr and gmaxwell's numbers on this).
2) Patch all branches to not *generate* blocks which trigger the lock count
limit. A larger block would still be accepted as valid, however, if it is
on the longest chain.
3) Simultaneously, provide an additional non-standard patch to mining pool
operators (>>50% network hash) *rejecting* blocks that trigger the lock
count limit. This keeps miners in collusion with each other to stay on a
'compatibility fork'.
4) At some point in the future once we've crossed an acceptable adoption
threshold, the miners remove the above patch in a coordinated way.
Does that not get us past this crisis without a hard-fork?
(Aside: I'm for BOTH raising the block-size limit and off-chain
transactions, but like it or not there are political sides to that debate
and we should keep politics out of crisis management.)

@_date: 2013-03-13 11:27:13
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] 0.8.1 ideas 
This may be a semantic issue. I meant that it's not a hard-fork of the
bitcoin protocol, which I'm taking to mean the way in which we all
*expected* every version of the Satoshi client to behave: the rules which
we have documented informally on the wiki, this mailing list, and in code
comments, etc. I'm just trying to prevent protocol-creep.
Luke-Jr is suggesting that we add-to/modify the bitcoin protocol rules
which all verifying implementations must adhere to. I'm suggesting that we
instead change the old codebase to do what we expected it to do all along
(what 0.8 does and what every other verifying implementation does), and
through miner collusion buy ourselves enough time for people to update
their own installations.
I know there's people here who will jump in saying that the bitcoin
protocol is the behavior of the Satoshi client, period. But which Satoshi
client? 0.7 or 0.8? How do you resolve that without being arbitrary? And
regardless, we are moving very quickly towards a multi-client future. This
problem is very clearly a *bug* in the old codebase. So let's be forward
thinking and do what we would do in any other situation: fix the bug,
responsibly notify people and give them time to react, then move on. Let's
not codify the bug in the protocol.

@_date: 2013-03-23 15:30:22
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] A bitcoin UDP P2P protocol extension 
If you're considering a datagram protocol, you might be interested in some
more modern alternatives to UDP:
UDT: Breaking the Data Transfer Bottleneck
Stream Control Transmission Protocol

@_date: 2013-03-23 19:27:31
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] A bitcoin UDP P2P protocol extension 
Nearly all of these new(er) user-mode transports run over UDP, so you can
hole-punch and port forward just the same. Some which don't can
nevertheless be tunneled, to the same effect.
Ultimately I don't have any skin in this game though. Just trying to save
someone from reinventing a perfectly good wheel ;)

@_date: 2013-05-20 16:59:39
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] UUID to identify chains (payment protocol and 
At the developer round-table it was asked if the payment protocol would alt-chains, and Gavin noted that it has a UTF-8 encoded string identifying the network ("main" or "test"). As someone with two proposals in the works which also require chain/coin identification (one for merged mining, one for colored coins), I am opinionated on this. I believe that we need a standard mechanism for identifying chains, and one which avoids the trap of maintaining a standard registry of string-to-chain mappings.
Any chain can be uniquely identified by its genesis block, 122 random bits is more than sufficient for uniquely tagging chains/colored assets, and the low-order 16-bytes of the block's hash are effectively random. With these facts in mind, I propose that we identify chains by UUID.
So as to remain reasonably compliant with RFC 4122, I recommend that we use Version 4 (random) UUIDs, with the random bits extracted from the double-SHA256 hash of the genesis block of the chain. (For colored coins, the colored coin definition transaction would be used instead, but I will address that in a separate proposal and will say just one thing about it: adopting this method for identifying chains/coins will greatly assist in adopting the payment protocol to colored coins.)
The following Python code illustrates how to construct the chain identifier from the serialized genesis block:
     from hashlib import sha256
     from uuid import UUID
     def chain_uuid(serialized_genesis_block):
         h = sha256(serialized_genesis_block).digest()
         h = sha256(h).digest()
         h = h[:16]
         h = ''.join([
             h[:6],
             chr(0x40 | ord(h[6]) & 0x0f),
             h[7],
             chr(0x80 | ord(h[8]) & 0x3f),
             h[9:]
         ])
         return UUID(bytes=h)
And some example chain identifiers:
     mainnet:  UUID('6fe28c0a-b6f1-4372-81a6-a246ae63f74f')
     testnet3: UUID('43497fd7-f826-4571-88f4-a30fd9cec3ae')
     namecoin: UUID('70c7a9f0-a2fb-4d48-a635-a70d5b157c80')
As for encoding the chain identifier, the simplest method is to give "network" the "bytes" type, but defining a "UUID" message type is also possible. In either case bitcoin mainnet would be the default, so the extra 12 bytes (vs: "main" or "test") would only be an issue for alt-chains or colored coins.
Kind regards,
Mark Friedenbach

@_date: 2013-05-20 21:00:44
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] UUID to identify chains (payment protocol 
This was meant to go to everyone:
That is true, and perhaps we have enough clout to push an RFC specifying a double-SHA256 Version 6, or at least get it reserved. I proposed Version 4 (random) because any UUID library should allow you to specify the 122 supposedly random bits of that version, whereas conceivably there might exist UUID libraries that require a SHA1 pre-image to create a Version 5 UUID (I know of no examples though). Regardless, making an official double-SHA256 UUID version RFC is an option worth considering.
I think there are perhaps two issues being conflated here (and in Mike's response): the UI identifying the network/coin to the user, and the matching of the protocol-supplied value to the underlying network/coin by the client/daemon. The former necessarily involves manual adjustments (e.g, localization), but it's preferable for the latter to be a self-validating reference to the block chain. This is a trivial difference for multi-chain wallets (what are you doing receiving requests for coins in chains you don't know about?), but is important for colored coins. Let me explain:
I will be proposing soon a colored coin architecture that allows issuance of new coins by anyone for a fee, by means of a special category of transaction. The hash of that issuing transaction would then be used to generate a UUID identifying the asset for the payment protocol and other purposes as well, analogous to how the hash of the genesis block identifies the host currency, bitcoin. It is expected that there will be many such coins issued, as they can be used to represent individual loans or lines of credit. In this context, any colored-coin aware client could scan the block chain (or lookup a maintained index) to discover the UUID -> coin mapping with absolute certainty. However the mechanism for mapping the text "mtgoxUSD" to a specific coin is not clear, and using some sort of DNS-resolution system adds huge external dependencies. IMHO it is much better to have the identifier derived from block chain data directly (and therefore accessible and trusted by all nodes), and then carry out optional UI mappings like UUID(...) -> "mtgoxUSD" at a higher level.
Does that make sense?

@_date: 2013-11-02 14:51:22
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Message Signing based authentication 
Hash: SHA1
Or SIGHASH of a transaction spending those coins or updating the SIN...
> On 11/01/2013 10:01 PM,

@_date: 2013-11-04 11:11:34
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Committing to extra block data/a better 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
You mean... an authenticated prefix tree? Composable/commutative
properties are not needed as far as I can see, so you could make the
path validation, traversal, and proof size smaller by using level
I had previously proposed to this list a hash256-to-UUID mechanism
explicitly for this purpose. Recap: use 122 of the low 128 bits of the
aux-chain's genesis block to form a version=4 (random) or version=6
(previously unused) UUID. However since making that proposal I am now
leaning towards simply using the hash of the genesis block directly to
identify aux chains since level compression will allow longer keys
with the same path length.
I'm in the middle of writing BIPs to this end, among my many other
tasks. But basically it's the same as you describe ("OP_RETURN
<32-byte auth tree root>" for the last output), except keys don't
necessarily have to be UUIDs.
If there is general interest, I can make finishing this a higher priority.

@_date: 2013-11-04 11:53:05
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Committing to extra block data/a better 
Hash: SHA1
The bits make a difference if you are merged mining. You can use the
birthday attack to construct two data trees whose hash match the
(truncated) value, each containing separate aux block headers. This
allows you to double-count the bitcoin PoW for more than one aux block
on the same chain, potentially facilitating aux chain attacks.
If you want 128 bits of security for merged mined aux chains, you need
256 bits of hash in the coinbase.

@_date: 2013-11-14 13:15:54
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
For this reason I'm in favor of skipping mBTC and moving straight to
uBTC. Having eight, or even five decimal places is not intuitive to
the average user. Two decimal places is becoming standard for new
national currencies, and we wouldn't be too far from human scale
everyday numbers: 25.00uBTC ~= $0.01 currently. And I don't think very
many people on this list would consider bitcoin overvalued in the long
term perspective.
Better to go through a confusing renumbering only once.

@_date: 2013-11-14 14:21:56
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
Well.. they are huge. 20 cents suggested fee for a irrevocable

@_date: 2013-11-14 14:31:53
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
Whoops, this was meant for the list:
Drawing on analogues from national currencies, it's also possible to
alleviate the confusion by switching currency symbols, e.g. to XBT or
NBC (New Bitcoin).
1 XBC == 1 uBTC

@_date: 2013-11-14 15:11:26
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
"key id" (thanks sipa).
I know it's a more technical term, but that is rather the point. It
was a fundamental error to call hashed-pubkeys "addresses" as people
either associate this with "account" or physical addresses, which also
rarely change.
Security and privacy guarantees of the system are defeated when key
pairs are reused. We should ideally adopt terminology that lead people
to associations of ephemeral, temporary use. "key id" is at least
neutral in this regard. Can anyone think of something better?

@_date: 2013-11-14 16:18:45
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I was referencing a IRC conversation where sipa suggested "key id" as
a replacement for "address".
My only issue with "invoice" is possible confusion over the payment

@_date: 2013-11-15 16:48:42
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
No, no no. That's precisely the problem! Bitcoin pubkey-hashes are not
like email address, physical address, or paypal address. These latter
things are fixed pieces of information that stay constant over time.
Bitcoin keys, on the other hand, must be one-use-only. We want to
break this association, not strengthen it.

@_date: 2013-11-15 17:31:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
Hash: SHA1
It's not about being technically correct. It is about protecting the
user from grave breaches of privacy. It is for their own benefit that
they should not be reusing addresses, and if they understood why they
Unfortunately calling it a "bitcoin address" and including an "address
book" in the reference client has had the effect of making people
think that these objects are like paypal address, or email addresses,
but they are not and they should not be treated the same.

@_date: 2013-10-25 06:29:57
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Making fee estimation better 
Hash: SHA1
There's no reason the signing can't be done all at once. The wallet
app would create and sign three transactions, paying avg-std.D, avg,
and avg+std.D fee. It just waits to broadcast the latter two until it
has to.
October Webinars: Code for Performance

@_date: 2013-10-30 02:05:05
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
Hash: SHA1
If I understand the code correctly, it's not about rejecting blocks.
It's about noticing that >50% of recent blocks are declaring a version
number that is meaningless to you. Chances are, there's been a soft
fork and you should upgrade.

@_date: 2013-09-10 15:35:53
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP0039 Mnemonic code for generating 
Getting OT...
For a while I've wanted to combine one of these mnemonic code generators
with an NLP engine to do something like output a short story as the
passphrase, even a humorous onem with the key encoded in the story
itself (remember the gist of the story and that's sufficient to
reconstruct the key).
Also, obligatory link about the failures of unsanitized word lists:
It can really backfire to get one of these things wrong.

@_date: 2013-09-13 18:00:03
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] New Output Script Type 
Prefix the script with OP_RETURN. Otherwise you are still contributing
to blockchain bloat.

@_date: 2013-09-17 10:08:54
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Faster databases than LevelDB 
Hash: SHA1
Also somewhat related, I have been looking for some time now to
abstract out the UTXO and block databases so that a variety of
key/value stores could be used as a backend, configured by a command
line parameter. In particular, it would be interesting for some server
applications to support HyperDex, which is basically a distributed,
fault-tolerant version of LevelDB:
By the same mechanism you could just as easily support a Sophia backend.
LIMITED TIME SALE - Full Year of Microsoft Training For Just $49.99!

@_date: 2013-09-29 10:49:00
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] smart contracts -- possible use case? yes 
This kind of thing - providing external audits of customer accounts
without revealing private data - would be generally useful beyond
taxation. If you have any solutions, I'd be interested to hear them
(although bitcoin-dev is probably not the right place yet).

@_date: 2014-04-06 16:23:43
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Feedback request: colored coins protocol 
No, I'm afraid it has significant flaws. The two chief flaws are (1)
there is absolutely no reason to include asset tagging information if it
is not validated - that just bloats the block chain, and (2) you
shouldn't be using fixed increments for share sizes either. It's not
future-proof as the minimum output size changes based on the minimum fee
(currently 540 satoshis, not 5,400, and it will float in the near
future). And needing a capital of 54 btc for a million shares is totally
Flavien, I know that I've seen you on the Bitcoin-X mailing list, where
these issues have been mostly worked out:
Have you seen the padded order-based coloring scheme worked out here?
Kind regards,
Mark Friedenbach

@_date: 2014-04-07 08:06:08
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Feedback request: colored coins protocol 
Flavien, capital is wealth or resources available for the stated purpose
of the company. These bitcoins represent nothing more than a speculative
floor owned by the investors, not the company.

@_date: 2014-04-07 09:27:07
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
Right now running a full-node on my home DSL connection (<1Mbps) makes
other internet activity periodically unresponsive. I think we've already
hit a point where resource requirements are pushing out casual users,
although of course we can't be certain that accounts for all lost nodes.

@_date: 2014-04-07 10:01:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
Oh, absolutely. But the question "why are people not running full
nodes?" has to do with the current implementation, not abstract
capabilities of a future version of the bitcoind code base.

@_date: 2014-04-07 11:48:26
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
The point is that the node has decided not to prune transactions from
that block, so that it is capable of returning full blocks within that

@_date: 2014-04-07 12:13:04
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
And how do you find those blocks?
I have a suggestion: have nodes advertise which range of full blocks
they possess, then you can perform synchronization from the adversed ranges!

@_date: 2014-04-09 08:47:37
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
Storing zero full blocks does nothing to aid the network.

@_date: 2014-04-09 13:36:35
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
I've advocated for this in the past, and reasonable counter-arguments I
was presented with are: (1) bittorrent is horribly insecure - it would
be easy to DoS the initial block download if that were the goal, and (2)
there's a reasonable pathway to doing this all in-protocol, so there's
no reason to introduce external dependencies.

@_date: 2014-04-10 09:59:00
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Feedback request: colored coins protocol 
At this point, I don't think what you are doing is even colored coins
anymore. You might want to look into Counterparty or Mastercoin.

@_date: 2014-04-10 12:36:39
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Chain pruning 
You took the quote out of context:
"a full node can copy the chain state from someone else, and check that
its hash matches what the block chain commits to. It's important to
note that this is a strict reduction in security: we're now trusting
that the longest chain (with most proof of work) commits to a valid
UTXO set (at some point in the past)."
The described synchronization mechanism would be to determine the
most-work block header (SPV level of security!), and then sync the UTXO
set committed to within that block. This is strictly less security than
building the UTXO set yourself because it is susceptible to a 51% attack
which violates protocol rules.

@_date: 2014-04-16 08:23:54
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Warning message when running wallet in 
XP is no longer receiving security patches from Microsoft, and hasn't been
for some time. There are known remote exploits that aren't going to be
fixed, ever.

@_date: 2014-04-16 09:35:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Warning message when running wallet in 
It's not really our place to police that ... plus it's perfectly safe to
be running Bitcoin Core as a full node on XP. It's just the wallet
functionality that people should be careful about. We're talking about
such a small intersection of people who are running XP, have systems
powerful enough to run Bitcoin Core, and use the wallet functionality.

@_date: 2014-04-16 09:44:55
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Warning message when running wallet in 
We don't support XP. In fact we don't support *any* distribution, but I
will assume you mean "provide a binary which runs on X." Can you find
any reference to Windows XP on the website? I can't.

@_date: 2014-04-16 14:39:27
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Warning message when running wallet in 
NO. Bitcoin Core will never have an auto-update functionality. That
would be a single point of failure whose compromise could result in the
theft of every last bitcoin held in a Bitcoin Core wallet.

@_date: 2014-04-17 09:35:52
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Timed testing 
Not necessarily. Running a private server involves listening to the p2p
network for incoming transactions, performing validation on receipt and
organizing a mempool, performing transaction selection, and relaying
blocks to auditors - none of which is tested in a reindex.
A reindex would give you an optimistic upper bound though, if that's all
you care about.

@_date: 2014-04-20 20:58:58
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Economics of information propagation 
As soon as we switch to headers
first - which will be soon - there will be no difference in propagation
time no matter how large the block is. Only 80 bites will be required to
propagate the block header which establishes priority for when the block is
fully validated.
On Apr 20, 2014 6:56 PM, "Jonathan Levin"

@_date: 2014-04-21 09:00:09
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Economics of information propagation 
That wasn't what I was saying. Right now the primacy of a block is
determined by the time at which the `block` message is received, which
is delays due to both the time it takes to transmit the block data and
the time it takes to validate. Headers-first, on the other hand, has the
option of basing primacy on the time the block header is received, which
is O(1) time to transmit and to SPV-validate. Mining on that block
doesn't actually commence until the full block is received and validated.
To see how this works, take an example: two blocks with a common parent
are found relatively close to each other, block A and block B. A is
found first but is a large block with the maximum block size and many
slow scripts. B is found a few seconds later and is an empty block. In
the current regime it is entirely possible that block B, the later but
smaller block, would get received and processed first by more mining
peers than the larger block A, exactly as described in Jonathan Levin's
With headers-first, however, the cost of propagation of the block header
is the same and we should expect block A to win out over block B nearly
every time. Miners will continue working on the old, known valid parent
block until the contents of block A are received and processed. So the
smaller block B is still found, and since it's data moves across the
network faster, miners even briefly mine on block B. But as soon as they
receive and process the contents of block A, they switch to that.
The earlier, larger block A will only become stale if *two* blocks are
found in the extra time it takes for block A to propagate the network.
That is a substantially different risk, and probably a negligible
concern to most miners.

@_date: 2014-04-21 09:38:17
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Economics of information propagation 
Yes, it certainly can be improved in this way. You can even extend the
idea to distribute partial proofs of work (block headers + Merkle lists
which represent significant but not sufficient work), and 'prime' your
memory pools with the transactions contained within.
This is, btw, basically what p2pool does, which is why last time I
calculated you get roughly 1% better return from p2pool than a zero-fee
mining pool would get you, specifically because of the lower stale rate.

@_date: 2014-04-22 08:32:23
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
Testnet vs mainnet is quite a separate issue than bitcoin vs altcoin.
Unfortunately few of the alts ever figured this out.

@_date: 2014-04-22 10:03:59
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Presenting a BIP for Shamir's Secret 
What I was saying is that while it may or may not make sense to have
separate prefixes for testnet, it makes no sense to have per-alt prefixes.

@_date: 2014-04-22 23:35:06
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Double-spending unconfirmed transactions 
Because you only want to double-spend if you loss the bet. If you tried
to double-spend every bet, there'd be no point :)

@_date: 2014-04-26 10:00:23
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP - Selector Script 
I think you're misunderstanding the point. The way you get IsStandard
changed is that you make an application-oriented BIP detailing the use
of some new standard transaction type (say, generalized hash-locked
transactions for atomic swaps). We then discuss that proposal for its
technical merits and reach consensus about the best way to do, for
example, cross-chain atomic swaps. It is then implemented.
So please, focus on some BIP(s) detailing applications of hash-locked
transactions, and we will engage more constructively -- I promise that I
will as cross-chain atomic swaps scratch my itch as well.

@_date: 2014-04-26 13:39:28
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proof-of-Stake branch? 
There's no need to be confrontational. I don't think anyone here objects
to the basic concept of proof-of-stake. Some people, myself included,
have proposed protocols which involve some sort of proof of stake
mechanism, and the idea itself originated as a mechanism for eliminating
checkpoints, something which is very much on topic and of concern to
many here.
The problems come when one tries to *replace* proof-of-work mining with
proof-of-stake "mining." You encounter problems related to the fact that
with proof-of-stake nothing is actually at stake. You are free to sign
as many different forks as you wish, and worse have incentive to do so,
because whatever fork does win, you want it to be yours. In the worst
case this results in double-spends at will, and in the best case with
any of the various proposed protections deployed, it merely reduces to
proof-of-work as miners grind blocks until they find one that names them
or one of their sock puppets as the signer of the next block.
I sincerely doubt you will find a solution to this, as it appears to be
a fundamental issue with proof-of-stake, in that it must leverage an
existing mechanism for enforced scarcity (e.g. proof-of-work) in order
to work in a consensus algorithm. Is there some solution that you have
in mind for this?

@_date: 2014-04-26 18:22:07
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proof-of-Stake branch? 
That makes double-spends trivially easy: sign two blocks, withholding
one. Then at a later point in time reveal the second signed block
(demonstrating your own fraud) and force a reorg.

@_date: 2014-04-26 18:43:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] About Compact SPV proofs via block header 
First of all, let's define what an SPV proof is: it is a succinct
sequence of bits which can be transmitted as part of a non-interactive
protocol that convincingly establishes for a client without access to
the block chain that for some block B, B has an ancestor A at some
specified height and work distance back, and the cost of creating a
false proof is at least as much work as it claims to represent.
The previous email you quote demonstrates how with additional backlink
commitments this can be done in logarithmic space: using an average of
log(N) headers to construct an SPV proof from block A to block B where N
is the height differential. It can be verified without access to the
block chain or other peers. Note that with back links the cost of
creating a fraudulent SPV proof is the same as 51% attacking bitcoin
itself. The protocol you outlined does not have this property.
Other than that, honestly I'm not really sure what you are trying to
accomplish. An interactive proof is does not meet the above requirements
and is not usable for the driving application of two-way pegs. Maybe you
had some other application in mind? I've looked at your SmartSPV
proposal, but fail to see how it doesn't reduce to simply blind trust in
your view of the network from your peers. SPV proofs on the other hand
put an economic cost to fraud.

@_date: 2014-04-26 23:43:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] About Compact SPV proofs via block header 
I don't think there's an official definition of "SPV proof." I wasn't
trying to make a argument "from definition" (that would be fallacious!).
Rather I suspected that we had different concepts in mind and wanted to
That said, I do think that the definition I gave matches how the term is
used in the Satoshi whitepaper, and the way in which SPV clients like
BitcoinJ work. "Best chain" is typically taken to mean the most-work,
*valid* chain. Without invoking moon math or assumptions of honest peers
and jamming-free networks, the only way to know a chain is valid is to
witness the each and every block. SPV nodes on the other hand, simply
trust that the most-work chain is a valid chain, based on economic
arguments about the opportunity cost of mining invalid blocks. These SPV
nodes use block headers as proofs to determine the most-work block
connected to the genesis block or most recent checkpoint. So yes,
operationally at least this is what the community seems to mean by "SPV
Now regarding your use case:
This linear scan of block headers is what I would prefer to avoid. By
using back-links you make it have log(N) space usage.
Start Your Social Network Today - Download eXo Platform

@_date: 2014-04-27 02:38:06
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposal for extra nonce in block header 
I'm not convinced of the necessity of this idea in general, but if it
were to be implemented I would recommend serializing the nVersion field
as a VarInt (Pieter Wuille's multi-byte serialization format) and using
the remaining space of the 4 bytes as your extra nonce.
That would allow serialization of numbers up to 0x1020407f (slightly
over 28 bits) before the 4-byte field is exhausted. For version numbers
less than 0x204080 there will be at least one byte of padding space left
over for extra-nonce usage (two bytes if less than 0x4080, three bytes
if less than 0x80). For version values up to 127, the format is exactly
identical when the padding bytes are zero.

@_date: 2014-04-27 10:05:45
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] About Compact SPV proofs via block header 
I should have said "without invoking moon math or interactive protocols
requiring honest peers over jamming-free networks." The interactive
protocol was more the point than the honest peers and jamming-free
network. Yes, without an honest peer and an un-jammed network, you might
never learn about the most-work chain in the first place. But having the
security of the proof not depend on query access to an honest full node
is absolutely necessary for some applications and certainly desirable in
Although strictly speaking what I said may not be 100% true. The single
alternative solution I've seen involves some sort of Fiat?Shamir
transform that could give you a probabilistic proof by including random
additional paths through the block chain chosen based on the combined
hash of the headers. However this is disadvantageous as it massively
increases the proof size and verification time, and you have to include
a lot of data to achieve assurance that more work was required to
generate the fraud than an honest chain.
Not necessarily. By requiring connectivity you know that what you are
receiving is built off of the main chain, for example, and you can still
make assumptions about resulting opportunity costs.
Unless you're connected to attacker nodes which are wildly divergent
from each other. It's relatively easy to create a massive fake history
of difficulty-1 blocks.
If you assume honest peers things get very easy. But that's not a safe
assumption to be making. With back-link block-history commitments, on
the other hand, an interactive protocol allows you to do a binary search
to find common ancestors, and have trust that the intermediate links
actually exist.

@_date: 2014-04-28 10:29:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] About Compact SPV proofs via block header 
No, that's sortof tangential. What you are solving is some higher level
application on top of SPV proofs, compact or otherwise. SPV proofs have
many broad applications, such as 2-way pegs where proof-of-work is used
to reach consensus over the most-work side-chain header, and a non-51%
attack is detectable from observed difficulty and interblock times. Do
you need an honest peer to learn about the best chain? Yes. Do you need
to *trust* that you have an honest peer? No, because a non-51% attack
against you is probabilistically detectable with existing tools.
Maybe SmartSPV is useful, maybe not. The application domain is not
something I've been concerned with in the past. But what you describe is
a higher-level protocol that uses block headers to determine which chain
to trust. My simple point from the start has been that you can use
back-link commitments and compact SPV proofs to accomplish what you want
fewer messages, less bandwidth, and equal security. The two proposals
are not in conflict with each other.

@_date: 2014-08-06 12:31:24
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] deterministic transaction expiration 
I highly doubt that is the best approach.
If this nExpiry field is a consensus rule, then the Merkle tree or the
appropriate paths through needs to be included with the transaction as
part of the network and on-disk data structures, so that proper
validation can be done. This would be both more disruptive and less
efficient than simply adding an nExpiry field to the transaction format,
as we do in Freimarkets.
If the field is pre-consensus (a mempool gentleman's agreement), then it
has no business in the transaction structure at all and should be
packaged in some sort of envelope container.

@_date: 2014-08-06 13:21:56
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] deterministic transaction expiration 
Note that this would be a massive, *massive* change that would
completely break bitcoin output frangibility. Merchants would have to
start demanding input history back to a certain depth in order to ensure
they are not exposing themselves to undue reorg-expiry risk.
There are useful applications of a consensus-enforced expiry,
particularly within a private (signed block) side chain, and for that
reason it is useful to have a discussion about the merits of an nExpiry
field or BLOCK_HEIGHT / BLOCK_TIME opcode, and methods for achieving
either. However I don't see this ever becoming part of the public
bitcoin network.

@_date: 2014-08-06 13:30:11
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] deterministic transaction expiration 
How can that possibly be the case? The information is hidden behind the
Merkle root in the transaction. The validator needs to know whether
there is an expiry and what it is. What's it supposed to do, guess?

@_date: 2014-08-09 13:17:58
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] CoinShuffle: decentralized CoinJoin 
On Sat, Aug 9, 2014 at 6:10 AM, Sergio Lerner That's not clear to me. The 2nd party doesn't know how the 3rd, 4th, 5th,
etc. parties shuffled the outputs, since it doesn't have their decryption

@_date: 2014-08-11 10:06:48
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] CoinShuffle: decentralized CoinJoin 
There should not be a requirement at this level to ensure validity. That
would too constrain use cases of implementations of your protocol. It is
not difficult to imagine use cases where parties generate chained
transactions on top of unconfimed transactions. Although malleability
currently makes this difficult to do safely in general, it is not
inconceivable that there are circumstances where it would nevertheless be
safe or otherwise desireable.
It is a good security recommendation that clients validate the inputs to a
shuffle they are participating in. What this means depends on the client --
checking the UTXO set for a full node, making some getutxos queries for a
SPV client. But this should remain a recommendation, not a requirement.
On Mon, Aug 11, 2014 at 4:38 AM, Tim Ruffing <

@_date: 2014-02-12 12:27:52
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
Not to mention that it would be potentially very insecure to have
consensus depend on data (scriptSigs) which are not hashed in the Merkle
structure of a block.
Not that anyone on this list has suggested such a change, but I've seen
it raised multiple times on the forum....

@_date: 2014-02-14 15:01:41
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] working with the blockchain: transaction 
Still straightforward: get a list of transaction hashes for the block
from bitcoind, then query these transactions from the UTXO changestate

@_date: 2014-02-20 18:41:05
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Base-32 error correction coding 
Hash: SHA1
What follows is a proposed BIP for human-friendly base-32
serialization with error correction encoding. A formatted version is
viewable as part of a gist with related code:
An implementation of this BIP and associated APIs is made available as
a pull request, with comprehensive testing:
This format is anticipated to be useful for helpdesk-related data
(e.g. the proposed normalized transaction ID), and future wallet
backup & paper wallet serialization formats.
== Abstract ==
The BIP proposes an human-centered encoding format for base-32 data
serialization. It differs from the presumptive default hexadecimal or
base58 encodings in the following ways:
1. Visually distinctive in that it includes the full range of
alphanumeric digits in its base-32 encoding, except the characters 0,
l, v, and 2. which are too easily confused with 1, i, u, r, or z in
font or handwriting.
2. Automatic correction of up to 1 transcription error per 31 coded
digits (130 bits of payload data). For a 256-bit hash or secret key,
this enables seamless recovery from up to two transcription errors so
long as they occur in separate halves of the coded representation.
3. Highly probable detection of errors beyond the error correction
threshold, with a false negative rate on the order of 25 bits, or 1 in
33 million likelihood.
4. Case-insensitive encoding ensures that it may be displayed in an
easier to read uniform case, and it is faster and more comfortable to
vocally read off a base-32 encoded number than the alternatives of
hexadecimal or base58.
In addition to the error correction code transformation of base-32
data, a padding scheme is specified for extending numbers or bit
vectors of any length to a multiple of 5 bits suitable for base-32
== z-base-32 ==
The bitcoin reference client already has one implementation of base-32
encoding following the RFC 3548 standard, using the following alphabet:
    const char *pbase32 = "abcdefghijklmnopqrstuvwxyz234567";
For error correction coded strings this BIP specifies usage of Phil
Zimmermann's z-base-32 encoding alphabet[], which provides better
resistance to transcriptive errors than the RFC 3548 standard:
    const char *pzbase32 = "ybndrfg8ejkmcpqxot1uwisza345h769";
The same RFC 3548 coder is used for z-base-32, except that unnecessary
'=' padding characters are stripped before performing the alphabet
substitution. For example, the hexadecimal string 'ae653be0049be3' is
RFC 3548 encoded as 'vzstxyaetprq====', and z-base-32 encoded as
== CRC-5-USB error correction coding ==
Herein we describe an error correction encoding using cyclic
redundancy check polynomial division[], which requires 5 error
correction digits per 26 digits of input, instead of the theoretically
optimal 4, but is much, much easier to implement correctly then
available non-patented error correction codes. Cyclic redundancy check
polynomial division provides a very straightforward, patent-free
mechanism for reliably detecting transcription errors in input, and
performing up to 1-digit corrections per 26 digit block.
=== Encoding ===
The input to this error correction encoder is a sequence of 26 base-32
digits. These digits are decoded into 5-bit unsigned integers with
values equal to their offset into the base-32 alphabet string. If the
input is less than 26 digits in length, it is extended with
zero-valued digits. If For example, the string 'vzstxyaetprq' using
the RFC 3548 alphabet becomes the code point sequence:
    <21 25 18 19 23 24 0 4 19 15 17 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0>'
And the result is converted into z-base-32: 'i31uzayruxtoweuz1'.
=== Decoding and error recovery ===
The process of decoding and error detection and recovery is similar to
encoding, and this section will not explain steps that were adequately
covered in the encoder description.
First, the input is converted from z-base-32 into a sequence of up to
31 (26+5) 5-bit integers, with zero-valued padding inserted between
the end of the input and the 5-digit checksum. Using our running
example of 'i31uzayruxtoweuz1', this result is the following:
    <21 25 18 19 23 24 0 4 19 15 17 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20
8 19 23 18>'
The binary representation of each element is exploded horizontally,
and the matrix transposed to yield the following 5 x 31 bit matrix:
    <1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1
     0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
     1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0
     0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1
     1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0>
A CRC calculation of each row should yield zero if the encoded string
was transcribed correctly. If, however, a digit was wrong, that would
result in some or all of the bits of the corresponding column being
flipped. A property of the CRC generating polynomial used is that in
the case of a single digit error, each row with a flipped bit would
result in the same checksum, and that checksum value can be used as
the index into a table indicating which bit was flipped:
    // EC[0] has no meaning, because offsets are 1-based
    const unsigned char EC[32] =
        { 32,  4,  3, 17,  2, 30, 16, 24,
           1,  6, 29,  8, 15, 27, 23, 12,
           0, 25,  5, 18, 28, 13,  7,  9,
          14, 10, 26, 19, 22, 21, 11, 20 };
If all checksum values are zero, the decoder assumes the input is
correct. If all non-zero checksum values are equal valued, the decoder
assumes the corresponding digit was transcribed incorrectly and flips
the bit in that column of the rows with non-zero checksums.
If any of the five checksum values are non-zero, and not equal to each
other, then there is an unrecoverable error in the input. The
probability of two or more digits being incorrect and yet by chance
the checksums being zero or equal valued is less than 1 in 33 million.
Now that errors have been detected and single-digit errors corrected,
the padding bits and CRC checksum bits are removed. The matrix is
transposed, it's rows imploded, and the resulting sequence of up to 26
characters converted into base-32 using the RFC 3548 alphabet:
== CodedBase32 integer encoding ==
Although providing an error correction coder for base-32 data
interesting and useful in contexts where base-32 is already deployed,
many applications involve encoding of integers which are powers of two
in length. This section provides a standard scheme for the encoding of
any sized bitstring into a multiple of 5 bits in length, suitable for
direct encoding into base-32.
First the size in bits of the integer is rounded up to the next
highest power of two greater than or equal to 128. This value with a
factor of 128 removed is known as n, and its base-2
logorithm as e. Pseudocode:
    int n = max(next_power_of_two(BITS), 128) / 128;
    int e = log2(n);
A total of 2*n padding bits are prefixed to the
bitstring. These consist of e 1 bits, a single 0 bit, and
2*n-e-1 user-specified bits (the "extra" field).
The bitstring is now a multiple of 5 in length and can be directly
converted into base-32.

@_date: 2014-02-24 09:23:12
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] On OP_RETURN in upcoming 0.9 release 
Given our standardization on 128-bit security / 256-bit primitives, I
can't think of any crypto related data payload which requires more than
40 bytes. Even DER encoded compressed public keys will fit in there. A
signature won't fit, but why would you need one in there?
There's no need to design for 64-byte hashes, and the 80-char line
length comparison is a good point. As an Engineer I'd want to have a
little more room as a 32-byte hash or EC point + 8 bytes identifying
prefix data is the bare minimum, but it is also very important that we
send a message: This is for payment related applications like stealth
addresses only. Don't burden everybody by putting your junk on the block

@_date: 2014-02-24 14:35:57
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] On OP_RETURN in upcoming 0.9 
Hash: SHA1
OP_RETURN outputs are provably unspendable *and* not included in the
UTXO set, so they're not dust (the client makes this check and handles
TX_NULL_DATA outputs separately).

@_date: 2014-02-28 11:25:27
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] On OP_RETURN in upcoming 0.9 release 
Hash: SHA1
Transaction fees are a DoS mitigating cost to the person making the
transaction, but they are generally not paid to the people who
actually incur costs in validating the blockchain. Actual transaction
processing costs are an externality that is completely unpaid for.
When I add a 1Kb transaction to the blockchain, there is an attached
fee which probabilistically goes to one of the miners. But every other
full node on the network also receives this transaction, processes it,
and adds it to local storage. From now until the heat death of the
universe that 1Kb of data will be redundantly stored and transmitted
to every single person who validates the block chain. None of these
countless people are reimbursed for their storage, bandwidth, and
processing costs. Not even a single satoshi.
Yes, transaction fees are broken. But it is their very nature which is
broken (sending coins to the miners, not the greater validator set),
and no little tweak like the one Warren links to will fix this.
But, in the absence of a reformed fee regime - which it is not clear
is even possible - one could at least make the hand-wavey argument
that people who validate the block chain receive benefit from it as a
payment network. Therefore processing of the block chain is "paid for"
by the utility it provides once fully synced.
However even this weak argument does not extend to general data
storage. If you want to put all of wikileaks or whatever in the block
chain, then you are extracting a rent from every full node which is
forced to process and store this data for eternity without
compensation or derived utility. You are extorting users of the
payment network into providing a storage service at no cost, because
the alternative (losing bitcoin as a payment network) would cost them
That is not ethical behavior. That is not behavior which responsible
developers should allow in the reference client.
Flow-based real-time traffic analytics software. Cisco certified tool.

@_date: 2014-01-03 10:23:41
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] An idea for alternative payment scheme 
Hash: SHA1
There is a standard mechanism for doing that called deterministic
signatures and is described in RFC 6979. It uses the private key and
the HMAC construction to generate a ECDSA k value.

@_date: 2014-01-06 16:21:25
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Hash: SHA1
There's really two tree structures being talked about here. Correct me
if I'm wrong Thomas, but last time I looked at your code it was still
implementing a 256-way PATRICIA trie, correct? This structure lends
itself to indexing either scriptPubKey or H(scriptPubKey) with
approximately the same performance characteristics, and in the
"Ultimate blockchain compression" thread there is much debate about
which to use.
In the process of experimentation I've since moved from a 256-way
PATRICIA trie to a bitwise, non-level-compressed trie structure - what
I call proof-updatable trees in the BIP. These have the advantage of
allowing stateless application of one proof to another, and as
consequence enable mining & mempool operations without access to the
UTXO set, so long as proofs are initially provided in the transaction
& block wire format.
The "disadvantage" is that performance is closely tied to key length,
making H(scriptPubKey) the much more desirable option. I'm sure you
see that as an advantage, however :)
This would be quite easy to do, separate from the UTXO structure but
using the same trie format.

@_date: 2014-01-07 17:04:58
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP proposal: Authenticated prefix trees 
Hash: SHA1
Not really. Just use a suffix to determine the number of bits used in
the final key byte. For example, the string "abc" would have the key
    0x61626308 // "abc\x08"
Dropping the final bit would mean masking it off and having a
different terminating value:
    0x61626207 // "abb\x07"
That way you keep the lexical ordering of keys necessary for database
iteration, and the efficient binary encoding.
It is an iterative change, I believe. You might be confusing this idea
with Peter Todd's TXO commitment proposal using MMR trees, which is a
drastic change with its own set of tradeoffs. Just to be clear, here's
what I'm proposing:
1) Restructure the current UTXO index to be a Merkle tree, basically
by splitting coins into individual outputs and adding interior nodes
to the leveldb database.
2) Add hash commitments of this structure to the coinbase.
It's still mapping txid's to unspent outputs, just as before - this
has nothing to do with the script keyed "wallet index." It's just now
nodes can prefix optional proofs to block or transaction messages
which prove by reference to the current best block's hash the spend
status of the inputs of a transaction, or all the inputs of all the
transactions of a block.
If the more expensive proof-updatable hashing is used, then these
proofs can even be composed or "rebased" onto a new block by applying
the contents of an "operational proof" representing the diff between
two blocks / the application of a series of transactions.
This means that a node which does not have access to the UTXO set can
nevertheless receive transactions or entire blocks with prefixed
proofs and check the validity of the transaction with just the
information available (proof + transaction contents).
All that is required after the above soft-fork is a protocol version
update and/or a service bit to indicate the ability to send or receive
proof-prefixed messages. I'd call that an incremental update.
[Aside: adding the wallet index requires storing the entire UTXO set
in duplicated form, indexed this time by scriptPubKey or
H(scriptPubKey), and including proofs of this structure as well. It is
unlikely that any soft-fork would occur forcing consensus over the
wallet index, but it could be done as a meta-chain or as an index
covering just the contents of the block.]

@_date: 2014-01-15 16:24:03
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Stealth Addresses 
Hash: SHA1
Say it like it is. This is the only suggestion so far that I really like.
No amount of finger wagging got people to stop using the block chain
for data storage, but news of the OP_RETURN change to relay rules in
0.9 got people to at least be less damaging in how they do it.
Having an officially named "reusable address" format won't stop people
from doing dumb things (e.g. vanity addresses), but maybe
they'll stop using traditional addresses for it.

@_date: 2014-01-17 01:19:03
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Stealth Addresses 
Hash: SHA1
Too close to private key, IMHO.
And ninjas.

@_date: 2014-01-17 13:04:16
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bitcoin Core 0.9rc1 release schedule 
Hash: SHA1
CPFP is *extremely* important. People have lost money because this
feature is missing. I think it's critical that it makes it into 0.9
If I get a low-priority donation from a blockchain.info wallet, that
money can disappear if it doesn't make it into a block in 24 hours -
bc.i will forget the transaction and happily respend its inputs on the
next transaction that user makes.
I wouldn't mind paying $1 in fees to receive a $50 donation. But
without CPFP there's no way to do that.

@_date: 2014-01-20 14:01:40
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP0039: Final call 
Hash: SHA1
Since you are taking the hash of Unicode data, I would strongly
recommend using a canonical form, e.g. Normalized Form C.
CenturyLink Cloud: The Leader in Enterprise Cloud Services.

@_date: 2014-01-20 15:18:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BIP0039: Final call 
Hash: SHA1
Proper Unicode handling is a serious issue however. You don't want
someone to move from one input method / machine to another and
suddenly find that their coins are inaccessible, because of an issue
of decomposed vs. compatibility forms or whatever.

@_date: 2014-07-17 09:35:20
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Mining Hashrate Caps 
Can someone explain to these guys and the public why promising to limit
yourselves to *only* a 50% chance of successfully double-spending a 6
confirm transaction is still not acceptable?
z=0	P=1
z=1	P=0.828861
z=2	P=0.736403
z=3	P=0.664168
z=4	P=0.603401
z=5	P=0.550625
z=6	P=0.50398
z=7	P=0.462301
z=8	P=0.424782
z=9	P=0.390828
z=10	P=0.359976
z=11	P=0.331858
z=12	P=0.306167
z=13	P=0.282649
z=14	P=0.261083
z=15	P=0.24128
z=16	P=0.223076
z=17	P=0.206324
z=18	P=0.190896
z=19	P=0.176676

@_date: 2014-06-05 07:28:04
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Future Feature Proposal - getgist 
The correct approach here is header hash-tree commitments which enable
compact (logarithmic) SPV proofs that elide nearly all intervening
headers since the last checkpoint. You could then query the hash tree
for references to any of the headers you actually need.
See this message for details:

@_date: 2014-06-13 23:01:28
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Going to tag 0.9.2 final 
Not when failure is defined as, e.g., extra text pushing a UI element
down such that the button the user needs to click is no longer visible.
You don't test that except by having a human being run through some
example workflows, which is presumably happening during the release process.

@_date: 2014-06-17 15:28:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Fidelity bonds for decentralized instant 
Not with current script, but there are mechanisms by which you can do a
digital signature where signing two pieces of information reveals the
ECDSA k parameter, thereby allowing anyone to recover the private key
and steal the coins.
Practically speaking, these are not very safe systems to use. For
example, imagine accidentally loading up the same wallet on two machines
or the wallet software crashing after signing and sending the
transaction, and the user recreates & sends on recovery.
It also invalidates reasonably legitimate use cases for repeating
addresses (in the absence of other solutions), and its not really
possible to prevent people from sending multiple coins to the same
address (which could then be stolen).

@_date: 2014-06-19 10:17:50
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BlockPow: A Practical Proposal to prevent 
Sergio, why is preventing mining pools a good thing? The issue is not
mining pools, which provide a needed service in greatly reducing
variance beyond what any proposal like this can do.
The issue is centralized transaction selection policies, which is
entirely orthogonal. And the solution already exists: getblocktemplate.
We just need more or better infrastructure which makes use of this
technology to perform local transaction selection.
If you have a proposal for eliminating hosted mining while keeping
variance-reducing pools, that would be an interesting read.

@_date: 2014-06-19 13:39:13
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BlockPow: A Practical Proposal to prevent 
Do you need to do full validation? There's an economic cost to mining
invalid blocks, and even if that were acceptable there's really no
reason to perform such an attack. The result would be similar to a block
withholding attack, but unlike block withholding it would be trivially
detectable if/when full validation was performed.
To protect yourself and to detect incorrect mining software you could
stochastically validate a randomly selected sample of shares, as your
hardware requirements allow.

@_date: 2014-06-19 14:07:17
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] BlockPow: A Practical Proposal to prevent 
It's not the pool operator's business what software the miner is running
to select transactions for his block, so long as the miner follows the
template and doesn't generate invalid blocks. We already discussed
invalid blocks, and checking the template requires parsing the coinbase
transaction and verifying the merkleroot. The most expensive operations
are the hashes in the merkleroot verification, but you have to do that
in stratum too because of the extranonce, right?

@_date: 2014-03-01 10:28:22
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Decentralized digital asset exchange with 
Only if you view bitcoin as no more than a payment network.

@_date: 2014-03-13 10:18:51
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
This ship may have already sailed, but...
Using milli- and micro- notation for currency units is also not very
well supported. Last time this thread was active, I believe there was a
suggestion to use 1 XBT == 1 uBTC. This would bring us completely within
the realm of supported behavior in accounting applications.

@_date: 2014-03-14 09:01:33
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] moving the default display to mbtc 
A cup of coffee in Tokyo costs about 55 yen. You see similar magnitude
numbers in both Chinas, Thailand, and other economically important East
Asian countries. Expect to pay hundreds of rupees in India, or thousands
of rupees in Indonesia.
This concept that money should have low, single digits for everyday
prices is not just Western-centric, it's English-centric. An expresso in
Rome would have cost you a few (tens of?) thousand lira in recent
memory. It was pegging of the Euro to the U.S. dollar that brought
European states in line with the English-speaking world (who themselves
trace lineage to the pound sterling).
No, there is no culturally-neutral common standards for currency and
pricing. But there are ill-advised, ill-informed "standards" in
accounting software that we nevertheless must live with. These software
packages do not handle more than two decimal places gracefully. That
gives technical justifications for moving to either uBTC or accounting
in Satoshis directly. An argument for uBTC is that it retains alignment
with the existing kBTC/BTC/mBTC/uBTC conventions.
However another limitation of these accounting software practices is
that they do not always handle SI notation very well, particularly
sub-unit prefixes. By relabeling uBTC to be a new three-digit symbol
(XBT, XBC, IBT, NBC, or whatever--I really don't care), we are now fully
compliant with any software accounting package out there.
We are still very, very early in the adoption period. These are changes
that could be made now simply by a few big players and/or the bitcoin
foundation changing their practice and their users following suit.

@_date: 2014-03-17 10:24:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Compact SPV proofs via block header 
Hash: SHA1
In simple payment verification (SPV) proofs it is currently necessary
that every intervening block header be provided between two blocks in
order to establish both connectivity and proof of work. By committing to
a hash tree of past block headers in each block, these back references
can be exploited to demonstrate in logarithmic space that an elided
sequence of block headers actually represents the claimed work. This is
particularly useful in the construction of 2-way pegging between chains,
and as an efficiency optimization for SPV clients and headers-first
The miner of a block is allowed to choose some subset of the block
history which it creates direct links back to. These links include the
block hash, height, and work distance from the new block, and are
organized into a hash tree structure. The root of this hash tree is
committed to the coinbase string, or some other identifiable position in
the block. Bitcoin is soft-forked to include verification of the
accuracy of the contents of this structure - if present - as a
validation rule.
When constructing an SPV proof, the prover is allowed to choose
back-links from this structure whose relative work distance is less than
or equal to the apparent work of the proof-of-work which contains the
back-link structure. Apparent work in this context means the expected
work that would be required to meet or exceed the actual block hash.
Since back-links can only be used if the apparent work is greater than
or equal to the distance back, constructing such a fraudulent proof is
expected to require just as much or more work as recreating the
underlying sequence of blocks.
The result is skip list structure where "lucky" proofs of work are used
to skip back further than a single block (recall that if you search for
a 64-bit leading-0 proof of work, half the time you get 65 leading-0's,
a quarter of those cases have 66 leading-0's, and so on). When
constructing very long proofs, the solver will follow links back to the
nearest lucky block, then use the back-links contained within that block
to skip back to a prior lucky block, and so on until it reaches a block
which points directly to the desired target block. Given the
distribution of "lucky" blocks, it is expected that such compact proofs
require revelation of log2 N links in order to prove the work required
to build a chain of length N.
==Back link selection==
In fully general form, this compact SPV proof scheme works no matter the
back-links chosen by miners, so long as they are either revealed or
selected in a deterministic way such that full nodes can check their
validity. In choosing which back-links to include, the primary trade-off
is that more back-links results in better connectivity, whereas a
limited number of links results in smaller tree structures and therefore
fewer hashes.
At one extreme you can commit to every single header in the entire
history of the block chain, by building an incremental data structure
such as a binary heap. Such a structure would require log N storage per
chain and log N hash operations per block update, where N is the total
number of blocks in the chain. The root hash of this structure is then
committed to the block chain in a known location.
At the other extreme one could allow the miner instead to commit
whatever back-links he or she desires, and force these the hash tree
structure to be revealed prior to block validation. This allows the
miner to be selective in choosing back-links which provide the most
value, although there is not yet a clearly optimal mechanism for
choosing these links.
This is an area which requires more research with the purpose of
determining the best structure for the hash tree of block header
commitments, and the selection of back-links.
==Use cases==
For SPV clients, a client that has just come online could quickly
ascertain which block represents the most work, and retrieve compact
proofs-of-inclusion for its transactions without having to download
every intervening block header. This further eliminates the need for
checkpoints in an SPV client as it can instead obtain very compact
proofs back to the genesis block instead of the most recent checkpoint,
at a comparable cost. Similar optimizations apply to the initial stages
of headers-first synchronization of full nodes.
Assuming the availability of (U)TxO hash-tree commitments, a compact SPV
proof would allow a mobile client to very quickly fast-forward to the
current most-work block from which it could then query the spend status
of its wallet outputs.
For merged-mined or slow proof-of-work side chains, the savings from
not including every intervening block header could be very significant
in bandwidth and processing time.
Compact SPV proofs allow side chains or private accounting servers to
experiment with very short block intervals without having a detrimental
effect on SPV proof sizes, as the compact proofs scale logarithmically
with the number of blocks.
Finally the most important and driving use case: symmetrical two-way
pegging between bitcoin and side-chains is made efficient enough to be
reduced to practice by the availability of compact SPV proofs[1]. The
compact SPV proofs allow both the necessary proofs-of-spend and
proofs-of-reorg to fit within current blockchain size limitations, and
provide incentives for keeping this data out of the block chain until it
is absolutely necessary.
This specific compact SPV proof proposal arose from pegging discussion
involving a number of people  Greg Maxwell, Matt
Corallo, Pieter Wuille, Luke-Jr, Jorge Timon, and Mark Friedenbach. It
is believed that the first explanation of this general idea is due to
Andrew Miller in his 7 Aug 2012 forum post titled "The High-Value-Hash

@_date: 2014-03-22 10:04:30
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
Please, by all means: ignore our well-reasoned arguments about
externalized storage and validation cost and alternative solutions.
Please re-discover how proof of publication doesn't require burdening
the network with silly extra data that must be transmitted, kept, and
validated from now until the heat death of the universe. Your failure
will make my meager bitcoin holdings all the more valuable! As despite
persistent assertions to the contrary, making quality software freely
available at zero cost does not pay well, even in finance. You will not
find core developers in the Bitcoin 1%.
Please feel free to flame me back, but off-list. This is for *bitcoin*

@_date: 2014-03-22 09:55:18
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
Jeff, there are *plenty* of places that lack local Internet access for
one or both participants.
Obviously making the case where both participants lack access to the
bitcoin network is difficult to secure, but not impossible (e.g. use a
telephany-based system to connect to a centralized double-spend
database, as VISA does).
I expect the case where one participant has Internet access (the
merchant) and the other does not to be very, very common. The majority
of transactions I do on a daily basis are like this, and I live in
Silicon Valley!

@_date: 2014-03-23 16:53:48
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
This isn't distributed-systems-development, it is bitcoin-development.
Discussion over chain parameters is a fine thing to have among people
who are interested in that sort of thing. But not here.

@_date: 2014-03-24 13:57:14
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Handling miner adoption gracefully for 
A discussion over such a system would be on-topic. Indeed I have made my
own proposals for systems with that capability in the past:
There's no reason to invoke alts however. There are ways where this can
be done within the bitcoin ecosystem, using bitcoins:
Bitcoin is not a centralized system, and neither is its development. I
don't even know how to respond to that. Bringing up altchains is a total
red herring.
This is *bitcoin*-development. Please don't make it have to become a
moderated mailing list.

@_date: 2014-03-25 14:03:57
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Tree-chains preliminary summary 
I'm afraid I'm going to be the jerk that requested more details and then
only nitpicks seemingly minor points in your introduction. But its
because I need more time to digest the contents of your proposal. Until
This isn't true. The re-org issue is fairly handled in the 2-way pegging
scheme that Greg Maxwell developed and Adam Back described a week ago on
this list. Depending on the implementation it could even be configurable
by the person performing the peg too - allowing the transfer to specify
the confirmation depth required during the quieting period in order to
protect against re-orgs up to a sufficient depth. I think this is worked
out quite well with sufficient enumeration of edge cases, and I don't
think they are particularly tricky to handle or lead to money-losing
situations under the explicit security assumptions.
More importantly, to your last point there is absolutely no way this
scheme can lead to inflation. The worst that could happen is theft of
coins willingly put into the pegging pool. But in no way is it possible
to inflate the coin supply.
I will look at your proposal in more depth. But I also think you should
give 2-way pegging a fair shake as pegging to side chains and private
accounting servers may eliminate the need.

@_date: 2014-03-29 13:10:00
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Best practices for dust remining 
NONE|ANYONECANPAY. This is what dust-be-gone does.

@_date: 2014-05-03 11:55:12
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bug with handing of OP_RETURN? 
I don't think such a pull request would be accepted. The point was to
minimize impact to the block chain. Each extras txout adds 9 bytes
minimum, with zero benefit over serializing the data together in a
single OP_RETURN.

@_date: 2014-05-03 12:15:35
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Bug with handing of OP_RETURN? 
Is it more complex? The current implementation using template matching
seems more complex than `if script.vch[0] == OP_RETURN &&
script.vch.size() < 42`

@_date: 2014-05-21 13:30:25
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
Honest question: what would signed commits do to help us here anyway?
What's the problem being solved?
Unfortunately git places signatures in the history itself, so it's not
like we could use easily use signatures to indicate acceptance after
code review, like we could if we were using monotone for example. Git
just wasn't designed for a commit-signing workflow.

@_date: 2014-05-23 09:38:02
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
I know the likelihood of this happening is slim, but if these are the
desired features we should consider switching to monotone (monotone.ca)
which has a much more flexible DAG structure and workflow built around
programmable multi-sig signing of commits. We could still maintain the
github account as a two-way repository interface, but acceptance of a
pull request would require some threshold signature sign-off in monotone.
I would seriously suggest anybody on this list exploring monotone if you
haven't already, at least for your personal projects if it is too late
to make that choice for bitcoin. Besides the benefits of using it, we
should be supporting build infrastructure that enables less trusted,
less centralized development.

@_date: 2014-05-29 17:06:40
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] bitcoind minor bug in wallet and possible 
Please make a pull request on github. It'll likely get merged quickly.
