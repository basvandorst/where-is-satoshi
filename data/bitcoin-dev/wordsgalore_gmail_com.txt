
@_date: 2017-11-02 19:31:55
@_author: Scott Roberts 
@_subject: [bitcoin-dev] Bitcoin Cash's new difficulty algorithm 
Bitcoin cash will hard fork on Nov 13 to implement a new difficulty
algorithm.  Bitcoin itself might need to hard fork to employ a similar
algorithm. It's about as good as they come because it followed the
"simplest is best" route. Their averaging window is probably
significantly too long (N=144). It's:
next_D = sum (past 144 D's) * T / sum(past 144 solvetimes)
They correctly did not use max(timestamp) - min(timestamp) in the
denominator like others do.
They've written the code and they're about to use it live, so Bitcoin
will have a clear, simple, and tested path if it suddenly needs to
hard fork due to having 20x delays for the next 2000 blocks (taking it
a year to get unstuck).
Details on it and the decision process:
It uses a nice median of 3 for the beginning and end of the window to
help alleviate bad timestamp problems. It's nice, helps a little, but
will also slow its response by 1 block.  They also have 2x and 1/2
limits on the adjustment per block, which is a lot more than they will
ever need.
I recommend bitcoin consider using it and making it N=50 instead of 144.
I have seen that any attempts to modify the above with things like a
low pass filter, starting the window at MTP, or preventing negative
timestamps will only reduce its effectiveness. Bitcoin's +12 and -6
limits on the timestamps are sufficient and well chosen, although
something a bit smaller than the +12 might have been better.
One of the contenders to the above is new and actually better, devised
by Degnr8 and they call it D622 or wt-144.It's a little better than
they realize. It's the only real improvement in difficulty algorithms
since the rolling average.  It gives a linearly higher weight to the
more recent timestamps. Otherwise it is the same. Others have probably
come across it, but there is too much noise in difficulty algorithms
to find the good ones.
# Degnr8's D622 difficulty algorithm
# T=TargetTime, S=Solvetime
# modified by zawy
for i = 1 to N  (from oldest to most recent block)
    t += T[i] / D[i] * i
    j += i
next i
next_D = j / t * T
I believe any modification to the above strict mathematical weighted
average will reduce it's effectiveness. It does not oscillate anymore
than regular algos and rises faster and drops faster, when needed.

@_date: 2017-11-02 19:53:25
@_author: Scott Roberts 
@_subject: [bitcoin-dev] Bitcoin Cash's new difficulty algorithm 
Whatever their failings from their previous code or their adversarial
nature, they got this code right and I'm only presenting it as a real and
excellent solution for the impending threat to bitcoin. As a big core fan,
I really wanted to delete the word Cash from my post because I was afraid
someone would turn this technical discussion into a political football.
On Thu, Nov 2, 2017 at 11:31 PM, Scott Roberts via bitcoin-dev
This is the bitcoin development mailing list, not the "give free
review to the obviously defective proposals of adversarial competing
systems" mailing list. Your posting is off-topic.

@_date: 2017-11-02 21:59:47
@_author: Scott Roberts 
@_subject: [bitcoin-dev] Bitcoin Cash's new difficulty algorithm 
The current DA is only sufficient if the coin has the highest
hashpower. It's also just really slow.  If miners somehow stick with
SegWit2x despite the higher rewards in defecting back to bitcoin, then
bitcoin will have long block delays. High transaction fees will
probably help them defect back to us. But if SegWit2x manages to be
more comparable in price than BCH (despite the futures), hashpower
could very well oscillate back and forth between the two coins,
causing delays in both of them. The first one to hard fork to fix the
difficulty problem will have a large advantage, as evidenced by what
happens in alts.   In any event someday BTC may not be the biggest kid
on the block and will need a difficulty algorithm that alts would find
acceptable. Few alts use anything like BTC's because they are not able
to survive the resulting long delays.   I am recommending BTC
developers watch what happens as BCH goes live with a much better
algorithm, in case BTC needs to hard fork for the same reason and
needs a similar fix. Ignore the trolls.

@_date: 2017-10-12 04:51:33
@_author: Scott Roberts 
@_subject: [bitcoin-dev] New difficulty algorithm needed for SegWit2x 
Since there is no surviving argument in this thread contrary to my original
post, I'll begin work on a BIP.

@_date: 2017-10-12 11:42:02
@_author: Scott Roberts 
@_subject: [bitcoin-dev] New difficulty algorithm part 2 
This is a good point I was not thinking about, but your math assumes
1/2 price for a coin that can do 2x more transactions. Holders like
Roger Ver have an interest in low price and more transactions. A coin
with 2x more transactions, 22% lower price, and 22% lower fees per
coin transferred will attract more merchants, customers, and miners
(they get 50% more total fees) and this will in turn attract more
hodlers and devs. This assumes it outweighs hodler security concerns.
Merchants and customers, to the extent they are not long term hodlers,
are not interested in price as much as stability, so they are somewhat
at odds with hodlers.
Bitcoin consensus truth is based on "might is right". Buyers and
sellers of goods and services ("users") can shift some might to miners
via fees, to the chagrin of hodlers who have more interest in security
and price increases. Some hodlers think meeting user needs is the
source of long term value. Others think mining infrastructure is. You
seem to require hodlers to correctly identify and rely solely on good
developers. Whatever combination of these is the case, bad money can
still drive out good, especially if the market determination is not
A faster measurement of hashrate for difficulty enables the economic
determination to be more efficient and correct. It prevents the
biggest coin from bullying forks that have better ideas. Conversely,
it prevents miners from switching to an inferior coin simply because
it provides them with more "protection money" from fees that enables
them to bully Bitcoin Core out of existence, even in the presence of a
slightly larger hodler support.
Devs are a governing authority under the influence of users, hodlers,
and miners. Miners are like banks lobbying government for higher total
fees. Hodlers are the new 1%, holding 90% of the coin, lobbying both
devs and users for security, but equally interested in price
increases. Users are "the people" that devs need to protect against
both hodlers and miners. They do not care about price as long as it is
stable. They do not want to become the 99% owning 10% of the coin or
have to pay unecessary fees merely for their coin to be the biggest
bully on the block.  A faster responding difficulty will take a lot of
hot air out of the bully. It prevents miners from being able to
dictate that only coins with high fees are allowed.  They are less
able to destroy small coins that have a fast defense.
The 1% and banks would starve the people that feed them to death if
they were allowed complete control of the government. Are hodlers and
miners any wiser? Devs need to strive for an expansion of the coin
quantity to keep value constant which is the foundation of the 5
characteristics of an ideal currency.  Therefore devs should seek
peaceful and sustainable forks of bitcoin. This will enable constant
value, security, and low transaction fees per coin transfer. Alts
aside, the current situation of discouraging forks forbids constant
value via limited quantity.  It also forces a choice between high
security and low fees.  Forks with a faster difficulty will be more
capable of retaining value.
Users, devs, hodlers, and miners are naturally aligned and at odds in
different ways. A flow chart of the checks and balances should enable
better development towards a self-controlling feedback system, but the
goals need to be known before it could be designed and implemented.
Hodlers say price increases is the goal. Users say efficient transfer
of value. Miners say fees (at least that's the end game after mining).
I'm with users despite trying to be the 1% (which reminds me of a book
about how people often vote based on feeling good about their morality
and concern for society as a whole, despite it being contrary to their
personal best interests if that vote wins.)

@_date: 2017-10-13 07:35:09
@_author: Scott Roberts 
@_subject: [bitcoin-dev] New difficulty algorithm part 2 
Yes, the current price ratio indicates there is no need for a new
difficulty algorithm. I do not desire to fork before a disaster, or to
otherwise employ a new difficulty before a fork is otherwise needed.
A 2-week delay in difficulty response is a 2 week error in
measurement. Slow response generally means less intelligence.
My goal is not to have a bunch of BTC clones that merchants and buyers
use equally, but to have a  better difficulty algorithm in place to be
used in the next BTC "Core" fork. If not for the current situation,
then for future security.
You mean multiple forks is inflationary. The current limit in quantity
is deflationary because the use of the coin is rising faster than its
mining is producing (see velocity of money). Constant value is defined
as being neither. Bitcoin's deflationary quality created a massive
marketing advantage as well as paid the creator about million dollars
an hour. If it suddenly were able to be a constant value coin, its use
in the marketplace and as a real store of value would skyrocket and
the cries of "Ponzi scheme" would stop. The trick is in determining
constant value without a 3rd party such as an index of a basket of
commodities (which both Keynes and von Mises wanted, but was scuttled
by the U.S. at Bretton Woods).

@_date: 2017-10-13 09:57:33
@_author: Scott Roberts 
@_subject: [bitcoin-dev] bitcoin-dev Digest, Vol 29, Issue 21 
I generally agree with ZmnSCPxj that
good ideas => good devs => hodlers => price => mining
Except that each step is not an absolute, and can be biased by things
like miners who seek profit via fees and other means that are not good
for everyone else. Llansky's belief itself influences price away from
the ideal. Marketing "easy profits for hodlers!" and first-to-market
monopoly are other elements that influence price and thereby guide
mining away from good ideas (like a constant value currency). Then
price pulls in good devs that pulls in more mining. So it can snowball
into a monster.
We need not debate cause and effect since it's distant from the list's
goals. The relevance to me is that the biases away from ZmnSCPxj's
ideal are a reason a more responsive difficulty is needed.
Mining is for determining truth of the blockchain, not to make sure
there is only 1 blockchain. ZmnSCPxj indicates we should not do
anything that has more precision or speed in determining the correct
difficulty if it reduces Bitcoin's ability to be a monopoly. Not
coincidentally, the monopoly helps ensure hodlers become the new 1%. A
fork clone that uses the faster difficulty would attack BTC's slow
difficulty if it achieves a comparable price. All other things being
equal, it would lower BTC's value until it forks to fix the
On Fri, Oct 13, 2017 at 8:27 AM, Ilan Oh via bitcoin-dev

@_date: 2018-08-29 05:54:17
@_author: Zawy 
@_subject: [bitcoin-dev] Getting around to fixing the timewarp attack. 
Rather than restricting every timestamp (or just the 2016*N+1
timestamps) to >= 1+ the previous timestamp as recorded on the
blockchain, the difficulty calculation could have the same restriction
but only in how the timestamps are used. I don't know about backwards
compatibility.  Either way, this would also prevent the powLimit
attack that is also capable of getting "unlimited" blocks in less than
4 weeks of > 50% selfish mining.  LTC, BCH, and LTC fixed to the
"Zeitgeist" or "timewarp" attack on GeistGeld in 2011 described by
Artforz in different ways, but all are still vulnerable by the
powLimit attack that I described here:
Other solutions may not prevent this other attack.

@_date: 2018-12-05 09:08:56
@_author: Zawy 
@_subject: [bitcoin-dev] How much is too much time between difficulty 
It's possible to let the difficulty linearly drop as the solvetime
goes beyond some limit (credit AS). If the limit is greater than any
delay in the past it could be backwards-compatible.
A simple daily-rolling average DA like BCH is probably the best option
if a faster DA is ever needed.
As a point of research interest (not likely to be needed by BTC), I've
taken the first above idea of "intra-block" timestamp-based difficulty
adjustment to the limit and made it symmetrical (higher D for fast
solves) and continuous. The result is a "tightening of the Poisson"
that increases "availability" (predictable solution times) at an
expense in "consistency" (orphans). It requires a very tight future
time limit to reduce timestamp manipulation. My objective was to help
small coins deal with persistent 20x hash rate changes that result in
long delays. About 3 coins have it on testnet.

@_date: 2018-09-17 10:09:23
@_author: Zawy 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
The 51% problem is deep. Any discussion of a solution to it should
begin with a link to an article that shows a profound discovery has
been made. Selfish mining prevention and pollution should be on
bitcoin-discussion, but it appears that list is not active.
The problem with Andrew's idea below is that it is a positive feedback
loop that amplifies oscillations. If h goes up or down due to price
changes or random solvetime variation, then the net reward goes in the
same direction, which motivates miners to cause h to go even further
in the same direction, which is a positive feedback loop until some
limit is reached. To make matters worse, miner profit motivation in
choosing which coin to mine is a non-linear function: a 30% drop in
difficulty (or 30% increase in this reward function) in an alt coin
can cause a 300% increase in hashrate.
Average of 144 past blocks to determine h are needed so that it does
not vary too much.  A selfish mine of 72 blocks would result in only a
12.5% loss compared to not using this pro-oscillation function. I've
tried similar reward functions in trying to reduce on-off mining.
There may also be a problem of issuing too many or too few coins,
depending on how fast h rises in the long term.
An alternative is to increase difficulty with this or a similar
function instead of reward. From a miner's perspective, there is not a
difference (they are only interested in the (price+fees)/difficulty
ratio. This would have the same problems.
The problem has been solved to the best of our ability by the Nakamoto
consensus. The math is straightforward, so you can't get around it's
failings unless it's a profound solution or we shift trust to some
place else. Currently we have to choose and trust a small group of
coins (or 1) to be the best choice(s), and to trust that the reward
plus fees we pay for mining (compared to coin value) is enough to
prevent a 51% attack.

@_date: 2019-05-18 13:40:16
@_author: Zawy 
@_subject: [bitcoin-dev] Code not following proof of security 
If MAX_FUTURE_BLOCK_TIME in chain.h is set smaller than
DEFAULT_MAX_TIME_ADJUSTMENT in timedata.h, the POW security can be
undermined by a 33% Sybil attack on the nodes. Blocks with accurate
timestamps would be rejected which allows various attacks. Code should
reflect a proof of security, so it should be coded as
DEFAULT_MAX_TIME_ADJUSTMENT = MAX_FUTURE_BLOCK_TIME / 2
(or sufficiently commented) otherwise future developers could make a
change that hurts security. "Unintended consequences due to how
disparate code interacts" is the result of code not following a proof
of security. I came across this while trying to "derive" POW security
from within Lamport's 1982 framework. The problem is that POW security
requires clock synchronization. But using median of network time for
it is a consensus mechanism that is subject to Byzantine attacks. So
POW requires an absolute bound on time (enforced by an oracle) that is
at least as stringent as  the allowed timestamp variation.  The rule
to revert to node time if network time is >70 minutes off is the real
bound that honest nodes can impose unilaterally, limiting the
potential damage of consensus (if MAX_FUTURE_BLOCK_TIME is not too
small). This fail-safe uses node operators as the oracle, who can all
approximately agree as to what time it is without asking each other. A
can unilaterally reject the chain if the current timestamp is not
realistic. Cryptonote appears to have done away with network time
without ill effect. The only other option to "the node operator is the
oracle" is to assume all internal clocks have a max drift, but this
would disconnect timestamps from real time to the extent of that drift
(if I'm reading Halpern, etc 1984 IBM correctly). I'm assuming Mike
Hearn was wrong in saying the centralization of NTP (or GPS) is
This affects coins who reduced MAX_FUTURE_BLOCK_TIME without either
removing the time consensus mechanism or reducing the
DEFAULT_MAX_TIME_ADJUSTMENT. Many have done this in order to have
faster responding difficulty algorithms, otherwise a large
MAX_FUTURE_BLOCK_TIME allows a sizable manipulation of difficulty.
Therefore, MAX_FUTURE_BLOCK_TIME should itself should be a function of
the size of the difficulty window for proof of security (instead of a
constant). I suspect more constants = less proof of security. For
MFBT = WindowTimespan / 10 would limit timestamp manipulation to 10% per window.
