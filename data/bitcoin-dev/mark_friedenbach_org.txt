
@_date: 2015-04-16 10:34:31
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
At this moment anyone can alter the txid. Assume transactions are 100%

@_date: 2015-08-07 11:25:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Please don't put words into Pieter's mouth. I guarantee you everyone
working on Bitcoin in their heart of hearts would prefer everyone in the
world being able to use the Bitcoin ledger for whatever purpose, if there
were no cost.
But like any real world engineering issue, this is a matter of tradeoffs.
At the extreme it is simply impossible to scale Bitcoin to the terrabyte
sized blocks that would be necessary to service the entire world's
financial transactions. Not without sacrificing entirely the protection of
policy neutrality achieved through decentralization. And as that is
Bitcoin's only advantage over traditional consensus systems, you would have
to wonder what the point of such an endeavor would be.
So *somewhere* you have to draw the line, and transactions below that level
are simply pushed into higher level or off-chain protocols.
The issue, as Pieter and Jorge have been pointing out, is that technical
discussion over where that line should be has been missing from this debate.
On Fri, Aug 7, 2015 at 10:47 AM, Ryan Butler via bitcoin-dev <

@_date: 2015-08-07 12:15:34
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Surely you have some sort of empirical measurement demonstrating the
validity of that statement? That is to say you've established some
technical criteria by which to determine how much centralization pressure
is too much, and shown that Pieter's proposal undercuts expected progress
in that area?

@_date: 2015-08-07 14:27:48
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] If you had a single chance to double the 
Because halving the block interval comes with costs to SPV proofs (which
double the number of headers) and mining centralization (doubles the
selfish mining effect of latency) for the questionable benefit of a block
expectation time that is still measured in minutes, not seconds.
Doubling the block size is safer than halving the block interval, for the
same effect in aggregate transactions per second.
On Fri, Aug 7, 2015 at 2:18 PM, Sergio Demian Lerner via bitcoin-dev <

@_date: 2015-08-07 16:01:05
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] If you had a single chance to double the 
Actually I gave a cached answer earlier which on further review may need
updating. (Bad Mark!)
I presume by "what's more likely to matter is seconds" you are referencing
point of sale. As you mention yourself, lightning network or green address
style payment escrow obviates the need for short inter-block times.
But with lightning there is a danger of channels being exhausted in the
time between blocks, causing the need for new channels to be established.
So lightning does in fact benefit from moderately shorter inter-block
times, although how much of an issue this will be is anyone's guess now.
Still the first two points about larger SPV proofs and selfish mining still
hold true, which sets the bar particularly high for justifying more
frequent blocks.

@_date: 2015-08-07 16:17:31
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] If you had a single chance to double the 
Then I would suggest working on payment channel networks. No decrease of
the interblock time will ever compete with the approximately instant time
it takes to validate a microchannel payment.
On Fri, Aug 7, 2015 at 4:08 PM, Sergio Demian Lerner <

@_date: 2015-08-08 11:56:32
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] The use of tx version field in BIP62 and 68 
It is not a bug that you are unable to selectively choose these features
with higher version numbers. The version selection is in there at all
because there is a possibility that there exists already signed
transactions which would violate these new rules. We wouldn't want these
transactions to become unspendable. However moving forward there is no
reason to selectively pick and choose which of these new consensus rules
you want to apply your transaction.
On Aug 8, 2015 11:51 AM, "jl2012 via bitcoin-dev" <

@_date: 2015-08-09 09:02:40
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Alternative chain support for payment protocol 
A sha256 hash is 32 bytes, but otherwise I agree with this proposal.
Genesis block hash is the logical way to identify chains, moving forward.
On Aug 9, 2015 7:12 AM, "Ross Nicoll via bitcoin-dev" <

@_date: 2015-08-09 11:54:05
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] What Lightning Is 
Tom, you appear to be misunderstanding how lightning network and
micropayment hub-and-spoke models in general work.
advanced it to the channel (or (2) below applies).  Nothing requires the
payment hub to do this.
On the contrary the funds were advanced by the hub on the creation of the
channel. There is no credit involved. if the funds aren't already available
for Bob to immediately claim his balance, the payment doesn't go through in
the first place.
On Sun, Aug 9, 2015 at 11:46 AM, Tom Harding via bitcoin-dev <

@_date: 2015-08-10 23:03:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Mon, Aug 10, 2015 at 10:34 PM, Thomas Zander via bitcoin-dev <
This is where things diverge. It's fine to pick a new limit or growth
trajectory. But defend it with data and reasoned analysis.
Can you at least understand the conservative position here? "1MB sounds
good to me" is how we got into this mess. We must make sure that we avoid
making the same mistakes again, creating more or worse problems then we are

@_date: 2015-08-11 00:08:42
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Mon, Aug 10, 2015 at 11:31 PM, Thomas Zander via bitcoin-dev <
I'm sorry, I really don't want to sound like a jerk, but not a single word
of that mattered. Yes we all want Bitcoin to scale such that every person
in the world can use it without difficulty. However if that were all that
we cared about then I would be remiss if I did not point out that there are
plenty of better, faster, and cheaper solutions to finding global consensus
over a payment ledger than Bitcoin. Architectures which are algorithmically
superior in their scaling properties. Indeed they are already implemented
and you can use them today:
So why do I work on Bitcoin, and why do I care about the outcome of this
debate? Because Bitcoin offers one thing, and one thing only which
alternative architectures fundamentally lack: policy neutrality. It can't
be censored, it can't be shut down, and the rules cannot change from
underneath you. *That* is what Bitcoin offers that can't be replicated at
higher scale with a SQL database and an audit log.
It follows then, that if we make a decision now which destroys that
property, which makes it possible to censor bitcoin, to deny service, or to
pressure miners into changing rules contrary to user interests, then
Bitcoin is no longer interesting. We might as well get rid of mining at
that point and make Bitcoin look like Stellar or Open-Transactions because
at least then we'd scale even better and not be pumping millions of tons of
CO2 into the atmosphere from running all those ASICs.
On the other side, 3Tb harddrives are sold, which take 8Mb blocks without
Straw man, storage is not an issue.
Neither one of those assertions is clear. Keep in mind the goal is to have
Bitcoin survive active censorship. Presumably that means being able to run
a node even in the face of a hostile ISP or government. Furthermore, it
means being location independent and being able to move around. In many
places the higher the bandwidth requirements the fewer the number of ISPs
that are available to service you, and the more visible you are.
It may also be necessary to be able to run over Tor. And not just today's
Tor which is developed, serviced, and supported by the US government, but a
Tor or I2P that future governments have turned hostile towards and actively
censor or repress. Or existing authoritative governments, for that matter.
How much bandwidth would be available through those connections?
It may hopefully never be necessary to operate under such constraints,
except by freedom seeking individuals within existing totalitarian regimes.
However the credible threat of doing so may be what keeps Bitcoin from
being repressed in the first place. Lose the capability to go underground,
and it will be pressured into regulation, eventually.
To the second point, it has been previously pointed out that large miners
stand to gain from larger blocks, for the same basic underlying reasons as
selfish mining. The incentive is to increase blocks, and miners are able to
do so at will and without cost. I would not be so certain that we wouldn't
see large blocks sooner than that.
This is basically already deployed thanks to Matt's relay network. Further
improvements are not going to have dramatic effects.
No, it doesn't. And 8GB/block is ludicrously large -- it would absolutely,
without any doubt destroy the very nature of Bitcoin, turning it into a
fundamentally uninteresting reincarnation of the existing financial system.
And still be unable to compete with VISA/Mastercard.
So why then the pressure to go down a route that WILL lead to failure by
your own metrics?
I humbly suggest that maybe we should play the strengths of Bitcoin instead

@_date: 2015-08-11 11:48:57
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Michael, why does it matter that every node in the world process and
validate your morning coffee transaction? Why does it matter to anyone
except you and the coffee vendor?
On Tue, Aug 11, 2015 at 11:46 AM, Michael Naber via bitcoin-dev <

@_date: 2015-08-11 12:00:46
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fees and the block-finding process 
More people using Bitcoin does not necessarily mean more transactions being
processed by the block chain. Satoshi was forward-thinking enough to
include a powerful script-signature system, something which has never
really existed before. Though suffering from some limitations to be sure,
this smart contract execution framework is expressive enough to enable a
wide variety of new features without changing bitcoin itself.
One of these invented features is micropayment channels -- the ability for
two parties to rapidly exchange funds while only settling the final balance
to the block chain, and to do so in an entirely trustless way. Right now
people don't use scripts to do interesting things like this, but there is
absolutely no reason why they can't. Lightning network is a vision of a
future where everyone uses a higher-layer protocol for their transactions
which only periodically settle on the block chain. It is entirely possible
that you may be able to do all your day-to-day transactions in bitcoin yet
only settle accounts every other week, totaling 13kB per year. A 1MB block
could support that level of usage by 4 million people, which is many orders
of magnitude more than the number of people presently using bitcoin on a
day to day basis.
And that, by the way, is without considering as-yet uninvented applications
of existing or future script which will provide even further improvements
to scale. This is very fertile ground being explored by very few people.
One thing I hope to come out of this block size debate is a lot more people
(like Joseph Poon) looking at how bitcoin script can be used to enable new
and innovative resource-efficient and privacy-enhancing payment protocols.
The network has room to grow. It just requires wallet developers and other
infrastructure folk to step up to the plate and do their part in deploying
this technology.

@_date: 2015-08-11 12:13:14
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CFP Bitcoin Scalability Workshop (Sept 12-13), 
I want to put a big thank-you out to Pindar, Warren, and others in the
organizing committee who I know must have put in a lot of hours to make
this happen. I will be attending, and I hope to see many of you there too.
It is my sincere hope that the academic structure of a workshop will help
break down some of the communication walls that have arisen in this debate,
and help us all work towards finding a compromise towards scaling bitcoin,
something we all want to see happen.
On Tue, Aug 11, 2015 at 1:45 AM, Pindar Wong via bitcoin-dev <

@_date: 2015-08-13 11:12:43
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
As per the rules of BIP 1, I hereby request that the BIP editor please
assign an official number to this work. The idea has been discussed before
on the bitcoin-dev mailing list:
And a reference implementation is available here:
On Thu, Aug 13, 2015 at 4:06 AM, Btc Drak via bitcoin-dev <

@_date: 2015-08-13 17:47:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
On Thu, Aug 13, 2015 at 4:42 PM, Joseph Poon via bitcoin-dev <
This proposal includes no such provision.
Since we talked about it, I spent considerable time thinking about the
supposed risk and proposed mitigations. I'm frankly not convinced that it
is a risk of high enough credibility to worry about, or if it is that a
protocol-level complication is worth doing.
The scenario as I understand it is a hub turns evil and tries to cheat
every single one of its users out of their bonds. Normally a lightning user
is protected form such behavior because they have time to broadcast their
own transactions spending part or all of the balance as fees. Therefore
because of the threat of mutually assured destruction, the optimal outcome
is to be an honest participant.
But, the argument goes, the hub has many channels with many different
people closing at the same time. So if the hub tries to cheat all of them
at once by DoS'ing the network, it can do so and spend more in fees than
any one participant stands to lose. My issue with this is that users don't
act alone -- users can be assured that other users will react, and all of
them together have enough coins to burn to make the attack unprofitable.
The hub-cheats-many-users case really is the same as the
hub-cheats-one-user case if the users act out their role in unison, which
they don't have to coordinate to do.
Other than that, even if you are still concerned about that  scenario, I'm
not sure timestop is the appropriate solution. A timestop is a
protocol-level complication that is not trivial to implement, indeed I'm
not even sure there is a way to implement it at all -- how do you
differentiate in consensus code a DoS attack from regular old blocks
filling up? And if you could, why add further complication to the consensus
A simpler solution to me seems to be outsourcing the response to an attack
to a third party, or otherwise engineering ways for users to
respond-by-default even if their wallet is offline, or otherwise assuring
sufficient coordination in the event of a bad hub.

@_date: 2015-08-14 14:29:31
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
With the assumed malleability-fix CHECKSIG2 version of lightning, watching
for and responding to bad behavior is fully outsourceable. You can
synchronize channel state (signed refund transactions) with a third party
that watches for replay of old transactions on the mainnet, and starts the
refund process if it observes them, paying the fees necessary to get on the
With the CLTV/CSV-only form of the hash time-lock contracts that Rusty has
developed, this is indeed something the users' wallets would have to be
online to observe happening and respond to. I presume that we are
eventually going to get a CHECKSIG2 with some kind of malleability-immune
signing scheme in the long term, and that we are not interested in
introducing new consensus behavior to cover that short stopgap.
A regrettable choice of words. In this case it is game theoretic
cooperation, not coordination. The users need only expect that each other
would react the same way, being willing to burn money as fees that would
otherwise be stolen. They don't actually have to communicate with each
other in order to cooperate.
You are correct though that hubs-with-hashpower complicate this situation.
Although a hub with hashpower also creates risk in the timestop scenario
On Fri, Aug 14, 2015 at 11:53 AM, Matt Corallo

@_date: 2015-08-15 15:55:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
I would like very much to know how it is that we're supposed to be making
money off of lightning, and therefore how it represents a conflict of
interest. Apparently there is tons of money to be made in releasing
open-source protocols! I would hate to miss out on that.
We are working on lightning because Mike of all people said, essentially, "
if you're so fond of micro payment channels, why aren't you working on
them?" And he was right! So we looked around and found the best proposal
and funded it.
On Aug 15, 2015 3:28 PM, "Ken Friece via bitcoin-dev" <

@_date: 2015-08-15 16:40:14
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Baseless accusations also have no place on this mailing list. They are
unprofessional, and poisonous to the consensus-building process we all seek
to engage in.
The Lightning Network effort at Blockstream is purposefully structured to
avoid any conflict of interest. ALL code related to lightning is available
on Github. There is absolutely nothing that we are holding back, and the
protocol itself is entirely p2p. There is no privileged entity, Blockstream
or otherwise.
On Sat, Aug 15, 2015 at 4:07 PM, Eric Lombrozo via bitcoin-dev <

@_date: 2015-08-16 09:52:32
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Minimum Block Size 
Levin, it is a complicated issue for which there isn't an easy answer. Part
of the issue is that "block size" doesn't actually measure resource usage
very reliably. It is possible to support a much higher volume of typical
usage transactions than transactions specifically constructed to cause DoS
issues. But if "block size" is the knob being tweaked, then you must design
for the DoS worst case, not the average/expected use case.
Additionally, there is an issue of time horizons and what presumed
improvements are made to the client. Bitcoin Core today can barely handle
1MB blocks, but that's an engineering limitation. So are we assuming fixes
that aren't actually deployed yet? Should we raise the block size before
that work is tested and its performance characteristics validated?
It's a complicated issue without easy answers, and that's why you're not
seeing straightforward statements of "2MB", "8MB", or "20MB" from most of
the developers.
But that's not to say that people aren't doing anything. There is a
workshop being organized for September 12-13th that will cover much of
these "it's complicated" issues. There will be a follow-on workshop in the
Nov/Dec timeframe in which specific proposals will be discussed. I
encourage you to participate:
On Sun, Aug 16, 2015 at 9:41 AM, Levin Keller via bitcoin-dev <

@_date: 2015-08-18 18:08:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP: Using Median time-past as endpoint for 
You are absolutely correct! My apologies for the oversight in editing. If
you could dig up the link though that would be really helpful.
On Tue, Aug 18, 2015 at 6:04 PM, Peter Todd via bitcoin-dev <

@_date: 2015-08-18 23:10:25
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
We can use nVersion & 0x8 to signal support, while keeping the consensus
rule as nVersion >= 4, right? That way we don't waste a bit after this all
clears up.
On Aug 18, 2015 10:50 PM, "Peter Todd via bitcoin-dev" <

@_date: 2015-08-19 09:21:36
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I am indifferent on this issue (the bit inversion), but so far only Jorge
has spoken up. I opted for this detail during implementation in order to
preserve existing semantics, even if those semantics are not commonly used.
This was the conservative choice, driven in part because I didn't want the
proposal to be held up by the other side saying "this is confusing because
it changes how sequence numbers work! it used to count up but now it counts
I can see both sides and as I said I'm indifferent, so I went with the
conservative choice of not messing with existing semantics. However if
there is strong preferences from _multiple_ people on this matter it is not
too late to change. If anyone feels strongly about this, please speak up.
On Wed, Aug 19, 2015 at 3:37 AM, Jorge Tim?n <

@_date: 2015-08-19 09:32:49
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
I think you misunderstand my suggestion Tier. I am suggesting the same as
BtcDrak, I think:
Modify IsSuperMajority to take an (optional) mask field. Bits set in that
mask are zero'd for the purpose of the IsSuperMajority calculation. For
this vote we mask bits 0x20000007.
Thus to signal support for CLTV/CSV/etc. (on Core, XT, or not-XT), you set
bit 4. On Core this would mean a minimum version of 0x8, on XT/not-XT a
minimum version of 0x20000008.
However the vote is still over whether to enforce BIP 65, 68, etc. for
blocks with nVersion>=4. So when this all clears up we haven't needlessly
wasted an additional bit.
On Wed, Aug 19, 2015 at 6:22 AM, Tier Nolan via bitcoin-dev <

@_date: 2015-08-20 10:42:45
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CLTV/CSV/etc. deployment considerations due to 
No, the nVersion would be >= 4, so that we don't waste any version values.
On Thu, Aug 20, 2015 at 10:32 AM, jl2012 via bitcoin-dev <

@_date: 2015-08-23 19:37:20
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
A power of 2 would be far more efficient here. The key question is how long
of a relative block time do you need? Figure out what the maximum should be
( I don't know what that would be, any ideas?) and then see how many bits
you have left over.
On Aug 23, 2015 7:23 PM, "Jorge Tim?n" <

@_date: 2015-08-23 19:54:34
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Sorry this was meant for the list:
There are only 32 bits in the version field. If you're going to spend a bit
for perpetuity to indicate whether or not a feature is active, you'd better
have a good reason to make that feature optional.
I haven't seen a compelling use case for having BIP 68 be optional in that
way. As you note, BIP 68 semantics is already optional by toggling the most
significant bit, and that doesn't permanently burn a version bit.
On Aug 23, 2015 7:41 PM, "jl2012 via bitcoin-dev" <

@_date: 2015-08-25 15:08:32
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
To follow up on this, let's say that you want to be able to have up to 1
year relative lock-times. This choice is somewhat arbitrary and what I
would like some input on, but I'll come back to this point.
 * 1 bit is necessary to enable/disable relative lock-time.
 * 1 bit is necessary to indicate whether seconds vs blocks as the unit of
 * 1 year of time with 1-second granularity requires 25 bits. However since
blocks occur at approximately 10 minute intervals on average, having a
relative lock-time significantly less than this interval doesn't make much
sense. A granularity of 256 seconds would be greater than the Nyquist
frequency and requires only 17 bits.
 * 1 year of blocks with 1-block granularity requires 16 bits.
So time-based relative lock time requires about 19 bits, and block-based
relative lock-time requires about 18 bits. That leaves 13 or 14 bits for
other uses.
Assuming a maximum of 1-year relative lock-times. But what is an
appropriate maximum to choose? The use cases I have considered have only
had lock times on the order of a few days to a month or so. However I would
feel uncomfortable going less than a year for a hard maximum, and am having
trouble thinking of any use case that would require more than a year of
lock-time. Can anyone else think of a use case that requires >1yr relative
On Sun, Aug 23, 2015 at 7:37 PM, Mark Friedenbach

@_date: 2015-08-27 16:32:55
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
So I've created 2 new repositories with changed rules regarding
This repository inverts (un-inverts?) the sequence number. nSequence=1
means 1 block relative lock-height. nSequence=LOCKTIME_THRESHOLD means 1
second relative lock-height. nSequence>=0x80000000 (most significant bit
set) is not interpreted as a relative lock-time.
This repository not only inverts the sequence number, but also interprets
it as a fixed-point number. This allows up to 5 year relative lock times
using blocks as units, and saves 12 low-order bits for future use. Or, up
to about 2 year relative lock times using seconds as units, and saves 4
bits for future use without second-level granularity. More bits could be
recovered from time-based locktimes by choosing a higher granularity (a
soft-fork change if done correctly).
On Tue, Aug 25, 2015 at 3:08 PM, Mark Friedenbach

@_date: 2015-08-28 16:38:21
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
It is in their individual interests when the larger block that is allowed
for them grants them more fees.
On Aug 28, 2015 4:35 PM, "Chris Pacia via bitcoin-dev" <

@_date: 2015-08-28 17:29:23
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
Ah, then my mistake. It seemed so similar to an idea that was proposed
before on this mailing list:
that my mind just filled in the gaps. I concur -- having miners -- or any
group -- vote on block size is not an intrinsically good thing. The the
original proposal due to Greg Maxwell et al was not a mechanism for
"voting" but rather a feedback control that made the maximum block size
that which generated the most fees.

@_date: 2015-12-09 01:41:23
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
A far better place than the generation transaction (which I assume means
coinbase transaction?) is the last transaction in the block. That allows
you to save, on average, half of the hashes in the Merkle tree.
On Tue, Dec 8, 2015 at 11:55 PM, Justus Ranvier via bitcoin-dev <

@_date: 2015-12-09 14:59:43
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Greg, if you have actual data showing that putting the commitment in the
last transaction would be disruptive, and how disruptive, that would be
appreciated. Of the mining hardware I have looked at, none of it cared at
all what transactions other than the coinbase are. You need to provide a
path to the coinbase for extranonce rolling, but the witness commitment
wouldn't need to be updated.
I'm sorry but it's not clear how this would be an incompatible upgrade,
disruptive to anything other than the transaction selection code. Maybe I'm
missing something? I'm not familiar with all the hardware or pooling setups
out there.
On Wed, Dec 9, 2015 at 2:29 PM, Gregory Maxwell via bitcoin-dev <

@_date: 2015-12-09 16:46:31
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
My apologies for the apparent miscommunication earlier. It is of interest
to me that the soft-fork be done which is necessary to put a commitment in
the most efficient spot possible, in part because that commitment could be
used for other data such as the merged mining auxiliary blocks, which are
very sensitive to proof size.
Perhaps we have a different view of how the commitment transaction would be
generated. Just as GBT doesn't create the coinbase, it was my expectation
that it wouldn't generate the commitment transaction either -- but
generation of the commitment would be easy, requiring either the coinbase
txid 100 blocks back, or the commitment txid of the prior transaction (note
this impacts SPV mining). The truncation shouldn't be an issue because the
commitment txn would not be part of the list of transactions selected by
GBT, and in any case the truncation would change the witness data which
changes the commitment.
On Wed, Dec 9, 2015 at 4:03 PM, Gregory Maxwell via bitcoin-dev <

@_date: 2015-12-12 23:18:46
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
A segwit supporting server would be required to support relaying segwit
transactions, although a non-segwit server could at least inform a wallet
of segwit txns observed, even if it doesn't relay all information necessary
to validate.
Non segwit servers and wallets would continue operations as if nothing had
If this means essentially that a soft fork deployment of SegWit will
require SPV wallet servers to change their logic (or risk not being able to
send payments) then it does seem to me that a hard fork to deploy this non
controversial change is not only cleaner (on the data structure side) but
safer in terms of the potential to affect the user experience.
? Regards,
On Sat, Dec 12, 2015 at 1:43 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2015-12-17 17:33:26
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
There are many reasons to support segwit beyond it being a soft-fork. For
* the limitation of non-witness data to no more than 1MB makes the
quadratic scaling costs in large transaction validation no worse than they
currently are;
* redeem scripts in witness use a more accurate cost accounting than
non-witness data (further improvements to this beyond what Pieter has
implemented are possible); and
* segwit provides features (e.g. opt-in malleability protection) which are
required by higher-level scaling solutions.
With that in mind I really don't understand the viewpoint that it would be
better to engage a strictly inferior proposal such as a simple adjustment
of the block size to 2MB.
On Thu, Dec 17, 2015 at 1:32 PM, jl2012 via bitcoin-dev <

@_date: 2015-12-19 15:50:41
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
Not entirely correct, no. Edge cases also matter. Segwit is described as
4MB because that is the largest possible combined block size that can be
constructed. BIP 102 + segwit would allow a maximum relay of 8MB. So you
have to be confident that an 8MB relay size would be acceptable, even if a
block full of actual transactions would be closer to 3.5MB.
On Fri, Dec 18, 2015 at 6:01 PM, sickpig--- via bitcoin-dev <

@_date: 2015-12-21 12:50:03
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
I am fully in support of the plan laid out in "Capacity increases for the
bitcoin system".
This plan provides real benefit to the ecosystem in solving a number of
longstanding problems in bitcoin. It improves the scalability of bitcoin
Furthermore it is time that we stop bikeshedding, start implementing, and
move forward, lest we lose more developers to the toxic atmosphere this
hard-fork debacle has created.
On Mon, Dec 21, 2015 at 12:33 PM, Pieter Wuille via bitcoin-dev <

@_date: 2015-02-21 12:30:11
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
Thank you Jorge for the contribution of the Stag Hunt terminology. It is
much better than a politically charged "scorched earth".

@_date: 2015-07-01 21:52:57
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] REQ BIP # / Discuss - Sweep incoming unconfirmed 
This is called child pays for parent and there is a three year old pull
request implementing it:
The points regarding sweep transaction UI is out of scope for a BIP I'm
afraid. I suggest talking with wallet authors, and if agreement can be
found writing a pull request.

@_date: 2015-07-05 09:17:19
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP 68 (Relative Locktime) bug 
Can you construct an example? Are there use cases where there is a need for
an enforced lock time in a transaction with inputs that are not confirmed
at the time the lock time expires?

@_date: 2015-07-05 10:07:12
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP 68 (Relative Locktime) bug 
Note that you can put 0 in the sequence number field and it would work just
as expected under the old rules. I will perhaps suggest instead that
Bitcoin Core post-0.11 switch to doing this instead for that case.

@_date: 2015-07-28 17:46:20
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Does it matter even in the slightest why the block size limit was put in
place? It does not. Bitcoin is a decentralized payment network, and the
relationship between utility (block size) and decentralization is
empirical. Why the 1MB limit was put in place at the time might be a
historically interesting question, but it bears little relevance to the
present engineering issues.
On Tue, Jul 28, 2015 at 5:43 PM, Jean-Paul Kogelman via bitcoin-dev <

@_date: 2015-07-30 10:13:13
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Block size following technological growth 
The median is used here because that is the consensus rule -- a block
cannot have a timestamp older than the median time of the last 11 blocks.
By linking the changeover to this rule we avoid perverse incentives about
miners lying in their timestamps, or the threshold being crossed, then
reverted, then crossed again, etc.
Maybe a different percentile would have been a better choice, but that ship
sailed in 2009. The rule is what it is right now, and we benefit the most
from using the same rule as consensus for the threshold.
On Jul 30, 2015 9:57 AM, "Gary Mulder via bitcoin-dev" <

@_date: 2015-07-30 11:02:43
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure 
They aren't really so closely related as you are implying, since bitcoin is
a trustlessly decentralized system. At present every participant needs to
be able to validate the entire chain in order to be certain that their copy
of the ledger state is correct, and miners need to be able to incrementally
validate blocks in particularly short timeframes or else.
It is possible for a decentralized system like bitcoin to scale via
distribution in a way that introduces minimal trust, for example by
probabilistic validation and distribution of fraud proofs. However changes
to bitcoin consensus rules (mostly soft-forks) are required in order to
make this possible.
I don't want to discourage thinking about scaling bitcoin in such ways, as
it is a viable medium term proposal. However right now with the bitcoin
that exists today parallel distribution and decentralization are at odds
with each other.
On Thu, Jul 30, 2015 at 10:42 AM, Thomas Zander via bitcoin-dev <

@_date: 2015-06-01 18:49:33
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced transaction 
I have written a reference implementation and BIP draft for a soft-fork
change to the consensus-enforced behaviour of sequence numbers for the
purpose of supporting transaction replacement via per-input relative
lock-times. This proposal was previously discussed on the mailing list in
the following thread:
In short summary, this proposal seeks to enable safe transaction
replacement by re-purposing the nSequence field of a transaction input to
be a consensus-enforced relative lock-time.
The advantages of this approach is that it makes use of the full range of
the 32-bit sequence number which until now has rarely been used for
anything other than a boolean control over absolute nLockTime, and it does
so in a way that is semantically compatible with the originally envisioned
use of sequence numbers for fast mempool transaction replacement.
The disadvantages are that external constraints often prevent the full
range of sequence numbers from being used when interpreted as a relative
lock-time, and re-purposing nSequence as a relative lock-time precludes its
use in other contexts. The latter point has been partially addressed by
having the relative lock-time semantics be enforced only if the
most-significant bit of nSequence is set. This preserves 31 bits for
alternative use when relative lock-times are not required.
The BIP draft can be found at the following gist:
The reference implementation is available at the following git repository:
I request that the BIP editor please assign a BIP number for this work.
Mark Friedenbach

@_date: 2015-06-01 21:16:03
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced 
You are correct! I am maintaining a 'checksequenceverify' branch in my git
repository as well, an OP_RCLTV using sequence numbers:
Most of the interesting use cases for relative lock-time require an RCLTV
opcode. What is interesting about this architecture is that it possible to
cleanly separate the relative lock-time (sequence numbers) from the RCLTV
opcode (OP_CHECKSEQUENCEVERIFY) both in concept and in implementation. Like
CLTV, the CSV opcode only checks transaction data and requires no
contextual knowledge about block headers, a weakness of the other RCLTV
proposals that violate the clean separation between libscript and
libconsensus. In a similar way, this BIP proposal only touches the
transaction validation logic without any impact to script.
I would like to propose an additional BIP covering the CHECKSEQUENCEVERIFY
opcode and its enabling applications. But, well, one thing at a time.
On Mon, Jun 1, 2015 at 8:45 PM, Stephen Morse

@_date: 2015-06-02 08:44:53
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced 
Oh it is far worse than that. There is nothing preventing those coins from
being spent somewhere else, so actually an expiry would enable coin theft
in a reorg -- you make your spends expire right after they hit the chain
the first time, and then if there is a reorg the spend and subsequent
transactions are invalidated. This is an exploitable means of theft.
For this reason it is very important to male sure that once a transaction
makes it on the chain, it cannot be invalidated by means of a reorg.

@_date: 2015-06-04 14:54:12
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Tough questions for Peter Todd, 
Viacoin}
Why is this your business or the business of anyone on this list? Take it
somewhere else.

@_date: 2015-06-05 21:46:17
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Rusty, this doesn't play well with SIGHASH_SINGLE which is used in
assurance contracts among other things. Sometimes the ordering is set by
the signing logic itself...

@_date: 2015-06-06 02:45:23
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Certainly, but I would drop discussion of IsStandard or consensus rules.

@_date: 2015-06-12 11:20:21
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Peter it's not clear to me that your described protocol is free of miner
influence over the vote, by artificially generating transactions which they
claim in their own blocks, or conforming incentives among voters by opting
to be with the (slight) majority in order to minimize fees.
Wouldn't it in fact be simpler to use the dynamic block size adjustment
algorithm presented to the list a few weeks back, where the miner opts for
a larger block by sacrificing fees? In that way the users "vote" for larger
blocks by including sufficient fees to offset subsidy, but as it is an
economic incentives miners gain nothing by inflating the fees in their own

@_date: 2015-06-14 19:47:15
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
There's another important use case which you mentioned Greg, that also
requires special exemption: compact commitments via mid-state compression.
The use case is an OP_RETURN output sorted last, whose last N bytes are a
commitment of some kind. A proof of the commitment can then use mid state
compression to elide the beginning of the transaction.
How do you make a special exemption for this category of outputs? I can't
think of a very clean way of doing so that doesn't require an ugly
advertising of sort-order exemptions.
The fact that we have two different existing use cases which conflict with
soft-fork enforcement, I'm quiet concerned that there are either other
things we aren't thinking of or haven't invented yet which would be

@_date: 2015-06-15 17:41:00
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
We are not reaching consensus about any proposal, Garzik's or otherwise.

@_date: 2015-06-16 18:00:45
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] [BIP draft] Consensus-enforced 
Given that we have had more than two weeks of public discussion, code is
available and reviewed, and several community identified issues resolved, I
would like to formally request a BIP number be assigned for this work. Will
the BIP editor be so kind as to do so to allow the BIP consensus process to
Thank you,
Mark Friedenbach
On Mon, Jun 1, 2015 at 6:49 PM, Mark Friedenbach

@_date: 2015-06-18 08:03:01
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
There is a quote from United States Supreme Court Justice Potter Stewart to
describe his threshold test for obscenity which is relevant here: "I know
it when I see it."
It is hard certainly, and perhaps even impossible to write down a system of
rules that is used to resolve every dispute among core developers. But that
doesn't mean it isn't obvious to all the participants what is going on. If
a contentious change is proposed, and if after sufficient debate there are
still members of the technical community with reasoned, comprehensible
objections who are not merely being obstinate in the views -- a neutral
observer would agree that their concerns have not been met -- then there is
a lack of consensus.
If there was some sort of formal process however, the system wouldn't work.
Rules can be gamed, and if you add rules to a process then that process can
be gamed. Instead we all have a reasonable understanding of what "technical
consensus" is, and we all know it when we see it. Where we do not see it,
we do not proceed.

@_date: 2015-06-18 14:49:51
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
Matt, I for one do not think that the block size limit should be raised at
this time. Matt Corallo also started the public conversation over this
issue on the mailing list by stating that he was not in favor of acting now
to raise the block size limit. I find it a reasonable position to take that
even if you feel the block size limit should be raised at some time in the
future, there are reasons why now is not the best time to do it.
On Thu, Jun 18, 2015 at 2:42 PM, Matt Whitlock

@_date: 2015-06-18 15:33:00
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
Or alternatively, fix the reasons why users would have negative experiences
with full blocks, chiefly:
  * Get safe forms of replace-by-fee and child-pays-for-parent finished and
in 0.12.
  * Develop cross-platform libraries for managing micropayment channels,
and get wallet authors to adopt
  * Use fidelity bonds, solvency proofs, and other tricks to minimize the
risk of already deployed off-chain solutions as an interim measure until:
  * Deploy soft-fork changes for truly scalable solutions like Lightning
Not raising the block size limit does not mean doing nothing to solve the

@_date: 2015-06-19 18:09:53
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
What retail needs is escrowed microchannel hubs (what lightning provides,
for example), which enable untrusted instant payments. Not reliance on
single-signer zeroconf transactions that can never be made safe.
On Fri, Jun 19, 2015 at 5:47 PM, Andreas Petersson

@_date: 2015-06-22 14:52:32
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
Can you please add a discussion of the tradeoffs of decentralization vs
block size?
On Mon, Jun 22, 2015 at 11:18 AM, Gavin Andresen

@_date: 2015-06-24 18:50:59
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP Process and Votes 
I'm sorry but this is absolutely not the case, Milly. The reason that
people get defensive is that we have a carefully constructed process that
does work (thank you very much!) and is well documented. We talk about it
quite often in fact as it is a defining characteristic of how bitcoin is
developed which differs in some ways from how other open source software is
developed -- although it remains the same in most other ways.
Changes to the non-consensus sections of Bitcoin Core tend to get merged
when there are a few reviews, tests, and ACKs from recognized developers,
there are no outstanding objections, and the maintainer doing the merge
makes a subjective judgement that the code is ready.
Consensus-changes, on the other hand, get merged into Bitcoin Core only
after the above criteria are met AND an extremely long discussion period
that has given all the relevant stakeholders a chance to comment, and no
significant objections remain. Consensus-code changes are unanimous. They
must be.
The sort of process that exists in standards bodies for example, with
working groups and formal voting procedures, has no place where changes
define the nature and validity of other people's money. Who has the right
to reach into your pocket and define how you can or cannot spend your
coins? The premise of bitcoin is that no one has that right, yet that is
very much what we do when consensus code changes are made. That is why when
we make a change to the rules governing the nature of bitcoin, we must make
sure that everyone is made aware of the change and consents to it.
Everyone. Does this work? Does this scale? So far, it does. Uncontroversial
changes, such as BIP 66, are deployed without issue. Every indication is
that BIP 66 will complete deployment in the very near future, and we intend
to repeat this process for more interesting changes such as BIP65:
This isn't about no one stepping forward to be the "decider." This is about
no one having the right to decide these things on the behalf of others. If
a contentious change is proposed and not accepted by the process of
consensus, that is because the process is doing its job at rejecting
controversial changes. It has nothing to do with personality, and
everything to do with the nature of bitcoin itself.

@_date: 2015-06-24 23:15:38
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP Process and Votes 
You don't need to ask permission for testnet. Here is one with 100MB blocks:

@_date: 2015-06-26 11:31:43
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] The need for larger blocks 
Jeff, block size limits large enough to prevent fee pressure is absolutely,
unequivocally unsustainable. We are already running against technological
limits in the tradeoff between decentralization and utility. Increases of
the block size limit in advance of fee pressure only delay the problem --
it does not and cannot solve it!
We must be careful to use the block size limit now to get infrastructure to
support a world with full blocks -- it's not that hard -- while still
having a little room to grow fast if things unexpectedly break.

@_date: 2015-06-26 12:12:00
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] The need for larger blocks 
This is a hard fork. It is not about miners, at all. 2013 showed that when
there is true consensus mining can be coordinated on the order of hours or
days. This is about pushing through a coercive change to the
decentralization tradeoffs of bitcoin without unanimous consent.

@_date: 2015-06-27 09:28:26
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
I really suggest you look into the layer2 systems Adam pointed to, as you
appear to be misinformed about their properties. There are many proposals
which really do achieve global consensus using the block chain, just in a
delayed (and cached) fashion that is still 100% safe.
It is possible to go off-chain without losing the trustlessness and
security of the block chain.

@_date: 2015-06-28 08:23:49
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Original Vision 
There's a couple of things that Patrick could have been referring to when
he said "Fraud proofs need to be at least more efficient than full node
validation. Currently they are not."
One of the issues is that you cannot efficiently encode or validate a proof
of a negative. If a transaction input is a double-spend, you can build a
semi-reasonable sized proof of the prior spend (or very reasonably sized
with block header commitments). However if a transaction spends an output
which never existed in the first place, there is no reasonable way to
assert this other than witnessing the entire block history, as a full node
UTXO commitments are the nominal solution here. You commit the validator
state in each block, and then you can prove things like a negative by
referencing that state commitment. The trouble is this requires maintaining
a hash tree commitment over validator state, which turns out to be insanely
expensive. With the UTXO commitment scheme (the others are not better) that
ends up requiring 15 - 22x more I/O during block validation. And I/O is
presently a limiter to block validation speed. So if you thought 8MB was
what bitcoin today could handle, and you also want this commitment scheme
for fraud proofs, then you should be arguing for a block size limit
decrease (to 500kB), not increase.

@_date: 2015-06-28 09:15:28
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Original Vision 
Assuming randomly-picked outputs, it's actually worse. The slowdown factor
has to do with the depth of the tree, and TXO and STXO trees are always
growing. It's still complexity O(log N), but with TXO/STXO N is the size of
the entire block chain history, whereas with UTXO it's just the set of
unspent transaction outputs.
Of course that's not a fair assumption since in an insertion-ordered tree
using the Merkle mountain range data structure would have significantly
shorter paths for recent outputs. But the average case might be about the
same, and it comes with a slew of other tradeoffs that make it hard to
compare head-to-head in the abstract. Ultimately both need to be written
and benchmarked.
But it is not the case that TXO/STXO gives you constant time updates. The
append-only TXO tree might be close to that, but you'd still need the spent
or unspent tree which is not insertion ordered. There are alternatives like
updating the TXO tree and requiring blocks and transactions to carry proofs
with them (so validators can be stateless), but that pushes the same
(worse, actually) problem to whoever generated or assembled the proof. It
may be a tradeoff worth making, but it's not an easy answer...

@_date: 2015-06-28 10:12:35
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Think in terms of participants, not addresses. A participant in the
lightning network has a couple of connections to various hubs, from which
the participant is able to send or receive coin. The user is able to send
coins to anyone connected to the lightning network by means of an atomic
transaction through any path of the network. But the only payment from them
that ever hits the chain is their settlement with the hub.
Imagine there was a TCP/IP data chain and corresponding lightning network.
Everyone connected to the network has an "IP" channel with their ISP.
Through this channel they can send data to anywhere on the network, and a
traceroute shows what hops the data would take. But when settlement
actually occurs all the network sees is the net amount of data that has
gone through each segment -- without any context. There's no record
preserved on-chain of who sent data to whom, just that X bytes went through
the pipe on the way to somewhere unspecified.
So it is with lightning payment networks. You open a channel with a hub and
through that channel send coins to anyone accessible to the network.
Channels only close when a participant needs the funds for non-lightning
reasons, or when hubs need to rebalance. And when they do, observers on the
chain learn nothing more than how much net coin moved across that single
link. They learn nothing about where that coin eventually ended up.
So back to your original question, each channel can be considered to have a
pseudonymous identity, and each new channel given a new identity. Channel
closures can even be coinjoin'd when the other party is cooperating. But
ultimately, lightning usefully solves a problem where participants have
semi-long lived payment endpoints.

@_date: 2015-06-28 10:45:58
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Gavin, do you use a debit card or credit card? Then you do fit that use
case. When you buy a coffee at Starbucks, it is your bank that pays
Starbuck's bank. So it is with micropayment hubs.
On Sun, Jun 28, 2015 at 1:12 PM, Mark Friedenbach Very few of my own personal Bitcoin transactions fit that use-case.
In fact, very few of my own personal dollar transactions fit that use-case
(I suppose if I was addicted to Starbucks I'd have one of their payment
cards that I topped up every once in a while, which would map nicely onto a
payment channel). I suppose I could setup a payment channel with the
grocery store I shop at once a week, but that would be inconvenient (I'd
have to pre-fund it) and bad for my privacy.
I can see how payment channels would work between big financial
institutions as a settlement layer, but isn't that exactly the
centralization concern that is making a lot of people worried about
increasing the max block size?
And if there are only a dozen or two popular hubs, that's much worse
centralization-wise compared to a few thousand fully-validating Bitcoin
Don't get me wrong, I think the Lightning Network is a fantastic idea and a
great experiment and will likely be used for all sorts of great payment
innovations (micropayments for bandwidth maybe, or maybe paying workers by
the hour instead of at the end of the month). But I don't think it is a
scaling solution for the types of payments the Bitcoin network is handling

@_date: 2015-06-28 13:16:18
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP Process and Votes 
Milly you are absolutely wrong as has been pointed out by numerous people
numerous times. Your idea of how bitcoin development decision making works
is demonstrably false. Please stop filling our inboxes with trolling
accusations, or else this will have to become a moderated list. And no one
wants that.

@_date: 2015-05-08 13:33:53
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
It is my professional opinion that raising the block size by merely
adjusting a constant without any sort of feedback mechanism would be a
dangerous and foolhardy thing to do. We are custodians of a multi-billion
dollar asset, and it falls upon us to weigh the consequences of our own
actions against the combined value of the entire bitcoin ecosystem. Ideally
we would take no action for which we are not absolutely certain of the
ramifications, with the information that can be made available to us. But
of course that is not always possible: there are unknown-unknowns, time
pressures, and known-unknowns where information has too high a marginal
cost. So where certainty is unobtainable, we must instead hedge against
unwanted outcomes.
The proposal to raise the block size now by redefining a constant carries
with it risk associated with infrastructure scaling, centralization
pressures, and delaying the necessary development of a constraint-based fee
economy. It also simply kicks the can down the road in settling these
issues because a larger but realistic hard limit must still exist, meaning
a future hard fork may still be required.
But whatever new hard limit is chosen, there is also a real possibility
that it may be too high. The standard response is that it is a soft-fork
change to impose a lower block size limit, which miners could do with a
minimal amount of coordination. This is however undermined by the
unfortunate reality that so many mining operations are absentee-run
businesses, or run by individuals without a strong background in bitcoin
protocol policy, or with interests which are not well aligned with other
users or holders of bitcoin. We cannot rely on miners being vigilant about
issues that develop, as they develop, or able to respond in the appropriate
fashion that someone with full domain knowledge and an objective
perspective would.
The alternative then is to have some sort of dynamic block size limit
controller, and ideally one which applies a cost to raising the block size
in some way the preserves the decentralization and/or long-term stability
features that we care about. I will now describe one such proposal:
  * For each block, the miner is allowed to select a different difficulty
(nBits) within a certain range, e.g. +/- 25% of the expected difficulty,
and this miner-selected difficulty is used for the proof of work check. In
addition to adjusting the hashcash target, selecting a different difficulty
also raises or lowers the maximum block size for that block by a function
of the difference in difficulty. So increasing the difficulty of the block
by an additional 25% raises the block limit for that block from 100% of the
current limit to 125%, and lowering the difficulty by 10% would also lower
the maximum block size for that block from 100% to 90% of the current
limit. For simplicity I will assume a linear identity transform as the
function, but a quadratic or other function with compounding marginal cost
may be preferred.
  * The default maximum block size limit is then adjusted at regular
intervals. For simplicity I will assume an adjustment at the end of each
2016 block interval, at the same time that difficulty is adjusted, but
there is no reason these have to be aligned. The adjustment algorithm
itself is either the selection of the median, or perhaps some sort of
weighted average that respects the "middle majority." There would of course
be limits on how quickly the block size limit can adjusted in any one
period, just as there are min/max limits on the difficulty adjustment.
  * To prevent perverse mining incentives, the original difficulty without
adjustment is used in the aggregate work calculations for selecting the
most-work chain, and the allowable miner-selected adjustment to difficulty
would have to be tightly constrained.
These rules create an incentive environment where raising the block size
has a real cost associated with it: a more difficult hashcash target for
the same subsidy reward. For rational miners that cost must be
counter-balanced by additional fees provided in the larger block. This
allows block size to increase, but only within the confines of a
self-supporting fee economy.
When the subsidy goes away or is reduced to an insignificant fraction of
the block reward, this incentive structure goes away. Hopefully at that
time we would have sufficient information to soft-fork set a hard block
size maximum. But in the mean time, the block size limit controller
constrains the maximum allowed block size to be within a range supported by
fees on the network, providing an emergency relief valve that we can be
assured will only be used at significant cost.
Mark Friedenbach
* There has over time been various discussions on the bitcointalk forums
about dynamically adjusting block size limits. The true origin of the idea
is unclear at this time (citations would be appreciated!) but a form of it
was implemented in Bytecoin / Monero using subsidy burning to increase the
block size. That approach has various limitations. These were corrected in
Greg Maxwell's suggestion to adjust the difficulty/nBits field directly,
which also has the added benefit of providing incentive for bidirectional
movement during the subsidy period. The description in this email and any
errors are my own.
On Fri, May 8, 2015 at 12:20 AM, Matt Whitlock

@_date: 2015-05-08 13:55:30
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Block Size Increase 
The problems with that are larger than time being unreliable. It is no
longer reorg-safe as transactions can expire in the course of a reorg and
any transaction built on the now expired transaction is invalidated.

@_date: 2015-05-08 13:40:50
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Block Size Increase 
Transactions don't expire. But if the wallet is online, it can periodically
choose to release an already created transaction with a higher fee. This
requires replace-by-fee to be sufficiently deployed, however.

@_date: 2015-05-08 15:45:19
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
There are already solutions to this which are waiting to be deployed as
default policy to bitcoind, and need to be implemented in other clients:
replace-by-fee and child-pays-for-parent.

@_date: 2015-05-08 16:58:20
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
In a fee-dominated future, replace-by-fee is not an opt-in feature. When
you create a transaction, the wallet presents a range of fees that it
expects you might pay. It then signs copies of the transaction with spaced
fees from this interval and broadcasts the lowest fee first. In the user
interface, the transaction is shown with its transacted amount and the
approved fee range. All of the inputs used are placed on hold until the
transaction gets a confirmation. As time goes by and it looks like the
transaction is not getting accepted, successively higher fee versions are
released. You can opt-out and send a no-fee or base-fee-only transaction,
but that should not be the default.
On the receiving end, local policy controls how much fee should be spent
trying to obtain confirmations before alerting the user, if there are fees
available in the hot wallet to do this. The receiving wallet then adds its
own fees via a spend if it thinks insufficient fees were provided to get a
confirmation. Again, this should all be automated so long as there is a hot
wallet on the receiving end.
Is this more complicated than now, where blocks are not full and clients
generally don't have to worry about their transactions eventually
confirming? Yes, it is significantly more complicated. But such
complication is unavoidable. It is a simple fact that the block size cannot
increase so much as to cover every single use by every single person in the
world, so there is no getting around the reality that we will have to
transition into an economy where at least one side has to pay up for a
transaction to get confirmation, at all. We are going to have to deal with
this issue whether it is now at 1MB or later at 20MB. And frankly, it'll be
much easier to do now.

@_date: 2015-05-10 11:10:47
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
Micropayment channels are not pie in the sky proposals. They work today on
Bitcoin as it is deployed without any changes. People just need to start
using them.

@_date: 2015-05-10 15:31:46
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
I'm on my phone today so I'm somewhat constrained in my reply, but the key
takeaway is that the proposal is a mechanism for miners to trade subsidy
for the increased fees of a larger block. Necessarily it only makes sense
to do so when the marginal fee per KB exceeds the subsidy fee per KB. It
correspondingly makes sense to use a smaller block size if fees are less
than subsidy, but note that fees are not uniform and as the block shrinks
the marginal fee rate goes up..
Limits on both the relative and absolute amount a miner can trade subsidy
for block size prevent incentive edge cases as well as prevent a sharp
shock to the current fee-poor economy (by disallowing adjustment below 1MB).
Also the identity transform was used only for didactic purposes. I fully
expect there to be other, more interesting functions to use.

@_date: 2015-05-26 13:56:06
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Cost savings by using replace-by-fee, 
Please let's at least have some civility and decorum on this list.

@_date: 2015-05-26 18:50:29
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Consensus-enforced transaction replacement 
Sequence numbers appear to have been originally intended as a mechanism for
transaction replacement within the context of multi-party transaction
construction, e.g. a micropayment channel. The idea is that a participant
can sign successive versions of a transaction, each time incrementing the
sequence field by some amount. Relay nodes perform transaction replacement
according to some policy rule making use of the sequence numbers, e.g.
requiring sequence numbers in a replacement to be monotonically increasing.
As it happens, this cannot be made safe in the bitcoin protocol as deployed
today, as there is no enforcement of the rule that miners include the most
recent transaction in their blocks. As such, any protocol relying on a
transaction replacement policy can be defeated by miners choosing not to
follow that policy, which they may even be incentivised to do so (if older
transactions provide higher fee per byte, for example). Transaction
replacement is presently disabled in Bitcoin Core.
These shortcomings can be fixed in an elegant way by giving sequence
numbers new consensus-enforced semantics as a relative lock-time: if a
sequence number is non-final (MAX_INT), its bitwise inverse is interpreted
as either a relative height or time delta which is added to the height or
median time of the block containing the output being spent to form a
per-input lock-time. The lock-time of each input constructed in this manor,
plus the nLockTime of the transaction itself if any input is non-final must
be satisfied for a transaction to be valid.
For example, a transaction with an txin.nSequence set to 0xffffff9b [==
~(uint32_t)100] is prevented by consensus rule from being selected for
inclusion in a block until the 100th block following the one including the
parent transaction referenced by that input.
In this way one may construct, for example, a bidirectional micropayment
channel where each change of direction increments sequence numbers to make
the transaction become valid prior to any of the previously exchanged
This also enables the discussed relative-form of CHECKLOCKTIMEVERIFY to be
implemented in the same way: by checking transaction data only and not
requiring contextual information like the block height or timestamp.
An example implementation of this concept, as a policy change to the
mempool processing of Bitcoin Core is available on github:

@_date: 2015-05-27 08:26:52
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Mike, this proposal was purposefully constructed to maintain as well as
possible the semantics of Satoshi's original construction. Higher sequence
numbers -- chronologically later transactions -- are able to hit the chain
earlier, and therefore it can be reasonably argued will be selected by
miners before the later transactions mature. Did I fail in some way to
capture that original intent?

@_date: 2015-05-28 02:56:36
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
I have no problem with modifying the proposal to have the most significant
bit signal use of the nSequence field as a relative lock-time. That leaves
a full 31 bits for experimentation when relative lock-time is not in use. I
have adjusted the code appropriately:

@_date: 2015-05-28 07:59:13
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Why 3? Do we have a version 2?
As for doing it in serialization, that would alter the txid making it a
hard fork change.

@_date: 2015-05-28 08:38:43
@_author: Mark Friedenbach 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Oh ok you mean a semantic difference for the purpose of explaining. It
doesn't actually change the code.
Regarding saving more bits, there really isn't much room if you consider
time-based relative locktimes and long-lived channels on the order of a
year or more.

@_date: 2015-11-04 14:47:35
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] A validation-cost metric for aggregate limits and fee 
At the first Scaling Bitcoin workshop in Montreal I presented on the topic
of "bad blocks" that take an excessive amount of time to validate. You can
read a transcript of this talk here:
The core message was that the assumption made by the design parameters of
the system, namely that validation costs scale linearly with transaction or
block size, is wrong. In particular, in certain kinds of transactions there
are validation costs which scale quadraticly with size. For example, the
construction of SIGHASH_ALL results in each input signing a different
message digest, meaning that the entire transaction (minus the scriptSigs)
is rehashed for each input. As another example, the number of signature
operation performed during block validation is unlimited if the validations
are contained within the scriptPubKey (this scales linearly but with a very
large constant factor). The severity of these issues increase as the
aggregate limits in place on maximum transaction and block size increase.
There have been various solutions suggested, and I would like to start a
public discussion to see if consensus can be reached over a viable approach.
Gavin, for example, has written code that tracks the number of bytes hashed
and enforces a separate limit for a block over this aggregate value. Other
costs could be constrained in a similar whack-a-mole way. I have two
concerns with this approach:
1. There would still exist a gap between the average-case validation cost
of a full block and the worst case validation cost of a block that was
specifically constructed to hit every limit.
2. Transaction selection and by extension fee determination would become
much more complicated multi-dimensional optimization problems. Since fee
management in particular is code replicated in a lot of infrastructure, I
would be very concerned over making optimal behavior greatly more difficult.
My own suggestion, which I submit for consideration, is to use a linear
function of the various costs involved (signatures verified, bytes hashed,
inputs consumed, script opcodes executed, etc.). The various algorithms
used for transaction selection and fee determination can then be reused,
using the output of this new linear function as the "size" of the
Separately, many others including Greg Maxwell have advocated for a
"net-UTXO" metric instead of, or in combination with a validation-cost
metric. In the pure form the block size limit would be replaced with a
maximum UTXO set increase, thereby applying a cost in extra fee required to
create unspent outputs. This has the distinct advantage of making dust
outputs considerably more expensive than regular spend outputs.
For myself, I remain open to the possibility of adding a UTXO set size
corrective factor to a chiefly validation-cost metric. It would be nice to
reward users for cleaning up scattered small output, reward miners for
including dust-be-gone outputs, and make spam attacks more costly. But
doing so requires setting aside some unused validation resources in order
to reward miners who clean up the UTXO, which means it widens the gap
between average and worst case block validation times. Also, worry over the
size of the UTXO database is only a concern for how Bitcoin Core is
currently structured -- with e.g. UTXO or STXO commitments it could be the
case that in the future full nodes do not store the UTXO and instead carry
proofs of their inputs as prunable witness data. If we choose a net-UTXO
metric however, we will be stuck with it for some time.
I will be submitting a talk proposal for Scaling Bitcoin on this topic, but
I would like to get some feedback from the developer community first.
Anyone have any thoughts to add?

@_date: 2015-11-25 15:05:50
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
Looks like I'm the long dissenting voice here? As the originator of the
name CHECKSEQUENCEVERIFY, perhaps I can explain why the name was
appropriately chosen and why the proposed alternatives don't stand up.
First, the names are purposefully chosen to illustrate what they do:
What does CHECKLOCKTIMEVERIFY do? It verifies the range of tx.nLockTime.
What does CHECKSEQUENCEVERIFY do? It verifies the range of txin.nSequence.
Second, the semantics are not limited to relative lock-time / maturity
only. They both leave open ranges with possible, but currently undefined
future consensus-enforced behavior. We don't know what sort of future
behavior these values might trigger, but the associated opcodes are generic
enough to handle them:
CHECKLOCKTIMEVERIFY will pass an nSequence between 1985 and 2009, even
though such constraints have no meaning in Bitcoin.
CHECKSEQUENCEVERIFY is explicitly written to permit a 5-byte push operand,
while checking only 17 of the available 39 bits of both the operand and the
nSequence. Indeed the most recent semantic change of CSV was justified in
part because it relaxes all constraints over the values of these bits
freeing them for other purposes in transaction validation and/or future
extensions of the opcode semantics.
Third, single-byte opcode space is limited. There are less than 10 such
opcodes left. Maybe space won't be so precious in a post-segwitness world,
but I don't want to presume that just yet.
As for the alternatives, they capture only the initial use case of
nSequence. My objection would relax if nSequence were renamed, but I think
that would be too disruptive and unnecessary. In any case, the imagined use
cases for CHECKSEQUENCEVERIFY has to do with sequencing execution pathways
of script, so it's not a stretch in meaning. Previously CHECKMATURITYVERIFY
was a hypothicated opcode that directly checked the minimum age of inputs
of a transaction. The indirect naming of CHECKSEQUENCEVERIFY on the other
hand is due to its indirect behavior. RELATIVELOCKTIMEVERIFY was also a
hypothicated opcode that would check a ficticious nRelativeLockTime field,
which does not exist. Again my objection would go away if we renamed
nSequence, but I actually think the nSequence name is better...
On Tue, Nov 24, 2015 at 2:30 AM, Btc Drak via bitcoin-dev <

@_date: 2015-10-05 17:19:06
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Alex, decreasing granularity is a soft-fork, increasing is a hard-fork.
Therefore I've kept the highest possible precision (1 second, 1 block) with
the expectation that at some point in the future if we need more low-order
bits we can soft-fork them to other purposes, we can decrease granularity
at that time.
On Mon, Oct 5, 2015 at 3:03 PM, Alex Morcos via bitcoin-dev <

@_date: 2015-10-15 11:31:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Adam, there is really no justification I can see to lower the interblock
interval on the Bitcoin blockchain, primarily due to the effects of network
latency. Lowering the interblock interval and raising the block size are
not equal alternatives - you can always get more throughput in bitcoin by
raising the block size than by lowering the interblock time. And that's
without considering the effect shorter intervals would have on e.g. SPV
client bandwidth or sidechain connectivity proofs. So I find it very
unlikely that such granularity would ever be needed on the Bitcoin block
chain, although if were to happen then extra bits from nSequence could be
used in a soft-fork compatible way.
However it is true that various sidechains such as Liquid will have a much
shorter interblock interval than 10min, as well as customer demand for
protocols with shorter timeouts. It would be nice if such systems did not
HAVE to resort to complex bit shifting to support more precision, and if
protocols written for bitcoin could be reused on such systems with minimal
or no modification.
To that end, it might be preferable to move the flag bit indicating use of
seconds from bit 16 to bit 23 and (by convention only) reserve bits 17..22
to provide higher granularity in a sidechain environment. This keeps the
size of a stack push to 3 bytes while also keeping sufficient room for
high-order bits of relative lock-time in a sidechain that supports shorter
block intervals.
Another alternative is to put the units flag in the least significant bit,
which has the advantage of allowing both units of lock-time to make use of
1-2 byte pushes, but the disadvantage of making lock times of 64..127
2-bytes instead of 1-byte.
On Thu, Oct 15, 2015 at 9:37 AM, Adam Back via bitcoin-dev <

@_date: 2015-09-10 14:32:37
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Named Bitcoin Addresses 
Are you aware of the payment protocol?
On Sep 10, 2015 2:12 PM, "essofluffy . via bitcoin-dev" <

@_date: 2015-09-17 00:23:41
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Eric, that would be, I think, my sequencenumbers2 branch in which nSequence
is an explicit relative lock-time field (unless the most significant bit is
set). That has absolutely clear semantics. You should comment on where this is being discussed.

@_date: 2015-09-17 15:07:34
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fill-or-kill transaction 
Note that this violates present assumptions about transaction validity,
unless a constraint also exists that any output of such an expiry block is
not spent for at least 100 blocks.
Do you have a clean way of ensuring this?
On Thu, Sep 17, 2015 at 2:41 PM, jl2012 via bitcoin-dev <

@_date: 2015-09-18 00:30:37
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
I am unlikely to attend at that time, but there is no time that will fit
everybody's schedules. I approve of the idea and look forward to reading
the logs.
On Thu, Sep 17, 2015 at 9:07 PM, Wladimir J. van der Laan via bitcoin-dev <

@_date: 2015-09-18 01:55:55
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Correction of a correction, in-line:
On Wed, Sep 16, 2015 at 5:51 PM, Matt Corallo via bitcoin-dev <
Perhaps it is accurate to say that there wasn't consensus at all except
that (1) we think we can work together on resolving this impasse (yay!),
and (2) it is conceivable that changing from block size to some other
metric might provide the basis for a compromise on near-term numbers.
As an example, I do not think the net-UTXO metric provides any benefit with
respect to scalability, and in some ways makes the situation worse (even
though it helpfully solves an unrelated problem of spammy dust outputs).
But there are other possible metrics and I maintain hope that data will
show the benefit of another metric or other metrics combined with net-UTXO
in a way that will allow us to reach consensus.
As a further example, I also am quite concerned about 2-4-8MB with either
block size or net-UTXO as the base metric. As you say, it depends on how
the non-blocksize limit accounting/adjusting is done... But if a metric
were chosen that addressed my concerns (worst case propagation and
validation time), then I could be in favor of an initial bump that allowed
a larger number of typical transactions in a block.
But where I really need to disagree is on the requirement for a 2nd hard
fork. I will go on record as being definitively against this. While being
conservative with respect to exponentials, I would very much like to make
sure that there is a long-term growth curve as part of any proposal. I am
willing to accept a hard-fork if the adopted plan is too conservative, but
I do not want to be kicking the can down the road to a scheduled 2nd hard
fork that absolutely must occur. That, I feel, could be a more dangerous
outcome than an exponential that outlasts conservative historical trends.
I commend Jeff for writing a Chatham-rules summary of the outcome of some
hallway conversations that occurred. On the whole I think his summary does
represent the majority view of the opinions expressed by core developers at
the workshop. I will caution though that on nearly every issue there were
those expressed disagreement but did not fight the issue, and those who
said nothing and left unpolled opinions. Nevertheless this summary is
informative as it feeds forwards into the design of proposals that will be
made prior to the Hong Kong workshop in December, in order that they have a
higher likelihood of success.

@_date: 2015-09-20 11:43:19
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Replying to this specific email only because it is the most recent in my
mail client.
Does this conversation have to happen on-list? It seems to have wandered
incredibly far off-topic.
On Sun, Sep 20, 2015 at 5:25 AM, Mike Hearn via bitcoin-dev <

@_date: 2015-09-23 10:18:14
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CI Build for Bitcoin - Some Basic Questions about 
The builds made by Travis are for the purpose of making sure that the
source code compiles and tests run successfully on all supported platforms.
The binaries are not used anywhere else because Travis is not a trusted
The binaries on bitcoin.org are built using the gitian process and signed
by a quorum of developers.
On Wed, Sep 23, 2015 at 10:13 AM, Roy Osherove via bitcoin-dev <

@_date: 2015-09-23 11:10:12
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] CI Build for Bitcoin - Some Basic Questions about 
Well the gitian builds are made available on bitcoin.org. If you mean a
build server where gitian builds are automatically done and made available,
well that rather defeats the point of gitian.
The quorum signatures are accumulated here:
 (it's a manual process).

@_date: 2015-09-27 13:27:57
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Agree with all CLTV and nVersionBits points. We should deploy a lock-time
soft-fork ASAP, using the tried and true IsSuperMajoirty test.
However your information regarding BIPs 68 (sequence numbers), 112
(checksequenceverify) and 113 (median time past) is outdated. Debate
regarding semantics has been settled, and there are working implementations
ready for merge on github. See pull requests   and  I
don?t know what the hold up has been regarding further reviews and merging,
but it is ready.
If you believe there are reasons   or  should not be
merged, please speak up. Otherwise it appears there is consensus on these
changes. They are related, and there is no reason not to include them in
the soft-fork, delaying applications using these features by 6-12 months.
On Sun, Sep 27, 2015 at 11:50 AM, Peter Todd via bitcoin-dev <

@_date: 2015-09-29 07:59:46
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Is it possible for there to be two chains after a 
You don't need to appeal to human psychology. At 75% threshold, it takes
only 25.01% of the hashpower to report but not actually enforce the fork to
cause the majority hashpower to remain on the old chain, but for upgraded
clients to start rejecting the old chain. With 95% the same problem exists
but with a threshold of 45.01%. BIP 66 showed this not to be a hypothetical
On Tue, Sep 29, 2015 at 7:17 AM, Jonathan Toomim (Toomim Bros) via

@_date: 2015-09-29 10:07:12
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] On bitcoin-dev list admin and list noise 
This mailing list was never meant to be a place "to hold the bitcoin
development community accountable for its actions [sic]." I know other
developers that have switched to digest-only or unsubscribed. I know if
this became a channel for PR and populist venting as you describe, I would
leave as well. This mailing list is meant to be a place to discuss ongoing
bitcoin development issues relating to the protocol and its instantiation
in bitcoin core. Please don't decrease the utility of this list by
expanding scope.
On Tue, Sep 29, 2015 at 9:38 AM, Santino Napolitano via bitcoin-dev <

@_date: 2016-01-19 08:27:09
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Segregated Witness App Development 
Wouldn't this be entirely on topic for  It's probably better
not to fragment the communication channels and associated infrastructure
(logs, bots, etc.)
On Tue, Jan 19, 2016 at 3:54 AM, Eric Lombrozo via bitcoin-dev <

@_date: 2017-04-15 09:42:25
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
If I understand correctly, the crux of your argument against BIP148 is that
it requires the segwit BIP9 activation flag to be set in every block after
Aug 1st, until segwit activates. This will cause miners which have not
upgrade and indicated support for BIP141 (the segwit BIP) to find their
blocks ignored by UASF nodes, at least for the month or two it takes to
activate segwit.
Isn't this however the entire point of BIP148? I understand if you object
to this, but let's be clear that this is a design requirement of the
proposal, not a technical oversight. The alternative you present (new BIP
bit) has the clear downside of not triggering BIP141 activation, and
therefore not enabling the new consensus rules on already deployed full
nodes. BIP148 is making an explicit choice to favor dragging along those
users which have upgraded to BIP141 support over those miners who have
failed to upgrade.
On an aside, I'm somewhat disappointed that you have decided to make a
public statement against the UASF proposal. Not because we disagree -- that
is fine -- but because any UASF must be a grassroots effort and
endorsements (or denouncements) detract from that.
Mark Friedenbach

@_date: 2017-08-13 13:56:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Would anyone object to adding a dlopen message 
Jonas, I think his proposal is to enable extending the P2P layer, e.g.
adding new message types. Are you suggesting having externalized
message processing? That could be done via RPC/ZMQ while opening up a
much more narrow attack surface than dlopen, although I imagine such
an interface would require a very complex API specification.
On Sun, Aug 13, 2017 at 1:00 PM, Jonas Schnelli via bitcoin-dev

@_date: 2017-08-22 13:20:41
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] UTXO growth scaling solution proposal 
A fun exercise to be sure, but perhaps off topic for this list?

@_date: 2017-08-22 20:26:19
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] UTXO growth scaling solution proposal 
Lock time transactions have been valid for over a year now I believe. In any case we can't scan the block chain for usage patterns in UTXOs because P2SH puts the script in the signature on spend.

@_date: 2017-08-28 14:33:52
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] P2WPKH Scripts, P2PKH Addresses, 
No, and the whole issue of compressed vs uncompressed is a red herring. If Alice gives Bob 1MsHWS1BnwMc3tLE8G35UXsS58fKipzB7a, she is saying to Bob ?I will accept payment to the scriptPubKey [DUP HASH160 PUSHDATA(20)[e4e517ee07984a4000cd7b00cbcb545911c541c4] EQUALVERIFY CHECKSIG]?.
Payment to any other scriptPubKey may not be recognized by Alice.

@_date: 2017-12-18 09:30:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Why not witnessless nodes? 
Sign-to-contract enables some interesting protocols, none of which are in wide use as far as I?m aware. But if they were (and arguably this is an area that should be more developed), then SPV nodes validating these protocols will need access to witness data. If a node is performing IBD with assumevalid set to true, and is also intending to prune history, then there?s no reason to fetch those witnesses as far as I?m aware. But it would be a great disservice to the network for nodes intending to serve SPV clients to prune this portion of the block history.

@_date: 2017-12-18 09:38:01
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Clarification about SegWit transaction size and 
Addresses are entirely a user-interface issue. They don?t factor into the bitcoin protocol at all.
The bitcoin protocol doesn?t have addresses. It has a generic programmable signature framework called script. Addresses are merely a UI convention for representing common script templates. 1.. addresses and 3? addresses have script templates that are not as optimal as could be constructed with post-segwit assumptions. The newer bech32 address just uses a different underlying template that achieves better security guarantees (for pay-to-script) or lower fees (for pay-to-pubkey-hash). But this is really a UI/UX issue.
A ?fork? in bitcoin-like consensus systems has a very specific meaning. Changing address formats is not a fork, soft or hard.
There are many benefits to segregated witness. You may find this page helpful:

@_date: 2017-12-18 14:03:44
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Clarification about SegWit transaction size and 
Why would I send you coins to anything other than the address you provided to me? If you send me a bech32 address I use the native segwit scripts. If you send me an old address, I do what it specifies instead. The recipient has control over what type of script the payment is sent to, without any ambiguity.

@_date: 2017-12-19 13:58:40
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Sign / Verify message against SegWit P2SH 
For what it?s worth, I think it would be quite easy to do better than the implied solution of rejiggering the message signing system to support non-P2PKH scripts. Instead, have the signature be an actual bitcoin transaction with inputs that have the script being signed. Use the salted hash of the message being signed as the FORKID as if this were a spin-off with replay protection. This accomplishes three things:
(1) This enables signing by any infrastructure out there ? including hardware wallets and 2FA signing services ? that have enabled support for FORKID signing, which is a wide swath of the ecosystem because of Bitcoin Cash and Bitcoin Gold.
(2) It generalizes the message signing to allow multi-party signing setups as complicated (via sighash, etc.) as those bitcoin transactions allow, using existing and future tools based on Partially Signed Bitcoin Transactions; and
(3) It unifies a single approach for message signing, proof of reserve (where the inputs are actual UTXOs), and off-chain colored coins.
There?s the issue of size efficiency, but for the single-party message signing application that can be handled by a BIP that specifies a template for constructing the pseudo-transaction and its inputs from a raw script.

@_date: 2017-12-21 08:29:13
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Sign / Verify message against SegWit P2SH 
It doesn?t matter what it does under the hood. The api could be the same.

@_date: 2017-12-21 16:30:52
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Total fees have almost crossed the block reward 
Every transaction is replace-by-fee capable already. Opt-in replace by fee as specified in BIP 125 is a fiction that held sway only while the income from fees or fee replacement was so much smaller than subsidy.

@_date: 2017-12-28 12:49:38
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Transaction aging to relieve user concerns. 
However users can?t know with any certainty whether transactions will ?age out? as indicated, since this is only relay policy. Exceeding the specified timeout doesn?t prevent a miner from including it in the chain, and therefore doesn?t really provide any actionable information.

@_date: 2017-06-19 13:24:25
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP148 temporary service bit (1 << 27) 
It is essential that BIP-148 nodes connect to at least two other BIP-148 nodes to prevent a network partition in August 1st. The temporary service but is how such nodes are able to detect each other.

@_date: 2017-06-20 10:22:18
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
I think it is very na?ve to assume that any shift would be temporary.
We have a hard enough time getting miners to proactively upgrade to
recent versions of the reference bitcoin daemon. If miners interpret
the situation as being forced to run non-reference software in order
to prevent a chain split because a lack of support from Bitcoin Core,
that could be a one-way street.
On Tue, Jun 20, 2017 at 9:49 AM, Hampus Sj?berg via bitcoin-dev

@_date: 2017-06-20 15:48:23
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
Why do you say activation by August 1st is likely? That would require an entire difficulty adjustment period with >=95% bit1 signaling. That seems a tall order to organize in the scant few weeks remaining.

@_date: 2017-06-20 19:11:15
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
80% have set "NYA" in their coinbase string. We have no idea what that
means. People are equating it to BIP 91 -- but BIP 91 did not exist at
the time of the New York agreement, and differs from the actual text
of the NYA in substantive ways. The "Segwit2MB" that existed at the
time of the NYA, and which was explicitly referenced by the text is
the proposal by Sergio Demian Lerner that was made to this mailing
list on 31 March. The text of the NYA grants no authority for
upgrading this proposal while remaining compliant with the agreement.
This is without even considering the fact that in the days after the
NYA there was disagreement among those who signed it as to what it
I feel it is a very dangerous and unwarranted assumption people are
making that what we are seeing now is either 80% support for BIP-91 or
for the code in the btc1 repo.

@_date: 2017-05-30 14:24:20
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Hypothetical 2 MB hardfork to follow BIP148 
The 1MB classic block size is not redundant after segwit activation.
Segwit prevents the quadratic hashing problems, but only for segwit
outputs. The 1MB classic block size prevents quadratic hashing
problems from being any worse than they are today.
On Tue, May 30, 2017 at 6:27 AM, Jorge Tim?n via bitcoin-dev

@_date: 2017-10-31 18:46:54
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
I don?t think you need to set an order of operations, just treat the jet as TRUE, but don?t stop validation. Order of operations doesn?t matter. Either way it?ll execute both branches and terminate of the understood conditions don?t hold.
But maybe I?m missing something here.

@_date: 2017-11-01 08:08:46
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
Yes, if you use a witness script version you can save about 40 witness bytes by templating the MBV script, which I think is equivalent to what you are suggesting. 32 bytes from the saved hash, plus another 8 bytes or so from script templates and more efficient serialization.
I believe the conservatively correct approach is to do this in stages, however. First roll out MBV and tail call to witness v0. Then once there is experience with people using it in production, design and deploy a hashing template for script v1. It might be that we learn more and think of something better in the meantime.

@_date: 2017-11-03 09:19:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity proposal - Jets? 
To reiterate, none of the current work focuses on Bitcoin integration, and many architectures are possible.
However the Jets would have to be specified and agreed to upfront for costing reasons, and so they would be known to all validators. There would be no reason to include anything more then the identifying hash in any contract using the jet.

@_date: 2017-11-15 09:54:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Making OP_CODESEPARATOR and FindAndDelete in 
As good of an idea as it may or may not be to remove this feature from the code base, actually doing so would be crossing a boundary that we have not previously been willing to do except under extraordinary duress. The nature of bitcoin is such that we do not know and cannot know what transactions exist out there pre-signed and making use of these features.
It may be a good idea to make these features non standard to further discourage their use, but I object to doing so with the justification of eventually disabling them for all transactions. Taking that step has the potential of destroying value and is something that we have only done in the past either because we didn?t understand forks and best practices very well, or because the features (now disabled) were fundamentally insecure and resulted in other people?s coins being vulnerable. This latter concern does not apply here as far as I?m aware.

@_date: 2017-11-27 13:06:35
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Making OP_CODESEPARATOR and FindAndDelete in 
It is relevant to note that BIP 117 makes an insecure form of CODESEPARATOR delegation possible, which could be made secure if some sort of CHECKSIGFROMSTACK opcode is added at a later point in time. It is not IMHO a very elegant way to achieve delegation, however, so I hope that one way or another this could be resolved quickly so it doesn?t hold up either one of those valuable additions.
I have no objections to making them nonstandard, or even to make them invalid if someone with a better grasp of history can attest that CODESEPARATOR was known to be entirely useless before the introduction of P2SH?not the same as saying it was useless, but that it was widely known to not accomplish what a early-days script author might think it was doing?and the UTXO set contains no scriptPubKeys making use of the opcode, even from the early days. Although a small handful could be special cased, if they exist.

@_date: 2017-09-30 19:23:47
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
The CLEANSTACK rule should be eliminated, and instead the number of items on the stack should be incorporated into the signature hash. That way any script with a CHECKSIG is protected from witness extension malleability, and those rare ones that do not use signature operations can have a ?DEPTH 1 EQUALVERIFY? at the end. This allows for much simpler tail-call evaluation as you don?t need to pass arguments on the alt-stack.

@_date: 2017-09-30 22:04:32
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
Clean stack should be eliminated for other possible future uses, the most obvious of which is recursive tail-call for general computation capability. I?m not arguing for that at this time, just arguing that we shouldn?t prematurely cut off an easy implementation of such should we want to. Clean stack must still exist as policy for future soft-fork safety, but being a consensus requirement was only to avoid witness malleability, which committing to the size of the witness also accomplishes.
Committing to the number of witness elements is fully sufficient, and using the number of elements avoids problems of not knowing the actual size in bytes at the time of signing, e.g. because the witness contains a merkle proof generated by another party from an unbalanced tree, and unbalanced trees are expected to be common (so that elements can be placed higher in the tree in accordance with their higher expected probability of usage). Other future extensions might also have variable-length proofs.

@_date: 2017-10-01 11:34:07
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
I would also suggest that the 520 byte push limitation be removed for v1 scripts as well. MERKLEBRANCHVERIFY in particular could benefit from larger proof sizes. To do so safely would require reworking script internals to use indirect pointers and reference counting for items on stack, but this is worth doing generally, and introducing a per-input hashing limit equal to a small multiple of the witness size (or retaining the opcount limit).

@_date: 2017-10-01 12:27:21
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
For what benefit? If your script actually uses all the items on the stack, and if your script is not written in such a way as to allow malleability (which cannot be prevented in general), then they?re equivalent. Using weight instead of depth only needlessly restricts other parties to select a witness size up-front.
And to be clear, signing witness weight doesn?t mean the witness is not malleable. The signer could sign again with a different ECDSA nonce. Or if the signer is signing from a 2-of-3 wallet, a common scenario I hope, there are 3 possible key combinations that could be used. If using MBV, a 3-element tree is inherently unbalanced and the common use case can have a smaller proof size.
Witnesses are not 3rd party malleable and we will maintain that property going forward with future opcodes.
Any time all parties are not online at the same time in an interactive signing protocol, or for which individual parties have to reconfigure their signing choices due to failures. We should not restrict our script signature system to such a degree that it becomes difficult to create realistic signing setups for people using best practices (multi-key, 2FA, etc.) to sign. If I am a participant in a signing protocol, it would be layer violating to treat me as anything other than a black box, such that internal errors and timeouts in my signing setup don?t propagate upwards to the multi-party protocol.
For example, I should be able to try to 2FA sign, and if that fails go fetch my backup key and sign with that. But because it?s my infrequently used backup key, it might be placed deeper in the key tree and therefore signatures using it are larger. All the other signers need care is that slot  in the witness is where my Merkle proof goes. They shouldn?t have to restart and resign because my proof was a little larger than anticipated ? and maybe they can?t resign because double-spend protections!

@_date: 2017-10-01 13:39:11
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
Script validation flags of the correct place to do this. We already have policy validation flags that check for these things. They were not made consensus rules with Segwit v0 mainly due to concern over scope creep in an already large overhaul, of my memory is correct. Script versions and quadratic hashing fixes where the minimum necessary to allow segwit to activate safely while still enabling future upgrades that would otherwise have been hard forks. We knew that we would be later changing the EC signature scheme to be something that supported signature aggregation, and that would be more appropriate time to discuss such changes. As we are considering to do now (although witness versions means we don?t need to omnibus the script upgrade here either, so a v1 before signature aggregation is ready is fine IMHO).
In any case if there is any general witness malleability due to opcode semantics that it?s not fixed by one of our existing policy flags, that is a bug and I would encourage you to report it.
Arguing that every single user should be forced to restart an interactive signing session. That?s a very strong statement based on something that I would say is a preference that depends on circumstances.
What about an optional commitment to witness size in bytes? The value zero meaning ?I don?t care.? I would argue that it should be a maximum however, and therefor serialized as part of the witness. The serialization of this would be very compact (1 plus the difference between actual and maximum, with zero meaning not used.)

@_date: 2017-10-01 17:35:38
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
I'm not sure I agree with the "minor version" nomenclature, or that we would necessarily end up with any consensus-visible fields beyond 2.  There are two separate soft-fork version fields that were, I think it is fair to say now, inappropriately merged in the "script version? feature of segregated witness as described in BIP141.
First there is the witness type, which combined with the length of the commitment that follows specifies how data from the witness stack is used to calculate/verify the witness commitment in the scriptPubKey of the output being spent.  For v0 with a 20-byte hash, it says that those 20 bytes are the HASH160 of the top element of the stack.  For v0 with a 32-byte hash, it says that those 32 bytes are the HASH256 of the top element of the stack.
Second there is the script version, which is not present as a separate field for witness type v0.  Implicitly though, the script version for v0,20-byte is that the witness consists of two elements, and these are interpreted as a pubkey and a signature.  For v0,32-byte the script version is that the witness consists of 1 or more elements; with max 520 byte size constraints for all but the top element, which has a higher limit of 10,000 bytes; and the top-most element is interpreted as a script and executed with the modified CHECKSIG behavior defined by BIP141 and the CLEANSTACK rule enforced.
These are separate roles, one not being derivative of the other.  In an ideal world the witness type (of which there are only 16 remaining without obsoleting BIP141) is used only to specify a new function for transforming the witness stack into a commitment for verification purposes.  Merklized script would be one example: v2,32-byte could be defined to require a witness stack of at least two elements, the top most of which is a Merkle inclusion proof of the second item in a tree whose root is given in the 32-byte payload of the output.  Maybe v3 would prove inclusion by means of some sort of RSA accumulator or something.
Such a specification says nothing about the features of the subscript drawn from the Merkle tree, or even whether it is bitcoin script at all vs something else (Simplicity, DEX, RISC-V, Joy, whatever).  All that is necessary is that a convention be adopted about where to find the script version from whatever data is left on the stack after doing the witness type check (hashing the script, calculating a Merkle root, checking inclusion in an RSA accumulator, whatever).  A simple rule is that it is serialized and prefixed to the beginning of the string that was checked against the commitment in the output.
So v0,32-byte says that the top item is hashed and that hash must match the 32-byte value in the output.  This new v1,32-byte witness type being talked about in this thread would have exactly the same hashing rules, but will execute the resulting string based on its prefix, the script version, which is first removed before execution.
Sure first script version used could be a cleaned up script with a bunch of the weirdness removed (CHECKMULTISIG, I'm looking at you!); CLTV, CSV, and MBV drop arguments; disabled opcodes and unassigned NOPs become "return true"; etc.  Maybe v2 adds new opcodes.  But we can imagine script version that do something totally different, like introduce a new script based on a strongly-typed Merklized lambda calculus, or a RISC-V executable format, or whatever.
This has pragmatic implications with the separation of witness type and script version: we could then define a "MAST" output that proves the script used is drawn from a set represented by the Merkle tree.  However different scripts in that tree can use different versions.  It would be useful if the most common script is the key aggregated everyone-signs outcome, which looks like a regular bitcoin payment, and then contingency cases can be handled by means of a complicated script written in some newly added general computation language or a whole emulated RISC-V virtual machine.
I see no reason to do either. Gate new behavior based on script execution flags, which are set based on the script version.  Script versions not understood are treated as "return true" to begin with.  The interpreter isn't even going to try to decode the script according to the old rules, let alone try to execute it, so there's no reason for the old soft-fork compatability tricks.
The new soft-fork trick is that you increment the script version number.  That is all.
This is not signature-time commitment of extra script. Not without CHECKSIGFROMSTACK or something like it.
Propose these as their own script updates.  Script versioning makes such new features cheap.  There's no reason to create some sort of complex omnibus overhaul that does everything.
Again, this is off-topic for this thread.  I don't think a v1 witness type upgrade should do any of these things.  The v1 witness type should add a proper script version in the witness, and remove or simplify limits or unnecessary verification rules that are no longer necessary and/or hindering progress.  That?s it.
For example, I don't think a v1 witness version should be coupled with my tail-call semantics or the introduction of MERKLEBRANCHVERIFY (but if MBV was released already we could have it drop its arguments, which would be nice).  However it should drop the CLEANSTACK rule in favor of something else (like signatures committing to the witness depth and/or weight) since the tail-call BIP demonstrates it to be an impediment to extensibility and alternatives are not.  And it should drop the 520 byte push limitation, as the MBV BIP demonstrates use cases that have serialized proofs larger than that, like a k-of-N threshold with N=16.

@_date: 2017-10-05 13:33:56
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Version 1 witness programs (first draft) 
Here?s an additional (uncontroversial?) idea due to Russell O?Connor:
Instead of requiring that the last item popped off the stack in a CHECKMULTISIG be zero, have it instead be required that it is a bitfield specifying which pubkeys are used, or more likely the complement thereof. This allows signatures to be matched to pubkeys in the order given, and batch validated, with no risk of 3rd party malleability.

@_date: 2017-10-09 19:19:11
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] New difficulty algorithm needed for SegWit2x 
The problem of fast acting but non vulnerable difficulty adjustment algorithms is interesting. I would certainly like to see this space further explored, and even have some ideas myself.
However without commenting on the technical merits of this specific proposal, I think it must be said upfront that the stated goal is not good. The largest technical concern (ignoring governance) over B2X is that it is a rushed, poorly reviewed hard fork. Hard forks should not be rushed, and they should receive more than the usual level of expert and community review.
I?m that light, doing an even more rushed hard fork on an even newer idea with even less review would be hypocritical at best. I would suggest reframing as a hardfork wishlist research problem for the next properly planned hard fork, if one occurs. You might also find the hardfork research group a more accommodating venue for this discussion:

@_date: 2017-10-10 21:08:52
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] New difficulty algorithm needed for SegWit2x 
You phrase the question as if ?deploying a hard fork to bitcoin? would protect the bitcoin chain from the attack. But that?s not what happens. If you are hard forking from the perspective of deployed nodes, you are an different ledger, regardless of circumstance or who did it. Instead of there being one altcoin fighting to take hashpower from bitcoin, there?d now be 2. It is not at all obvious to me that this would be a better outcome.
If that isn?t reason enough, changing the difficulty adjustment algorithm doesn?t solve the underlying issue?hashpower not being aligned with users? (or even its owners?) interests. Propose a fix to the underlying cause and that might be worth considering, if it passes peer review. But without that you?d just be making the state of affairs arguably worse.
And so yes, *if* this incentive problem can?t be solved, and the unaltered bitcoin chain dies from disuse after suffering a hashpower attack, especially a centrally and/or purposefully instigated one, then bitcoin would be failed a failed project.
The thesis (and value proposition) of bitcoin is that a particular category of economic incentives can be used to solve the problem of creating a secure trustess ledger. If those incentives failed, then he thesis of bitcoin would have been experimentally falsified, yes. Maybe the incentives can be made better to save the project, but we?d have to fix the source of the problem not the symptoms.

@_date: 2017-10-12 08:25:23
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] New difficulty algorithm part 2 
While this might be true of some, it is most certainly not true of many, and it is a very dangerous men to the safety and well being of people on this list.
You don?t get bitcoin for being a bitcoin developer, and there is no reason to suppose a developer has any more or less bitcoin than anyone else in the industry.
It is certainly the case that a large number of people and organizations who are not developers hold massive amounts of bitcoin (hundred of thousands each, millions in aggregate).

@_date: 2017-10-20 20:55:55
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] bitcoin-dev Digest, Vol 29, Issue 24 
You could do that today, with one of the 3 interoperable Lightning implementations available. Lowering the block interval on the other hand comes with a large number of centralizing downsides documented elsewhere. And getting down to 1sec or less on a global network is simply impossible due to the speed of light. If you want point of sale support, I suggest looking into the excellent work the Lightning teams have done.

@_date: 2017-10-27 21:40:01
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
I have completed updating the three BIPs with all the feedback that I have received so far. In short summary, here is an incomplete list of the changes that were made:
* Modified the hashing function fast-SHA256 so that an internal node cannot be interpreted simultaneously as a leaf.
* Changed MERKLEBRANCHVERIFY to verify a configurable number of elements from the tree, instead of just one.
* Changed MERKLEBRANCHVERIFY to have two modes: one where the inputs are assumed to be hashes, and one where they are run through double-SHA256 first.
* Made tail-call eval compatible with BIP141?s CLEANSTACK consensus rule by allowing parameters to be passed on the alt-stack.
* Restricted tail-call eval to segwit scripts only, so that checking sigop and opcode limits of the policy script would not be necessary.
There were a bunch of other small modifications, typo fixes, and optimizations that were made as well.
I am now ready to submit these BIPs as a PR against the bitcoin/bips repo, and I request that the BIP editor assign numbers.
Thank you,
Mark Friedenbach

@_date: 2017-10-30 08:31:22
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
So enthused that this is public now! Great work. Sent from my iPhone

@_date: 2017-10-30 14:56:00
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
Script versions makes this no longer a hard-fork to do. The script version would implicitly encode which jets are optimized, and what their optimized cost is.

@_date: 2017-10-30 15:32:42
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
I was just making a factual observation/correction. This is Russell?s project and I don?t want to speak for him. Personally I don?t think the particulars of bitcoin integration design space have been thoroughly explored enough to predict the exact approach that will be used.
It is possible to support a standard library of jets that are general purpose enough to allow the validation of new crypto primitives, like reusing sha2 to make Lamport signatures. Or use curve-agnostic jets to do Weil pairing validation. Or string manipulation and serialization jets to implement covenants. So I don?t think the situation is as dire as you make it sound.

@_date: 2017-10-31 13:46:49
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
Nit, but if you go down that specific path I would suggest making just
the jet itself fail-open. That way you are not so limited in requiring
validation of the full contract -- one party can verify simply that
whatever condition they care about holds on reaching that part of the
contract. E.g. maybe their signature is needed at the top level, and
then they don't care what further restrictions are placed.
On Tue, Oct 31, 2017 at 1:38 PM, Russell O'Connor via bitcoin-dev

@_date: 2017-09-06 17:38:55
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics for 
I would like to propose two new script features to be added to the
bitcoin protocol by means of soft-fork activation. These features are
a new opcode, MERKLE-BRANCH-VERIFY (MBV) and tail-call execution
In brief summary, MERKLE-BRANCH-VERIFY allows script authors to force
redemption to use values selected from a pre-determined set committed
to in the scriptPubKey, but without requiring revelation of unused
elements in the set for both enhanced privacy and smaller script
sizes. Tail-call execution semantics allows a single level of
recursion into a subscript, providing properties similar to P2SH while
at the same time more flexible.
These two features together are enough to enable a range of
applications such as tree signatures (minus Schnorr aggregation) as
described by Pieter Wuille [1], and a generalized MAST useful for
constructing private smart contracts. It also brings privacy and
fungibility improvements to users of counter-signing wallet/vault
services as unique redemption policies need only be revealed if/when
exceptional circumstances demand it, leaving most transactions looking
the same as any other MAST-enabled multi-sig script.
I believe that the implementation of these features is simple enough,
and the use cases compelling enough that we could BIP 8/9 rollout of
these features in relatively short order, perhaps before the end of
the year.
I have written three BIPs to describe these features, and their
associated implementation, for which I now invite public review and
Fast Merkle Trees
BIP: Code: BIP: Code: Tail-call execution semantics
BIP: Code: Note: I have circulated this idea privately among a few people, and I
will note that there is one piece of feedback which I agree with but
is not incorporated yet: there should be a multi-element MBV opcode
that allows verifying multiple items are extracted from a single
tree. It is not obvious how MBV could be modified to support this
without sacrificing important properties, or whether should be a
separate multi-MBV opcode instead.
Kind regards,
Mark Friedenbach

@_date: 2017-09-06 19:20:06
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fast Merkle Trees 
This design purposefully does not distinguish leaf nodes from internal nodes. That way it chained invocations can be used to validate paths longer than 32 branches. Do you see a vulnerability due to this lack of distinction?

@_date: 2017-09-07 10:42:13
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fast Merkle Trees 
I've been puzzling over your email since receiving it. I'm not sure it
is possible to perform the attack you describe with the tree structure
specified in the BIP. If I may rephrase your attack, I believe you are
seeking a solution to the following:
Want: An innocuous script and a malign script for which
   double-SHA256(innocuous)
is equal to either
   fast-SHA256(double-SHA256(malign) || r) or
   fast-SHA256(r || double-SHA256(malign))
where r is a freely chosen 32-byte nonce. This would allow the
attacker to reveal the innocuous script before funds are sent to the
MAST, then use the malign script to spend.
Because of the double-SHA256 construction I do not see how this can be
accomplished without a full break of SHA256. The trick of setting r
equal to the padding only works when a single SHA256 is used for leaf
values. This is why double-SHA256 is specified in the BIP, and I will
edit the text to make that more clear.
Which brings us to the point that I think your original request of
separating the hash function of leaves from internal nodes is already
in the specification. I misunderstood your request at first to be that
MERKLEBRANCHVERIFY should itself perform this hash, which I objected
to as it closes of certain use cases such as chained verification of
proofs. But it is explicitly the case that leaf values and internal
updates are calculated with different hash functions.
I'm not intrinsicly opposed to using a different IV for fast-SHA256 so
as to remove the incompatability with single-SHA256 as the leaf hash
function, if that is the consensus of the community. It just adds
complication to implementations and so I want to make sure that
complication is well justified.
Mark Friedenbach

@_date: 2017-09-07 13:04:30
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Fast Merkle Trees 
TL;DR I'll be updating the fast Merkle-tree spec to use a different
      IV, using (for infrastructure compatability reasons) the scheme
      provided by Peter Todd.
This is a specific instance of a general problem where you cannot
trust scripts given to you by another party. Notice that we run into
the same sort of problem when doing key aggregation, in which you must
require the other party to prove knowledge of the discrete log before
using their public key, or else key cancellation can occur.
With script it is a little bit more complicated as you might want
zero-knowledge proofs of hash pre-images for HTLCs as well as proofs
of DL knowledge (signatures), but the basic idea is the same. Multi-
party wallet level protocols for jointly constructing scriptPubKeys
should require a 'delinearization' step that proves knowledge of
information necessary to complete each part of the script, as part of
proving the safety of a construct.
I think my hangup before in understanding the attack you describe was
in actualizing it into a practical attack that actually escalates the
attacker's capabilities. If the attacker can get you to agree to a
MAST policy that is nothing more than a CHECKSIG over a key they
presumably control, then they don't need to do any complicated
grinding. The attacker in that scenario would just actually specify a
key they control and take the funds that way.
Where this presumably leads to an actual exploit is when you specify a
script that a curious counter-party actually takes the time to
investigate and believes to be secure. For example, a script that
requires a signature or pre-image revelation from that counter-party.
That would require grinding not a few bytes, but at minimum 20-33
bytes for either a HASH160 image or the counter-party's key.
If I understand the revised attack description correctly, then there
is a small window in which the attacker can create a script less than
55 bytes in length, where nearly all of the first 32 bytes are
selected by the attacker, yet nevertheless the script seems safe to
the counter-party. The smallest such script I was able to construct
was the following:
     CHECKSIGVERIFY HASH160  EQUAL
This is 56 bytes and requires only 7 bits of grinding in the fake
pubkey. But 56 bytes is too large. Switching to secp256k1 serialized
32-byte pubkeys (in a script version upgrade, for example) would
reduce this to the necessary 55 bytes with 0 bits of grinding. A
smaller variant is possible:
    DUP HASH160  EQUALVERIFY CHECKSIGVERIFY HASH160  EQUAL
This is 46 bytes, but requires grinding 96 bits, which is a bit less
Belts and suspenders are not so terrible together, however, and I
think there is enough of a justification here to look into modifying
the scheme to use a different IV for hash tree updates. This would
prevent even the above implausible attacks.

@_date: 2017-09-11 19:03:42
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
My apologies for a delay in responding to emails on this list; I have
been fighting a cold.
(Also my apologies to Johnson Lau, as I forgot to forward this to the list.)
I believe you meant "unclean stack," and you are correct. This was
also pointed out last tuesday by a participant at the in-person
CoreDev meetup where the idea was presented.
This doesn't kill the idea, it just complicates the implementation
slightly. A simple fix would be to allow tail-recursion to occur if
the stack is not clean (as can happen with legacy P2SH as you point
out, or yet to be defined version 1+ forms of segwit script), OR if
there is a single item on the stack and the alt-stack is not empty.
For segwit v0 scripts you then have to move any arguments to the alt
stack before ending the redeem script, leaving just the policy script
on the main stack.
I disagree with this design requirement.
The SigOp counting method used by bitcoin is flawed. It incorrectly
limits not the number of signature operations necessary to validate a
block, but rather the number of CHECKSIGs potentially encountered in
script execution, even if in an unexecuted branch. (Admitedly MAST
makes this less of an issue, but there are still useful compact
scripts that use if/else constructs to elide a CHECKSIG.) Nor will it
account for aggregation when that feature is added, or properly
differentiate between signature operations that can be batched and
those that can not.
Additionally there are other resources used by script that should be
globally limited, such as hash operations, which are not accounted for
at this time and cannot be statically assessed, even by the flawed
mechanism by which SigOps are counted. I have maintained for some time
that bitcoin should move from having multiple separate global limits
(weight and sigops, hashed bytes in XT/Classic/BCH) to a single linear
metric that combines all of these factors with appropriate
A better way of handling this problem, which works for both SigOps and
HashOps, is to have the witness commit to the maximum resources
consumed by validation of the spend of the coin, to relay this data
with the transaction and include it in the SigHash, and to use the
committed maximum for block validation. This could be added in a
future script version upgrade. This change would also resolve the
issue that led to the clean stack rule in segwit, allowing future
versions of script to use tail-call recursion without involving the
Nevertheless it is constructive feedback that the current draft of the
BIP and its implementation do not count SigOps, at all. There are a
couple of ways this can be fixed by evaluating the top-level script
and then doing static analysis of the resulting policy script, or by
running the script and counting operations actually performed.
Additionally, it is possible that we take this time to re-evaluate
whether we should be counting SigOps other than for legacy consensus
rule compliance. The speed of verification in secp256k1 has made
signature operations no longer the chief concern in block validation
This is correct. Your feedback will be incorporated.
Is there a repo that contains the latest implementation of BIP 114,
for comparison purposes?
Kind regards,
Mark Friedenbach

@_date: 2017-09-12 12:57:10
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
DEPTH makes this relatively easy to do. Just repeat the following for
the maximum number of stack elements that might be used:
  DEPTH 1SUB IF SWAP TOALTSTACK ENDIF
There are probably more compact alternatives.
Using a new script version is easier, but not faster. There's a number
of things that might be fixed in a v1 upgrade, and various design
decisions to sort out regarding specification of a witness version
(version in the witness rather than the scriptPubKey).
Tree signatures and MAST are immediately useful to many services,
however, and I would hate to delay usage by six months to a year or
more by serializing dependencies instead of doing them in parallel.
That is what I'm suggesting. And yes, there are changes that would
have to be made to the p2p layer and transaction processing to handle
this safely. I'm arguing that the cost of doing so is worth it, and a
better path forward.
4MB of secp256k1 signatures takes 10s to validate on my 5 year old
laptop (125,000 signatures, ignoring public keys and other things that
would consume space). That's much less than bad blocks that can be
constructed using other vulnerabilities.
That's alright, I don't think it's necessary to purposefully restrict
one to compare them head to head with the same features. They are
different proposals with different pros and cons.
Kind regards,
Mark Friedenbach

@_date: 2017-09-17 08:36:48
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] proposal: extend WIF format for segwit 
Bech32 and WIF payload format are mostly orthogonal issues. You can design a new wallet import format now and later switch it to Bech32.

@_date: 2017-09-18 17:46:30
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
As some of you may know, the MAST proposal I sent to the mailing list
on September 6th was discussed that the in-person CoreDev meetup in
San Francisco. In this email I hope to summarize the outcome of that
discussion. As chatham house rules were in effect, I will refrain from
attributing names to this summary..
* An introductory overview of the BIPs was presented, for the purpose
  of familiarizing the audience with what they are attempting to
  accomplish and how they do so.
* There was a discussion of a single vs multi-element MBV opcode. It
  was put forward that there should perhaps be different opcodes for
  the sake of script analysis, since a multi-element MBV will
  necessarily consume a variable number of inputs. However it was
  countered that if the script encodes the number of elements as an
  integer push to the top of the stack immediately before the opcode,
  then static analyzability is maintained in such instances. I took
  the action item to investigate what an ideal serialization format
  would be for a multi-element proof, which is the only thing holding
  back a multi-element MBV proposal.
* It was pointed out that the non-clean-stack tail-call semantics is
  not compatible with segwit's consensus-enforcement of the clean
  stack rule. Some alternatives were suggested, such as changing
  deployment mechanisms. After the main discussion session it was
  observed that tail-call semantics could still be maintained if the
  alt stack is used for transferring arguments to the policy script. I
  will be updating the BIP and example implementation accordingly.
* The observation was made that single-layer tail-call semantics can
  be thought of as really being P2SH with user-specified hashing. If
  the P2SH script template had been constructed slightly differently
  such as to not consume the script, it would even have been fully
  compatible with tail-call semantics.
* It was mentioned that using script versioning to deploy a MAST
  template allows for saving 32 bytes of witness per input, as the
  root hash is contained directly in the output being spent. The
  downside however is losing the permissionless innovation that comes
  with a programmable hashing mechanism.
* The discussion generally drifted into a wider discussion about
  script version upgrades and related issues, such as whether script
  versions should exist in the witness as well, and the difference in
  meaning between the two. This is an important subject, but only of
  relevance in far as using a script version upgrade to deploy MAST
  would add significant delay from having to sort through these issues
  first.
This feedback led to some minor tweaks to the proposal, which I will
be making, as well as the major feature request of a multi-element
MERKLE-BLOCK-VERIFY opcode which requires a little bit more effort to
accomplish. I will report back to this list again when that work is
Mark Friedenbach

@_date: 2017-09-19 00:33:54
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
Well in the sense that "cleanstack" doesn't do what it says, sure.
However cleanstack was introduced as a consensus rule to prevent a
possible denial of service vulnerability where a third party could
intercept any* transaction broadcast and arbitrarily add data to the
witness stack, since witness data is not covered by a checksig.
Cleanstack as-is accomplishes this because any extra items on the
stack would pass through all realistic scripts, remaining on the stack
and thereby violating the rule. There is no reason to prohibit extra
items on the altstack as those items can only arrive there
purposefully as an action of the script itself, not a third party
malleation of witness data. You could of course use DEPTH to write a
script that takes a variable number of parameters and sends them to
the altstack. Such a script would be malleable if those extra
parameters are not used. But that is predicated on the script being
specifically written in such a way as to be vulnerable; why protect
against that?
There are other solutions to this problem that could have been taken
instead, such as committing to the number of items or maximum size of
the stack as part of the sighash data, but cleanstack was the approach
taken. Arguably for a future script version upgrade one of these other
approaches should be taken to allow for shorter tail-call scripts.
* Well, almost any. You could end the script with DEPTH EQUAL and that
  is a compact way of ensuring the stack is clean (assuming the script
  finished with just "true" on the stack). Nobody does this however
  and burning two witness bytes of every redeem script going forward
  as a protective measure seems like an unnecessary ask.

@_date: 2017-09-20 12:29:17
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
To be clear, I don?t think it is so much that the version should be moved to the witness, but rather that there are two separate version values here ? one in the scriptPubKey which specifies the format and structure of the segwit commitment itself, and another in the witness which gates functionality in script or whatever else is used by that witness type. Segwit just unfortunately didn?t include the latter, an oversight that should be corrected on the on the next upgrade opportunity.
The address-visible ?script version? field should probably be renamed ?witness type? as it will only be used in the future to encode how to check the witness commitment in the scriptPubKey against the data provided in the witness. Upgrades and improvements to the features supported by those witness types won?t require new top-level witness types to be defined. Defining a new opcode, even one with modifies the stack, doesn?t change the hashing scheme used by the witness type.
v0,32-bytes is presently defined to calculate the double-SHA256 hash of the top-most serialized item on the stack, and compare that against the 32-byte commitment value. Arguably it probably should have hashed the top two values, one of which would have been the real script version. This could be fixed however, even without introducing a new witness type. Do a soft-fork upgrade that checks if the witness redeem script is push-only, and if so then pop the last push off as the script version (>= 1), and concatenate the rest to form the actual redeem script. We inherit a little technical debt from having to deal with push limits, but we avoid burning v0 in an upgrade to v1 that does little more than add a script version.
v1,32-bytes would then be used for a template version of MAST, or whatever other idea comes along that fundamentally changes the way the witness commitment is calculated.

@_date: 2017-09-20 15:51:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] An explanation and justification of the tail-call and 
Over the past few weeks I've been explaining the MERKLEBRANCHVERIFY
opcode and tail-call execution semantics to a variety of developers,
and it's come to my attention that the BIPs presentation of the
concept is not as clear as it could be. Part of this is the fault of
standards documents being standards documents whose first and foremost
responsibility is precision, not pedagogy.
I think there's a better way to explain this approach to achieving
MAST, and it's worked better in the face to face and whiteboard
conversations I've had. I'm forwarding it to this list in case there
are others who desire a more clear explanation of what the
MERKLEBRANCHVERIFY and tail-call BIPs are trying to achieve, and what
any of it has to do with MAST / Merklized script.
I've written for all audiences so I apologize if it starts of at a
newbie level, but I encourage you to skim, not skip as I quickly start
varying this beginner material in atypical ways.
Review of P2SH
It's easiest to explain the behavior and purpose of these BIPs by
starting with P2SH, which we are generalizing from. BIP 16 (Pay to
Script Hash) specifies a form of implicit script recursion where a
redeem script is provided in the scriptSig, and the scriptPubKey is a
program that verifies the redeem script hashes to the committed value,
with the following template:
  HASH160  EQUAL
This script specifies that the redeem script is pulled from the stack,
its hash is compared against the expected value, and by fiat it is
declared that the redeem script is then executed with the remaining
stack items as arguments.
Sortof. What actually happens of course is that the above scriptPubKey
template is never executed, but rather the interpreter sees that it
matches this exact template format, and thereby proceeds to carry out
the same logic as a hard-coded behavior.
Generalizing P2SH with macro-op fusion
This template-matching is unfortunate because otherwise we could
imagine generalizing this approach to cover other use cases beyond
committing to and executing a single redeem script. For example, if we
instead said that anytime the script interpreter encountered the
3-opcode sequence "HASH160 <20-byte-push> EQUAL" it switched to
interpreting the top element as if it were a script, that would enable
not just BIP 16 but also constructs like this:
  IF
    HASH160  EQUAL
  ELSE
    HASH160  EQUAL
  ENDIF
This script conditionally executes one of two redeem scripts committed
to in the scriptPubKey, and at execution only reveals the script that
is actually used. All an observer learns of the other branch is the
script hash. This is a primitive form of MAST!
The "if 3-opcode P2SH template is encountered, switch to subscript"
rule is a bit difficult to work with however. It's not a true EVAL
opcode because control never returns back to the top-level script,
which makes some important aspects of the implementation easier, but
only at the cost of complexity somewhere else. What if there are
remaining opcodes in the script, such as the ELSE clause and ENDIF in
the script above?  They would never be executed, but does e.g. the
closing ENDIF still need to be present? Or what about the standard
pay-to-pubkey-hash "1Address" output:
  DUP HASH160 <20-byte-key-hash> EQUALVERIFY CHECKSIG
That almost looks like the magic P2SH template, except there is an
EQUALVERIFY instead of an EQUAL. The script interpreter should
obviously not treat the pubkey of a pay-to-pubkey-hash output as a
script and recurse into it, whereas it should for a P2SH style
script. But isn't the distinction kinda arbitrary?
And of course the elephant in the room is that by choosing not to
return to the original execution context we are no longer talking
about a soft-fork. Work out, for example, what will happen with the
following script:
  [TRUE] HASH160  EQUAL FALSE
(It returns false on a node that doesn't understand generalized
3-opcode P2SH recursion, true on a node that does.)
Implicit tail-call execution semantics and P2SH
Well there's a better approach than trying to create a macro-op fusion
franken-EVAL. We have to run scripts to the end to for any proposal to
be a soft-fork, and we want to avoid saving state due to prior
experience of that leading to bugs in BIP 12. That narrows our design
space to one option: allow recursion only as the final act of a
script, as BIP 16 does, but for any script not just a certain
template. That way we can safely jump into the subscript without
bothering to save local state because termination of the subscript is
termination of the script as a whole. In computer science terms, this
is known as tail-call execution semantics.
To illustrate, consider the following scriptPubKey:
  DUP HASH160 <20-byte-hash> EQUALVERIFY
This script is almost exactly the same as the P2SH template, except
that it leaves the redeem script on the stack rather than consuming
it, thanks to the DUP, while it _does_ consume the boolean value at
the end because of the VERIFY. If executed, it leaves a stack exactly
as it was, which we assume will look like the following::
   ...  Now a normal script is supposed to finish with just true or false on
the stack. Any script that finishes execution with more than a single
element on the stack is in violation of the so-called clean-stack rule
and is considered non-standard -- not relayable and potentially broken
by future soft-fork upgrades. But so long as at least one bit of
 is set, it is interpreted as true and the script
interpreter would normally interpret a successful validation at this
point, albeit with a clean-stack violation.
Let's take advantage of that by changing what the script interpreter
does when a script finishes with multiple items remaining on the stack
and top-most one evaluates as true -- a state of affairs that would
pass validation under the old rules. Now instead the interpreter
treats the top-most item on the stack as a script, and tail-call
recurse into it, P2SH-style. In the above example,  is
popped off the stack and is executed with  ...  remaining
on the stack as its arguments.
The above script can be interpreted in English as "Perform tail-call
recursion if and only if the HASH160 of the script on the top of the
stack exactly matches this 20-byte push." Which is, of course, what
BIP 16 accomplishes with template matching. However the implicit tail
call approach allows us to do much more than just P2SH!
For starters, it turns out that using HASH160 for P2SH was probably a
bad idea as it reduces the security of a multi-party constructed hash
to an unacceptable 80 bits. That's why segwit uses 256-bit hashes for
its pay to script hash format, for 128-bit security. Had we tail call
semantics instead of BIP 16, we could have just switched to a new
address type that decodes to the following script template instead:
  DUP HASH256 <32-byte-hash> EQUALVERIFY
Ta-da, we're back to full 128-bit security with no changes to the
consensus code, just a new address version to target this script
MAST with tail-call alone?
Or: an aside on general recursion
Our IF-ELSE Merklized Abstract Syntax Tree example above, rewritten to
use tail-call evaluation, might look like this (there are more compact
formulations possible, but our purpose here is not code golf):
  IF
    DUP HASH160  EQUALVERIFY
  ELSE
    DUP HASH160  EQUALVERIFY
  ENDIF
Either execution pathway leaves us with one of the two allowed redeem
scripts on the top of the stack, and presumably its arguments beneath
it. We then execute that script via implicit tail-call.
We could write scripts using IF-ELSE branches or other tricks to
commit to more than two possible branches, although this unfortunately
scales linearly with the number of possible branches. If we allow the
subscript itself to do its own tail-call recursion, and its subscript
and so on, then we could nest these binary branches for a true MAST in
the original sense of the term.
However in doing so we would have enabled general recursion and
inherit all the difficulties that come with that. For example, some
doofus could use a script that consists of or has the same effect as a
single DUP to cause an infinite loop in the script interpreter. And
that's just the tip of the iceberg of problems general recursion can
bring, which stem generally from resource usage no longer being
correlated with the size of the witness stack, which is the primary
resource for which there are global limits.
This is fixable with a gas-like resource accounting scheme, which
would affect not just script but also mempool, p2p, and other
layers. And there is perhaps an argument for doing so, particularly as
part of a hard-fork block size increase as more accurate resource
accounting helps prevent many bad-block attacks and let us set
adversarial limits closer to measured capacity in the expected/average
use case. But that would immensely complicate things beyond what could
achieve consensus in a reasonably short amount of time, which is a
goal of this proposal.
Instead I suggest blocking off general recursion by only allowing the
script interpreter to do one tail-call per input. To get log-scaling
benefits without deep recursion we introduce instead one new script
feature, which we'll cover in the next section. But we do leave the
door open to possible future general recursion, as we will note that
going from one layer of recursion to many would itself be a soft-fork
for the same reason that the first tail-call recursion is.
Merkle branch verify to the rescue!
In  and elsewhere there has been a desire for some
time to have an opcode that proves that an item was drawn from the set
used to construct a given Merkle tree. This is not a new idea although
I'm not aware of any actual proposal made for it until now. The most
simple version of the opcode, the version initially proposed, takes
three arguments:
     MERKLEBRANCHVERIFY 2DROP DROP
 is the 32-byte hash label of the root of the Merkle tree,
calculated using a scheme defined in the fast Merkle hash tree BIP.
 is 32 bytes of data which we are proving is part of the
Merkle hash tree -- usually the double-SHA256 hash of an item off the
 is the path through the Merkle tree including the hashes of
branches not taken, which is the information necessary to recalculate
the root hash thereby proving that  is in the Merkle tree.
The 2DROP and DROP are necessary to remove the 3 arguments from the
stack, as the opcode cannot consume them since it is soft-forked in.
There are two primary motivating applications of Merkle branch verify
(MBV), which will be covered next.
The MBV BIP will be extended to support extraction of more than one
item from the same Merkle tree, but for the rest of this explanation
we assume the current implementation of a single item proof, just for
MBV and MAST
This new opcode combines with single tail-call execution semantics to
allow for a very short and succinct MAST implementation:
  OVER HASH256  MERKLEBRANCHVERIFY 2DROP DROP
That's it. This script expects an input stack in the following format:
   ...   At the end of execution the script has verified that  is
part of the Merkle tree previously committed to, and  is
dropped from the stack. This leaves the stack ready for a tail-call
recursion into .
MBV and Key Aggregation
If the signature scheme supports key aggregation, which it happens
that the the new signature aggregation scheme being worked on will
support as a side effect, then there is a very cool and useful
application that would be supported as well: tree signatures as
described by Pieter Wuille[1].  This looks almost exactly the same as
the MAST script, but with a CHECKSIG tacked on the end:
  OVER HASH256  MERKLEBRANCHVERIFY 2DROP DROP CHECKSIG
This script expects an input stack of the following form:
And proves that the pubkey is drawn from the set used to construct the
Merkle hash tree, and then its signature is checked. While it should
be clear this has 1-of-N semantics, what might be less obvious is that
key aggregation allows any signature policy expressible as a monotone
Boolean function (anything constructible with combinations of AND, OR,
and k-of-N thresholds) to be transformed to a 1-of-N over a set of key
aggregates. So the above script is a generic template able to verify
any monotone Boolean function over combinations of pubkeys, which
encompasses quite a number of use cases!
[1] An argument for permission-less innovation
The driving motivation for the tail-call and MBV proposals, and the
reason they are presented and considered together is to enable
Merklized Abstract Syntax Trees. However neither BIP actually defines
a MAST template, except as an example use case of the primitive
features. This is very much on purpose: it is the opinion of this
author that new bitcoin consensus features should follow the UNIX
philosophy as expressed by Peter Salus and Mike Gancarz and
paraphrased by yours truly:
  * Write features that do one thing and do it well.
  * Write features to work together.
  * Simple is beautiful.
By using modularity and composition of powerful but simple tools like
MERKLEBRANCHVERIFY and single tail-call recursion to construct MAST we
enable a complex and desirable feature while minimizing changes to the
consensus code, review burden, and acquired technical debt.
The reusability of the underlying primitives also means that they can
be combined with other modular features to support use cases other
than vanilla MAST, or reused in series to work around various limits
that might otherwise be imposed on a templated form of MAST. At the
moment the chief utility of these proposals is the straightforward
MAST script written above, but as primitives they do allow a few other
use cases and also combine well with features in the pipeline or on
the drawing board. For example, in addition to MAST you can:
1. Use MERKLEBRANCHVERIFY alone to support honeypot bounties, as
   discussed in the BIP.
2. Use a series of MERKLEBRANCHVERIFY opcodes to verify a branch with
   split proofs to stay under script and witness push limitations.
3. Combine MERKLEBRANCHVERIFY with key aggregation to get
   Wuille-Maxwell tree signatures which support arbitrary signing
   policies using a single, aggregatable signature.
4. Combine tail-call execution semantics with CHECKSIGFROMSTACK to get
   delegation and signature-time commitment to subscript policy.
5. Combine MERKLEBRANCHVERIFY with a Merkle proof prefix check opcode
   and Lamport signature support to get reusable Lamport keys.
I believe these benefits and possible future expansions to be strong
arguments in favor of extending bitcoin in the form of small, modular,
incremental, and reusable changes that can be combined and used even
in ways unforeseen even by their creators, creating a platform for
unrestricted innovation.
The alternative approach of rigid templates achieves the stated goals,
perhaps even with slightly better encoding efficiency, but at the cost
of requiring workaround for each future innovation. P2SH is just such
an example -- we couldn't even upgrade to 128-bit security without
designing an entirely different implementation because of the
limitations of template pattern matching.
Efficiency gains from templating
Finally, I need to point out that there is one efficiency gain that a
proper template-matching implementation has over user-specified
schemes: reduction of witness data. This is both a practical side
effect of more efficient serialization that doesn't need to encode
logic as opcodes, as well as the fact that since the hashing scheme is
fixed, one layer of hashes can be removed from the serialization. In
the case of MAST, rather than encode the Merkle root hash in the
redeem script, the hash is propagated upwards and compared against the
witness commitment. The amount space saved from adopting a template is
about equal to the size of the redeem script, which is approximately
40 bytes of witness data per MAST input.
That is arguably significant enough to matter, and in the long term I
think we will adopt a MAST template for those efficiency gains. But I
feel strongly that since MAST is not a feature in wide use at this
time, it is strongly in our interests to deploy MBV, tail-call, as
well overhaul the CHECKSIG operator before tackling what we feel an
ideal MAST-supporting witness type would look like, so that with some
experience under our belt we can adopt a system that is as simple and
as succinct as possible while supporting the necessary use cases
identified by actual use of the features.
Kind regards,
Mark Friedenbach

@_date: 2017-09-22 14:11:03
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
There?s no need to put it in the transaction itself. You put it in the witness and it is either committed to as part of the witness (in which case it has to hold for all possible spend paths), or at spend time by including it in the data signed by CHECKSIG.

@_date: 2017-09-22 14:39:45
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
You generally know the witness size to within a few bytes right before signing. Why would you not? You know the size of ECDSA signatures. You can be told the size of a hash preimage by the other party. It takes some contriving to come up with a scheme where one party has variable-length signatures of their chosing

@_date: 2017-09-22 15:07:33
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
There is no harm in the value being a maximum off by a few bytes.

@_date: 2017-09-27 12:03:44
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Address expiration times should be added to 
While there is a lot that I would like to comment on, for the moment I will just mention that you should consider using the 17 bit relative time format used in CSV as an offset from the birthdate of the address, a field all addresses should also have.
This would also mean that addresses cannot last more than a year without user override, which might actually be a plus, but you could also extend the field by a few bits too if that was deemed not acceptable. An address should not be considered valid longer than anticipated lifetime of the underlying cryptosystem in any case, so every address should have an expiry.

@_date: 2017-09-27 14:09:16
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Address expiration times should be added 
First, there?s been no discussion so far for address expiration to be part of ?the protocol? which usually means consensus rules or p2p. This is purely about wallets and wallet information exchange protocols.
There?s no way for the sender to know whether an address has been used without a complete copy of the block chain and more indexes than even Bitcoin Core maintains. It?s simply not an option now, let alone as the blockchain grows into the future.

@_date: 2017-09-28 18:06:29
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
This article by Ron Lavi, Or Sattath, and Aviv Zohar was forwarded to
me and is of interest to this group:
    "Redesigning Bitcoin's fee market"
    I'll briefly summarize before providing some commentary of my own,
including transformation of the proposed mechanism into a relatively
simple soft-fork.  The article points out that bitcoin's auction
model for transaction fees / inclusion in a block is broken in the
sense that it does not achieve maximum clearing price* and to prevent
strategic bidding behavior.
(* Maximum clearing price meaning highest fee the user is willing to
   pay for the amount of time they had to wait.  In other words, miner
   income.  While this is a common requirement of academic work on
   auction protocols, it's not obvious that it provides intrinsic
   benefit to bitcoin for miners to extract from users the maximum
   amount of fee the market is willing to support.  However strategic
   bidding behavior (e.g. RBF and CPFP) does have real network and
   usability costs, which a more "correct" auction model would reduce
   in some use cases.)
Bitcoin is a "pay your bid" auction, where the user makes strategic
calculations to determine what bid (=fee) is likely to get accepted
within the window of time in which they want confirmation.  This bid
can then be adjusted through some combination of RBF or CPFP.
The authors suggest moving to a "pay lowest winning bid" model where
all transactions pay only the smallest fee rate paid by any
transaction in the block, for which the winning strategy is to bid the
maximum amount you are willing to pay to get the transaction
Unlike other proposed fixes to the fee model, this is not trivially
broken by paying the miner out of band.  If you pay out of band fee
instead of regular fee, then your transaction cannot be included with
other regular fee paying transactions without the miner giving up all
regular fee income.  Any transaction paying less fee in-band than the
otherwise minimum fee rate needs to also provide ~1Mvbyte * fee rate
difference fee to make up for that lost income.  So out of band fee is
only realistically considered when it pays on top of a regular feerate
paying transaction that would have been included in the block anyway.
And what would be the point of that?
As an original contribution, I would like to note that something
strongly resembling this proposal could be soft-forked in very easily.
The shortest explanation is:
    For scriptPubKey outputs of the form "", where
    the pushed data evaluates as true, a consensus rule is added that
    the coinbase must pay any fee in excess of the minimum fee rate
    for the block to the push value, which is a scriptPubKey.
Beyond fixing the perceived problems of bitcoin's fee auction model
leading to costly strategic behavior (whether that is a problem is a
topic open to debate!), this would have the additional benefits of:
    1. Allowing pre-signed transactions, of payment channel close-out
       for example, to provide sufficient fee for confirmation without
       knowledge of future rates or overpaying or trusting a wallet to
       be online to provide CPFP fee updates.
    2. Allowing explicit fees in multi-party transaction creation
       protocols where final transaction sizes are not known prior to
       signing by one or more of the parties, while again not
       overpaying or trusting on CPFP, etc.
    3. Allowing applications with expensive network access to pay
       reasonable fees for quick confirmation, without overpaying or
       querying a trusted fee estimator.  Blockstream Satellite helps
       here, but rebateable fees provides an alternative option when
       full block feeds are not available for whatever reason.
Using a fee rebate would carry a marginal cost of 70-100 vbytes per
instance.  This makes it a rather expensive feature, and therefore in
my own estimation not something that is likely to be used by most
transactions today.  However the cost is less than CPFP, and so I
expect that it would be a heavily used feature in things like payment
channel refund and uncooperative close-out transactions.
Here is a more worked out proposal, suitable for critiquing:
1. A transaction is allowed to specify an _Implicit Fee_, as usual, as
   well as one or more explicit _Rebateable Fees_.  A rebateable fee
   is an ouput with a scriptPubKey that consists of a single, minimal,
   nonzero push of up to 42 bytes.  Note that this is an always-true
   script that requires no signature to spend.
2. The _Fee Rate_ of a transaction is a fractional number equal to the
   combined implicit and rebateable fee divided by the size/weight of
   the transaction.
   (A nontrivial complication of this, which I will punt on for the
    moment, is how to group transactions for fee rate calculation such
    that CPFP doesn't bring down the minimum fee rate of the block,
    but to do so with rules that are both simple, because this is
    consensus code; and fair, so as to prevent unintended use of a
    rebate fee by children or siblings.)
3. The smallest fee rate of any non-coinbase transaction (or
   transaction group) is the _Marginal Fee Rate_ for the block and is
   included in the witness for the block.
4. The verifier checks that each transaction or transaction grouping
   provides a fee greater than or equal to the threshold fee rate, and
   at least one is exactly equal to the marginal rate (which proves
   the marginal rate is the minimum for the block).
This establishes the marginal fee rate, which alternatively expressed
is epsilon less than the fee rate that would have been required to get
into the block, assuming there was sufficient space.
5. A per-block _Dust Threshold_ is calculated using the marginal fee
   rate and reasonable assumptions about transaction size.
6. For each transaction (or transaction group), the _Required Fee_ is
   calculated to be the marginal fee rate times the size/weight of the
   transaction.  Implicit fee is applied towards this required fee and
   added to the _Miner's Fee Tally_.  Any excess implicit fee
   remaining is added to the _Implicit Fee Tally_.
7. For each transaction (group), the rebateable fees contribute
   proportionally towards towards meeting the remaining marginal fee
   requirement, if the implicit fee failed to do so.  Of what's left,
   one of two things can happen based on how much is remaining:
     A. If greater than or equal to the dust threshold is remaining in
        a specific rebateable fee, a requirement is added that an
        output be provided in the coinbase paying the remaining fee to
        a scriptPubKey equal to the push value (see  above).
        (With due consideration for what happens if a script is reused
         in multiple explicit fees, of course.)
     B. Otherwise, add remaining dust to the implicit fee tally.
8. For the very last transaction in the block, the miner builds a
   transaction claiming ALL of these explicit fees, and with a single
   zero-valued null/data output, thereby forwarding the fees on to the
   coinbase, as far as old clients are concerned.  This is only
   concerns consensus in that this final transaction does not change
   any of the previously mentioned tallies.
   (Aside: the zero-valued output is merely required because all
    transactions must have at least one output. It does however make a
    great location to put commitments for extensions to the block
    header, as being on the right-most path of the Merkle tree can
    mean shorter proofs.)
9. The miner is allowed to claim subsidy + the miner's fee tally + the
   explicit fee tally for themselves in the coinbase.  The coinbase is
   also required to contain all rebated fees above the dust threshold.
In summary, all transactions have the same actual fee rate equal to
the minimum fee rate that went into the creation of the block, which
is basically the marginal fee rate for transaction inclusion.
A variant of this proposal is that instead of giving the implicit fee
tally to the miner (the excess non-rebateable fees beyond the required
minimum), it is carried forward from block to block in the final
transaction and the miner is allowed to claim some average of past
fees, thereby smoothing out fees or providing some other incentive.

@_date: 2017-09-28 19:45:02
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
And in doing so either reduce the claimable income from other transactions (miner won?t do that), or require paying more non-rebateable fee than is needed to get in the block (why would the user do that?)
This is specifically addressed in the text you quoted. Discounted by the fact rebates would not be honored by other miners. The rebate would have to be higher than what they could get from straight fee collection, making it less profitable than doing nothing. You?d still have to pay the minimum fee rate of the other transactions or you?d bring down the miners income. Otherwise this is nearly the same cost as the rebate fee, since they both involve explicit outputs claimed by the miner, but the rebate goes back to you. So why would you not want to do that instead?
A different way of looking at this proposal is that it creates a penalty for out of band payments.

@_date: 2017-09-28 20:30:27
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
Only if your keys are online and the transaction is self-signed. It wouldn?t let you pre-sign a transaction for a third party to broadcast and have it clear at just the market rate in the future. Like a payment channel refund, for example.

@_date: 2017-09-29 08:22:14
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
This is correct. Under assumptions of a continuous mempool model however this should be considered the outlier behavior, other than a little bit of empty space at the end, now and then. A maximum fee rate calculated as a filter over past block rates could constrain this outlier behavior from ever happening too.

@_date: 2017-09-30 16:51:49
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Merkle branch verification & tail-call semantics 
============================== START ==============================
10s of seconds if no further restrictions are placed. It would be trivial to include a new per input rule that reduces it to ~1s without cutting off any non-attack script (require sigops per input to be limited to witness/sig size). secp256k1 is now fast enough that we don?t need a separate sigop limit.

@_date: 2018-01-09 21:40:30
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP 117 Feedback 
The use of the alt stack is a hack for segwit script version 0 which has the clean stack rule. Anticipated future improvements here are to switch to a witness script version, and a new segwit output version which supports native MAST to save an additional 40 or so witness bytes. Either approach would allow getting rid of the alt stack hack. They are not part of the proposal now because it is better to do things incrementally, and because we anticipate usage of MAST to better inform these less generic changes.
Your suggestion of ?single blob on the stack? seems to be exactly this proposal afaict? Note the witness data needs to be passed separately because signatures can?t be included in that single blob if that blob is hashed and compared against something in the scriptPubKey.
The sigop and opcode limit drop can be justified with some back of the envelope calculations. Non-scriptPubKey scripts are fundamentally limited by blocksize/weight and the most damage you can do, as an adversary, is limited by space. The most expensive thing you can do check a signature. Our assumptions about block size safety are basically due to how much computation you can stuff in a block with checksigs ? all the analysis there applies.
My suggestion is to limit the number of checksigs allowed in a script to size(script+witness)/64, but I wanted this to come up in review rather than complicate the code right off the bat.
I will make a strong assertion: static analyzing the number of opcodes and sigops gets us absolutely nothing. It is cargo cult safety engineering. No need to perpetuate it when it is now in the way.
Sent from my iPhone

@_date: 2018-01-10 07:57:34
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] BIP 117 Feedback 
I havent the hubris to suggest that we know exactly what a templated MAST *should* look like. It's not used in production anywhere. Even if we did have the foresight, the tail-call semantics allow for other constructions besides MAST and for the sake of the future we should allow such permission-less innovation. The proper sequence of events should be to enable features in a generic way, and then to create specialized templates to save space for common constructions. Not the other way around.
We've been down the route of templating new features, and have made mistakes. P2SH is a canonical example, which BIP 117 is fixing. P2SH only provides 80 bits of security to a multi-party construction. Had we been designing BIP 16 now we probably would have used double-SHA256 instead of RIPEMD160. I will also assert that had we been using single-use tail-call semantics *instead* of BIP 16, recognition of this limitation would have resulted in us immediately defining a longer '3...' address which used HASH256 instead, and we would have immediately benefited from the fix. Instead we had to wait years until segwit gave us the opportunity to fix it at the same time.
To take another example, in some ideal sense we probably shouldn't even be developing a programmable signature script framework. We should instead template a generic lightning-derived layer-2 protocol and use that for everything, including both payments (supporting cut-through) and payment channels for smart contracts. This may not be the majority technical opinion yet, but as someone working in this space I believe that's where we are headed: a single layer-2 protocol that's generic enough to use for all payment caching and smart contracts, while achieving a full anonymity set for all contracts, as closures look identical on the wire. Even if that's where things are headed, I hope it is clear that we are not yet at such a stage to standardize what that looks like. We still need many years of use of specialized lightning protocols and work to be done to make them more generic and applicable to other protocols.
I believe the situation to be similar with MAST. Even if we have a better idea of what the MAST constructions will look like, *nobody* uses MAST in production yet, and there are bound to be implementation issues in rolling it out, or unique variants we do not have the foresight to see now. As a concrete example, BIP 116 has been modified since the initial proposal to allow multiple branches to be proven at once from a single Merkle tree root. To be honest, I don't know exactly how this will be used. We were able to come up with a couple of examples to justify inclusion of the feature, but I anticipate that someone down the line will come up with an even more creative use. Maybe a payment channel that uses a single tree to simultaneously commit to both the policy script and a sequence number. Or something like that. If we provide a templated, special-cased MAST now before it sees wide use then we will be locking in the protocol that we anticipate people using without having any production experience or ecosystem-wide review. Frankly that strikes me as a poor engineering decision.
Finally, even if we had perfect foresight into how a feature will be used, which we don't, it is still the case that we would want to enable permission-less innovation with the generic construct, even if using it comes with a 40-byte or so witness hit. I make the argument for this in the "intuitive explanation of MAST" email I sent to this list back in September of last year. I will reproduce the argument below:
The driving motivation for the tail-call and MBV proposals, and the reason they are presented and considered together is to enable Merklized Abstract Syntax Trees. However neither BIP actually defines a MAST template, except as an example use case of the primitive features. This is very much on purpose: it is the opinion of this author that new bitcoin consensus features should follow the UNIX philosophy as expressed by Peter Salus and Mike Gancarz and paraphrased by yours truly:
  * Write features that do one thing and do it well.
  * Write features to work together.
  * Simple is beautiful.
By using modularity and composition of powerful but simple tools like MERKLEBRANCHVERIFY and single tail-call recursion to construct MAST we enable a complex and desirable feature while minimizing changes to the consensus code, review burden, and acquired technical debt.
The reusability of the underlying primitives also means that they can be combined with other modular features to support use cases other than vanilla MAST, or reused in series to work around various limits that might otherwise be imposed on a templated form of MAST. At the moment the chief utility of these proposals is the straightforward MAST script written above, but as primitives they do allow a few other use cases and also combine well with features in the pipeline or on
the drawing board. For example, in addition to MAST you can:
1. Use MERKLEBRANCHVERIFY alone to support honeypot bounties, as
   discussed in the BIP.
2. Use a series of MERKLEBRANCHVERIFY opcodes to verify a branch with
   split proofs to stay under script and witness push limitations.
3. Combine MERKLEBRANCHVERIFY with key aggregation to get
   Wuille-Maxwell tree signatures which support arbitrary signing
   policies using a single, aggregatable signature.
4. Combine tail-call execution semantics with CHECKSIGFROMSTACK to get
   delegation and signature-time commitment to subscript policy.
5. Combine MERKLEBRANCHVERIFY with a Merkle proof prefix check opcode
   and Lamport signature support to get reusable Lamport keys.
I believe these benefits and possible future expansions to be strong arguments in favor of extending bitcoin in the form of small, modular, incremental, and reusable changes that can be combined and used even in ways unforeseen even by their creators, creating a platform for unrestricted innovation.
The alternative approach of rigid templates achieves the stated goals, perhaps even with slightly better encoding efficiency, but at the cost of requiring workaround for each future innovation. P2SH is just such an example -- we couldn't even upgrade to 128-bit security without designing an entirely different implementation because of the limitations of template pattern matching.

@_date: 2018-01-18 13:00:03
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] ScriptPubkey consensus translation 
The downsides could be mitigated somewhat by only making the dual interpretation apply to outputs older than a cutoff time after the activation of the new feature. For example, five years after the initial activation of the sigagg soft-fork, the sigagg rules will apply to pre-activation UTXOs as well. That would allow old UTXOs to be spent more cheaply, perhaps making some dust usable again, but anyone who purposefully sent funds to old-style outputs after the cutoff are not opened up to the dual interpretation.

@_date: 2018-01-22 11:59:39
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Blockchain Voluntary Fork (Split) Proposal 
I believe you have the causality on that backwards. The tokens which are worth more value will attract more mining hash rate. Miners respond to cash-out value, they don?t set it.

@_date: 2018-01-23 06:39:37
@_author: Mark Friedenbach 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
I had the opposite response in private, which I will share here. As recently as Jan 9th feedback on BIP 117 was shared on this list by Pieter Wuille and others suggesting we adopt native MAST template instead of the user programmable combination of BIPs 116 and 117. Part of my response then was, I quote:
I havent the hubris to suggest that we know exactly what a templated MAST *should* look like. It's not used in production anywhere. Even if we did have the foresight, the tail-call semantics allow for other constructions besides MAST and for the sake of the future we should allow such permission-less innovation. The proper sequence of events should be to enable features in a generic way, and then to create specialized templates to save space for common constructions. Not the other way around. [1]
I take this advance as further evidence in favor of this view. As recently as 24 hours ago if you had asked what a native-MAST template would have looked like, the answer would have been something like Johnson Lau?s BIP 114, with some quibbling over details. Taproot is a clearly superior approach. But is it optimal? I don?t think we can claim that now. Optimality of these constructs isn?t something easily proven, with the nearest substitute being unchanging consensus over extended periods of time.
Every time we add an output type specialization, we introduce a new codepath in the core of the script consensus that must be maintained forever. Take P2SH: from this point forward there is no reason to use it in new applications, ever. But it must be forever supported. In an alternate universe we could have deployed a native MAST proposal, like BIP 114, only to have Taproot-like schemes discovered after activation. That would have been a sucky outcome. It is still the case that we could go for Taproot right now, and then in six months or a year?s time we find an important tweak or a different approach entirely that is even better, but the activation process had already started. That would be a sucky outcome we haven?t avoided yet.
This is not an argument against template specialization for common code paths, especially those which increase fungibility of coins. I do think we should have a native MAST template eventually, using Taproot or something better. However if I may be allowed I will make an educated guess about the origin of Taproot: I think it?s no coincidence that Greg had this insight and/or wrote it up simultaneous with a push by myself and others for getting MAST features into bitcoin via BIPs 98, 116, and 117, or 114. Cryptographers tend to only think up solutions to problems that are on their minds. And the problems on most people?s minds are primarily those that are deployable today, or otherwise near-term applicable.
BIPS 116 and 117 each provide a reusable component that together happens to enable a generic form of MAST. Even without the workarounds required to avoid CLEANSTACK violations, the resulting MAST template is larger than what is possible with specialization. However let?s not forget that (1) they also enable other applications like honeypots, key trees, and script delegation; and relevant to this conversation (2) they get the MAST feature available for use in production by the wider community. I don?t think I?d personally be willing to bet that we found the optimal MAST structure in Greg?s Taproot until we have people doing interesting production work like multisig wallets, lightning protocol, and the next set of consensus features start putting it into production and exploring edge cases. We may find ways Taproot can be tweaked to enable other applications (like encoding a hash preimage as well) or simplify obscure corner cases.
I feel quite strongly that the correct approach is to add support for generic features to accomplish the underlying goal in a user programmable way, and THEN after activation and some usage consider ways in which common use cases can be made more efficient through output specialization. To take a more obvious example, lightning protocol is still an active area or research and I think it is abundantly clear that we don?t know yet what the globally optimal layer-2 caching protocol will be, even if we have educated guesses as to its broad structure. A proposal right now to standardize a more compact lightning script type would be rightly rejected. It is less obvious but just as true that the same should hold for MAST.
I have argued these points before in favor of permission less innovation first, then application specialization later, in [1] and at the end of the rather long email [2]. I hope you can take the time to read those if you still feel we should take a specialized template approach instead of the user programmable BIPSs 116 and 117.
[1]  [2]
