
@_date: 2011-12-16 23:41:56
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Protocol extensions 
Hey, guys.
I haven't posted here before so I'll introduce myself. My name's Eric,
I've been developing cryptocurrency-related
software for several months now, I've implemented some libraries for
dealing with core bitcoin datastructures, made
some custom builds of bitcoind and interfaced it with a few apps I've written.
In doing so, I've come to appreciate just how little of the potential
for the bitcoin protocol is being exploited right now...
not only in terms of the script features but in terms of the potential
commands and node types that could exist.
For instance, the protocol spec at
 only has 16 commands
listed and
only one service type...despite having a full 12 bytes for a command
code and a full eight bytes for a services
The fact that only one node service type is specified is probably due
to the fact that the satoshi client was written
to be a standalone monolithic app that took care of all the essential
needs for a network of peers.
i.e. block chain storage/management, transaction signing/verification,
key generation/wallet management, block mining, etc...
However, I think there's an urgent need for breaking up all these
different tasks into separate components that can run as independent
services on different types of devices.
One of the big issues I'm dealing with now pertains to block chain
storage. As of right now, it is implemented as sequential
disk files using Berkeley DB in the satoshi client. Then you have
other projects that have been using SQL tables, etc...
But I believe the direction this really needs to move towards is some
sort of distributed hash table...and the database queries
should be performed using the bitcoin protocol itself. Perhaps adding
a few more commands. As things stand right now,
the only way to query for transactions or blocks is by their hash. And
once a transaction gets incorporated into a block and
removed from the transaction pool, one can no longer query it by the
transaction hash without stepping outside the bitcoin protocol.
We need access to the disk file that stores the blocks whether it be
via Berkeley DB or SQL or whatever.
I propose an extension to the bitcoin protocol to provide methods for
performing more sophisticated queries, such as "Give me
an inventory of transactions involving this particular public key" or
"Give me an inventory all transactions in the last n blocks with
unredeemed outputs." This could be done by adding a few more commands.
Furthermore, I propose a new network services type for nodes that
serve as block chain/transaction pool storage.
Of couse, any peer that wishes to verify the integrity of the block
chain would still have to download at the very least
all the block headers...and to be completely sure, also all the blocks
themselves...and verify everything. But it would be
very nice to be able to run thin services that can rely on other
network peers to do this work. It is still possible to attain
a high level of confidence in the integrity by querying multiple peers
for similar objects and comparing. It is also possible
to run your own dedicated block chain storage servers which you trust.
There are other ideas I have for other types of services, too.
Anyhow, I'm just throwing this out there...if anyone's interested I'd
love to develop these ideas further and help put together some
-Eric Lombrozo

@_date: 2011-12-20 22:19:46
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Protocol extensions 
There are other issues besides IP address anonymization that would
need to be addressed. I'm sure at least a good number of you have read
 and have seen Dan Kaminsky's
i.e. all fund aggregations (transactions with multiple inputs using
different public keys) make it easy to associate all the public keys
to a single entity. Large movements of bitcoin to addresses that
haven't been seen before are often interesting events. Then you can
correlate transactions with trades on exchanges or with other data
sources for time and amount.
However, going back to what had been said earlier, the bitcoin
protocol itself is not really designed to address these issues. It is
designed with the goal of rapidly propagating transactions over a
network and getting a bunch of peers to be able to independently
verify that they occurred in a particular order and that the
signatures are valid.
The subject of how to anonymize cryptocurrencies is a separate one,
IMHO...and one which needs to address not only how to hide the
identity of those who relay transactions but also how to organize and
manipulate wallets as to thwart attempts at block chain analysis. And
these topics, although interesting in and of themselves, was not what
this thread was intended to address. This thread was intended to
address the issue of extending the protocol to allow for independently
running thin or specialized services that can all interface via the
bitcoin protocol without requiring one to step outside the protocol
with special gateway access.

@_date: 2011-12-21 03:42:40
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Protocol extensions 
Is it just me or does it seem inevitable that at some point supernodes
will emerge that other nodes trust to validate transactions for them?
Supernodes needn't even store the entire block chain and transaction
pool...it would be sufficient that they keep lists of IP addresses of
other trustworthy nodes and partition them into a hashspace.
Anonymous peers have no reputation to defend...but a trusted supernode
would, which could provide just enough incentive for the supernode to
do its best to ensure the nodes it vouches for are indeed legit. Of
course, unless the supernode is validating the entire block chain and
transaction pool itself, it could only assess the trustworthiness of
other nodes by performing random sampling.
Michael, I really like your ideas and the clarity you bring to the
issue. Regarding the potential attack vector you mention, would it be
possible to partition the hashspace to minimize the risk that an
attacker can manage to disproportionately gain control over a part of
the hashspace?

@_date: 2011-12-21 03:50:47
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Why are my posts being put in new threads? 
I've made a couple recent posts that were intended for the Protocol
extensions thread but have been put in new threads. What part of the
email message is used to identify the thread to which it belongs? I
would have thought the subject, but apparently it isn't.

@_date: 2012-12-12 14:09:38
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development]  Zero-length scripts 
I've noticed a few transactions that have zero-length input and/or output scripts. There's a couple examples in block 0000000000000159a27442ee8b7f9ffad0cd799b003eafe007de9fbb47bd6ce7:
Txs: cdb553214a51ef8d4393b96a185ebbbc2c84b7014e9497fea8aec1ff990dae35, af32bb06f12f2ae5fdb7face7cd272be67c923e86b7a66a76ded02d954c2f94d
Is there ever a legitimate reason to create a transaction with a zero-length script? Should the protocol even allow it?
-Eric Lombrozo

@_date: 2012-12-21 00:53:16
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development]  Multiwallet support 
I started working on a new feature to allow for watch-only addresses in wallets. In order to integrate this feature nicely into bitcoin / bitcoin, it will be necessary to disable signing and privkey export operations for watch-only addresses. Since disabling these things for only some of the keys in a wallet but not others is an API nightmare and complicates CreateTransaction logic, I propose adding multiple wallet capabilities and specifying upon creation whether a wallet is a:
1)  full signing wallet
2) watch-only wallet
In order to do the above, it will be necessary to add multiple wallet support. Anyhow, that was my initial motivation for multiple wallets - but obviously, there are a number of other reasons why people might want multiple wallet support.
Adding the ability to specify multiple wallets with associated names and passphrases in the config file should be fairly straightforward. However, exposing multiple wallets via RPC will be tricky as the existing RPC is not designed to support multiple wallets.
As to not break compatibility with the existing RPC calls, we can have a main wallet which is always used as the default wallet. If the user wants to use a different wallet, the name of the wallet would have to be specified in the call. Unfortunately, it doesn't look like we can use many of the existing RPC calls (sendfrom, sendmany,sendtoaddress, etc...) since they all have optional parameters already and it would be awkward to just tack on the wallet name parameter at the end. Also, walletpassphrase is problematic as it is not stateless. So it looks like we need a whole separate set of calls which require a wallet name and passphrase (if the wallet is encrypted).
For instance,
I welcome any proposals or suggestions as to how this should be done.

@_date: 2012-12-21 10:11:21
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Multiwallet support 
I like that idea. I'm close to having something working along those lines. Hopefully I'll be able to push something by tonight.

@_date: 2012-03-21 22:13:13
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Adding callback hooks to the satoshi client 
Hey, guys.
I've been writing a number of apps that require realtime event
notifications, where the JSON-RPC API clearly doesn't suffice.
There are two approaches I've been taking to this end:
1) Writing my own library for dealing with raw bitcoin structures and
connecting to bitcoin nodes via the bitcoin protocol.
2) Making custom builds of the satoshi client putting callback hooks
in key points.
Neither of these two approaches is ideal. (1) involves a lot of code
duplication, (2) involves patching the satoshi client source
each time I grab a later version, with the everpresent risk of
something breaking and the need to continue maintaining these patches.
Moreover, unfortunately many of these key points happen to be in files
like main.cpp which see frequent changes.
I would like to propose adding these callback hooks to the main
branch. I am willing to help locate these key points, reorganize the
to place these methods in separate source files, define a callback
mechanism, and contribute source code.
-Eric Lombrozo

@_date: 2012-03-22 08:23:17
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Adding callback hooks to the satoshi 
The callback architecture could be such that other code would never
need to enter into the wallet-handling process/memory space. For
instance, client applications could subscribe a particular URL to get
sent an HTTP POST.
For the apps I've been working on, there really isn't any need to
access the wallet space. I was talking more about events like "A new
transaction was just seen" or "A new block was just seen", like what
libcoin seems to support (sorry, Michael, I haven't really had a
chance to look at it in depth but I will). Then there are other types
of events for other bitcoin messages could also be useful: new addr,
new node connected, node disconnected, bitcoin alert, etc...
Then there are events for dealing with potential attacks: DoS attempt,
double-spend attempts (two transactions seen with valid signatures
claiming the same output), node sending malformed messages, etc...
And then there are alerts pertaining to the status of the bitcoind
process itself: bitcoind started, bitcoind ready to accept
connections, bitcoind stopping, etc...
None of these events require the callback subscriber to have any
access to the bitcoind process/memory space and all the I/O could be
done via IPC or over network sockets.

@_date: 2012-03-24 01:41:25
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Adding callback hooks to the satoshi 
I'd also like to be able to get detailed information of message format
errors for debugging other nodes that connect to bitcoind. For
instance, if a transaction is rejected because the signature is
invalid, I want to know this. If it's because the amount is out of
range or because the output couldn't be connected, I want to know
this, too. I especially want to know if it was because the transaction
is claiming an output that has already been claimed by another
For now, I've had to resort to sticking tracers and stubs into
bitcoind. It would be really nice to not only be able to write an
error log but to also let the connecting node know what went wrong.
Obviously these types of messages should *not* be part of the bitcoin
protocol itself since it invites all kinds of attacks. But it would be
wonderful to have a side channel for this type of data, and it could
also be done using callbacks.
The callback mechanism could be configurable in a similar fashion to
the RPC in the bitcoin.conf file.
-Eric Lombrozo

@_date: 2013-12-27 13:36:29
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Testnet block explorer 
I've built a shell around the bitcoind JSON-RPC, along with a websockets server that provides realtime transaction and block feeds which can be used with bitcoin mainnet and testnet as well as any of the alt chains and formats it similar to blockchain.info with the bootstrap look-and-feel, i.e. A goal of this project was simplicity in deployment to a new server.
The JSON-RPC is missing address indexing and an "unspent" API, so these things are still missing from this shell.

@_date: 2013-12-27 13:48:06
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Testnet block explorer 
I'll add testnet to it as well - sorry, Ben, for lifting the css (I'm a programmer, not a graphic designer) - if anyone would like to help me make the styling original, I would be more than happy to collaborate.

@_date: 2013-09-06 15:47:50
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Simple contacts exchange (was: Social 
Why not just use the transaction hash itself for the lookup? Also, presumably you'd want to encrypt the data so that only the recipient of the transaction can do this lookup.

@_date: 2014-08-09 18:20:09
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] BIP32 - invalidation 
Does bitcoin properly handle the case of a hash collision? no - because it is considered too unlikely. The case of I_L >= n is also astronomically unlikely, so it's more a matter of improved performance and simpler data structures under expected circumstances and taking that less than 1 in 2^127 chance that it will fail, in which case we can recover by moving everything over to a new tree.
-Eric Lombrozo

@_date: 2014-03-01 04:29:48
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Making the H in HD keychains useful 
I've been trying to find ways to make HD keychain wallets (BIP0032) really usable from an application development perspective. I think we all know a number of solid use cases and possible applications for the D in HD, but nobody seems to have really found a way to make use of the H in a way that is actually manageable from a usability standpoint.
After pondering it a bit more, I think I've stumbled upon at least a couple issues that seem to give hints as to how we can change this.
Hierarchical organizations do not generally tend to be designed up front, cast in stone. In the real world, hierarchies tend to evolve organically, growing new branches as entities differentiate themselves to different purposes. Organizations grow over time. Sometimes branches merge, sometimes branches die. This means that for HD keychains to be truly useful, they too need to be sufficiently flexible to adapt to the needs of a growing and evolving organization. It needs to be simple to create and move branches around as the need for them arises without having to plan the structure a priori.
A significant problem I'm runnign into in trying to build applications around the BIP0032 standard is the lack of a clear separation between signing keys and hierarchical nodes. That's to say, a child of a node can either be used as a signing key or as a parent for new branches to the tree. From a usability standpoint, what this means is that one must be very careful in how one allocates keys from the very beginning - if one mixes signing keys with new branching nodes in the same generation, the whole thing becomes a horrendous mess. Moreover, it is impossible to generally distinguish these two fundamentally different types of objects (at least from a use model perspective) just from the extended key representation, something that is certain to create significant confusion as we try to design applications that can share these types of objects.
An organization might begin as a single individual who just wants to generate signing keys for him/herself. Later on, this individual might bring on another individual or two and create new branches for them. With the current HD keychain structure, unless this individual made sure to set aside these new branches from the start, the individual is now forced to mix the new branches in at the same level of the hierarchy as the signing keys. Instead, it should be possible to branch off any node without having to worry at all about whether or not that node has been used to generate signing keys at all.
A possible workaround to this issue is to always allocate a specific child for hierarchical derivation and the rest of the children for signing keys. Then to create subbranches, the specific child would be used as the new parent, effectively alternating generations between signing keys and organizational nodes. However, this solution seems pretty ugly.
A better solution, IMO, is to only use BIP0032 for organizational hierarchy and have a different mechanism for generating a sequence of signing keys from a given node. This different mechanism could be used standalone by those not needing the full set of hierarchical features. For those who do want to use the hierarchical features, it could be seeded by the keys in the BIP0032 hierarchy. These individual signing keys would NEVER be represented in the same format as the organizational hierarchy nodes, thus ensuring applications can share these structures without risk of confusion.
Until we make this clear distinction between organizational hierarchy (which parallels real-world organizations) and signing keys (which are merely cryptographic primitives, preferably never even shown directly to most endusers), I think we'll fail to find good ways to make the H in HD keychains useful.
-Eric Lombrozo

@_date: 2014-03-05 13:31:01
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] New side channel attack that can recover 
If we don't mind sacrificing some performance when signing, there's a fairly simple way to implement a constant-time constant-cache-access-pattern secp256k1.
It is based on the idea of branchless implementations of the field and group operations.
Multiprecision arithmetic can be implemented branch-free by assuming fixed sized limbs and always performing dummy carry operations even when they aren't needed.
The most critical field operation that could potentially leak data is the modular inverse. Again, if we don't mind a slow implementation, a simple constant-time implementation involves exponentiation by the field modulus minus two - which is a known constant.
As for group operations in secp256k1, the main sources of leaks are the branchings that exist in typical implementations as well as optimizations for special inputs,
i.e. To avoid leaking any information, we can use the most general operation, Point Addition, in the following way:
Always carry through the full point addition algorithm even if we get POINT_AT_INFINITY. Also, always carry through the POINT_DOUBLE operation even on unequal inputs. Store the three possible results (POINT_ADDITION, POINT_AT_INFINITY, and POINT_DOUBLE) and then do a branchless conditional swap with the output location as a final step.
Branchless swaps can be performed using bitwise operations such as the examples here: In the case of bitcoin, signature verification is where performance optimization is really helpful - and here there are no risks of sidechannel leaks, so we can go ahead and use the most optimal implementations. But for signing, the amount of throughput required is generally not that large and constant-time implementations will be more than adequate on typical hardware.
-Eric Lombrozo

@_date: 2014-03-05 14:14:20
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Everything you say is true.
However, branchless does reduce the attack surface considerably - if nothing else, it significantly ups the difficulty of an attack for a relatively low cost in program complexity, and that might still make it worth doing.
As for uniform memory access, if we avoided any kind of heap allocation, wouldn't we avoid such issues?
Anyhow, without having gone into the full details of this particular attack, it seems the main attack point is differences in how squaring and multiplication (in the case of field exponentiation) or doubling and point addition (in the case of ECDSA) are performed. I believe using a branchless implementation where each phase of the operation executes the exact same code and accesses the exact same stack frames would not be vulnerable to FLUSH+RELOAD.
"To be able to recover the sequence of point additions and doublings, the spy program should distinguish
between consecutive doubling operations and must be able to order them with respect to point additions.
Our spy program achieves this by setting the time slot to less than half the length of the group operations.
With the selected curve, group add operations take 7,928 cycles on average, while group double operation
take 7,601 cycles. Setting the time slot to 3,000 cycles ensures that there is an empty time slot within any
group operation, allowing our spy to correctly distinguish consecutive doubles"
The approach I've suggested makes doubling operations indistinguishable from point additions from the perspective of cache access.

@_date: 2014-03-05 14:26:31
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] New side channel attack that can recover 
Oh, I absolutely agree that this type of attack is NOT the weakest link in security. There are MANY far easier targets in bitcoind and typical use scenarios of it. If we want to dramatically improve the security of a typical bitcoin wallet, the FLUSH+RELOAD attack is probably not where our efforts would be best rewarded trying to prevent.
However, this thread IS about this particular attack vector - and my suggestion IS specific to this thread.
-Eric Lombrozo

@_date: 2014-03-11 19:48:13
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Multisign payment protocol? 
Ciphrex CoinVault ( is currently using parallel trees with lexicographic sorting of keys.
CoinVault is also using a partially signed transaction format whereby 0-length placeholders are used for missing signatures in the transaction scripts. Once all the required signatures to satisfy the policy are present, the remaining zero-length placeholders are removed so the transaction can be broadcast to the network. These partially signed transactions can be shared with other parties to an account or other signing devices for the purpose of requesting additional signatures.

@_date: 2015-08-03 01:20:39
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
There have already been two notable incidents requiring manual intervention and good-faith cooperation between core devs and mining pool operators that would have either never gotten resolved alone or would have ended up costing a lot of people a lot of money had no action been taken (March 2013 and July 2015). They were both caused by consensus disagreement that directly or indirectly were brought about by bigger blocks. There is *strong* evidence?and a great deal of theory explaining it?that links larger blocks with the propensity for consensus forks that require manual intervention.
Please, can we stop saying this is merely about decentralization and trustlessness? The very model upon which the security of the system is based *broke*?as in, we were only able to recover because a few individuals deliberately manipulated the consensus rules to fix it manually. Shouldn?t we more highly prioritize fixing the issues that can lead to these incidents than trying to increase throughput? Increasing block size cannot possibly make these forking tendencies better?but it very well could make them worse.
- Eric

@_date: 2015-08-03 01:38:41
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
Bah, I don?t know if you?re just trolling me, Hector?but I?ll give you the benefit of the doubt and act like you aren?t.
We already have much more efficient, far more scalable systems that allow this kind of cooperation you speak of without the inconveniences of blockchains and such. These incidents do, fortunately, present some of the better sides of humanity?but?the design of the network *broke* - and for reasons that are now well understood to be only worsened by larger blocks. These incidents are *not supposed to happen* - and if they do, it means we?ve botched something up and need to fix it. And by fix it, I mean fix the protocol so that given our best understanding of things in the present we can significantly reduce the potential for its occurrence in the future.
The correct incentives here were not due to people potentially losing a lot of money. The incentives here were well-intentioned altruism. Some miners lost money as a result of these actions?and they didn?t put up a fight. if you want to design a system around the assumption that this is how all such incidents will be resolved, please don?t spoil this for the rest of us.
- Eric

@_date: 2015-08-03 02:01:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] A reason we can all agree on to increase block 
If it?s an as-of-yet unidentified issue that comes up, yes?obviously we can?t plan for everything and we?ll make some mistakes. But here we?re talking about specific well-known issues (or at least well-known today) for which several people have proposed potential solutions. However, these things have been all but ignored in the public discourse.
I agree. But again, once we?ve identified specific issues, it is irresponsible to continue to pretend they don?t exist?and to more highly prioritize changes that can only make the problem worse.
Again, for the record, I am not against ultimately allowing bigger blocks. I think it would be a good thing to be able to do this?and my main concerns are not around things like equipment costs or typical household bandwidth. I just think security is a more important feature than greater throughput and prioritize it thusly.
I don?t disagree?clearly even the miners that lost money believed that consensus was more valuable to them than a few bitcoins. However, it seems to be EXTREMELY dangerous to assume that it will always work out this way. What?s to stop a mining majority from deciding on-the-fly they want to keep a particular consensus rule that benefits them even if the core developers disagree?

@_date: 2015-08-03 10:22:16
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Incentivising full nodes by having SPV nodes to 
I proposed something along these lines in the lightning-dev mailing list:
It's probably a little off-topic there...but I'm very interested in
discussing such ideas.
On Aug 3, 2015 10:06 AM, "Luv Khemani via bitcoin-dev" <

@_date: 2015-08-03 10:54:12
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Incentivising full nodes by having SPV nodes to 
The point is that without concrete direct incentives, many might find it
rational to just leech off others...which might actually work for a
while...but this is obviously not stable equilibrium...and it's very hard
to do any game theory on it.
Current SPV implementations necessarily diverge from the p2p model and
strongly tend toward a client/server architecture. It was originally
thought that mining would provide an incentive to run servers...but we all
know that's not at all how things have turned out.
Using offchain protocols with blockchain settlement guarantees, it is
possible for clients to request proofs directly from servers without
requiring any additional authentication layers...and can perhaps even be
onion-routed to hide the client's identity.
Checking SPV proofs is cheap...it only requires downloading headers,
checking PoW, and constructing a lookup table mapping block hashes to
merkle roots. This structure can easily be stored entirely in RAM on
typical consumer devices (it's at most 80 bytes per block, or a little over
30MB right now and grows at a very manageable rate). The most expensive
step to prove inclusion of a specific transaction in the blockchain is then
O(log n) sha256 operations, where n is the number of transactions in the
Using p2sh, we ensure the txout script is a mere 22 bytes. Full validation
nodes can fully prune the partial merkle tree structure when the output is
spent. SPV proof serving nodes would still need to store full blocks...but
as long as they are getting paid for this market forces could lead to a
stable balance between clients and servers...or at least adaptive dynamics
that prevent deficits or surpluses.
If I screwed something up in this analysis someone please correct me.
- Eric
On Aug 3, 2015 10:29 AM, "Eric Voskuil via bitcoin-dev" <

@_date: 2015-08-04 20:07:48
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Eli Dourado on "governance" 
Rather than speculating on fake markets, why don?t we use theory, empirical data, and engineering to fix the damn problems?

@_date: 2015-08-15 14:32:10
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
You deeply disappoint me, Mike.
Not only do you misrepresent many cogent, well thought out positions from a great number of people who have published and posted a number of articles detailing an explaining in-depth technical concerns?you also seem to fancy yourself more capable of reading into the intentions of someone who disappeared from the scene years ago, before we even were fully aware of many things we now know that bring the original ?plan? into question.
I ask of you, as a civilized human being, to stop doing this divisive crap. Despite your protestations to the contrary, YOU are the one who is proposing a radical departure from the direction of the project. Also, as several of us have clearly stated before, equating the fork of an open source project with a fork of a cryptoledger is completely bogus - there?s a lot of other people?s money at stake. This isn?t a democracy - consensus is all or nothing. The fact that a good number of the people most intimately familiar with the inner workings of Satoshi?s invention do not believe doing this is a good idea should give you pause.
Please stop using Bitcoin as your own political football?for the sake of Bitcoin?and for your own sake. Despite your obvious technical abilities (and I sincerely do believe you have them) you are discrediting yourself and hurting your own reputation.
- Eric

@_date: 2015-08-15 15:16:10
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
I fully agree that core developers are not the only people who should have a say in this. But again, we?re not talking about merely forking some open source project - we?re talking about forking a ledger representing real assets that real people are holding?and I think it?s fair to say that the risk of permanent ledger forks far outweighs whatever benefits any change in the protocol might bring. And this would be true even if there were unanimous agreement that the change is good (which there clearly IS NOT in this case) but the deployment mechanism could still break things.
If anything we should attempt a hard fork with a less contentious change first, just to test deployability.
Again, let?s figure out a hard fork mechanism and test it with a far less contentious change first
For the record, I do not work for Blockstream. Neither do a bunch of other people who have published a number of concerns. Very few of the concerns I?ve seen from the technical community seem to be motivated primarily by profit motives.
It should also be pointed out that *not* making drastic changes is the default consensus policy?and the burden of justifying a change falls on those who want to make the change. Again, the risk of permanent ledger forks far outweighs whatever benefits protocol changes might bring.
Miners are NOT in direct competition with the lightning network and sidechains - these claims are patently false. I recommend you take a look at these ideas and understand them a little better before trying to make any such claims. Again, I do not work for Blockstream?and my agenda in this post is not to promote either of these ideas?but with all due respect, I do not think you properly understand them at all.
I don?t think the concern here is so much that some people want to increase block size. It?s the *way* in which this change is being pushed that is deeply problematic.

@_date: 2015-08-15 16:07:45
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Please take the lightning 101 discussion to another thread.
The main point I was trying to make was that Mike is clearly misrepresenting the views of a great number of people who have deep, intimate knowledge of how things work and are almost certainly not primarily motivated by their own potential for profits.

@_date: 2015-08-17 05:33:14
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Or can?t you create a transaction that?s still within the op count and sig ops limits but is larger than 1MB?

@_date: 2015-08-17 07:03:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
In the entire history of Bitcoin we?ve never attempted anything even closely resembling a hard fork like what?s being proposed here.
Many of us have wanted to push our own hard-forking changes to the protocol?and have been frustrated because of the inability to do so.
This inability is not due to any malice on anyone?s part?it is a feature of Satoshi?s protocol. For better or worse, it is *very hard* to change the rules?and this is exactly what imbues Bitcoin with one of its most powerful attributes: very well-defined settlement guarantees that cannot be suddenly altered nor reversed by anyone.
We?ve managed to have a few soft forks in the past?and for the most part these changes have been pretty uncontroversial?or at least, they have not had nearly the level of political divisiveness that this block size issue is having. And even then, we?ve encountered a number of problems with these deployments that have at times required goodwill cooperation between developers and mining pool operators to fix.
Again, we have NEVER attempted anything even remotely like what?s being proposed - we?ve never done any sort of hard fork before like this. If even fairly uncontroversial soft forks have caused problems, can you imagine the kinds of potential problems that a hard fork over some highly polarizing issue might raise? Do you really think people are going to want to cooperate?!?
I can understand that some people would like bigger blocks. Other people might want feature X, others feature Y?and we can argue the merits of this or that to death?but the fact remains that we have NEVER attempted any hard forking change?not even with a simple, totally uncontroversial no-brainer improvement that would not risk any sort of ill-will that could hamper remedies were it not to go as smoothly as we like. *THIS* is the fundamental problem - the whole bigger block thing is a minor issue by comparison?it could be any controversial change, really.
Would you want to send your test pilots on their first flight?the first time an aircraft is ever flown?directly into combat without having tested the plane? This is what attempting a hard fork mechanism that?s NEVER been done before in such a politically divisive environment basically amounts to?but it?s even worse. We?re basically risking the entire air force (not just one plane) over an argument regarding how many seats a plane should have that we?ve never flown before.
We?re talking billlions of dollars? worth of other people?s money that is on the line here. Don?t we owe it to them to at least test out the system on a far less controversial, far less divisive change first to make sure we can even deploy it without things breaking? I don?t even care about the merits regarding bigger blocks vs. smaller blocks at this point, to be quite honest - that?s such a petty thing compared to what I?m talking about here. If we attempt a novel hard-forking mechanism that?s NEVER been attempted before (and which as many have pointed out is potentially fraught with serious problems) on such a politically divisive, polarizing issue, the result is each side will refuse to cooperate with the other out of spite?and can easily lead to a war, tanking the value of everyone?s assets on both chains. All so we can process 8 times the number of transactions we currently do? Even if it were 100 times, we wouldn?t even come close to touching big payment processors like Visa. It?s hard to imagine a protocol improvement that?s worth the risk.
I urge you to at least try to see the bigger picture here?and to understand that nobody is trying to stop anyone from doing anything out of some desire for maintaining control - NONE of us are able to deploy hard forks right now without facing these problems. And different people obviously have different priorities and preferences as to which of these changes would be best to do first. This whole XT thing is essentially giving *one* proposal special treatment above those that others have proposed. Many of us have only held back from doing this out of our belief that goodwill amongst network participants is more important than trying to push some pet feature some of us want.
Please stop this negativity - we ALL want the best for Bitcoin and are doing our best, given what we understand and know, to do what?s right.

@_date: 2015-08-17 07:30:24
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
The hope is that eventually the network will be sufficiently resilient and robust to be able to handle anything that?s thrown at it. But it?s still a baby?and this is a serious problem indeed, because on the one hand we don?t want any central authority but on the other it still needs some guardians?and we don?t have anything resembling the kind of institution that could possibly be entrusted to nurture and care for this baby until it is ready to go out on its own.
Imagine, when the US Constitution was being written, if suddenly everyone started to just propose their own different version of it and insisting (under threat of fork) on their own versions before any sort of government could be created. Yes, I know the US federal government is not exactly the paragon of decentralization?but regardless of your views on the US government, it?s still a somewhat analogous situation. Until the system was in place, some people (who at the time were unelected) had to bootstrap the process.
For better or worse, Satoshi has left the picture?and no clear succession model was put in place. The Bitcoin Foundation, which for a time attempted to be a guardian institution, ended up self-destructing. It was an utter failure.
We don?t have any sort of institution like this?and we don?t really want one. But the system is still not fully in place. Importantly, we lack any mechanisms to be able to make potentially controversial changes without serious risks.
It would be amazing if despite this trial-by-fire we still survived and managed to pull through. And if we do we?ll be stronger for it. But quite sincerely, I would have wanted the system to be a little more mature before putting it through this trial. At least I would have liked to have gone through a test hard-fork using a far less politically divisive issue.
Anyhow, completely separate from my views on governance, etc?my main point is that we?re ALL trying to do what?s best given our understanding and resources?and we?ve all poured our hearts and souls into this. We might disagree on certain things, but let?s stop this negativity and misrepresentation and try to figure out a way forward that is less likely to lead to a war.
- Eric

@_date: 2015-08-17 08:07:39
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
I specifically asked you to stop misrepresenting - I?m NOT in favor of guided decentralization, I never said anything like that. *THIS* is the problem?you?re reading intentions into others that simply are NOT there. If you don?t really understand something, ask.
I want complete decentralization - but for practical reasons, which should be obvious, we cannot start at this point. Bitcoin came into existence because Satoshi wrote a whitepaper and implemented the idea - and it was his rules. There was no voting, no committee, no proof-of-work, no nothing?it was a complete dictatorship in the beginning.
Again, misrepresentation - ?you fear the choice, not the change? - why should anyone ask *you* what I fear? Why don?t you ask *me*?

@_date: 2015-08-17 09:37:01
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
I don?t think what theymos did is very constructive.I understand his position?but it only hurts the cause, unfortunately - the PR battle is not the same thing as a discussion on technical merits. He hurts the PR battle and plays into Mike?s hand by doing that. The actual underlying issue actually has little to do with block size - it has to do with Mike and Gavin feeling that the core devs are being obstructionist.
Regardless of the technical merits of XT, the fact that we?ve never done a hard fork before, not even for things some other devs have wanted?and not due to any malice on anyone?s part but because simply that?s just the nature of decentralized consensus with well-defined settlement guarantees?this is the problem - Mike and Gavin think they?re somehow special and their fork should be pushed while the rest of us resist pushing our own controversial pet ideas because we want civility and understand that at this stage in Bitcoin?s development trying to fork the blockchain over highly divisive issues is counterproductive and destructive.
But the fact of the matter is that in the PR battle, arguments against the fork actually play into Mike?s hand, and that?s the problem.
The whole block size thing is too nuanced and too easily spun simplistically. It?s too easy to spin resistance to bigger blocks (even though the resistance is actually much more towards untested hardforking mechanisms and serious security concerns) as ?obstructionism? and it?s too easy to spin bigger blocks as ?scalability? because most of the people can?t tell the fucking difference.
The fact is most of the people don?t really understand the fundamental issue and are taking sides based on charismatic leadership and authority which is actually entirely counter to the spirit of decentralized consensus. It?s beyond ironic.
If you guys want to win the PR battle, the key is to make it clear that you are not obstructionist and are giving everyone equal treatment?Bitcoin was designed such that changing the rules is *hard* and this is a feature. Bitcoin simply does not have a reliable and tested hard forking mechanism?and a hard fork for such a politically divisive issue will almost certainly lead to a lack of cooperation and refusal to work together out of spite. All of us would like to be able to process more transactions on the network. It?s not a matter of whether we think higher capacity is a bad thing - it?s more that some of us are concerned that Bitcoin is not sufficiently mature to be able to handle such a schism with so much hostility.
Let?s face it, folks - from a PR standpoint, the block size issue is irrelevant. Nobody really understands it except for a handful of people - I?ve tried to explain it, I?ve even written articles about it - but most people just don?t get it. Most people don?t really get scalability either - they seem to think that scalability is just doing the same thing you?ve always done manyfold.
Block size is an especially dangerous issue politically because it?s one of those that requires deep understanding yet superficially sounds really simple. It?s perfect Dunning-Kruger bait.
So let?s be a little smarter about this.

@_date: 2015-08-17 09:55:43
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Annoucing Not-BitcoinXT 
I should add that in the interest of peace and goodwill, I extend an offer to both Mike and Gavin to make their grievances heard?but only on the condition that we make a good effort to avoid misrepresentation and misreading of the other side?s intentions.

@_date: 2015-08-18 11:52:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
I?m 100% in favor of attempting a hard fork using a far less controversial, far less risky consensus rule change. We should stop wasting our energy arguing over stuff we don?t really know and understand and can?t predict very well - and we should especially avoid using a highly contentious change as our first hard fork deployment.
I?m also in favor of trying a small block increase before attempting any major jumps. I don?t think we should be focusing so much on long-term block size adjustment rules right now - much more critical is to develop a hard fork mechanism and to make sure we can deploy it. So something along these lines is probably a step in the right direction.

@_date: 2015-08-18 13:51:38
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
Problem is if you know most of the people running the testnet personally (as is pretty much the case with many current testnets) then the deployment amounts to ?hey guys, let?s install the new version?

@_date: 2015-08-18 14:17:41
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
People have already been testing big blocks on testnets.
The biggest problem here isn?t whether we can test the code in a fairly sterile environment. The biggest problem is convincing enough people to switch without:
1) Pissing off the other side enough to the point where regardless of who wins the other side refuses to cooperate
2) Screwing up the incentive model, allowing people to sabotage the process somehow
3) Setting a precedent enabling hostile entities to destroy the network from within in the future
These kinds of things seem very hard to test on a testnet.

@_date: 2015-08-18 19:53:16
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin is an experiment. Why don't we have an 
As an aside, combining reward halving with block size limit doubling would have probably been a good idea :)

@_date: 2015-08-19 12:28:09
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
It?s good to know that Gavin still manages to keep his priorities straight. Of course, vacationing at the moment that the most controversial change in the history of Bitcoin which threatens to split the community is officially ?announced? is probably exactly what he should be doing.
I?m glad to know that we?ll continue to have this amazing leadership under the XT fork.

@_date: 2015-08-19 12:48:05
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
Unfortunately, I think that from a PR angle, removing Gavin from commit privileges right now will probably play into his hand. Sadly.
Say what you will regarding Gavin and Mike?s technical merits, they?ve been quite clever on the PR front. Framing this issue as ?obstructionism from the core devs? and relying on the fact that many people out there can?t seem to tell the difference between a source code fork and a blockchain fork.

@_date: 2015-08-19 13:04:16
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin XT Fork 
I would fully like to see the consensus stuff split off into a separate organization from everything else. Let XT continue to support additional p2p messages or relay policies or whatever. Let Mike and Gavin argue for their improved wallet or whatever - I have absolutely no problem with that.
But the consensus code should NOT be subject to the same commit policies?and we should make an effort to separate the two clearly. And we should find a way to communicate the difference succinctly and clearly to laypeople (which is something I think the XT opponents have been horrible at doing so far).

@_date: 2015-08-19 13:11:24
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] bitcoin-dev list etiquette 
With all due respect, right now the biggest challenge facing Bitcoin is not technical but political. I would love to see this list go back to technical discussions, but unfortunately, until this political stuff is resolved, even technical discussion is purely philosophical as there?s little chance of actually making good progress on consensus?which in a space where everything depends on consensus pretty much makes everything else moot.

@_date: 2015-08-19 13:19:16
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] bitcoin-dev list etiquette 
Those who are too smart to engage in politics are punished by being governed by those who are dumber.
- Plato

@_date: 2015-08-20 20:45:56
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Core Devs : can you share your thoughts about all 
I think everyone who has contributed any commits to the Bitcoin Core project should have a say in this.

@_date: 2015-08-21 20:07:15
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
Unfortunately we have no way of rigorously proving functional equivalence
other than code review and unit testing. The simpler the consensus code
(and the more we can write it in a style that affords provability of
correctness) the easier it will be in the future to compare implementations.
Prior to swapping out implementations, we should at the least run it
through the gauntlet and perhaps run both implementations side-by-side.
All I/O should be treated abstractly in the API.
In C++ I really like using a nearly bare-bones signal template for most
async message handling, i.e.
This greatly facilitates support for async bidirectional I/O, etc...with
minimal overhead.
But others might have other stylistic preferences.
- Eric
On Fri, Aug 21, 2015, 12:46 PM Jorge Tim?n <

@_date: 2015-08-23 01:23:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
I've been pushing for greater modularization since I first got into
bitcoin. I got quickly frustrated when I was only able to get through very
few things (i.e. moving core structure serialization classes to a separate
unit not called main). Working on Bitcoin has an added layer of frustration
that goes beyond most open source projects: even though we're clearly in
userland working at the application layer, a good layered protocol design
is still lacking. We have no standards process separate from what basically
amount to updates to one specific reference implementation. And we all need
to agree on any major change, since a blockchain that is easily forked in
contentious ways pretty much defeats its own purpose.
I went off to develop my own stack, where I could more easily avoid
politics and focus on engineering. But I now understand the politics are
inevitable. Bitcoin is inherently a cooperative project. Several people
have poured themselves passionately into the reference codebase, most of
whom did it (at least initially) purely as unpaid volunteers. There's a lot
of love that's gone into this. But it's become pretty clear that the
modularization is no longer merely a matter of good engineering - it is
essential to resolving serious political challenges.
Perhaps the most frustrating thing of all is watching people pushing for
relatively superficial yet highly controversial changes while we still lack
the proper infrastructure to handle these kinds of divergences of opinion
without either stagnating or becoming polarized.
I could continue working to reimplement an entire stack from scratch, as
several others have also done - but besides the serious effort duplication
this entails, it doesn't really seem like it will ultimately be a
convergent process. It's too easy to let ego and habit dictate one's
preferences rather than rational engineering considerations.
I know that some might feel I'm just preaching to the choir, but we should
probably take a step back from implementation hackery and try to specify
some core protocol layers, focusing on interfaces. Specifically, we need a
consensus layer that doesn't try to specify networking, storage, wallets,
UI, etc. Let different people improve upon these things independently in
their own implementations. What matters is that we all converge on a common
history and state. At the same time, let's open up more competition on all
these other things that are separate from the consensus layer.
If only we were to dedicate a fraction of the effort we've put into this
whole block size circus into what's actually important...and I blame myself
as well...
On Sat, Aug 22, 2015, 4:05 AM Tamas Blummer via bitcoin-dev <

@_date: 2015-08-23 02:19:26
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Libconsensus separated repository (was Bitcoin 
One thing it occurs to me (and I don't know if this has been suggested
before) we could do is separate the BIP process into at several distinct
1) Commit structure changes/consensus rule change proposals
- Consensus-building process (how are proposals debated, improved, vetted,
and selected)
- Update/deployment mechanisms for rule changes
- Specific hard fork proposals
- Specific soft fork proposals
2) Peer policies
- Seeding and discovery mechanisms
- Relay policies
- p2p message support
3) RPC
4) Everything else

@_date: 2015-08-24 17:58:55
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
When I was working on mSIGNA I became a little torn on the whole filtering
mechanism. I fully support connection filtering...but in practice always
run my own full node instances to connect to due to the three fatal flaws:
1) no mechanism for short proofs of tx nonexclusion, txout unspentness,
block validity, nor the ability to find the first instance of the use of a
scriptPubKey without full blockchain scanning, 2) poor privacy, 3) lack of
incentives to run servers.
I always felt that BIP37 was necessarily a step towards a client/server
Having said that, I have found the filter mechanism useful, if only because
no "special" server is required. However, in practice I'd rather make the
distinction between trustless peers and a client/server model more explicit.
On Mon, Aug 24, 2015, 10:41 AM Wladimir J. van der Laan via bitcoin-dev <

@_date: 2015-08-24 18:15:39
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
It would be very useful to not only be able to switch filtering on and off
globally...but to be able to switch on a per-connection basis. But then
again, perhaps it would be smarter to ditch the whole bloom filter thing in
favor of an actual client/server architecture with proper authentication
and access controls.
The RPC was supposed to be this client/server architecture...but in
practice it sucks so bad for doing anything beyond administering a node
instance you fully control yourself that I eschewed it entirely in my
wallet design.
On Mon, Aug 24, 2015, 11:07 AM Matt Corallo via bitcoin-dev <

@_date: 2015-08-24 18:33:40
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Revisiting NODE_BLOOM: Proposed BIP 
Indeed, so I don't really have a problem with this proposal.
On Mon, Aug 24, 2015, 11:30 AM Wladimir J. van der Laan

@_date: 2015-08-24 21:01:26
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Splitting BIPs 
Seems like a lot of effort and goodwill is being wasted on contention over
things we don't really need to agree upon. In order to help us better
prioritize, I propose adding an extra attribute to BIPs indicating their
"level" which is split into five as follows:
1. Consensus (hard/soft fork)
2. Peer Services
3. RPC
4. Implementations
5. Applications
I posted an example of what such a table might look like here: http://
If other folks also think this is a good idea I'll start working on a BIP
draft for this.

@_date: 2015-08-24 23:25:54
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Splitting BIPs 
Also, the current "type" attribute needs modification. There are different
degrees of "standard". Just because a lot of people do X doesn't need to
mean that doing X is "officially" endorsed by any other devs. At most
levels below 1, disagreements might be entirely tolerable for many things.

@_date: 2015-08-27 20:51:55
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Splitting BIPs 
I posted a new draft of the proposal:
The subsections still need to be fleshed out a bit more. I'd love any
comments or suggestions.

@_date: 2015-08-29 17:51:56
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Consensus based block size retargeting algorithm 
In principle I am sympathetic to dynamic block size proposals...but in
practice it seems we're barking up the wrong tree. Without mechanisms for
incentivizing validators...and checks and balances between the interests of
regular users (who want to reduce fees and confirmation time), miners (who
want to balance hashing and propagation time costs with revenue), and
validator nodes (who currrently lack any direct incentives), I think we're
talking about significant protocol complications with potential benefits
that are hard to model at best.
On Sat, Aug 29, 2015, 3:16 AM Btc Drak via bitcoin-dev <

@_date: 2015-12-16 18:44:56
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Segregated Witness in the context of 
There are no good short-term scaling solutions...this is a very hard problem that necessarily requires a lot of out-of-the-box thinking, something 2015 has seen a LOT of...and I'm optimistic about the ideas presented thus far.
At least SW *is* a scaling solution (albeit most of the important benefits are long term). The issue of fee events has nothing to do with scaling - it has to do with economics...specifically whether we should be subsidizing transactions, who should pay the bill for it, etc. My own personal opinion is that increasing validation costs works against adoption, not for it...even if it artificially keeps fees low - and we'll have to deal with a fee event sooner or later anyhow. You may disagree with my opinion, but please, let's stop confounding the economic issues with actual scaling.

@_date: 2015-12-17 13:18:57
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Segregated Witness in the context of 
Doesn't a good soft fork signaling mechanism along with an activation warning system for non-upgraded nodes (i.e. BIP9, or even block version ISM for that matter) essentially fix this? I don't quite get why this should be an issue.

@_date: 2015-12-18 03:02:36
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] On the security of softforks 
First of all, that's an expensive beer!
Second of all, any consensus rule change risks non-full-validating or non-upgraded nodes seeing invalid confirmations...but assuming a large supermajority (i.e. > 95%) of hashing power is behind the new rule, it is extremely unlikely that very many invalid confirmations will ever be seen by anyone. The number of confirmations you require depends on your use case security requirements...and especially during a new rule activation, it is probably not a good idea for non-validating nodes or non-upgraded nodes to accept coins with low confirmation counts unless the risk is accounted for in the use case (i.e. a web hosting provider that can shut the user out if fraud is later detected).
Third of all, as long as the rule change activation is signaled in blocks, even old nodes will be able to detect that something is fishy and warn users to be more cautious (i.e. wait more confirmations or immediately upgrade or connect to a different node that has upgraded, I honestly don't see an issue here - unless you're already violating fundamental security assumptions that would make you vulnerable to exploitation even without rule changes.
- Eric
------ Original Message ------
Sent: 12/17/2015 6:47:14 PM

@_date: 2015-12-23 15:22:30
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Segregated Witness BIPs 
I've been working with jl2012 on some SEGWIT BIPs based on earlier discussions Pieter Wuille's implementation. We're considering submitting three separate BIPs:
CONSENSUS BIP: witness structures and how they're committed to blocks, cost metrics and limits, the scripting system (witness programs), and the soft fork mechanism.
PEER SERVICES BIP: relay message structures, witnesstx serialization, and other issues pertaining to the p2p protocol such as IBD, synchronization, tx and block propagation, etc...
APPLICATIONS BIP: scriptPubKey encoding formats and other wallet interoperability concerns.
The Consensus BIP is submitted as a draft and is pending BIP number assignment: The other two BIPS will be drafted soon.

@_date: 2015-12-26 08:23:38
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Fixing block withholding is relatively simple, but (so far) requires a
SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
do this hard-fork in conjunction with any blocksize increase, which will
have the desirable side effect of clearly show consent by the entire
ecosystem, SPV clients included.
I think we can generalize this and argue that it is impossible fix this without reducing the visible difficulty and blinding the hasher to an invisible difficulty. Unfortunately, changing the retargeting algo to compute lower visible difficulty (leaving all else the same) or interpreting the bits field in a way that yields a lower visible difficulty is a hard fork by definition - blocks that didn't meet the visible difficulty requirement before will now meet it.
jl2012's key discovery here is that if we add an invisible difficulty while keeping the retarget algo and bits semantics the same, the visible difficulty will decrease automatically to compensate. In other words, we can artificially increase the block time interval, allowing us to force a lower visible difficulty at the next retarget without changing the retarget algo nor the bits semantics. There are no other free parameters we can tweak, so it seems this is really the best we can do.
Unfortunately, this also means longer confirmation times, lower throughput, and lower miner revenue. Note, however, that confirmations would (on average) represent more PoW, so fewer confirmations would be required to achieve the same level of security.
We can compensate for lower throughput and lower miner revenue by increasing block size and increasing block rewards. Interestingly, it turns out we *can* do these things with soft forks by embedding new structures into blocks and nesting their hash trees into existing structures. Ideas such as extension blocks [ have been proposed before...but they add significant complications to the protocol and require nontrivial app migration efforts. Old nodes would not get forked off the network but backwards compatibility would still be a problem as they would not be able to see at least some of the transactions and some of the bitcoins in blocks. But if we're willing to accept this, even the "sacred" 21 million asymptotic limit can be raised via soft fork!
So in conclusion, it *is* possible to fix this attack with a soft fork and without altering the basic economics...but it's almost surely a lot more trouble than it's worth. Luke-Jr's solution is far simpler and more elegant and is perhaps one of the few examples of a new feature (as opposed to merely a structure cleanup) that would be better to deploy as a hard fork since it's simple to implement and seems to stand a reasonable chance of near universal support...and soft fork alternatives are very, very ugly and significantly impact system usability...and I think theory tells us we can't do any better.
- Eric

@_date: 2015-12-26 08:26:54
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
Note: my stupid email client didn't indent Peter Todd's quote correctly. The first paragraph is his, the second is my response.
------ Original Message ------
Sent: 12/26/2015 12:23:38 AM

@_date: 2015-12-26 09:38:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
For simplicity, assume total network hashpower is constant. Also, assume the soft fork activates at the beginning of a retarget period.
At the moment the soft fork activates, the effective difficulty is increased (by adding a second independent PoW check that must also be satisfied) which means more hashes on average (and proportionally more time) are required to find a block. At the end of the retarget period,  the difficulty is lowered so that if the second PoW difficulty were to be kept constant the block interval would again average 10 mins.
If we were to keep the second PoW difficulty constant, we would restore the same total PoW-to-time-unit ratio and the retarget difficulty would stabilize again so each block would once more require the same number of hashes (and same amount of time) on average as before.
But we don't keep the second PoW difficulty constant - we increase it so once again more hashes on average are required to find a block by the same proportion as before. And we keep doing this.
Now, the assumption that hashpower is constant is obviously unrealistic. If this is your bone of contention, then yes, I agree my model is overly simplistic.
My larger point was to explore the extent of what's possible with only a soft fork - and we can actually go pretty far and even compensate for these economic shifts by increasing block size and rewards. The whole thing is clearly a huge mess - and I wouldn't recommend actually doing it.

@_date: 2015-12-26 10:30:04
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
I should have stated that we're assuming the actual total hashrate remains constant. Obviously this is not what would actually happen - the rest of the post discusses ways to counter the economic forces at play pushing total hashrate down using only soft forks. The increased variance is still unaccounted for (pool operators would have to deal with this somehow)...and we still have larger block intervals even with compensation. And the practicality of deployment and usability are clearly problematic, to understate things.
It's merely an exercise seeking the theoretical limit of what's actually possible to do with a soft fork.

@_date: 2015-02-22 03:41:26
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
It seems to me we're confusing two completely different motivations for
double-spending. One is the ability to replace a fee, the other is the
ability to replace outputs.
If the double-spend were to merely add or remove inputs (but keep at least
one input in common, of course), it seems fairly safe to assume it's the
former, a genuine fee replacement. Even allowing for things like coinjoin,
none of the payees would really care either way.
Conversely, if at least one of the inputs were kept but none of the outputs
were, we can be confident it's the the latter.
It is possible to build a wallet that always does the former when doing fee
replacement by using another transaction to create an output with exactly
the additional desired fee.
If we can clearly distinguish these two cases then the fee replacement case
can be handled by relaying both and letting miners pick one or the other
while the output replacement case could be handled by rewarding everything
to a miner (essentially all outputs are voided...made unredeemable...and
all inputs are added to coinbase) if the miner includes the two conflicting
transactions in the same block.
Wouldn't this essentially solve the problem?
- Eric Lombrozo

@_date: 2015-02-22 04:06:13
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
I should note that my proposal does require a change to the consensus
rules...but getting bitcoin to scale will require this no matter what.
- Eric Lombrozo

@_date: 2015-02-22 05:41:56
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
In case it wasn't clear in my earlier post, there's of course a third
possibility - namely, some outputs are kept but not all. Here, it is
generally impossible to tell whether the motivation was fee replacement,
output replacement, or both. My proposal is to always treat these instances
as output replacement and punish the sender. The sender needs to make it
unambiguously clear it's only a fee replacement by creating a new
transaction that produces an output with the desired extra fee and then
adding an input that spends it to the original transaction.
- Eric Lombrozo

@_date: 2015-02-22 15:29:36
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] replace-by-fee v0.10.0rc4 
That won't work because in general it is impossible to know which
transaction is the original. Did we add outputs to transaction A? Or remove
outputs from transaction B?
I think you're unnecessarily complicating use cases.
As for 0-conf security, there are instances where 0-conf transactions make
a lot of sense - i.e. paying for utilities, ISP, web hosting, or other such
services which could be immediately shut off upon detection of a

@_date: 2015-01-14 10:00:39
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] convention/standard for sorting public 
I think everyone is pretty much following this standard now.
- Eric

@_date: 2015-01-14 15:53:26
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] convention/standard for sorting public 
I would highly recommend NOT using Base58 for anything except stuff that is
to be copy/pasted by the enduser.
Internally, pubkeys are DER-encoded integers.
- Eric

@_date: 2015-01-14 17:09:54
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] convention/standard for sorting public 
Ciphrex was using this convention well before BitPay...and BitPay's BIP32
implementation was at least partly taken from ours.
- Eric

@_date: 2015-07-05 11:50:23
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Thoughts on Forks, Scalability, 
Blockchain validation has become too expensive to properly secure the
network as per our original security model. The level of validation
required to comply with our security model has become completely
impractical for most use cases. Block space is still cheap only because of
block reward subsidy (which decreases exponentially with time). The
economics are already completely jacked - larger blocks will only worsen
this disparity.
The only practical way for the network to function at present (and what has
essentially ended up happening, if often tacitly) is by introducing trust,
in validators, miners, relayers, explorer websites, online wallets,
etc...which in and of itself wouldn't be the end of the world were it not
for the fact that the raison d'etre of bitcoin is trustlessness - and the
security model is very much based on this idea. Because of this, there's
been a tendency to deny that bitcoin cannot presently scale without trust.
This is horrible because our entire security model has gone out the
window...and has been replaced with something that isn't specified at all!
We don't really know the boundaries of our model, as the fork a couple of
days ago demonstrated. Right now we're basically trusting a few devs and
some mining pool operators that until now have been willing to cooperate
for the benefit of the network. It is dangerous to assume this will
continue perpetually. Even assuming the best intentions, an incident might
occur that this cooperation cannot easily repair.
We need to either solve the validation cost/bottleneck issue...or we need
to construct a new security model that takes these trust assumptions into
- Eric Lombrozo

@_date: 2015-07-05 12:55:47
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Thoughts on Forks, Scalability, 
I should clarify that by "most use cases" I'm not envisioning a bunch of
cryptogeeks [us, or at least myself and a few of us] happily buying up hard
disks, waiting hours, days, weeks to spawn up new full nodes. I'm
envisioning a world where every person has access to this technology and
finds it practical, convenient, and safe ti use.
- Eric Lombrozo

@_date: 2015-07-05 13:53:51
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Thoughts on Forks, Scalability, 
Perhaps I didn't write that so well if I gave that impression. Perhaps
taking a look at some of my work in this space would make you think
otherwise. (yes, I've implemented an entire SPV stack from scratch...look
it up.)
But all patronizing aside, your claim that "the reason an attacker can fool
SPV clients into accepting invalid blocks is because there exists no
mechanism via which honest nodes can prove the invalidity of blocks" is
exactly to the point...and building such a mechanism would address the
first of the two options I give: make it cheap to securely validate or take
trust into account.
- Eric Lombrozo
On Jul 5, 2015 1:34 PM, "Justus Ranvier" <

@_date: 2015-07-05 14:08:12
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Thoughts on Forks, Scalability, 
I was saying that somewhat tongue-in-cheek...sorry that didn't come through
in the text.
On Jul 5, 2015 2:05 PM, "Justus Ranvier" <

@_date: 2015-07-13 10:49:40
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] SPV Mining reveals a problematic incentive issue. 
My other email client messed up. I apologize for the blank message.
Even though the cost of mining bad blocks is high enough to deter most
deliberate attacks, if we're not properly validating blocks, this
deterrence does not stop bugs nor version issues and it opens up attack
vectors like someone hacking into a mining pool server.
It is imperative we continue to look for ways to make secure validation
cheaper. I would make this  priority. Not only is it crucial to the
integrity of our security model - it is crucial for scalability and
decentralization as well.
- Eric Lombrozo

@_date: 2015-07-22 12:17:44
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
This is only because of the fact that only a negligible portion of miner income comes from fees - the vast majority still continues to be subsidized by block rewards. The original design of the protocol was such that this subsidy would be decreased over time to let fees become the predominant source of income for miners. Until we have fee pressures, there?s no incentive for the industry to find solutions to real problems that need solving. I think you underestimate the ingenuity of people when pressed for real solutions. The main barrier to Bitcoin adoption is NOT this issue?and I believe the priorities are misplaced here. We?ve had over six years to start working on solutions but we keep ?kicking the can down the road? - until when?!?! I believe unless there?s a strong need to find a solution no solutions will really be found.
The current userbase and market is still tiny - we have to think bigger than this. We already go through loads of pain to use the current system?and quite frankly, there are a number of other significant issues that I think are far bigger obstacles to widespread adoption than ?I have to pay a fee?. For example, the current cost of verification is too high to continue to ensure the security of the network (as the July 4th fork clearly illustrated)?and places huge centralization pressures on validation?and simply will not support hundreds of millions of users or billions of users. Increasing block size actually worsens the scaling properties, it does not improve them. We need better scaling solutions - almost certainly this will require avoiding the need for global consensus for the vast majority of transactions (nested consensus or off-chain direct party-to-party contract negotiation, the lightning network, etc. The focus on reducing fee pressure by increasing block size is a distraction from far more fundamental issues, IMHO.
Wrong - the economic policy of bitcoin has always been, from the beginning, to subsidize blocks initially and transition to fees. Artificially continuing to rely on block reward subsidies is what is a new economic policy. We?re already six years in, pretty soon another halving is coming - how long are we going to wait to start transitioning? The lower block reward subsidies are, the more pain fee pressures will cause.
I think about the billions of people out there in the world that could be using this technology that simply have no access to it right now. The majority or them which are unbanked, etc?
More the reason to go through the steps needed while we?re still small to correct the core issues.

@_date: 2015-07-22 14:56:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
It would be truly awesome to be able to give people more choice on consensus rules. Unfortunately, cryptoledgers do not fork gracefully (yet). Until we?re able to merge blockchain forks like we?re able to merge git repo forks, the safest option is no fork.

@_date: 2015-07-22 15:09:26
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Blockchain reorgs are part of the consensus rules. We?re talking not about forks caused by network partitions?but forks caused by the use of distinct consensus rules.
You cannot merge two chains that have incompatible transactions in them without throwing away one of the two conflicting transactions (along with all dependencies). In the reorg process, this occurs naturally?and we allow for it by using confirmation count as a metric of irreversibility. Until one chain wins (by overwhelming consensus) or all chains include a particular transaction in question, we cannot treat that transaction as irreversible. Propose a model in which we can still reliably measure irreversibility in the presence of multiple chains and you might have a point.

@_date: 2015-07-22 16:53:39
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
FWIW, I had worked on something similar a while back:  I like the idea in principle?but we should require a new genesis block, different magic bytes, and a different network port at the very least. :)

@_date: 2015-07-22 17:13:42
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Only being partly serious - I strongly am in favor of a sufficiently modularized codebase that swapping out consensus rules is fairly straightforward and easy to test. I?m not in favor of encouraging forking an existing blockchain without having mechanisms in place to gracefully merge back without significant network disruptions. We do not have this yet.
That?s exactly what my coinparams_new branch does. Adding a parameter for maximum block size would be straightforward.
Yes, indeed - this would be a special case.
I do not encourage anyone to try to fork an existing blockchain without first securing overwhelming (near unanimous) consensus?or without having yet built a mechanism that can merge divergent chains gracefully.

@_date: 2015-07-22 17:37:11
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I take it you mean allowing bitcoin to *not* become space-constrained - because all real-world computers are space-constrained?

@_date: 2015-07-22 17:43:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
In general, new consensus rules are not trivial to implement. Block size limits are exceptional in being so simple to change in the code. So what you?re proposing sounds more like a plugin model supporting dynamic linking than a configuration file.
The real problem isn?t so much the difficulty of creating forks of the codebase - but the fact that unless a fork has overwhelming support, blockchains cannot guarantee irreversibility of transactions?which defeats their entire purpose.

@_date: 2015-07-22 18:53:36
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Mike, you might be surprized to learn that there are other hard fork proposals out there besides increasing block size.

@_date: 2015-07-23 10:43:06
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Let?s be absolutely clear about one thing - block size increases are *not* about scaling the network. Can we please stop promoting this falsehood? It doesn?t matter by what number we multiply the block size?we can NEVER satisfy the full demand if we insist on every single transaction from every single person everywhere in the world being on the blockchain?it?s just absurd.
Increasing block size only temporarily addresses one significant issue - how to postpone having to deal with transaction fees, which by design, are how the cost of operating the Bitcoin network (which is already very expensive) is supposed to be paid for ultimately. Suggesting we avoid dealing with this constitutes a new economic policy - dealing with it is the default economic policy we?ve all known about from the beginning?so please stop claiming otherwise.
Exactly. There?s been tremendous progress here in addressing scalability, yet I don?t see you participating in that discussion, Gavin.
I agree with what you?re saying, Jorge?but It?s even worse than that. The July 4th fork illustrated that the security model of the network itself could be at risk from the increasing costs in validation causing people to rely on others to validate for them?and increasing block size only makes the problem worse.
- Eric Lombrozo

@_date: 2015-07-23 11:21:25
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
The issue isn?t really whether it?s 1MB or 2MB or 4MB or 8MB or whatever. First of all, the burden of justifying this change should be on those proposing a hardfork. The default is to not have a hard fork. Second of all, it?s not really about *whether* the block size is increased?but about *when* and *how* it is increased. There?s a good argument to be made that right now it is more important to address issues such as the fact that validation is so expensive (which as others and myself have pointed out has led to a collapse of the security model in the past, requiring manual intervention to temporarily ?fix?)?and the fact that we don?t yet have great solutions to dealing with fees, which are a crucial component of the design of the protocol.

@_date: 2015-07-23 12:14:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Very well put, Jameson. And the cost of bearing this load must be paid for. And unless we?re willing to accept that computational resources are finite and subject to the same economic issues as any other finite resource, our incentive model collapses the security of the network will be significantly at risk. Whatever your usability concerns may be regarding fees, when the security model?s busted usability issues are moot.
Larger blocks support more transactions?but they also incur ?(n) overhead in bandwidth, CPU, and space. These are finite resources that must be paid for somehow?and as we all already know miners are willing to cut corners on all this and push the costs onto others (not to mention wallets and online block explorers). And who can really blame them? It?s rational behavior given the skewed incentives.
Mainstream usage of cryptocurrency will be enabled primarily by direct party-to-party contract negotiation?with the use of the blockchain primarily as a dispute resolution mechanism. The block size isn?t about scaling but about supply and demand of finite resources. As demand for block space increases, we can address it either by increasing computational resources (block size) or by increasing fees. But to do the former we need a way to offset the increase in cost by making sure that those who contribute said resources have incentive to do so.

@_date: 2015-07-23 12:39:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Regarding rhetoric, fair enough, Gavin - I?m human and I could be wrong. It is my educated best guess, a conclusion I?ve drawn given my understanding of computer science, economics, and what?s been happening in this space.

@_date: 2015-07-23 12:51:29
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I also wanted to point out I fully agree with you that there are still many optimizations we could do to reduce costs, and think many of these things are certainly worth doing. However, there?s only so much we can do in this regard. Sooner or later we still run up against theoretical limitations. These optimizations can reduce costs by some factor?but they are highly unlikely to overcome the ?(n) validation complexity barring some major algorithmic breakthrough (and perhaps allowing for nondeterminism, perhaps accepting a negligible but finite error probability).

@_date: 2015-07-23 13:52:26
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I should also point out, improvements in hardware and network infrastructure can also reduce costs?and we could very well have a model where resource requirements can be increased as technology improves. However, currently, the computational cost of validation is clearly growing far more quickly than the cost of computational resources is going down. There are 7,000,000,000 people in the world. Payment networks in the developed world already regularly handle thousands of transactions a second. Even with highly optimized block propagation, pruning, and signature validation, we?re still many orders shy of being able to satisfy demand. To achieve mainstream adoption, we?ll have to pass through a period of quasi-exponential growth in userbase (until the market saturates?or until the network resources run out). Unless we?re able to achieve a validation complexity of O(polylog n) or better, it?s not a matter of having a negative attitude about the prospects?it?s just math. Whether we have 2MB or 20MB or 100MB blocks (even assuming the above mentioned optimizations and that the computational resources exist and are willing to handle it) we will not be able to satisfy demand if we insist on requiring global validation for all transactions.

@_date: 2015-07-23 16:57:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Agreed. But I believe the economic and security arguments I gave regarding fees and incentives still hold and are largely separate from the scalability issue. Please correct me if I overlooked something.
An increase in block size at this time will exacerbate security concerns around nodes relying on other nodes to validate (particularly miners and wallets). It?s not really a matter of having limited developer resources that need to be budgeted, as you seem to suggest.
Regarding developments on properly handling fees, there must exist the economic need for it before there?s an earnest effort to solve it. Increasing the block size right now will, in all likelihood, delay this effort. I?d much prefer to first let the fee market evolve because it?s a crucial component to the protocol?s design and its security model?and so we can get a better sense for fee economics. Then we might be able to figure out better approaches to block size changes in the future that makes sense economically?perhaps with mechanisms that can dynamically adjust it to reflect resource availability and network load.

@_date: 2015-07-23 17:04:07
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I should also add that I think those who claim that fee pressure will scare away users and break the industry are *seriously* underestimating human ingenuity in the face of a challenge. We can do this - we can overcome this obstacle?we can find good solutions to a fee market. Unless someone can come up with another way to pay for the operation of the network, we NEED to do this. What makes anyone think it will be easier to do later rather than now? The longer we wait, the lower block rewards get, the larger the deployed infrastructure, the larger our userbase, the HARDER it will be to solve it. We should solve it now - we will be much better off for it?and so will our users.

@_date: 2015-07-23 17:32:21
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Not sure what you mean by QoS here. Either your transaction is included or it isn?t. It?s not like you can upgrade to a master suite with a view or anything.

@_date: 2015-07-23 17:56:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
The scheme I?ve been considering is the use of services (separate from miners) that guarantee inclusion for you for some predetermined price and then do the bidding on your behalf. Via contracts you can guarantee you get included within a certain number of blocks or you receive a full refund?or even possibly receive compensation for failure to deliver.

@_date: 2015-07-23 18:08:45
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
By using third parties separate from individual miners that do bidding on your behalf you get a mechanism that allows QoS guarantees and shifting the complexity and risk from the wallet with little computational resources to a service with abundance of them. Using timelocked contracts it?s possible to enforce the guarantees.
Negotiating directly with miners via smart contracts seems difficult at best.

@_date: 2015-07-23 18:28:25
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I suppose you can use a timelocked output that is spendable by anyone you could go somewhat in this direction?the thing is it still means the wallet must make fee estimations rather than being able to get a quick quote.

@_date: 2015-07-23 18:37:20
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I think it?s pretty clear by now that the assumption that all nodes have pretty similar computational resources leads to very misplaced incentives. Ultimately, cryptocurrencies will allow direct outsourcing of computation, making it possible to distribute computational tasks in an economically sensible way.
Wallets should be assumed to have low computational resources and intermittent Internet connections for the foreseeable future if we ever intend for this to be a practical payment system, methinks.

@_date: 2015-07-23 18:55:25
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
I agree that the fewer the necessary parties the better - however, some entities are much better positioned to offer certain services on the network than others - and the fact we can trustlessly negotiate smart contracts with them is one of the most significant developments in the cryptospace - it?s one of the most revolutionary aspects of this technology?it accomplishes something we?ve never really been able to do before.
Notice that third parties can encapsulate complex tasks and provide a far simpler interface. Crypto contracts provide the incentives for them to do this. And by having competition and transparency, these services automatically get optimized via human ingenuity. We don?t need to design top-down for it.

@_date: 2015-07-24 13:23:27
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
Thanks for bringing up the CCSS, Adam and Peter.
I was actually working on a post inviting everyone in this mailing list to come and participate?but you guys beat me to it. :)
The CCSS is an open standard, born out of the belief that sharing the industry's best practices amongst each other and with the community at large benefits everyone.
To read more about it and how you can contribute, please visit  The standard:   The github repository:  - Eric

@_date: 2015-07-24 13:28:28
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
For the record, there?s pretty much unanimous agreement that running a full node should be a requirement at the higher levels of certification (if not the lower ones as well). I?m not sure exactly what pushback you?re referring to.

@_date: 2015-07-24 13:31:46
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Roadmap 2015, 
Peter, it?s a work in evolution, it?s not complete yet. It?s still missing a bunch of stuff - please feel free to contribute.

@_date: 2015-07-28 15:25:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I only got into Bitcoin in 2011, after the block size limit was already in place. After going through some more of the early history of Bitcoin to better understand the origins of this, things are starting to come into better perspective.
Initially there was no block size limit - it was thought that the fee market would naturally develop and would impose economic constraints on growth. But this hypothesis failed after a sudden influx of new uses. It was still too easy to attack the network. This idea had to wait until the network was more mature to handle things.
Enter a ?temporary? anti-spam measure - a one megabyte block size limit. Let?s test this out, then increase it once we see how things work. So far so good?
1) We never really got to test things out?a fee market never really got created, we never got to see how fees would really work in practice.
2) Turns out the vast majority of validation nodes have little if anything to do with mining - validators do not get compensated?validation cost is externalized to the entire network.
3) Miners don?t even properly validate blocks. And the bigger the blocks get, the greater the propensity to skip this step. Oops!
4) A satisfactory mechanism for thin clients to be able to securely obtain reasonably secure, short proofs for their transactions never materialized.

@_date: 2015-07-28 17:44:20
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I?m using spam and DoS somewhat synonymously here, although you?re correct - DoS is a more accurate term.

@_date: 2015-07-28 17:55:20
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I agree that the historical reasons are irrelevant from an engineering perspective. But they still set a context for the discussion?and might help shed some insight into the motivations behind some of the participants. It?s also good to know these things to counter arguments that start with ?But Satoshi said that??
What?s critically important to note is that several of the assumptions that were being made at the time this limit was decided have turned out wrong?and that these other issues should probably be of greater concern and more highly prioritized in any discussion considering the merits of deploying potentially incompatible consensus rule changes. It seems if these other issues were fixed perhaps no block size limit would be required at all (as was originally hoped).
- Eric

@_date: 2015-07-28 19:40:21
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
In the interest of promoting some constructive discussion on this, let me start by making a few proposals to correct the listed issues.
Note: many of these ideas are neither my own nor really all that new, but it seems in the past we?ve given up too easily on actually moving forward on them despite their critical importance.
1) A fee market never really got created, we don?t really know how transaction fees would  work in practice.
The only way to see how fees would work in practice is to have scarcity. If the network is still not sufficiently mature to be able to handle actual resource limits securely, the safest way to do this is to artificially impose limits. Some economists might bicker about the problems with production quotas and what not?but how else are we to solve the real, non-trivial engineering problems without risking system collapse? The eventual goal would be to remove these artificial limits once we?re confident that the economic incentives are properly aligned to maintain security. We?re still quite far from this goal, though, and it would be irresponsible, IMHO, to insist on letting the system hit its real limits.
2) Turns out the vast majority of validation nodes have little if anything to do with mining - validators do not get compensated?validation cost is externalized to the entire network.
3) Miners don?t even properly validate blocks. And the bigger the blocks get, the greater the propensity to skip this step. Oops!
Issues (2) and (3) are inextricably related so I?ll cover both together.
The obvious problem here is that as long as the cost of checking validators is the same as the cost of validating itself, there?s really little we can do to properly have any sort of division of labor. Requiring, at the very least, random checks might be a start. Perhaps some clever use of SNARKs might eventually be secure and practical.
It might also be possible to directly pay validators for satisfying random checks or providing SNARKs. If only we could trustlessly and securely outsource this work we?d make tremendous progress.
Of all the issues I?ve listed, these are perhaps the ones for which practical solutions seem most tentative at present.
4) A satisfactory mechanism for thin clients to be able to securely obtain reasonably secure, short proofs for their transactions never materialized.
The first part of the solution to this issue is the use of better data structures. Satoshi?s SPV can prove that transactions are included in blocks?and that outputs are spent. But it has no mechanism for proving that a given transaction is *not* included in any block?or that some particular output remains unspent. The structures to which we?re committing extremely inefficient for querying some of the most important things required for validation?i.e. whether an output exists and whether it is spent.
The second part is shifting the responsibility for constructing proofs to the parties who already have the greatest incentives to store the necessary data to construct these proofs to allow efficient prunability. Outsourceability of proofs would also be highly desirable.
If we want to be able to raise the block size limit?or perhaps get rid of it altogether, I would suggest we start by addressing these specific issues and work to find practical solutions. Since raising the block size limit is already a hard forking consensus rule change, at least the need for hard forks isn?t what?s stopping us.
- Eric

@_date: 2015-07-28 20:37:57
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
In retrospect I regret not having made this note more emphatic:
GUYS, WE?VE KNOWN ABOUT THESE PROBLEMS AND HAVE TALKED ABOUT THEM FOR YEARS ALREADY?AND IT SEEMS PRACTICALLY NOTHING HAS HAPPENED?WTF?!?!?!?

@_date: 2015-07-28 22:17:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Some of the most highly skilled technical people working on Bitcoin Core have been doing exactly that! The main incentive, of course, is that later on you get to work on something that?s actually pleasant to work on rather than a whole bunch of garbled crap that doesn?t work properly.
However, the great irony is that the devs who have long since recognized the importance of fixing these issues have also tended to be loathe to touching any of the consensus code unless it fixes some critical immediately exploitable security hole?while the devs who most clamor for consensus code changes have tended to all but ignore these issues entirely.
I sometimes wish it were the other way around.
- Eric

@_date: 2015-07-29 03:43:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Irrelevant what term was used - and as brilliant as Satoshi might have been at some things, he obviously got this one wrong.
Nobody threatened to start mining huge blocks given how relatively inexpensive it was to mine back then?
I thought I clarified this in an earlier post - I meant DoS. Please don?t digress on such stupid technicalities.
Guess what? SPV wallets are still not particularly widespread?and those that are out there are notoriously terrible at detecting network forks and making sure they are on the right one.
Something has to limit block sizes in practice. Perhaps Satoshi was not constrained by finite computational resources, but the rest of us sure are. The fact that without imposing a hardcoded limit Satoshi couldn?t figure out a way to keep the DoS-block guys away suggests he didn?t have this fully worked out.
I understand that initially it was desirable that transactions be free?but surely even Satoshi understood this couldn?t be perpetually self-sustaining?and that the ability to bid for inclusion in blocks would eventually become a crucial component of the network. Or were fees just added for decoration?
We?re already more than six years into this. When were these mechanisms going to be developed and tested? After 10 years? 20? Perhaps after 1024 years?( )
And Satoshi was dead wrong. As others have pointed out in this thread, while this is certainly of historical interest, it is irrelevant from an engineering perspective.
Right. Turns out the ledger structure is terrible for constructing the kinds of proofs that are most important to validators - i.e. whether an output exists, what its script and amounts are, whether it?s been spent, etc?
Despite Satoshi?s brilliance, software architecture was obviously not his strongest suit. But it didn?t really matter at the beginning since this was really an experiment?and he succeeded in making his point.
Yes, let?s wait until things are about to break before even beginning to address the issue?because we can ?easily create? anything we haven?t invented yet at the last minute.
Erm?most miners just trust mining pool operators to validate blocks for them?and some of the biggest pools have been blatantly cutting corners. Yes, a few pools might have temporarily bled a little?but properly validating is probably not the equilibrium strategy?and as time goes on, they are likely to start cutting corners again. Whether they ultimately bleed money isn?t really the point - many believe that cutting corners is actually a rational strategy. If you want to discuss the game theory behind this, fine?but the fact some of the biggest mining pool operators are on record saying they are likely to continue doing this is enough to seriously put to question one of the most fundamental assumptions behind the network security model.
You have my respect for BIP37, Mike. I know you can do amazing work. You actually made SPV semi-useful despite inheriting such crappy data structures. This is indeed to be respected.
Not done the work?
I?m one of the very few developers in this space that has actually tried *hard* to make your BIP37 work. Amongst the desktop wallets listed on bitcoin.org , there are only two that have always supported SPV (or at least I think MultiBit has always supported it, perhaps I?m wrong). One is MultiBit, the other one is mine. I give you credit for your work?perhaps you could be generous enough to extend me some credit too?

@_date: 2015-07-29 05:03:45
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
100% granted - it was not obvious?and we speak today with the benefit of hindsight.
I?ll clarify my argument, for the sake of anyone who thinks I?m looking to play word games rather than trying to figure out a good way forward.
Point is?processing blocks requires computational resources that someone needs to put up. Unless the people who are putting up these resources are properly incentivized to continue doing it, the network will fail.
Unfortunately, it was unforeseen that most nodes on the network would turn out to not be miners?and that most miners wouldn?t even run full nodes. Yes, I speak with the benefit of hindsight, had I been discussing this in 2008 I very well could have made the same mistake or worse. But it isn?t 2008, it?s 2015?and we?ve learned a thing or two since.
Given that things are what they are, it is clear that larger blocks externalize costs onto the rest of the network.
Waiting until we can no longer count on the altruistic goodwill of volunteers because they suddenly decide that they have better uses for their computers is probably not such a wonderful idea. But even worse is further burdening the network with externalized costs before we?ve solved these important issues?especially given the evidence that larger blocks tend to lead to network forks. No, I?m not talking about regular run-of-the-mill reorgs?I?m talking consensus forks - a network partition that cannot be reconciled without manual intervention, so please don?t distract the issue. Yes, each incident occurred for a very different reason?but you?d have to be blind to miss the correlation between bigger blocks and the propensity for forks.
What Satoshi might have thought in 2008-2009 is fascinating from a historical perspective, but his early pioneering insights don?t appear to be of much help in addressing these particular issues.

@_date: 2015-07-30 01:21:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I usually avoid troll-infested Dunning-Kruger-gone-wild fests like reddit, so I?ll leave that to others.
But I do want to clarify a couple things here, though, Andrew.
First of all, the issue is not about whether it is affordable for a highly motivated, technically skilled person to continue running a node even if we increase block size by a factor of X. This misses the point for at least a couple reasons:
- Regardless of what that X is, it isn?t really going to be what makes this technology accessible to the masses. We would likely need the X to be in the thousands before we start to really take on players like Visa. Despite what people might have thought in 2009, it turns out Bitcoin is probably pretty ill-suited as a database in which to store the entire transaction history of the entire world. It?s looking to be more of a censorship-resistant dispute resolution mechanism that provides very well-defined settlement guarantees with the potential for encoding complex rules. It?s possible to build higher level tiers on top of it that DO support high volume transaction processing WITHOUT costing thousands of times more, and these approaches are looking quite promising. However, it doesn?t seem very many people in this space quite grasp this paradigm shift yet.
- What matters is not how a relatively small number of well-intentioned people in the network behave. What matters is how the network behaves as a whole?and a number of the people most intimately familiar with the inner workings of the system (some of whom are in this thread) think that given what we now today about the Bitcoin network, increasing block size externalizes costs in dangerous ways. Remember that total cost includes not just equipment costs but also things like block propagation latency and specifically identified security risks. Some of these security risks were only appreciated relatively recently and were completely unknown in 2009.

@_date: 2015-07-30 02:15:25
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Secondly, there are a few well-identified problems with the protocol design that might be possible to fix that would perhaps allow us to remove the block size limit entirely without sacrificing security. I listed the ones that come to my mind at the beginning of this thread. I EMPHATICALLY state that in no way am I fundamentally opposed to raising or even getting rid of the block size limit. But I believe these problems should be addressed first. And it?s easier to address them and tackle them if we don?t have to worry about potential security risks and higher costs that come from insisting on bigger blocks right now.
- Eric

@_date: 2015-07-30 16:33:16
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Some of the risks are pretty hard to quantify. But I think this misses the bigger point - it very well *might* be possible to safely raise this limit or even get rid of it by first fixing some serious issues with the protocol. But over six years into the project and these issues continue to be all-but-ignored by most of the community (including at least a few core developers). I don?t think it?s really a matter of whether we agree on whether it?s good to raise the block size limit, Gavin. I think it?s a matter of a difference in priorities.
- Eric

@_date: 2015-07-30 17:22:11
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Please, Mark, let?s make this happen.
You can count on my full support.

@_date: 2015-07-31 13:45:38
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I would love to be able to increase block size. But I have serious doubts
about being able to do this safely at this time given what we presently
know about the Bitcoin network. And I'm pretty sure I'm not alone in this
Had we been working on fixing the known issues that most complicate bigger
blocks in the last six years, or even in the last three years after many
issues had already been well-identified, perhaps we'd be ready to increase
the limit. But other things have seemed more important, like specifying the
use of X.509 overlay protocols or adding complex filtering mechanisms to
the p2p protocol to make it practical to use tx merkle trees...and as a
result we're not ready for safely allowing larger blocks.
- Eric
On Jul 30, 2015 11:43 PM, "Thomas Zander via bitcoin-dev" <

@_date: 2015-07-31 13:57:14
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
Having said that, I must admit that the complex filtering mechanisms are
pretty clever...they almost make it practical to use SPV...now if only we
were committint to structures that can prove the validity of returned
datasets and miners actually validated stuff, it might also offer some
level of security.

@_date: 2015-07-31 14:43:43
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
I generally agree with this as well. I think it is crucial we avoid
controversial hardforks. The risks greatly outweigh the benefits.
This is a good start to making it less controversial.
- Eric
On Jul 31, 2015 2:31 PM, "Jorge Tim?n" <

@_date: 2015-06-12 13:04:23
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
Miners currently only collect an almost negligible portion of their revenue
from fees. While I certainly welcome any proposals that move us in the
direction of defining a smooth metaconsensus process, I think with the
curent economics, miners (and especially those with significant hashing
power) have more of an incentive to block txs/spam their votes than to
adapt to tx sender preferences...unless people increase their tx fees
significantly. But without wallets that can make decent suggestions in this
regard, this feature will be almost inaccessible to most regular users. And
the economics still aren't entirely clear.
- Eric Lombrozo
I'm imagining in Peter's proposal it's not the transaction votes that are
counted but only the votes in the blocks? So miners get to vote but they
risk losing money by having to exclude counter voting transactions. But
garbage transactions are no problem at all.
Note that users that want to cast a vote "pay" for that by increased
confirmation time (on average, hopefully slightly depending on the trend).

@_date: 2015-06-13 15:24:26
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
That?s exactly the problem with Bitcoin - it was supposed to be the case that users ARE the miners and node operators?but?alas?

@_date: 2015-06-13 21:50:39
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
What Stephen said is very much along the same lines of my earlier critique. This voting mechanism would be all but unusable to most endusers without some pretty elaborate tools?and unless users are willing to pay substantially higher fees than they?re currently paying, their votes will not really count all that much. And it?s not all that clear that most users would really be able to make very rational economic decisions even having elaborate tools. More likely, a small group would figure out ways to exploit this for their own benefit - at everyone else?s expense.
- Eric Lombrozo

@_date: 2015-06-13 22:08:16
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
With all due respect, there are a couple major differences between BIP34 and BIP66 on the one hand and BIP100 on the other.
1) BIP34 and BIP66 are soft forks. Miners choosing to switch to them will not seriously impact validation rules for non-mining users that do not make the switch. With BIP66, the worst that can happen to them is noncompliant transactions will no longer be accepted by the network?but even nodes that do not switch over will continue to remain synched with the network.
2) BIP100 has direct economic consequences?and particularly for miners. It lends itself to much greater corruptibility.
- Eric Lombrozo

@_date: 2015-06-13 22:20:37
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
I definitely think we need some voting system for metaconsensus?but if we?re going to seriously consider this we should look at the problem much more generally. Using false choices doesn?t really help, though ;)
- Eric Lombrozo

@_date: 2015-06-14 13:10:41
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
In addition, fees are complicated by the fact that they are used as an anti-spam measure for relay nodes?who don?t see ANY direct compensation whatsoever for providing that service. So we really have two different fees being tacked on?but the miners get to keep all of it?and the relay fee is being hard coded into the software.
Fee calculation heuristics for wallets are already far from trivial - this is another issue that needs to be addressed.
- Eric Lombrozo

@_date: 2015-06-14 17:53:05
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] comments on BIP 100 
I think the whole complexity talk is missing the bigger issue.
Sure, per block validation scales linearly (or quasilinearly?there?s an O(log n) term in there somewhere but it?s probably dominated by linear factors at current levels?asymptotic limits don?t always apply very well to finite systems). And there?s an O(n^2) bandwidth issue.
The real issue, though, is validation cost. The n in O(n) here does not represent block size - it represents the size of the entire block chain for every new validator that must be synchronized! It means we have no way to construct short proofs (or at least arguments that are computationally *hard* to forge) without requiring the validator to maintain the complete system state. And currently, there is no mechanism for directly compensating validators.
A full validator that goes offline even for a short period of time takes a while to fully catch up to the rest of the network - and starting up a new validator from scratch will continue to be painful?even for those of us who?ve turned this into routine by now, let alone new nontechnical users.
Satoshi?s SPV is not a real solution - it?s a mere suggestion that wasn?t fully thought out at the time of the Bitcoin white paper. Besides lacking a good validation security model, practical implementations of it weaken privacy and complicate client implementations substantially?and the worst part, it still doesn?t scale all that well. The validator still has to query every single block (even if filtered) back to the first transaction (which cannot be determined without doing a blockchain scan anyway).
So yes, we will most certainly need algorithmic improvements!
- Eric Lombrozo

@_date: 2015-06-14 20:59:05
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] User vote in blocksize through fees 
To be clear, Jeff, I am 100% in agreement with you that a mechanism like what you?re proposing is a million times better than having high priests that ram hard forks without proper consensus. And perhaps given the present circumstances it seems like the only alternative. However, in my mind this block size limit controversy is actually a fairly superficial aspect - a mere symptom, a manifestation of the real problem...
What I find somewhat irksome is that we?ve had six years to figure out a mechanism to enable hard forks (which we knew from the start would be inevitable) - and more to the point, we?ve known about this block size issue from the start as well?and only suddenly it becomes an issue of major urgency that we must bump up this parameter 20x?
- Eric Lombrozo

@_date: 2015-06-14 21:43:09
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] comments on BIP 100 
While things like zero-knowledge and homomorphic encryption would be awesome, they are not really needed to achieve the objective of an efficient proof that is hard to forge with at least a decently thought out security model (i.e. we can make information withholding far more difficult)?and we can dramatically improve search times and local storage requirements by doing some of the things that you?ve actually proposed, Peter, like shifting the responsibility of maintaining and constructing proofs over to transaction senders and committing proof hashes to the global state. At least the incentives would be far better aligned in such a scenario.
How do we deal with things like the discovery of an invalid proof a couple weeks after it?s already been committed? This is a tricky issue I?ve been giving a lot of thought to recently - but we?ll deal with this topic in a separate thread. :)
If we were to shift responsibility of constructing proofs over to transaction senders, today's ?validators? would indeed become nothing more than compensated servers. Clients would be able to query for proofs and verify them for themselves efficiently.
It?s a disaster. Even with 1MB blocks this is already the principal centralization pressure on Bitcoin.
We clearly need better data structures and algorithms. This talk of bigger blocks seems so petty by comparison, TBH.

@_date: 2015-06-15 02:39:37
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] comments on BIP 100 
still don't get it - the vast >majority of users don't run relaying nodes
that take part in gossiping. They run web or SPV >wallets. And the nodes
that do take part don't connect to every other node.
It's a little scary, IMO, that the fact that the majority of nodes don't
relay and only perform the most rudimtentary level of validation if any is
considered an acceptable feature of the protocol.
- Eric Lombrozo

@_date: 2015-06-15 18:20:28
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & 
This is certainly twisting words!
We all agree that the limit needs to eventually be lifted - but some of us certainly disagree with the means being used to do so by Mike and Gavin.
Most news publications keep the discussion rather shallow and like to keep the controversy pretty black and white - some of us have far more nuanced views!
- Eric Lombrozo

@_date: 2015-06-18 22:59:38
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
I don?t think the issue is between larger blocks on the one hand and things like lightning on the other - these two ideas are quite orthogonal.
Larger blocks aren?t really about addressing basic scalability concerns - for that we?ll clearly need architectural and algorithmic improvements?and will likely need to move to a model where it isn?t necessary for everyone to validate everyone else?s latte purchases. Larger blocks might, at best, keep the current system chugging along temporarily - although I?m not sure that?s necessarily such a great thing?we need to create a fee market sooner or later, and until we do this, block size issues will continue to crop up again and again and economic incentives will continue to be misplaced. It would be nice to have more time to really develop a good infrastructure for this?but without real market pressures, I?m not sure it will happen at all. Necessity is the mother of invention, after all. The question is how to introduce a fee market smoothly and with the overwhelming consensus of the community - and that's where it starts to get tricky.
On a separate note, as several others have pointed out in this thread (but I wanted to add my voice to this as well), maintenance of source code repositories is NOT the real issue here. The bitcoin/bitcoin project on github is a reference implementation of the Satoshi protocol?but it is NOT the only implementation?and it wasn?t really meant to be. Anyone is free to fork it, extend it, improve upon it, or create an entirely new network with its own genesis block?a separate cryptoledger.
The real issue regarding XT is NOT the forking of source code nor issues surrounding commit access to repositories. The real issue is the *forking of a cryptoledger*.
Open source repositories are meant to be forked - in fact, it is often encouraged. It is also encouraged that improvements be submitted for review and possibly merged back into the parent repository?although this doesn?t always happen.
However, we currently have no mechanisms in place to support merging of forked cryptoledgers. Software, and most other forms of digital content, generally increases in value with more copies made. However, money is scarce?by design. The entire value of the assets of a decentralized cryptoledger rests on the assumption that nobody can just unilaterally fork it and change the rules. Yes, convincing other people to do things a certain way is HARD?yes, it can be frustratingly slow?I?ve tried to push for many changes to the Bitcoin network?and have only succeeded a very small number of times. And yes, it?s often been quite frustrating. But trying to unilaterally impose a change of consensus rules for an existing cryptoledger sets a horrendous precedent?this isn?t just about things like block size limits, which is a relatively petty issue by comparison.
It would be very nice to have a similar workflow with consensus rule evolution as we do with most other open source projects. You create a fork, demonstrate that your ideas are sound by implementing them and giving others something that works so they can review them, and then merge your contributions back in. However, the way Bitcoin is currently designed, this is unfortunately impossible to do this with consensus rules. Once a fork, always a fork - a.k.a. altcoins. Say what you will about how most altcoins are crap - at least most of them have the decency of starting with a clean ledger.
- Eric Lombrozo

@_date: 2015-06-19 03:52:48
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
With all due respect, many of us DO run end user facing services?and would rather see a fundamental problem *fixed* rather than merely covered up temporarily?hoping nobody notices.
The user experience of Bitcoin is already horrendous?unless you use a centralized validator web wallet. Even SPV is fundamentally broken (and I would have pegged you for being one of the people most directly aware of this fact). If we?re going for centralized validation, why even use a blockchain in the first place? We already have much faster, more efficient technology that can do that kind of stuff at a fraction of the cost. If you have well-established entities running banking services, we have other mechanisms in place that can help keep them honest?other far more efficient protocols. We?re basically defeating the very purpose of this invention.
Then there are a bunch of other ?inconveniences? about the way Bitcoin currently works. For instance, have you ever received a bunch of small payments (i.e. a crowdsale) and then found yourself in the position of having to suddenly move a big chunk of that on the blockchain?only to discover all the txouts you were spending added up to hundreds of kB or more? Or have you ever had to send a small payment but only had one large output in your wallet?which meant that the entirety of those funds were tied up until the first transaction got signed and propagated? Yes, the protocol has MANY serious issues?of which the ?send and forget? fee model as opposed to the ?send and bid model? is just one.
Bitcoin was designed from the beginning with the idea that sooner or later fees would be a significant component of the network. The problem was never really fully addressed and solved - I?m glad to see that finally some good people in this space are starting to seriously think about solutions.
Mike, are you telling us you?d rather avoid user complaints at all costs even if that means building something shitty for them that doesn?t really serve its stated purpose? If those are your standards then no thanks, I don?t want to be part of your fork. And I don?t think I?m alone in this sentiment.
- Eric Lombrozo

@_date: 2015-06-19 05:02:40
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
Exactly, Adam.
Except, I think the genie is out of the bottle - these ideas are too powerful for them to be killed forever. They will probably survive even if this scenario comes to pass?but in a different network under a different name?and Bitcoin will be relegated to the history books and walls of museums.
Most of the potential brainpower available on this Earth to make serious, profound contributions to this movement haven?t even begun to touch it. Just because you happen to run a Bitcoin startup right now?even if you?ve received millions of dollars in funding?don?t think that the whole world has low standards and is lazy! Someone WILL eventually build something better than we can presently imagine.
First mover advantage and the network effect are vastly overrated. At the risk of stating cliches, the Mac came before the Windows PC?Yahoo! came before Google?MySpace came before Facebook?Bitcoin came before .
- Eric Lombrozo

@_date: 2015-06-19 05:02:40
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
Exactly, Adam.
Except, I think the genie is out of the bottle - these ideas are too powerful for them to be killed forever. They will probably survive even if this scenario comes to pass?but in a different network under a different name?and Bitcoin will be relegated to the history books and walls of museums.
Most of the potential brainpower available on this Earth to make serious, profound contributions to this movement haven?t even begun to touch it. Just because you happen to run a Bitcoin startup right now?even if you?ve received millions of dollars in funding?don?t think that the whole world has low standards and is lazy! Someone WILL eventually build something better than we can presently imagine.
First mover advantage and the network effect are vastly overrated. At the risk of stating cliches, the Mac came before the Windows PC?Yahoo! came before Google?MySpace came before Facebook?Bitcoin came before .
- Eric Lombrozo

@_date: 2015-06-19 05:49:51
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] improving development model (Re: Concerns 
IPv4 came before IPv6?you pick up on things quickly :)

@_date: 2015-06-19 08:37:10
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
OK, a few things here:
The Bitcoin network was designed (or should be designed) with the requirement that it can withstand deliberate double-spend attacks that can come from anywhere at any time?and relaxing this assumption without adequately assessing the risk (i.e. I?ve never been hacked before so I can assume it?s safe) is extremely dangerous at best and just horrid security practice at worst. Your users might not thank you for not getting hacked - but they surely will not like it when you DO get hacked?and lack a proper recovery plan.
Furthermore, the protocol itself makes no assumptions regarding the intentions behind someone signing two conflicting transactions. There are many potential use cases where doing so could make a lot of sense. Had the protocol been designed along the lines of, say, tendermint?where signing multiple conflicting blocks results in loss of one?s funds?then the protocol itself disincentivizes the behavior without requiring any sort of altruistic, moralistic assumptions. That would also mean we?d need a different mechanism for the use cases that things like RBF address.
Thirdly, taken to the extreme, the viewpoint of ?signing a conflicting transaction is fraud and vandalism? means that if for whatever reason you attempt to propagate a transaction and nobody mines it for a very long time, you?re not entitled to immediately reclaim those funds?they must remain in limbo forever.
- Eric Lombrozo

@_date: 2015-06-19 09:42:33
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
If we want a non-repudiation mechanism in the protocol, we should explicitly define one rather than relying on ?prima facie? assumptions. Otherwise, I would recommend not relying on the existence of a signed transaction as proof of intent to pay?

@_date: 2015-06-19 20:07:02
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
It all comes down to managing risk. If you?ve got a decent risk model with capped losses and safe recovery mechanisms?and it?s still profitable?it?s fine. But most payment processors and merchants right now probably don?t have particularly good risk models and are making many dangerous assumptions?and probably would not be able to gracefully handle very many risk scenarios.
- Eric Lombrozo

@_date: 2015-06-19 21:02:06
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Newspapers are often sold in vending machines that make it possible for anyone to just pay the price of one and take them all?and most of the time they are not that carefully monitored. Why? Because most people have better things to do than try to steal a few newspapers. They probably were much more closely monitored earlier in their history?but once it became clear that despite the obvious attack vector very few people actually try to game it, vendors figured it wasn?t really that big a risk. Same thing applies to people trying to steal a piece of bubble gum at the cash register at a convenience store by double-spending.
- Eric Lombrozo

@_date: 2015-06-20 16:47:53
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Just to be clear, Jorge, I wasn?t suggesting that unconfirmed transactions are part of any sort of global consensus. In fact, they very much AREN?T. Which is exactly why it is extremely dangerous to accept unconfirmed transactions as final unless you clearly have assessed the risks and it makes sense for the particular business use case.
- Eric Lombrozo

@_date: 2015-06-20 16:52:26
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
I should also add that I think many in this space believe they have assessed the risk as acceptable but haven?t really considered how to cap potential losses nor made contingency plans for when the inevitable attacks *do* come.
- Eric Lombrozo

@_date: 2015-06-20 16:56:14
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
I think the misunderstanding was in perhaps my earlier statement seemed like I was suggesting that it?s the protocol?s responsibility to protect merchants from double-spends. On the contrary - I think we agree - the protocol CANNOT make any guarantees to ANYONE until we do converge on a history. The ?design? I speak of here is more on the merchant side.
- Eric Lombrozo

@_date: 2015-06-20 17:19:08
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
We don?t even have a concept of identity in the Bitcoin protocol, let alone non-repudiation. What good is non-repudiation if there?s no way to even associate a signature with a legal entity?
Sure, we could use the ECDSA signatures in transactions as part of a non-repudiation scheme - but the recipient would have to also have a means to establish the identity of the sender and associate it with the the transaction.
Furthermore, in light of the fact that there *are* fully legitimate use cases for sending conflicting transactions?and the fact that determination of intent isn?t always entirely clear?we should refrain from attaching any further significance transaction signatures other than that ?the sender was willing to have it included in the blockchain if a miner were to have seen it and accepted it?but perhaps the sender would have changed their mind before it actually did get accepted.?
- Eric Lombrozo

@_date: 2015-06-20 17:36:47
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
The contract between the buyer and seller is actually outside the Bitcoin network. Yes, a merchant that gets cheated could seek some other recourse in such an event?but the behavior you?re claiming as ?obviously correct? is NOT obviously correct.  In fact, there are arguments against this ?obviously correct? way even if we were to accept the premise that the signature implies a promise to pay (which I think many reasonable individuals would also dispute). For instance, by relaying conflicting transactions it makes it potentially easier for others to discover the double-spend attempt (of course, this requires wallets to not be lazy about this?perhaps such relays could be flagged or placed in a special message type).
- Eric Lombrozo

@_date: 2015-06-20 17:54:39
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
One more thing I would like to add to this thread: I want to make it unequivocally clear that I believe what is making double-spends easier has relatively little to do with the protocol and almost everything to do with poor software and poor security policy on the merchant end. Perhaps it isn?t prudent to push out changes to the relay policy that make these exploits even easier right now - but we NEED to be applying some kind of pressure on the merchant end to upgrade their stuff to be more resilient so that we have more room for changes on things like relay policy without significant disruption to the network.
- Eric Lombrozo

@_date: 2015-06-21 00:42:43
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
Thanks for asking *the* question, Jeff. We often get caught up in these philosophical debates?but at the end of the day we need something concrete.
Even more important than the specific software you?re using is the security policy.
If you must accept zero confirmation transactions, there are a few concrete things you can do to reduce your exposure:
1) limit the transaction amounts for zero confirmation transactions - do not accept them for very high priced goods?especially if they require physical shipping.
2) limit the total amount of unconfirmed revenue you?ll tolerate at any given moment - if the amount is exceeded, require confirmations.
3) give merchants of subscription services (i.e. servers, hosting, etc?) the ability to shut the user out if a double-spend is detected.
4) collect legal information on purchasers (or have the merchants collect this information) so you have someone to go after if they try to screw you
5) create a risk profile for users?and flag suspicious behavior (i.e. someone trying to purchase a bunch of stuff that totally doesn?t fit into their purchasing habits).
6) get insurance (although right now reasonably-priced insurance is probably pretty hard to obtain since statistics are generally of little use?we?re entering uncharted territory).
7) set up a warning system and a ?panic? button so that if you start to see an attack you can immediately disable all zero confirmation transactions system-wide.
8) independently verify all inbound transactions and connect to multiple network nodes?check them against one another.
As for software tools to accomplish these things, we can talk about that offline :)
- Eric Lombrozo

@_date: 2015-06-21 01:35:50
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
I should also point out that pretty much all of these suggestions (except for maybe 8) would apply to ANY payment system?they are NOT specific to Bitcoin whatsoever. Any serious payment processor should have these sorts of policies engrained as part of company culture?or else one day (probably not too long from now) you?ll be out of business. The mere suggestion that changing relay policy would pose significant threats to the bottom line of a payment processor is about the height of amateurishness, IMHO.
- Eric Lombrozo

@_date: 2015-06-21 01:51:51
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] F2Pool has enabled full replace-by-fee 
I am glad to hear that. Yes, it absolutely makes sense to let the merchant to make business decisions still pending confirmation (i.e. should I actually ship?)
Then Coinbase is essentially taking on the role of an insurer?are they taking the appropriate precautions to limit potential losses? Can they make up for these losses with fees? And if not (or if they don?t really have a quantifiable risk model) could they survive a worst-case scenario with at most a surface wound? (i.e. a systemic attack involving many machines in many different places all attacking at once).
It would be absolutely the height of idiocy to guarantee payment on merchandise that has yet to ship, i.e. So I hope these reports are wrong :)
- Eric Lombrozo

@_date: 2015-06-25 06:41:01
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] BIP Process and Votes 
Wladimir is doing an amazing job under difficult circumstances. Give the guy a break, please.
- Eric Lombrozo

@_date: 2015-06-26 19:18:20
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] The need for larger blocks 
I?ve been pondering this whole scale issue considerably?and am left with the conclusion that blockchains are ultimately dispute resolution mechanisms. The vast majority of crypto negotiation will be taking place at levels lesser than global consensus in the future - global consensus is just far too expensive to require for every single cappuccino. There really is little need to take most cases globally?unless the participants disagree. I?ve commented in other places that blockchains are essentially a ?fix? to the prisoner?s dilemma - they make cooperation the equilibrium strategy.
Regardless of whatever linear factor we scale the blockchain by, it is simple math to see that any exponential growth (even if for a short time) in usage will overwhelm the current network. If we ever intend to take bitcoin mainstream, we will most likely experience at least a short time of exponential growth?at least until we either reach an inherent limitation or until we saturate. As Pieter said earlier, FAPP right now the demand for payments might as well be infinite. We?re nowhere near the ability to service it all.
The block size issue is really a usability issue at this point. There are two fundamental things we need to solve:
1) There?s no model for how we?ll introduce a fee market, even though the design of Bitcoin fundamentally depends on fees for its survival (at least in the current form of the design.)
2) There?s no mechanism for how to perform fee bidding and estimation. Most wallets simply have no way to do this without serious usability problems.
If we?re going to talk about block fees, let?s keep it in the context of these relevant issues and not confound it with the scalability issue?these are two very different issues.
- Eric Lombrozo

@_date: 2015-06-26 19:54:37
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] The need for larger blocks 
I should add that a strategy of ?let?s avoid fee pressure as much as possible. let?s avoid even thinking about how we?ll transition as much as possible.? strikes me as at least a tad bit myopic.
- Eric Lombrozo

@_date: 2015-06-27 04:18:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] The need for larger blocks 
The economic policy?s status quo has been to avoid fee pressure. But the consensus status quo obviously is not to have a hard fork.
There?s clearly a contradiction between these two policies, which is a big part of the reason this issue has come to this point. These two policies are fundamentally at odds.
- Eric Lombrozo

@_date: 2015-06-27 21:54:04
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Original Vision 
Fraud proofs actually don?t need to be made super efficient?but they do need to be secure, of course.
The trick is aligning incentives. In order for fraud proofs to be widely available there needs to be a market for them - there must be a way to buy one (because producing one is not free). What makes such a scheme actually practical is that very few of these fraud proofs ever need to actually be executed - it?s a classical Nimzowischian case of the threat being much stronger than the execution.
- Eric Lombrozo

@_date: 2015-06-27 22:32:57
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Original Vision 
Just to clarify, SPV is fundamentally busted as it currently exists. I?m talking about potential optimizations for future protocols.
- Eric Lombrozo

@_date: 2015-06-27 22:48:04
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Original Vision 
"Unfortunately no design for fraud proofs which is both efficient and
secure has been proposed?
Also to clarify, there?s no disagreement here, Patrick.

@_date: 2015-06-28 07:13:13
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] The need for larger blocks 
Either one branch wins overwhelmingly in a relatively short period of time?or both branches lose, I think.
- Eric

@_date: 2015-06-28 07:16:01
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] The need for larger blocks 
Furthermore, the actual way in which the conflict is resolved sets a precedent for how such disagreements are to be ?resolved? in the future.
So the means are also important to consider.
- Eric

@_date: 2015-06-28 17:59:40
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
There?s no question that a flooding mesh network requiring global consensus for every transactions is not the way. It?s also clear that a routable protocol capable of compensating hubs is basically the holy grail.
So what?s there to discuss?
- Eric

@_date: 2015-06-28 18:13:29
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
The Lightning network is essentially a contract negotiation scheme that rewards cooperation. Defection amounts to either broadcasting early or not responding to signature requests. If done right, either of these situations incurs a bigger cost to the uncooperative party than cooperation. This is why I say blockchains are like a fix to the prisoner?s dilemma.
The blockchain becomes essentially a dispute resolution mechanism and a way to anchor stuff. There?s no use case covered by the current method of ?flood the entire network and confirm on blockchain? that can?t be covered by a method of ?participate in a contract which guarantees me payment on the blockchain if anyone is uncooperative but which rarely requires touching the blockchain? methinks.
- Eric Lombrozo

@_date: 2015-05-06 16:06:00
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Block Size Increase 
I don?t really have a strong opinion on block size either?but if we?re going to do a hard fork, let?s use this as an opportunity to create a good process for hard forks (which we?ll inevitably need to do again in the future). The change in block size is a very simple change that still allows us to explore all the complexities involved with deployment of hard forks. Let?s not just do a one-off ad-hoc thing.
- Eric Lombrozo

@_date: 2015-05-07 04:30:49
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Block Size Increase 
IMHO, these issues are the elephant in the room and the talk of block size increases is just a distraction.
- Eric Lombrozo

@_date: 2015-05-23 17:44:20
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] BIP for deterministic pay-to-script-hash 
A few months back, William Swanson and I had worked on a more general script template format. Unfortunately, other work has prevented us from being able to fully complete it - but here?s the start:
 /
- Eric Lombrozo

@_date: 2015-05-31 03:01:53
@_author: Eric Lombrozo 
@_subject: [Bitcoin-development] Proposal: A measured response to save 
I mostly agree with your assessment...except for your last claim.
Not that I wouldn't like to find a way to avoid politics, but like I've
argued before, it is inevitable that sooner or later any consensus protocol
that seeks dynamism will encounter politics.
The block size discussion, while ultimately necessary, for now is in the
best case merely serving as an example of the kind of political issues we
*really* need to be finding some solution for...and in the worst case is a
distraction and evasion.
Some protocol updates will be merely technical optimizations or feature
enhancements that are fairly uncontroversial...but some will inevitably be
highly controversial with real-world economic consequences, winners and
losers. We lack a process for deciding these issues. No matter how
sophistocated we make the protocol, somethings will inevitably require
external input to make these issues decidable...it is a Goedelian
implication. This external input could be some sort of vote (of which
hashing power is a particular kind) or perhaps something else.
There's something to be said for building the dynamics of hard forks *into*
our model rather than avoiding it at all costs.  However, forks are the
easy part. The difficulty is in merging different branches. Perhaps we
should learn a thing or two from git. Perhaps the question we should be
asking is not "how do we avoid hard forks" but "how can we design the
network to allow for merging?"
- Eric Lombrozo

@_date: 2015-11-17 11:40:43
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Dynamic Hierarchical Deterministic Key Trees 
I've submitted a BIP proposal that solves the issue of needing to predefine HD wallet structures and not being able to arbitrarily nest deeper levels. Comments appreciated.
- Eric

@_date: 2015-11-21 03:29:46
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Hierarchical Deterministic Script Templates 
A while back, I started working with William Swanson on a script template format to allow for interoperability in accounts between different wallets. We made some progress, but both of us got pretty busy with other projects and general interest was still quite low.
It seems interest has picked up again, especially in light of recent developments (i.e. CLTV, relative CLTV, bidirectional payment channels, lightning), where nongeneralized script formats will not readily support the rapidly advancing state-of-the-art in script design.
I have started working on a draft for such a standard: Comments, suggestions, and collaboration are welcome.
- Eric

@_date: 2015-11-21 08:45:10
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Dynamic Hierarchical Deterministic Key Trees 
You could use a key for both signing and for derivation of a deeper level (and perhaps there are some applications for this, if you think of any please let me know), but the use cases being considered involve generation of signing key sequences from seeds that are easy to backup and easy to share with others to simplify multidevice synchronization, key management, account structures, etc... while also allowing for privacy by making it nontrivial to associate transactions for an account without knowing the seed/chain code.
As such, we generally refer to such sequences by a path to the immediate parent node in the tree and reserve the children themselves for the signing keys.
- Eric
------ Original Message ------
bitcoin-dev" Sent: 11/17/2015 5:10:17 AM

@_date: 2015-11-25 01:14:55
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
the semantics of this opcode with another existing feature in the system (coinbase maturity).
from an application developer standpoint, I think the concept of a timelock is more relevant. Maturity is a concept that was introduced for the sake of reducing the disruptive impact of reorgs. Miners would prefer to be able to spend the coins immediately, but instead they are forced to wait due to inherent limitations of the system. Timelocks, on the other hand, are typically used to control when funds can be moved. In these use cases, one or more of the parties involved explicitly want there to be a delay even if there were an idealized situation in which consensus is always reached instantaneously and there were never any Moreover, since we already have CLTV, adding RCLTV or some variant thereof makes the relationship between the two more explicit.
So my vote goes to RCLTV or RCHECKLOCKTIMEVERIFY.
As for whether to explicitly use CHECK_..._VERIFY, consider that with segregated witness it will be possible to add opcodes that can push values onto the stack (rather than just hard failing or NOP), so there's something to be said for naming consistency.
- Eric
------ Original Message ------
Sent: 11/24/2015 4:31:55 AM

@_date: 2015-11-25 23:41:03
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
As I said in an earlier post, a systems developer and an application developer have very different perspectives on this. From the former's perspective, it is entirely sensible to name things based on basic features of the system's design (i.e. a field in the txin or tx that gets checked) - but from an app developer's perspective, what matters is how they will use a particular feature in an actual app.
I think that part of what systems developers should strive to do is to abstract out the inner minutiae of the system's guts and expose to app developers the clearest interface with which to develop apps. This is even more the case when the details of the inner workings are completely irrelevant to the application logic and there's no real gains to be had from attempting to optimize for the inner workings when designing an clear that relative timelock is *the* critical exposed functionality intended here. Now, one could argue that the satoshi script is still a systems level component of the system...but with the advent of overlay protocols such as payment channels and the Lightning Network, it is clear that we now require a new abstraction layer for reasoning about the higher level logic of the system that doesn't burden the protocol designer with having to know the intimate and esoteric details of the lower system levels. Of course, many of those who work on these higher level protocols will also be experts in the underlying system design. However, it greatly increases the learning curve and can easily frustrate people looking to work on these ideas...and ultimately, knowing the inner details of how the nSequence field is structured and what the bits actually mean is irrelevant to someone trying to design scripts for such applications.
We've already deployed another opcode, CHECKLOCKTIMEVERIFY, which does refer to the field name. However, in this particular situation, the field name reflects *far* more closely what the app developer actually cares about than nSequence, which to the app developer might as well be called foo. As such, I stick with my original vote - we should call the opcode RCHECKLOCKTIMEVERIFY, which has the advantage of communicating fairly directly to developers and protocol designers the semantics they actually care about and also makes clear the relationship between absolute and relative timelock...that's to say, the ability for the script designer to lock specific coins until either a specific moment in time or until a certain delay has passed since the coin output was created (added to blockchain).
Let's face it - the entire motivation behind BIP68/BIP112 is relative timelock. Explicitly calling the opcode RCHECKLOCKTIMEVERIFY will make life easier for everyone and will help sell the idea and help it gain greater acceptance more quickly; while stubbornly adhering to an esoteric detail that is only there for historical reasons will only continue to delay the idea's acceptance and adoptance.
- Eric
------ Original Message ------
Sent: 11/25/2015 3:05:50 PM

@_date: 2015-11-26 13:32:58
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
After a little more though (and some comments from aj), I realize that the opcode naming convention is actually CHECK  VERIFY.
Therefore, the full opcode name should be CHECKRELATIVELOCKTIMEVERIFY.
However, this name is ridiculously long, so at least some part will require abbreviation.
In typical script example usage, most sensible seems to be to abbreviate both CLTV and CRLTV.
- Eric

@_date: 2015-11-27 00:10:49
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
After reading Rusty's post, I admit there's something to be said for the fact that both the script and the nSequence field play a combined role, and thus, making the interaction between the two more clear in the naming make sense.
It is somewhat unfortunate that currently, we can't just have a dedicated field for the purpose of relative locktime (or minimum age) without having to repurpose the only unused 32 bits in the txin.
HOWEVER...there might be ways around this issue using segwit.
I've been pondering the possibility of adding an extra input vector to the prunable extra data that  comprises the witness. Witness structures can provide additional data that is used in transaction validation but does not contribute to the tx hash.
Currently, the signature checking opcodes in the script already do this implicitly for computing the hash that is signed (but not the tx hash used in block merkletrees)...and this is the principal cause of undesirable malleability issues. Clearly the signatures themselves cannot contribute to the hash they are signing. So segwit makes this separation explicit by moving the signatures to a structure external to the script. Pieter Wuille's implementation ( generalizes this idea using a script witness structure that is a vector of arbitrary inputs. Clearly moving the signatures into such structure is an important feature...but other types of input to the script could be placed here as well.
I had considered the possibility of placing a minimum age (relative locktime) field in the input vector that could be checked for mempool acceptance without having to evaluate the script. Of course, the location of such a field would have to be known by the mempool and cannot be an arbitrary element of a generic input vector, which adds some minor but surmountable complications.
Greg Maxwell pointed out, however, that signing opcodes that sign hashes discarding this data would make it trivial for anyone to change this field without signing anything. The nSequence fields of txins, being part of the tx serialization that gets hashed, is therefore always signed.
This led me to consider the possibility of adding extra opcodes to the script that can incorporate additional data in the hash that gets signed. This data would go in another structure that does not contribute to the tx hash but is outside the witness. Then we could add extra prunable data fields that the signer can commit to.
If I've missed something critical in the above analysis, someone please correct me...but it seems that such a mechanism would allow adding extra prunable signed data fields to transactions, which might ultimately remove scarcity of tx data that we can repurpose via soft forks. If this is the case, I would suggest turning the nSequence field into a dedicated min age/rlt field to simplify the semantics and avoid ugliness in trying to reclaim unused bits.
I may be overlooking something important here, but unless there's a reason such data cannot be made prunable, I haven't been able to poke a hole yet.
- Eric

@_date: 2015-09-30 19:54:34
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
I can go along with making it optional but recommended for the first deployment and making it mandatory later on. It would be purely informational for now...but it will give us valuable data.
As has been said before, most of these BIP deployments will likely be accompanied by recommended default settings for miners. Assuming the BIP itself is not very controversial, the gravest dangers come not so much from miners (or pool operators, more accurately) deliberately choosing to lie...but more from either shortcuts taken in implementations and/or bugs. Collecting additional data will help spot faulty implementations and allow us to intervene.
Eventually, I imagine a much more sophisticated signaling mechanism where endusers can be given highly informative messages regarding changes and we can have a way of directing people to resources where they can learn more about the new features.
- Eric

@_date: 2015-10-05 16:18:12
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
I agree with you, Sergio, up until the part about someone having won a battle. There's a difference between sincere technical objections and someone just being a dick. I think in this case this line has been crossed (and I don't think I'm alone here).
- Eric

@_date: 2015-10-06 07:20:38
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
I prefer the term "clown".
Can we please move on?
------ Original Message ------
Sent: 10/6/2015 12:17:14 AM

@_date: 2015-10-07 09:02:14
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
That's why it's important to measure miner adoptance. Note that this isn't a vote - it's an adoption metric for what is presumably a fairly uncontroversial upgrade. If there's contentious controversy amongst miner all bets are off.
Our current mechanisms are imperfect in this regard...as we've seen in the past, miners have deliberately disabled checks despite signaling adoption in their blocks. But a real hashpower supermajority would make such attacks hard to pull off in practice.
- Eric

@_date: 2015-10-07 09:25:53
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
You're right about the potential for 1 bad confirmation even with very low frequency...but with an overwhelming supermajority of hashpower, 2 bad confirmations become quite unlikely, n bad confirmations becomes exponentially unlikely in n.
As part of such soft fork deployments, it's true that old nodes might see a bad confirmation on occasion (even assuming overwhelming supermajority hashpower adoptance). So yes, old nodes and SPV clients should probably require more confirmations right around such a transition...or should upgrade. It is entirely possible to make clients warn the user if the block version is unrecognized, which will help to prevent anyone from accepting bad blocks (although SPV security necessarily relies on miners to validate for them).

@_date: 2015-10-09 03:58:12
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Making soft forks pluggable 
Before I scare anyone away, please here me out:
It occurs to me it wouldn't be all that difficult to support the ability to define soft forks entirely as standalone units that can be trivially merged with Bitcoin Core. It would require a few changes in some places in the consensus code, but at least for a very wide class of potential soft forks, all cases could be covered via only a small number of hooks, primarily in main.cpp, consensus/*, script/interpreter.cpp, and primitives/*. (Other hooks could be added in non-consensus code such as rpcblockchain.cpp or the wallet). It would be possible to build unit tests for each soft fork independently and compare enforcement of different combinations (as well as simulate these deployment combinations on regtest).
Before I get too heavily invested in this idea, though, I'd like to see if there are any reasonable objections to such a thing. Of course, refactors are generally disruptive in the short-term...but I think what I'm talking about can be done without having to move very large chunks of code around, with very specifically defined hooks that can be easily documented to make backports fairly simple.
My biggest concern (other than being able to convince everyone that we won't break anything, which of course I'd have to do a good job of in terms of rigor) is whether supporting this feature is a good idea in the first place. There's something to be said for it not being *too* easy to write and deploy a soft fork...however, unless we open this up a little more and make such deployments more routine (and safe) it will take a very long time to deploy stuff. A significant motivation behind VersionBits (BIP0009) is to make such deployments faster, so if we're already doing that perhaps we might as well take this initiative even If others think this is a good idea I'll start writing up a detailed plan. (NOTE: The current versionbits deployment plan does not require this. I am working on an implementation of versionbits that could potentially support this plan but doesn't have to.)
If I'm very wrong, I am all ears to *sincere* objections.
- Eric

@_date: 2015-10-09 07:39:59
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Fw: Making soft forks pluggable 
I wanted to clarify that this goal is for AFTER the next release in case that didn't come across. The point is just to ascertain interest and start thinking ahead. VersionBits can be fully ready to go well before then and is well underway.
------ Forwarded Message ------
Sent: 10/8/2015 8:58:12 PM
Before I scare anyone away, please here me out:
It occurs to me it wouldn't be all that difficult to support the ability to define soft forks entirely as standalone units that can be trivially merged with Bitcoin Core. It would require a few changes in some places in the consensus code, but at least for a very wide class of potential soft forks, all cases could be covered via only a small number of hooks, primarily in main.cpp, consensus/*, script/interpreter.cpp, and primitives/*. (Other hooks could be added in non-consensus code such as rpcblockchain.cpp or the wallet). It would be possible to build unit tests for each soft fork independently and compare enforcement of different combinations (as well as simulate these deployment combinations on regtest).
Before I get too heavily invested in this idea, though, I'd like to see if there are any reasonable objections to such a thing. Of course, refactors are generally disruptive in the short-term...but I think what I'm talking about can be done without having to move very large chunks of code around, with very specifically defined hooks that can be easily documented to make backports fairly simple.
My biggest concern (other than being able to convince everyone that we won't break anything, which of course I'd have to do a good job of in terms of rigor) is whether supporting this feature is a good idea in the first place. There's something to be said for it not being *too* easy to write and deploy a soft fork...however, unless we open this up a little more and make such deployments more routine (and safe) it will take a very long time to deploy stuff. A significant motivation behind VersionBits (BIP0009) is to make such deployments faster, so if we're already doing that perhaps we might as well take this initiative even If others think this is a good idea I'll start writing up a detailed plan. (NOTE: The current versionbits deployment plan does not require this. I am working on an implementation of versionbits that could potentially support this plan but doesn't have to.)
If I'm very wrong, I am all ears to *sincere* objections.
- Eric

@_date: 2015-09-04 18:41:52
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Proposed minor change to BIP 01 to use a PR for 
I think it's a reasonable approach. Once the number is assigned, the change is made and the pull request is updated.
Only thing is it would be nice to be able to indicate which pull requests are number requests and which pull requests are ready for merging. Perhaps we should make a special label for number requests.
- Eric
------ Original Message ------
Sent: 9/3/2015 4:18:08 PM

@_date: 2015-09-15 12:00:11
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
I basically agree with what has been said here.
Refactoring efforts should be well-coordinated. Their short-term impact can be quite disruptive, although if done correctly, longer-term they make it even easier for downstream developers to add and merge changes.
By scheduling move-only changes, others can avoid making PRs immediately prior to or during these changes (which ironically involve considerable disruption to PRs while changing nothing for endusers). Furthermore, it would be useful to document the changes in ways that help other developers rebase properly.

@_date: 2015-09-16 18:52:09
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout 
The exact numbers (95% vs. 75% etc) don't need to be completely specified to start working on an implementation. What really matters for now is defining the states and trigger mechanisms. I'd rather we not argue over the optimal values for supermajority requirement at this point.

@_date: 2015-09-16 19:23:18
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
I'd rather replace the whole nSequence thing with an explicit relative locktime with clear semantics...but I'm not going to fight this one too much.

@_date: 2015-09-18 01:20:54
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Weekly development meetings on IRC 
I love the weekly meeting idea...but timezones might be an issue.
My general preference would be afternoons to late evenings pacific time, but that translates to late night/early morning for those in europe.

@_date: 2015-09-18 01:42:53
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] libconsensus and bitcoin development process 
You're aware that my entire stack was built around this model and I've even built a fully fledged desktop GUI, multisig account manager, and servers supporting pull and event subscription atop it, right?

@_date: 2015-09-18 10:28:08
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
To be quite frank, I'm a little disappointed we've fallen back on arguing over numbers pulled out of a hat rather than discussing far more fundamental issues such as the dev process generally, consensus building, and our basic understanding of what Bitcoin really is, its strengths and weaknesses, where it shows most promise, and communicating a more unified vision to the industry and the public.

@_date: 2015-09-20 21:16:36
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
You make a decent point...but please try to keep the discourse civil. It's already hard enough trying to figure this stuff out without fanning more flames.
------ Original Message ------
Sent: 9/20/2015 1:23:28 PM

@_date: 2015-09-20 21:45:04
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Sent: 9/20/2015 2:33:38 PM
Larger user base won't necessarily protect against governments if we still have chokepoints they can go after. Given that as a currency Bitcoin  currently represents a negligible portion of the world's economy, even growing the user base by some small factor is at best a token gesture in our fight against governmental threats. If governments successfully take down critical pieces of our network infrastructure, Bitcoin will fail and most people will continue doing business as usual (using fiat currency), most of them never even noticing anything noteworthy happened at all.
What we really need to grow is the number of nodes on the network that participate in its basic infrastructure - namely: miners, validators, etc...and the more centralized these activities become, the easier it will be for governments to clamp down.

@_date: 2015-09-20 22:21:55
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Sent: 9/20/2015 3:02:32 PM
Almost none of these merchants depend on Bitcoin in any significant way for revenue...and that's likely to remain the case for a good while. Merchants that have chosen to accept Bitcoin are typically using a handful of payment processors, again...chokepoints. And almost none of them are contributing any network resources back to Bitcoin.
Exchanges are indeed serious chokepoints. But increasing the number of users will probably have relatively little effect on this unless we also increase the number of exchanges and decentralize the exchanges. If all we had to do is increase the number of users, the same argument could be used to claim that banks would be less susceptible to governmental crackdowns if they just had more account holders.
Exchange decentralization is indeed another thing we must work towards - but that's probably beyond the scope of the more pressing issue which is building consensus in Bitcoin development.
I've pointed out this weakness of Bitcoin *numerous* times. That I failed to mention it here does not mean it hasn't been discussed elsewhere. Some of us have also been actively working towards developing a more modular, layered architecture and better implementations that will afford greater decentralization in software development with less need for critical code reviews, less pushback from downstream developers who must continuously rebase, a better process for building consensus in the community, and simpler app migration.
We need to increase the basic infrastructure nodes by a factor much larger than 2 or 3...more like 100 or 1000...and it's entirely doable with properly aligned incentives.

@_date: 2015-09-20 23:11:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Scaling Bitcoin conference micro-report 
Sent: 9/20/2015 3:51:36 PM
Core development process and decentralized dev/community consensus building (in particular for consensus-critical changes) is at the top of my priorities as issues right now...and one that I'd love to discuss more in depth...but it probably deserves its own thread. The political angle seems very difficult right now while the systems architecture stuff seems a bit more tractable...and it seems that without architectural changes it will be extremely hard to decentralize development and easily bring large numbers of new developers in.
Not necessarily. Right now we already pay around 3,600 bitcoins a day in inflationary subsidies, very little of which goes to the majority of critical infrastructure nodes and their operators. This is a problem with the current protocol design, one we'll hopefully be able to fix.
Having more core infrastructure nodes doesn't need to raise costs per transaction - but it will most likely require abandoning the current approach of having three basic node classes: miners (which tend towards centralized pools), full nodes (which must validate each of everyone's transaction and in return get paid nothing), and thin clients (which essentially amount to parasitic nodes that do not contribute any resources back to the network and must be subsidized).

@_date: 2015-09-23 00:07:22
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Long-term vision for bitcoind (was libconsensus 
Here's what I propose as a long-term plan:
1) libconsensus
We should probably start by defining an API for libconsensus. It should support an abstract DB model, track chain state, provide query mechanisms for blocks and transactions with optional pruning and indexing, expose a subscription mechanism for events such as NEW_TIP, REORG, etc, and contain a script interpreter.
We can develop the library in parallel with Bitcoin Core without too much refactoring of Bitcoin Core itself...just moving pieces of Bitcoin Core's consensus code into the new library, tracking code movements to make merging easier. Yes, this is a bit ugly as it requires code duplication...but it will temporarily avoid much of the downstream pushback we're getting. The idea is that we can prove out the library with some simple projects, then start removing the consensus stuff from Bitcoin Core once we have greater acceptance of the library and better 2) peer services
We develop a peer services library that performs the tasks of peer discovery and relay, with the ability to connect to appropriate peers and queue messages. It uses libconsensus for all validation functionality and as a datastore for the consensus state but maintains its own database for peer history and statistics. I believe Cory has been working on this already using libevent. I've already developed an async library for this as well.
3) API/RPC
We provide high level calls and pub/sub mechanisms. ZMQ has been implemented and added already, but we could support other transports as 4) Wallet
The wallet is split out into a separate process that connects to the stack via the API/RPC layer.
- Eric
------ Original Message ------
Sent: 9/22/2015 11:36:14 AM

@_date: 2015-09-23 00:10:50
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Long-term vision for bitcoind (was libconsensus 
I should also add that the mempool should exist in (2). This way the peer services layer can manage all relay policy and mempool management.
------ Original Message ------
Sent: 9/22/2015 5:07:22 PM

@_date: 2015-09-28 04:04:49
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
My initial reaction is just HUH?!?!? Is this some sophisticated form of humor I'm just not getting?

@_date: 2015-09-28 05:20:31
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Perhaps Adam won't go into the rationale...but I think it is important we clarify this.
For better or worse, the only "voting" system available to Bitcoin that cannot be trivially attacked is hashing power. Soft forks are essentially miner-enforced rule changes...rules they could have decided to enforce without the consensus of anyone else. For instance, as far as old nodes are concerned, a p2sh output can be redeemed by a simple preimage of the hash...with no signatures. The point, however, is that as long as the majority of hashpower enforces the new rule, such attempts to redeem the output will never end up on the blockchain. Therefore, transactions that attempt to redeem the output with a simple preimage are as good as invalid...and effectively have become invalid.
I concede that this mechanism has some issues. Moreover, I agree that it is important that the Bitcoin community be aware of these things. I've been proposing making these rule changes explicit in the BIPs ( I believe it is important that people weigh in on such rule changes. However, the above stated mechanism does not fall under the definition of "hard fork" we've come to accept.
Go ahead and object to soft forks...but at least try not to make arguments based on changing the definitions of terms we all generally agree upon.
- Eric

@_date: 2015-09-28 05:44:52
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
SPV wallets in their current form are inherently insecure. Moreover, while we at least have a soft fork mechanism that is not trivially exploitable (yes, it's got issues...but unlike SPV wallets, it isn't so easily exploitable), we have NO hard fork mechanism in place that isn't highly prone to systemic consensus failure.
But I think pretty much anyone who hasn't been in a coma for the last several years knows this...and I'll stop repeating the obvious.

@_date: 2015-09-28 23:17:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
Insults were not really my intention. Let's set aside our differences regarding SPV security and assume you understand the different implications for soft forks and hard forks.
Other than the fact that doing this as a soft fork requires an extra OP_DROP, how would doing this as a hard fork make any difference to SPV clients? If, as others have suggested, all clients warn the user on unrecognized nVersion and make unknown noops nonstandard, would this satisfy your concerns? The logic seems pretty straightforward.
- Eric

@_date: 2015-09-30 04:46:25
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
Good points, Greg.
The way I see it, this mechanism isn't really about "voting" - it's about deployment of fairly uncontroversial changes with the minimum amount of negative disruption. If we have reason to believe a particular BIP stands little chance of hitting the 95% mark relatively quickly, it's probably better not to deploy it...so this mechanism is most useful for adding fairly uncontroversial features provided as default settings in product releases - and measuring adoption as best we can before activating these features.
The current controversies around things like CLTV, CSV, etc... don't seem to revolve around these features themselves - there seems to be near-unanimous agreement that these features are good (and most disagreements regarding functionality are over quite minor nits, really). Instead the controversies are much more likely to be around deployment strategies.
While I would like to get some form of explicit acknowledgment from miners that a new rule is in effect, the truth of the matter is we still lack a means to determine whether or not miners are actually enforcing these rules...unless someone happens to mine a block that breaks the new rule. This is a bit frustrating...but that's just how it is.
To sum up, Version Bits is not a mechanism for vetting proposed changes and building consensus (that should take place BEFORE we assign bits). This is a deployment mechanism for fairly uncontroversial changes. Either a BIP is relatively quickly adopted with overwhelming support...or else perhaps it's best to wait until it has sufficient support before attempting deployment (or perhaps not deploy it at all) - and ultimately we want these transitions to run as smoothly as possible. As long as the BIPs are relatively uncontroversial, miners will most likely continue to choose to cooperate in the interest of the health of the network (and will use recommended default settings). Once clients have better support for this, perhaps we can do more sophisticated - Eric
------ Original Message ------
; "Pieter Wuille" ; "Eric Lombrozo" Sent: 9/29/2015 7:57:52 PM

@_date: 2015-09-30 05:09:51
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
I should add that hard forks do provide us with a means to determine whether or not miners are enforcing the new rules...but generally speaking they risk far greater disruption if anything fails to go as planned. Between the risk of clients accepting an occasional invalid "confirmation" or two and the risk of a total network partition, the former seems far less serious. I believe the concerns regarding old clients can be remedied to a very large extent by means of a good awareness campaign.
- Eric

@_date: 2015-09-30 07:16:01
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Design Competition 
I've also got a competition where the object is to build a spaceship using only a watermelon, two donkeys, some duct tape, and a fire ------ Original Message ------
Sent: 9/29/2015 11:37:07 PM

@_date: 2016-04-21 08:28:45
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Proposal to update BIP-32 
In practice the probability of this case triggering is on the order of 2^-128 or something astronomically tiny. I've been using BIP32 for a few years already as have many others...I don't think we've ever had to handle this case. Justifiably, many app developers feel like the additional complexity of properly handling this case is not worth the effort.
Having said that, if the handling of this case is simple to implement and easy to isolate in the program flow, I am in favor of doing something along the lines of what you propose.
- Eric

@_date: 2016-01-07 05:28:18
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] SegWit testnet is live 
I am pleased to report that as of December 31, 2015 we have been successfully running a segregated witness testnet, called segnet, and have already implemented rudimentary wallets with support.
For source code, please look at sipa's github repo:
And some example signing code at my repo:
Several wallets have already committed to supporting it including mSIGNA, GreenAddress, GreenBits, Blocktrail, and NBitcoin. More wallets are expected to be added to this list soon. If you're a wallet dev and are interested in developing and testing on segnet please contact me.
We're right on schedule and are very excited about the fundamental improvements to bitcoin that segwit will enable.

@_date: 2016-01-07 05:56:57
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] SegWit testnet is live 
I have been informed that Breadwallet has also committed to supporting segwit.
The list now includes Blocktrail, Breadwallet, GreenAddress, GreenBits, mSIGNA, and NBitcoin.

@_date: 2016-01-19 03:54:15
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Segregated Witness App Development 
Hello, folks.
I wanted to let all of you know a new IRC channel has been created called  where we welcome all discussion pertaining to integrating and supporting segregated witness transactions in wallets as well as comments or suggestions for improvement to the spec. Please come join us. :)

@_date: 2016-01-28 16:52:53
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] BIP Classification Process 
I think the current situation with forks could have been avoided with a better process that can distinguish between different layers for bitcoin modification proposals.
For instance, BIP64 was proposed by Mike Hearn, which does not affect the consensus layer at all. Many Core devs disliked the proposal and Mike had lots of pushback. Regardless of whether or not you agree with the merits of Mike?s ideas here, fact is having nodes that support BIP64 would not fundamentally break the Bitcoin network.
This issue prompted Mike to break off from Core and create XT as the applications he was developing required BIP64 to work. With this split, Gavin found a new home for his big block ideas?and the two teamed up.
We need to have a process that clearly distinguishes these different layers and allows much more freedom in the upper layers while requiring agreement at the consensus layer. Many of these fork proposals are actually conflating different features, only some of which would actually be consensus layer changes. When people proposing nonconsensus features get pushback from Core developers they feel rejected and are likely to team up with others trying to push for hard forks and the like.
A while back I had submitted a BIP -  BIP123 - that addresses this issue. I have updated it to include all the currently proposed and accepted BIPs and have submitted a PR:  I urge everyone to seriously consider getting this BIP accepted as a top priority before we get more projects all trying their hand at stuff and not understanding these critical distinctions.
- Eric

@_date: 2016-01-28 23:57:00
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] BIP Classification Process 
Codebase forks with nonconsensus features are totally fine! It?s the bitterness and resentment that arose out of the need to get everyone to agree on something that not everyone really needs to agree on that?s the problem.

@_date: 2016-03-30 01:54:33
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Segnet4 
============================== START ==============================
Hello everyone.
Pieter Wuille has pushed code for a new segwit testnet that features activation via BIP9 as well as support for BIP68, BIP112, and BIP113. In particular, it now supports Lightning Network app development and collaboration.
I encourage everyone to spin up a node and try it out.
For source code, please go to Pieter's github repo:
 Feedback is welcome here or on the  channel on Freenode.
- Eric

@_date: 2016-05-17 16:25:22
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Making UTXO Set Growth Irrelevant With 
We?ve been talking about doing this forever and it?s so desperately needed.

@_date: 2017-02-22 19:30:37
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] TXO commitments do not need a soft-fork to be 
This kind of thing is long overdue!
I think it?s a great idea to attempt this without soft forking TXO
commitments yet so we can see what works best.
- E
On Wed, Feb 22, 2017 at 5:11 PM, Peter Todd via bitcoin-dev <

@_date: 2017-01-07 00:55:19
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Classic 1.2.0 released 
Your release announcement does not make it clear that Bitcoin Classic is
incompatible with the current Bitcoin network and its consensus rules. It
is a hard fork on mainnet with no safe activation as well as including
other unsafe changes. There is also no BIP for the hard fork. There is also
no evidence of community wide consensus for such a hard fork. This is
dangerous and irresponsible.
It's wrong to announce software without correctly informing people about
the contents or risks. Furthermore, there are no release notes in
 nor
changelog. Without those, it is almost impossible for average users to know
what is under the hood or what has changed and time consuming for
developers to assess.
On Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <

@_date: 2017-01-07 16:28:34
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] Bitcoin Classic 1.2.0 released 
Can you guys please take this discussion elsewhere? Perhaps to
bitcoin-discuss? This is not the place to rehash discussions that have
taken place a million times already. The behavior of the network under
contentious hard forks has been discussed ad nauseum. This mailing list is
for the discussion of new ideas and proposals.
Much appreciated. Thanks.
On Sat, Jan 7, 2017 at 3:49 PM, Eric Voskuil via bitcoin-dev <

@_date: 2017-06-01 14:00:02
@_author: Eric Lombrozo 
@_subject: [bitcoin-dev] BIP Proposal: Compact Client Side Filtering for 
Thanks for sending this proposal! I look forward to having a great
discussion around this.
- Eric
On Thursday, June 1, 2017, Olaoluwa Osuntokun via bitcoin-dev <
