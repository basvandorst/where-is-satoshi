
@_date: 2014-07-17 11:45:06
@_author: David A. Harding 
@_subject: [Bitcoin-development] Mining Hashrate Caps 
Hi, Mark.
We were asked on the bitcoin-documentation mailing list about a month
ago to work on something like this and we're getting close to a pull
request for the Bitcoin.org website.  There's a preview here:
    (Remember, it's a preview and still being actively written/edited.)
Discussion about that doc belongs on the bitcoin-documentation mailing
list. Here's the particular thread:

@_date: 2014-05-21 16:25:02
@_author: David A. Harding 
@_subject: [Bitcoin-development] PSA: Please sign your git commits 
For all of my projects, I now I put this script in
.git/hooks/post-commit and post-merge:
     -eu
    if ! git log -n1 --show-signature | grep -q 'gpg: Good signature'
    then
        yes "FORGOT TO SIGN COMMIT MESSAGE"
        exit 1
    fi
So anytime I forget to sign, I get an obvious error and can immediately
run git commit --amend -S.
To automatically add a script like the one above to all new projects (plus
quickly add it old current projects), you can follow these instructions:
    I find signing my commits quite useful even on projects without a
default signing policy because it lets me diff from the last time I
provably reviewed the code.  Here's my script for that:
     -eu
    KEY=F29EC4B7
    last_signed_commit=$( git log --topo-order --show-signature --pretty=oneline \
 grep -m1 " gpg: Signature made.*RSA key ID $KEY" \
 sed 's/ .*//' \
 grep .
    ) || { echo "No signed commit found.  Dying..." ; exit 1 ; }
    set -x
    git diff $last_signed_commit
By diffing against the last signed commit I made, I also review any
commits that were made using my name but which I didn't actually make,
such as squashes and rebases of my commits (and, of course, forgeries).
For anyone who's bored and wants to read a lot of text, I think the
definitive work on git signing is this:

@_date: 2014-05-26 17:01:28
@_author: David A. Harding 
@_subject: [Bitcoin-development] Announce: Bitcoin.org Developer Documentation 
Hi all,
The first version of the Bitcoin.org Developer Documentation is now
live. The main URL (below) provides a portal to two main documents, an
overview-level guide and a more detailed reference. The portal page also
links to individual sections of the documentation and noteworthy
off-site documentation:
    In printed pages, the current version is about 130 pages long in total.
Notable parts include:
* Technical, but still plain-English, descriptions of the block chain
  and transactions, including several illustrations:
        * A description of several different "contracts", including arbitration
  contracts, micropayment channels, and coinjoin:
    * Information about wallet formats, including what we hope is a good
  introduction to BIP32 HD wallets for people not already familiar with
  ECDSA specifics:
        * A considerable amount of payment processing detail, currently aimed
  primarily at developers writing payment-receiving applications for
  merchants. It includes a step-by-step description of the BIP70 Payment
  Protocol using an actual CGI script example.
        * A description of every RPC included in Bitcoin Core 0.9
  cross-referenced with the other sections of the documentation and
  augmented with actual examples of the command in use.
    * Not a section, but still notable are the over 2,000 cross-references
  in the text---there are so many that we hide them by default. Hover
  your mouse over a paragraph to see the cross references (in blue) and
  hover your mouse over the link to see more information.
Additions and improvements to the text are being worked on right
now, with an average of 200 lines of text being added each day.
We are, however, in need of expert reviewers.  Issues can be opened on
the main Bitcoin.org repository:
    (If you find an issue while reading the documentation, please click the
Report An Issue link on the bottom left side---this will automatically
add the URL and nearest HTML anchor to your bug report so we know where
to find what you're talking about.)
We also welcome suggestions about what to write next:
    All work has been done by volunteers---and we're always looking for more
contributors.  Please feel free to subscribe to our mailing list and say
    And, most importantly, thank you to everyone from the -dev mailing list
who has helped us produce this content over the last 80 days!
-Dave Harding on behalf of Sa?vann Carignan, Greg Sanders, and all the
 documentation contributors

@_date: 2015-08-27 07:03:30
@_author: David A. Harding 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
That sentence doesn't quite parse ("say it nLockTime"), so please
forgive me if I'm misunderstanding you. Are you saying that you want
IsStandard() to require a transaction have a locktime of 0 (no
confirmation delay) if any of its inputs use a non-final sequence?
If so, wouldn't that make locktime useless for delaying confirmation in
IsStandard() transactions because the consensus rules require at least
one input be non-final in order for locktime to have any effect?

@_date: 2015-02-17 10:50:12
@_author: David A. Harding 
@_subject: [Bitcoin-development] More precise type information in API 
Hi Dario,
I'm the primary author of the Bitcoin.org JSON-RPC reference, and I'd
be happy to help.
Do you think it would be possible for you to submit a minimal pull
request against the docs adding the type information you need to just
one RPC call?  From there we can see how much work it would take to
generalize that across all 100+ printed pages worth of RPC docs.
I've tried to make this as easy as possible: if you hover your mouse
over the title of an RPC[1], an "Edit" link will appear that will take you
to a page on GitHub to edit the file describing that RPC call.  Or you
can checkout[2] the website repository and edit the file locally; the
individual RPCs are in _includes/ref/bitcoin-core/rpcs/rpcs/
    [1] For example,     [2] If you have any questions about the editing process, or anything else,
please feel free to email me at this address or PM harding on Freenode.

@_date: 2015-07-04 11:42:32
@_author: David A. Harding 
@_subject: [bitcoin-dev] List of approved pools 
Please add known good/bad pools here:
    I will update the Bitcoin.org alert to point there.
We're especially in need of information about pools that are still SPV
mining so that we can make accurate guesses about how many additional
confirmations are needed while the situation persists.
Thank you,

@_date: 2015-06-30 10:02:13
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP: Full Replace-by-Fee deployment schedule 
Bundle it in with BIP62 version-2 (or whatever) transactions.
- As you desire for RBF, the BIP62 transactions are already specified to
  be opt-in. Nobody has to use them.
- Although BIP62 transactions only prevent third-party mutation, some
  people might wrongly assume that they prevent all mutation---including
  double spending.
  We need to make it clear that even with BIP62 transactions, signers
  can still mutate their own transactions---and what better way to do
  that than make BIP62 transactions easier to double spend?
The downside I see is possible further delay of full BIP62. Although, I
guess it could go the other way too by having developers who want RBF
help push BIP62 into production.

@_date: 2015-11-13 10:59:16
@_author: David A. Harding 
@_subject: [bitcoin-dev] Ads on bitcoin.org website 
Bitcoin.org hosts the Bitcoin Core binaries refered to in release
announcements, so this subject is conceivably on-topic for bitcoin-dev.
However, I think questions about how Bitcoin.org operates are best asked
on bitcoin-discuss[1] (or somewhere else) and issues about the
advertising are best addressed in the PRs you referenced.
So, as one of the Bitcoin.org maintainers, here are my suggestions:
1. I'm on bitcoin-discuss.  Please feel free to ask any questions there.
2. I just opened a GitHub issue for discussing advertising on Bitcoin.org[2]
    - I will reply to your specific questions there after I've sent this
      mail.
3. If advertisements are added to Bitcoin.org and there is general
   dissatisfaction about that, maybe then we can come back to
   bitcoin-dev to discuss moving the Bitcoin Core binaries
   somewhere else.
[1] [2]

@_date: 2016-01-22 23:10:42
@_author: David A. Harding 
@_subject: [bitcoin-dev] nSequence multiple uses 
Hi Andrew,
Opt-in RBF requires setting nSequence to less than MAX-1 (not merely
less than MAX), so an nSequence of exactly MAX-1 (which appears in
hex-encoded serialized transactions as feffffff) enables locktime
enforcement but doesn't opt in to RBF.
For more information, please see BIP125:

@_date: 2016-03-02 12:14:28
@_author: David A. Harding 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
Having a well-reviewed hard fork patch for rapid difficulty adjustment
would seem to be a useful reserve for all sorts of possible problems.
That said, couldn't this specific potential situation be dealt with by a
relatively simple soft fork?
Let's say that, starting soon, miners require that valid block header
hashes be X% below the target value indicated by nBits. The X% changes
with each block, starting at 0% and increasing to 50% just before block
420,000 (the halving). This means the before the halving, every two
hashes are being treated as one hash, on average.
For blocks 420,000 and higher the code is disabled, immediately doubling
the effective hash rate at the same time the subsidy is halved,
potentially roughly canceling each other out to make a pre-halving hash
equal in economic value to a post-halving hash.
Of course, some (perhaps many) miners will not be profitable at the
post-halving subsidy level, so the steady increase in X% will force them
off the network at some point before the halving, hopefully in small
numbers rather than all at once like the halving would be expected to do.
For example, if the soft fork begins enforcement at block 410,000, then
X% can be increased 0.01% per block. Alice is a miner whose costs are
24BTC per block and she never claims tx fees for some reason, so her
profits now are always 25BTC per block. During the first difficulty
period after the soft fork is deployed, the cost to produce a hash will
increase like this,
    0: 0%           500: 5%         1000: 10%       1500: 15%       2000: 20%
    100: 1%         600: 6%         1100: 11%       1600: 16%
    200: 2%         700: 7%         1200: 12%       1700: 17%
    300: 3%         800: 8%         1300: 13%       1800: 18%
    400: 4%         900: 9%         1400: 14%       1900: 19%
Somewhere around block 417, Alice will need to drop out because her
costs are now above 25BTC per block.  With the loss of her hash rate,
the average interblock time will increase and the capacity will decrease
(all other things being equal). However, Bob whose costs are 20BTC per
block can keep mining through the period.
At the retarget, the difficulty will go down (the target goes up) to
account for the loss of Alice's hashes. It may even go down enough
that Alice can mine profitably for a few more blocks early in the new
period, but the increasing X% factor will make her uneconomical again,
and this time it might even make Bob uneconomical too near the end of
the period. However, Charlie whose costs are 12BTC per block will
never be uneconomical as he can continue mining profitably even after
the halving. Alice and Bob mining less will increase the percentage of
blocks Charlie produces before the retarget, steadily shifting the
dynamics of the mining network to the state expected after the halving
and hopefully minimizing the magnitude of any shocks.
This does create the question about whether this soft fork would be
ethical, as Alice and Bob may have invested money and time on the
assumption that their marginal hardware would be usable up until the
halving and with this soft fork they would become uneconomical earlier
than block 420,000. A counterargument here is such an investment was
always speculative given the vagaries of exchange rate fluctuation, so
it could be permissible to change the economics slightly in order to
help ensure all other Bitcoin users experience minimal disruption during
the halving.
Unless I'm missing something (likely) I think this proposal has the
advantage of fast rollout (if the mechanism of an adjusted target is as
simple as I think it could be) in a non-emergency manner without a hard
fork that would require all full nodes upgrade (plus maybe some SPV
software that check nBits, which they probably all should be doing
given it's in the block headers that they download anyway).
P.S. I see Tier Nolan proposed something similar while I was writing
     this. I think this proposal differs in its analysis to warrant a
     possible duplicate posting.

@_date: 2016-03-02 14:34:33
@_author: David A. Harding 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
To avoid duplication of looking up this statistic among readers, here
are the various recent difficulties:
    $ for i in $( seq 0 2016 60000 ) ; do echo -n $i blocks ago:' ' ; bitcoin-cli getblock $( bitcoin-cli getblockhash $(( 400857 - i )) ) | jshon -e difficulty ; done | column -t
    0      blocks  ago:  163491654908.95929
    2016   blocks  ago:  144116447847.34869
    4032   blocks  ago:  120033340651.237
    6048   blocks  ago:  113354299801.4711
    8064   blocks  ago:  103880340815.4559
    10080  blocks  ago:  93448670796.323807
    12096  blocks  ago:  79102380900.225983
    14112  blocks  ago:  72722780642.54718
    16128  blocks  ago:  65848255179.702606
    18144  blocks  ago:  62253982449.760818
    20160  blocks  ago:  60883825480.098282
    22176  blocks  ago:  60813224039.440353
    24192  blocks  ago:  59335351233.86657
    26208  blocks  ago:  56957648455.01001
    28224  blocks  ago:  54256630327.889961
    30240  blocks  ago:  52699842409.347008
    32256  blocks  ago:  52278304845.591682
    34272  blocks  ago:  51076366303.481934
    36288  blocks  ago:  49402014931.227463
    38304  blocks  ago:  49692386354.893837
    40320  blocks  ago:  47589591153.625008
    42336  blocks  ago:  48807487244.681381
    44352  blocks  ago:  47643398017.803436
    46368  blocks  ago:  47610564513.47126
    48384  blocks  ago:  49446390688.24144
    50400  blocks  ago:  46717549644.706421
    52416  blocks  ago:  47427554950.6483
    54432  blocks  ago:  46684376316.860291
    56448  blocks  ago:  44455415962.343803
    58464  blocks  ago:  41272873894.697021
<50% of current hash rate was last seen roughly six retarget periods (12
weeks) ago and <25% of current hash rate was last seen roughly 29 periods
(58 weeks) ago.
I think that's reasonably strong evidence for your thesis given that
the increases in hash rate from the introduction of new efficient
equipment are likely partly offset by the removal from the hash rate of
lower efficiency equipment, so the one-year tail of ~25% probably means
that less than 25% of operating equipment is one year old or older.
However, it is my understanding that most mining equipment can be run at
different hash rates. Is there any evidence that high-efficiency miners
today are using high clock speeds to produce more hashes per ASIC than
they will after halving?  Is there any way to guess at how many fewer
hashes they might produce?
Maybe I'm not thinking this through thoroughly, but I don't think it's
possible to significantly advance inflation unless the effective hash
rate increases by more than 300% at the halving.  With the proposal
being replied to, if all mining equipment operation before the
halving continued operating after it, the effective increase would be
200%. That doubling in effective hash rate would've been offset in
advance through a reduction in the effective hash rate in the weeks
before the halving.
Exacerbated unintentional selfish mining is a much more significant
concern IMO, even if it's only for a short retarget period or two. This
is especially the case given the current high levels of centralization
and validationless mining on the network today, which we would not want
to reward by making those miners the only ones effectively capable of
creating blocks until difficulty adjusted. I had not thought of this
aspect; thank you for bringing it up.
Yes, I very much don't like that aspect, which is why I made sure to
mention it.
I think having that code ready in general is a good idea, and a one-time
change in nBits is sounds like a good and simple way to go about it.
Thank you for your insightful reply,

@_date: 2016-03-18 18:52:55
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP 2 promotion to Final 
Arguing about which wiki is better doesn't feel productive to me. Can we
just let BIP authors decide for themselves? Draft-BIP2 already has a
provision for allowing authors to specify a backup wiki of their own
choosing; can we just make that the policy in all cases (and drop the
need for a backup wiki)?

@_date: 2017-12-13 16:36:07
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP Proposal: Utilization of bits denomination 
Wallets and other software is already using this term, so I think it's a
good idea to ensure its usage is normalized.
That said, I think the term is unnecessary and confusing given that
microbitcoins provides all of the same advantages and at least two
additional advantages:
- Microbitcoins is not a homonym for any other word in English (and
  probably not in any other language), whereas "bit" and "bits" have
  more than a dozen homonyms in English---some of which are quite common
  in general currency usage, Bitcoin currency usage, or Bitcoin
  technical usage.
- Microbitcoins trains users to understand SI prefixes, allowing them to
  easily migrate from one prefix to the next.  This will be important
  when bitcoin prices rise to $10M USD[1] and the bits denomination has
  the same problems the millibitcoin denomination has now, but it's also
  useful in the short term when interacting with users who make very
  large payments (bitcoin-scale) or very small payments
  (nanobitcoin-scale).[2]  Maybe a table of scale can emphasize this
  point:
      Wrong (IMO):        Right (IMO):
      ---------------     --------------
      BTC                 BTC
      mBTC                mBTC
      bits                ?BTC
      nBTC                nBTC
[1] A rise in price to $10M doesn't require huge levels of growth---it
only requires time under the assumption that a percentage of bitcoins will
be lost every year due to wallet mishaps, failure to inherit bitcoins,
and other issues that remove bitcoins from circulation.  In other words,
it's important to remember that Bitcoin is expected to become a
deflationary currency and plan accordingly.
[2] Although Bitcoin does not currently support committed
nanobitcoin-scale payments in the block chain, it can be supported in a
variety of ways by offchain systems---including (it is hypothesized)
trustless systems based on probabilistic payments.

@_date: 2018-12-09 17:41:57
@_author: David A. Harding 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
To what degree is this an actual problem?  If the mutated transaction
pays a feerate at least incremental-relay-fee[1] below the original
transaction, then the original transaction can be rebroadcast as an RBF
replacement of the mutated transaction (unless the mutated version has
been pinned[2]).
[1] $ bitcoind -help-debug | grep -A2 incremental
  -incrementalrelayfee=
       Fee rate (in BTC/kB) used to define cost of relay, used for mempool
       limiting and BIP 125 replacement. (default: 0.00001)
[2]

@_date: 2018-12-11 12:47:24
@_author: David A. Harding 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Indeed, you are correct (BIP125 rule 4[1]).
Thanks for the correction,
[1] For the curious, the relevant code from master's validation.cpp:
    // Finally in addition to paying more fees than the conflicts the
    // new transaction must pay for its own bandwidth.
    CAmount nDeltaFees = nModifiedFees - nConflictingFees;
    if (nDeltaFees < ::incrementalRelayFee.GetFee(nSize))
    {
        return state.DoS(0, false,
                REJECT_INSUFFICIENTFEE, "insufficient fee", false,
                strprintf("rejecting replacement %s, not enough additional fees to relay; %s < %s",
                      hash.ToString(),
                      FormatMoney(nDeltaFees),
                      FormatMoney(::incrementalRelayFee.GetFee(nSize))));
    }

@_date: 2018-01-28 12:29:48
@_author: David A. Harding 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
Imagine a miner is only concerned with creating the next block and his
mempool currently only has 750,000 vbytes in it.  If two 250-vbyte
transactions each paying a feerate of 100 nanobitcoins per vbyte (50k
total) are replaced with one 325-vbyte transaction paying a feerate of
120 nBTC (39k total), the miner's potential income from mining the next
block is reduced by 11k nBTC.
Moving away from this easily worked example, the problem can still exist
even if a miner has enough transactions to fill the next block.  For
replacement consideration only by increased feerate to be guaranteed
more profitable, one has to assume the mempool contains an effectively
continuous distribution of feerates.  That may one day be true of the
mempool (it would be good, because it helps keep block production
regular sans subsidy) but it's often not the case these days.

@_date: 2018-06-02 08:41:57
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I'm confused about why this would be the case.  If Alice's node
generates filters accurately and Mallory's node generates filters
inaccurately, and they both send their filters to Bob, won't Bob be able
to download any blocks either filter indicates are relevant to his
If Bob downloads a block that contains one of his transactions based on
Alice's filter indicating a possible match at a time when Mallory's
filter said there was no match, then this false negative is perfect
evidence of deceit on Mallory's part[1] and Bob can ban her.
If Bob downloads a block that doesn't contain any of his transactions
based on Mallory's filter indicating a match at a time when Alice's
filter said there was no match, then this false positive can be recorded
and Bob can eventually ban Mallory should the false positive rate
exceeds some threshold.
Until Mallory is eventually banned, it seems to me that the worst she
can do is waste Bob's bandwidth and that of any nodes serving him
accurate information, such as Alice's filters and the blocks Bob
is misled into downloading to check for matches.  The amount of
attacker:defender asymetry in the bandwidth wasted increases if
Mallory's filters become less accurate, but this also increases her
false positive rate and reduces the number of filters that need to be
seen before Bob bans her, so it seems to me (possibly naively) that this
is not a significant DoS vector.
[1] Per BIP158 saying, "a Golomb-coded set (GCS), which matches all
items in the set with probability 1"

@_date: 2018-06-09 06:34:45
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I'm not sure why commitment proof size is a significant issue.  Doesn't
the current BIP157 protocol have each filter commit to the filter for
the previous block?  If that's the case, shouldn't validating the
commitment at the tip of the chain (or buried back whatever number of
blocks that the SPV client trusts) obliviate the need to validate the
commitments for any preceeding blocks in the SPV trust model?
I think those are unrelated points.  The gain from using a more
efficient filter is saved bytes.  The gain from using block commitments
is SPV-level security---that attacks have a definite cost in terms of
generating proof of work instead of the variable cost of network
compromise (which is effectively free in many situations).
Comparing the extra bytes used by block commitments to the reduced bytes
saved by prevout+output filters is like comparing the extra bytes used
to download all blocks for full validation to the reduced bytes saved by
only checking headers and merkle inclusion proofs in simplified
validation.  Yes, one uses more bytes than the other, but they're
completely different security models and so there's no normative way for
one to "outweigh the gains" from the other.
It seems like you're claiming better security here without providing any
evidence for it.  The security model is "at least one of my peers is
honest."  In the case of outpoint+output filters, when a client receives
advertisements for different filters from different peers, it:
    1. Downloads the corresponding block
    2. Locally generates the filter for that block
    3. Kicks any peers that advertised a different filter than what it
       generated locally
This ensures that as long as the client has at least one honest peer, it
will see every transaction affecting its wallet.  In the case of
prevout+output filters, when a client receives advertisements for
different filters from different peers, it:
    1. Downloads the corresponding block and checks it for wallet
       transactions as if there had been a filter match
This also ensures that as long as the client has at least one honest
peer, it will see every transaction affecting its wallet.  This is
equivilant security.
In the second case, it's possible for the client to eventually
probabalistically determine which peer(s) are dishonest and kick them.
The most space efficient of these protocols may disclose some bits of
evidence for what output scripts the client is looking for, but a
slightly less space-efficient protocol simply uses randomly-selected
outputs saved from previous blocks to make the probabalistic
determination (rather than the client's own outputs) and so I think
should be quite private.  Neither protocol seems significantly more
complicated than keeping an associative array recording the number of
false positive matches for each peer's filters.

@_date: 2019-07-27 09:34:17
@_author: David A. Harding 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Timelocking bitcoins, especially for long periods, carries some special
risks in Bitcoin:
1. Inability to sell fork coins, also creating an inability to influence
the price signals that help determine the outcome of chainsplits.
2. Possible inability to transition to new security mechanisms if
a major weakness is discovered in ECC or a hash function.
An alternative to timelocks might be coin age---the value of a UTXO
multiplied by the time since that UTXO was confirmed.  Coin age may be
even harder for an attacker to acquire given that it is a measure of
past patience rather than future sacrifice.  It also doesn't require
using any particular script and so is flexible no matter what policy the
coin owner wants to use (especially if proof-of-funds signatures are
generated using something like BIP322).
Any full node (archival or pruned) can verify coin age using the UTXO
set.[1]  Unlike script-based timelock (CLTV or CSV), there is no current
SPV-level secure way to prove to lite clients that an output is still
unspent, however such verification may be possible within each lite
client's own security model related to transaction withholding attacks:
- Electrum-style clients can poll their server to see if a particular
  UTXO is unspent.
- BIP158 users who have saved their past filters to disk can use them to
  determine which blocks subsequent to the one including the UTXO may
  contain a spend from it.  However, since a UTXO can be spent in the
  same block, they'd always need to download the block containing the
  UTXO (alternatively, the script could contain a 1-block CSV delay
  ensuring any spend occurred in a later block).  If BIP158 filters
  become committed at some point, this mechanism is upgraded to SPV-level
  security.
This is the thing I most like about the proposal.  I suspect most
honest makers are likely to have only a small portion of their funds
under JoinMarket control, with the rest sitting idle in a cold wallet.
Giving makers a way to communicate that they fit that user template
would indeed seem to provide significant sybil resistance.
[1] See, bitcoin-cli help gettxout

@_date: 2019-07-31 07:59:40
@_author: David A. Harding 
@_subject: [bitcoin-dev] Improving JoinMarket's resistance to sybil 
Probably.  A stronger form of my argument would apply to single-wallet
(or wallet library) problems of the type we see with depressing
regularity, such as reused nonces, weak nonces, brainwallets, and weak
HD seeds.  In some cases, this leads directly to theft and loss---but in
others, the problem is detected by a friendly party and funds can be
moved to a secure address before the problem is publicly disclosed and
attackers try to exploit it themselves.
If funds are timelocked, there's a greater chance that the issue will
become publicly known and easily exploitable while the funds are
inaccessible.  Then, at the time the lock expires, it'll become a race
between attackers and the coin owner to see who can get a spending
transaction confirmed first.
Good point.  There's also the case that some Electrum-style indexers
don't index more than a certain number of outputs sent to the same
address.  E.g., I believe Electrs[1] stops indexing by default after 100
outputs to the same address.
[1] I don't think that works.  If Bob sends 100 BTC to bc1foo and then uses
that UTXO as his fidelity bond, Mallory can subsequently send some dust
to bc1foo to invalidate Bob's bond.
To use compact block filters in a way that prevents spamming, I think
we'd need a different filter type that allowed you to filter by

@_date: 2019-06-09 10:07:36
@_author: David A. Harding 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
Yes, the current incremental relay fee in Bitcoin Core is 0.00001000
Here's a scenario that I think shows it being at least 20x worse.
Let's imagine that you create two transactions:
  tx0: A very small transaction (~100 vbytes) that's just 1-in, 1-out.
       At the minimum relay fee, this costs 0.00000100 BTC
  tx1: A child of that transaction that's very large (~100,000 vbytes,
       or almost 400,000 bytes using special scripts that allow witness
       stuffing).  At the minimum relay fee, this costs 0.00100000 BTC
Under the current rules, if an attacker wants to fee-bump tx0 by the
minimum incremental fee (a trivial amount, ~0.00000100 BTC), the
attacker's replacement also needs to pay for the eviction of the huge
child tx1 by that same incremental fee (~0.00100000).
Thus the replacement would cost the attacker a minimum of about
0.00100100 (~1 mBTC) for the original transactions and 0.00100100 for
the replacement (about 2 mBTC total).
The attacker could then spend another 1 mBTC re-attaching the child and
yet another 1 mBTC bumping again, incuring about a 2 mBTC cost per
replacement round.  At writing, 2 mBTC is about $14.00 USD---an amount
that's probably enough to deter most attacks at scale.
* * *
Under the new proposed rule 6, Mallory's replacement cost would be the
amount to get the small tx0 to near the top of the mempool (say
0.00100000 BTC/KvB, so 0.00010000 BTC total).  Because this would evict
the expensive child, it would actually reduce the original amount paid
by the attacker by 90% compared to the previous section's example where
using RBF increased the original costs by 100%.
The 0.1 mBTC cost of this attack is about $0.70 USD today for the
roughly the same data relay use as one round of the currently possible
attack.  In short, if I haven't misplaced a decimal point or made some
other mistake, I think the proposed rule 6 would result in approximately
a 95% reduction in the cost paid by an attacker for wasting 400,000
bytes of bandwidth per node (x60,000 nodes = 24 GB across all nodes, not
counting INV overhead).
Although the attacker might only get one replacement per block per
transaction pair out of this version of the attack, they could execute
the attack many times in parallel using different tranaction pairs.  If
this is combined with the treadmill leapfrogging Russell O'Connor
described elsewhere in this thread, the attack could possibly be
repeated multiple times per block per transaction pair at only slightly
increased cost (to pay the increasing next-block transaction fees).
Although the BIP125 limit is 100, Bitcoin Core's current default is 25.[1]
(When RBF was implemented in Bitcoin Core, transaction ancestry was only
tracked for purposes of ensuring valid transaction ordering within
blocks; later when CPFP was implemented, ancestry was additionally used
to calculate each transaction's package fee---the value of it and all
its unconfirmed ancestors.  This requires more computation to update
the mempool metadata when the ancestry graph changes.)
Again, I'd be thinking here of something similar to O'Connor's
treadmilling attack where replacements can push each other out of the
top mempool and so create enough churn for a CPU exhaustion attack.
It's already hard for wallet software to determine whether or not its
transactions have successfully been relayed.  This proposal requires LN
wallets not only be able to guess where the next-block feerate boundary
is in other nodes' individual mempools (now and in the future for the
time it takes the transaction to propagate to ~100% of miners), but it
possibly requires that under the condition that the LN wallet can't
guess too low because it might not get another chance for relay in the
limited time available before contract expiration.
On top of that, there's O'Connor's suggestion to increase treadmilling
costs by only allowing bumps if they're in the top-half of the
next-block mempool.
Considered that way, I worry that these constraints produce a recipe for
paying extremely high feerates.  If that's an actual risk, is that
actually significantly better than dealing with the existing transaction
pinning issue where one needs to pay a high total fee in order to evict
a bunch of junk descendents?  Paying lots of fees may not be the optimal
solution to the problem of having to pay lots of fees.  :-)
[1] Excerpt from bitcoind -help-debug :
  -limitancestorcount=
       Do not accept transactions if number of in-mempool ancestors is  or
       more (default: 25)
  -limitdescendantcount=
       Do not accept transactions if any ancestor would have  or more
       in-mempool descendants (default: 25)

@_date: 2019-06-29 14:21:03
@_author: David A. Harding 
@_subject: [bitcoin-dev] Generalized covenants with taproot enable 
prevent credit inflation through fractional reserve
I believe these goals are obtainable today without any consensus
changes.  Bob can provably timelock bitcoins using CLTV or CSV in a
script that commits to the outpoint (txid, vout) of an output that will
be used as a colored coin to track the debt instrument.  The colored
coin, which has no appreciable onchain value itself, can then be
trustlessly traded, e.g. from Alice to Charlie to Dan as you describe.  Anyone with a copy of the script Bob paid, the confirmed transaction he
included it in, and the confirmed transaction history of the colored
coin can trustlessly verify the ownership record---including that no
inflation or fractional reserve occurred.
I believe the RGB working group has set for itself the goal[1] of making
trustless colored coin protocols more efficient when performed on top of
Bitcoin.  I'd also suggest reading about Peter Todd's concept of
single-use seals[2].  You may want to investigate these ideas and see
whether they can be integrated with your own.
[1] [2]

@_date: 2019-03-10 13:01:34
@_author: David A. Harding 
@_subject: [bitcoin-dev] Signet 
Sure, but anyone could also just connect their lite client to a trusted
node (or nodes) on signet.  The nodes would protect the clients from
missing/invalid-signature DoS and the clients wouldn't have to implement
any more network-level changes than they need to now for testnet.
For people who don't want to run their own trusted signet nodes, there
could be a list of signet nodes run by well-known Bitcoiners (and this
could even be made available via a simple static dns seeder lite clients
could use).
This post from Maxwell could be the idea Corallo is describing:
    I read it as:
  - Trusted signer Alice only signs extensions of her previous blocks
  - Trusted signer Bob periodically extends one of Alice's blocks
    (either the tip or an earlier block) with a chain that grows faster
    than Alice's chain, becoming the most-PoW chain.  At some point he
    stops and Alice's chain overtakes Bob's fork as the most-PoW chain
  - User0 who wants to ignore reorg problems starts his node with
    -signet -signers="alice", causing his node to only accept blocks
    from Alice.
  - User1 who wants to consider reorg problems starts his node with
    -signet -signers="alice,bob", causing his node to accept blocks from
    both Alice and Bob, thus experiencing periodic reorgs.
  - There can also be other signing keys for any sort of attack
    that can be practically executed, allowing clients to test their
    response to the attack when they want to but also ignore any
    disruption it would otherwise cause the rest of the time.
  - As an alternative to particular signing keys, there could just be
    flags put in the header versionbits, header nonce, or generation
    transaction indicating how the block should be classified (e.g.
    no_reorg, reorg_max6, reorg_max144, merkle_vulnerability, special0,
    special1, etc...)
(If something like this is implemented, I propose reserving one of the
signing keys/classification flags for use by any of Bitcoin's more
devious devs in unannounced attacks.  Having to occasionally dig
through weird log messages and odd blocks with other Bitcoin dorks on
IRC in order to figure out why things went horribly sideways in our
signet clients sounds to me like an enjoyable experience.  :-)

@_date: 2019-03-24 09:29:10
@_author: David A. Harding 
@_subject: [bitcoin-dev] New BIP - v2 peer-to-peer message transport 
Why is this optional and only specified here for some message types
rather than being required by v2 and specified for all message types?
There's only 26 different types at present[1], so it seems better to
simply make this a one-byte fixed-length field than it is to deal with
variable size, mapping negotiation, per-peer mapping in general, and
(once the network is fully v2) the dual-logic of being able to process
messages either from a short ID or a full command name.
[1] src/protocol.cpp:
const static std::string allNetMessageTypes[] = {
    NetMsgType::VERSION,
    NetMsgType::VERACK,
    NetMsgType::ADDR,
    NetMsgType::INV,
    NetMsgType::GETDATA,
    NetMsgType::MERKLEBLOCK,
    NetMsgType::GETBLOCKS,
    NetMsgType::GETHEADERS,
    NetMsgType::TX,
    NetMsgType::HEADERS,
    NetMsgType::BLOCK,
    NetMsgType::GETADDR,
    NetMsgType::MEMPOOL,
    NetMsgType::PING,
    NetMsgType::PONG,
    NetMsgType::NOTFOUND,
    NetMsgType::FILTERLOAD,
    NetMsgType::FILTERADD,
    NetMsgType::FILTERCLEAR,
    NetMsgType::REJECT,
    NetMsgType::SENDHEADERS,
    NetMsgType::FEEFILTER,
    NetMsgType::SENDCMPCT,
    NetMsgType::CMPCTBLOCK,
    NetMsgType::GETBLOCKTXN,
    NetMsgType::BLOCKTXN,

@_date: 2019-03-24 11:38:56
@_author: David A. Harding 
@_subject: [bitcoin-dev] New BIP - v2 peer-to-peer message transport 
Gregory Maxwell discussed this with me on IRC[1].  My summary of our
Although the BIP can easily allocate short-ids to all existing messages,
anyone who wants to add an additional protocol message later will need
to coordinate their number allocation with all other developers working
on protocol extensions.  This includes experimental and private
extensions.  At best this would be annoying, and at worst it'd be
another set of bikeshed problems we'd waste time arguing about.
Allowing nodes to continue using arbitrary command names eliminates this
coordination problem.   Yet we can also gain the advantage of saving
bandwidth by allowing mapping (with optional negotiation) of short-ids.
Now that I understand the motivation, this part of the proposal makes
sense to me.
[1]

@_date: 2019-11-07 16:15:41
@_author: David A. Harding 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Either a consensus rule or a standardness rule[1] would require anyone
using a bech32 library supporting v1+ segwit to upgrade their library.
Otherwise, users of old libraries will still attempt to pay v1 witness
outputs of length 31 or 33, causing their transactions to get rejected
by newer nodes or get stuck on older nodes.  This is basically the
problem  was meant to prevent.
If we're going to need everyone to upgrade their bech32 libraries
anyway, I think it's probably best that the problem is fixed in the
bech32 algorithm rather than at the consensus/standardness layer.
[1] [2] P.S. My thanks as well to the people who asked the question during
     review that lead to this discussion:

@_date: 2019-10-03 22:20:31
@_author: David A. Harding 
@_subject: [bitcoin-dev] Chain width expansion 
Hi Braydon,
Thank you for researching this important issue.  An alternative solution
proposed some time ago (I believe originally by Gregory Maxwell) was a
soft fork to raise the minimum difficulty.  You can find discussion of
it in various old IRC conversations[1,2] as well as in related changes
to Bitcoin Core such as PR  addining minimum chain work[3] and the
assumed-valid change added in Bitcoin Core 0.14.0[4].
[1] [2] [3] [4] The solutions proposed in section 4.2 and 4.3 of your paper have the
advantage of not requiring any consensus changes.  However, I find it
hard to analyze the full consequences of the throttling solution in
4.3 and the pruning solution in 4.2.  If we assume a node is on the
most-PoW valid chain and that a huge fork is unlikely, it seems fine.
But I worry that the mechanisms could also be used to keep a node that
synced to a long-but-lower-PoW chain on that false chain (or other false
chain) indefinitely even if it had connections to honest peers that
tried to tell it about the most-PoW chain.
For example, with your maximum throttle of 5 seconds between
`getheaders` requests and the `headers` P2P message maximum of 2,000
headers per instance, it would take about half an hour to get a full
chain worth of headers.  If a peer was disconnected before sending
enough headers to establish they were on the most-PoW chain, your
pruning solution would delete whatever progress was made, forcing the
next peer to start from genesis and taking them at least half an hour
too.  On frequently-suspended laptops or poor connections, it's possible
a node could be be operational for a long time before it kept the same
connection open for half an hour.  All that time, it would be on a
dishonest chain.
By comparison, I find it easy to analyze the effect of raising the
minimum difficulty.  It is a change to the consensus rules, so it's
something we should be careful about, but it's the kind of
basically-one-line change that I expect should be easy for a large
number of people to review directly.  Assuming the choice of a new
minimum (and what point in the chain to use it) is sane, I think it
would be easy to get acceptance, and I think it would further be easy
increase it again every five years or so as overall hashrate increases.

@_date: 2019-10-18 12:45:35
@_author: David A. Harding 
@_subject: [bitcoin-dev] Removal of reject network messages from Bitcoin 
I don't think a new BIP or a version number increment is necessary.
1. "Should support" isn't the same as "must support".  See
    ; by that reading,
   implementations with protocol versions above 70,002 are not required
   to support the reject message.
2. If you don't implement a BIP, as Bitcoin Core explicitly doesn't any
   more for BIP61[1], you're not bound by its conditions.
[1]   "BIP61
[...] Support was removed in v0.20.0"

@_date: 2019-10-20 14:06:08
@_author: David A. Harding 
@_subject: [bitcoin-dev] Draft BIP for SNICKER 
That'd be awesome!
Your logic seems correct for the watching half of the wallet, but I
think it's ok to consider requiring interaction with the cold wallet.
Let's look at the recovery procedure from the SNICKER documentation
that you kindly cited:
    1. Derive all regular addresses normally (doable watch-only for
    wallets using public BIP32 derivation)
    2. Find all transactions spending an output for each of those
    addresses.  Determine whether the spend looks like a SNICKER
    coinjoin (e.g. "two equal-[value] outputs").  (doable watch-only)
    3. "For each of those transactions, check, for each of the two equal
    sized outputs, whether one destination address can be regenerated
    from by taking c found in the method described above" (not doable
    watch only; requires private keys)
I'd expect the set of candidate transactions produced in step  to be
pretty small and probably with no false positives for users not
participating in SNICKER coinjoins or doing lots of payment batching.
That means, if any SNICKER candidates were found by a watch-only wallet,
they could be compactly bundled up and the user could be encouraged to
copy them to the corresponding cold wallet using the same means used for
PSBTs (e.g. USB drive, QR codes, etc).  You wouldn't even need the whole
transactions, just the BIP32 index of the user's key, the pubkey of the
suspected proposer, and a checksum of the resultant address.
The cold wallet could then perform step  using its private keys and
return a file/QRcode/whatever to the hot wallet telling it any shared
secrets it found.
This process may need to be repeated several times if an output created
by one SNICKER round is spent in a subsequent SNICKER round.  This can be
addressed by simply refusing to participate in chains of SNICKER
transactions or by refusing to participant in chains of SNICKERs more
than n long (requring a maximum n rounds of recovery).  It could also be
addressed by the watching-only wallet looking ahead at the block chain a
bit in order to grab SNICKER-like child and grandchild transactions of
our SNICKER candidates and sending them also to the cold wallet for
attempted shared secret recovery.
The SNICKER recovery process is, of course, only required for wallet
recovery and not normal wallet use, so I don't think a small amount of
round-trip communication between the hot wallet and the cold wallet is
too much to ask---especially since anyone using SNICKER with a
watching-only wallet must be regularly interacting with their cold
wallet anyway to sign the coinjoins.

@_date: 2019-10-27 12:54:02
@_author: David A. Harding 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I'm not sure any of the other replies to this thread addressed your
request for a reason behind the limits related to your proposal, so I
thought I'd point out that---subsequent to your posting here---a
document[1] was added to the Bitcoin Core developer wiki that I think
describes the risk of the approach you proposed:
The document goes on to describe at a high level how Bitcoin Core
attempts to mitigate this problem as well as other ways it tries to
optimize the mempool in order to maximize miner profit (and so ensure
that miners continue to use public transaction relay).
I hope that's helpful to you and to others in both understanding the
current state and in thinking about ways in which it might be improved.
[1]     Content adapted from slides by Suhas Daftuar, uploaded and formatted
    by Gregory Sanders and Marco Falke.

@_date: 2019-10-28 07:14:38
@_author: David A. Harding 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
A parent transaction near the limit of 100,000 vbytes could have almost
10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children
were limited to 10,000 vbytes each (the current max carve-out size),
that allows relaying 100 mega-vbytes or nearly 400 MB data size (larger
than the default maximum mempool size in Bitcoin Core).
As Matt noted in discussion on  about this issue, it's
possible to increase second-child carve-out to nth-child carve-out but
we'd need to be careful about choosing an appropriately low value for n.
For example, BOLT2 limits the number of HTLCs to 483 on each side of the
channel (so 966 + 2 outputs total), which means the worst case free
relay to support the current LN protocol would be approximately:
    (100000 + 968 * 10000) * 4 = ~39 MB
Even if the mempool was empty (as it sometimes is these days), it would
only cost an attacker about 1.5 BTC to fill it at the default minimum
relay feerate[1] so that they could execute this attack at the minimal
cost per iteration of paying for a few hundred or a few thousand vbytes
at slightly higher than the current mempool minimum fee.
Instead, with the existing rules (including second-child carve-out),
they'd have to iterate (39 MB / 400 kB = ~100) times more often to
achieve an equivalent waste of bandwidth, costing them proportionally
more in fees.
So, I think these rough numbers clearly back what Matt said about us
being able to raise the limits a bit if we need to, but that we have to
be careful not to raise them so far that attackers can make it
significantly more bandwidth expensive for people to run relaying full
[1] Several developers are working on lowering the default minimum in
Bitcoin Core, which would of course make this attack proportionally

@_date: 2019-09-16 06:48:21
@_author: David A. Harding 
@_subject: [bitcoin-dev] PoW fraud proofs without a soft fork 
This is a nifty idea.
I think "~1 MB" is probably a reasonable estimate for the average case
but not for the worst case.  To allow verification of the spends in
block N+1, each UTXO entry must contain its entire scriptPubKey.  I
believe the current consensus rules allow scriptPubKeys to be up to
10,000 bytes in size.  A specially-constructed block can contain a bit
more than 20,000 inputs, making the worst case size of just the UTXO
entries that needs to be communicated over 200 MB.
I think this also expands to a worst-case of over 200 MB.  A lying peer
will only be able to get you on one of these checks, so it's 200 MB per
lying peer.  For an honest peer communicating valid blocks, the worst
case is that they'll need to communicate both of these state
transactions, so over 400 MB.  That could be a bandwidth-wasting DoS
attack on honest listening nodes if there were a large number of SPV
clients using this type of fraud proofs.
Additionally, each node capable of providing fraud proofs will need to
persistently store the state transition proof for each new block.  I
assume this is equal to the block undo data currently stored by archival
full nodes plus the utreexo partial merkle branches.
This data would probably not be stored by pruned nodes, at least not
beyond their prune depth, even for pruned nodes that use utreexo.  That
would mean this system will only work with archival full nodes with an
extra "index" containing the utreexo partial merkle branches, or it will
require querying utreexo bridge nodes.
Given that both of those would require significant additional system
resources beyond the minimum required to operate a full node, such nodes
might be rare and so make it relatively easy to eclipse attack an SPV
client depending on these proofs.
Finally, this system depends on SPV clients implementing all the same
consensus checks that full nodes can currently perform.  Given that most
SPV clients I'm aware of today don't even perform the full range of
checks it's possible to run on block headers, I have serious doubts that
many (or any) SPV clients will actually implement full verification.  On
top of that, each client must implement those checks perfectly or they
could be tricked into a chainsplit the same as a full node that follows
different rules than the economic consensus.
One thing I didn't like in your original proposal---which I appologize
for keeping to myself---is that the SPV client will accept confirmations
on the bad chain until a fork is produced.  Even a miner with a minority
of the hash rate will sometimes be able to produce a 6-block chain before
the remaining miners produce a single block.  In that case, SPV clients
with a single dishonest peer in collusion with the miner will accept any
transctions in the first block of that chain as having six
confirmations.  That's the same as it is today, but today SPV users
don't think fraud proofs help keep them secure.
I think that, if we wanted to widely deploy fraud proofs depending on
forks as a signal, we'd have to also retrain SPV users to wait for much
higher confirmation counts before accepting transactions as reasonably

@_date: 2020-04-22 07:51:30
@_author: David A. Harding 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
I think it's important to remember than mempool behavior affects not
just miners but also relay nodes.  Miner costs, such as bandwidth usage,
can be directly offset by their earned block rewards, so miners can be
much more tolerant of wasted bandwidth than relay nodes who receive no
direct financial compensation for the processing and relay of
unconfirmed transactions.[1]
To avoid the excessive wasting of bandwidth.  Bitcoin Core's defaults
require each replacement pay a feerate of 10 nBTC/vbyte over an existing
transaction or package, and the defaults also allow transactions or
packages up to 100,000 vbytes in size (~400,000 bytes).  So, without
enforcement of BIP125 rule 3, an attacker starting at the minimum
default relay fee also of 10 nBTC/vbyte could do the following:
- Create a ~400,000 bytes tx with feerate of 10 nBTC/vbyte (1 mBTC total
  fee)
- Replace that transaction with 400,000 new bytes at a feerate of 20
  nBTC/vbyte (2 mBTC total fee)
- Perform 998 additional replacements, each increasing the feerate by 10
  nBTC/vbyte and the total fee by 1 mBTC, using a total of 400 megabytes
  (including the original transaction and first replacement) to
  ultimately produce a transaction with a feerate of 10,000 nBTC/vbyte
  (1 BTC total fee)
- Perform one final replacement of the latest 400,000 byte transaction
  with a ~200-byte (~150 vbyte) 1-in, 1-out P2WPKH transaction that pays
  a feerate of 10,010 nBTC/vbyte (1.5 mBTC total fee)
Assuming 50,000 active relay nodes and today's BTC price of ~$7,000
USD/BTC, the above scenario would allow an attacker to waste a
collective 20 terabytes of network bandwidth for a total fee cost of
$10.50.  And, of course, the attacker could run multiple attacks of this
sort in parallel, quickly swamping the network.
To use the above concrete example to repeat the point made at the
beginning of this email: miners might be willing to accept the waste of
400 MB of bandwidth in order to gain a $10.50 fee, but I think very few
relay nodes could function for long under an onslaught of such behavior.
[1] The reward to relay nodes of maintaining the public relay network is
    that it helps protect against miner centralization.  If there was no
    public relay network, users would need to submit transactions
    directly to miners or via a privately-controlled relay network.
    Users desiring timely confirmation (and operators of private relay
    networks) would have a large incentive to get transactions to the
    largest miners but only a small incentive to get the transaction to
    the smaller miners, increasing the economies of scale in mining and
    furthering centralization.
    Although users of Bitcoin benefit by reducing mining centralization
    pressure, I don't think we can expect most users to be willing to
    bear large costs in defense of benefits which are largely intangible
    (until they're gone), so we must try to keep the cost of operating a
    relay node within a reasonable margin of the cost of operating a
    minimal-bandwidth blocks-only node.

@_date: 2020-04-22 14:24:54
@_author: David A. Harding 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
IIUC, the main problem is honest Bob will broadcast a transaction
without realizing it conflicts with a pinned transaction that's already
in most node's mempools.  If Bob knew about the pinned transaction and
could get a copy of it, he'd be fine.
In that case, would it be worth re-implementing something like a BIP61
reject message but with an extension that returns the txids of any
conflicts?  For example, when Bob connects to a bunch of Bitcoin nodes
and sends his conflicting transaction, the nodes would reply with
something like "rejected: code 123: conflicts with txid 0123...cdef".
Bob could then reply with a a getdata('tx', '0123...cdef') to get the
pinned transaction, parse out its preimage, and resolve the HTLC.
This approach isn't perfect (if it even makes sense at all---I could be
misunderstanding the problem) because one of the problems that caused
BIP61 to be disabled in Bitcoin Core was its unreliability, but I think
if Bob had at least one honest peer that had the pinned transaction in
its mempool and which implemented reject-with-conflicting-txid, Bob
might be ok.

@_date: 2020-04-22 16:28:13
@_author: David A. Harding 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
You don't need a mempool to send a transaction.  You can just open
connections to random Bitcoin nodes directly and try sending your
transaction.  That's what a lite client is going to do anyway.  If the
pinned transaction is in the mempools of a significant number of Bitcoin
nodes, then it should take just a few random connections to find one of
those nodes, learn about the conflict, and download the pinned
If that's not acceptable, you could find some other way to poll a
significant number of people with mempools, e.g. BIP35 mempool messages
or reusing the payment hash in a bunch of 1 msat probes to LN nodes who
opt-in to scanning their bitcoind's mempools for a corresponding

@_date: 2020-04-23 05:59:57
@_author: David A. Harding 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
If the problem is that miners might have information not available to
the network in general, you could just bribe them for that knowledge.
E.g. as Bob's refund deadline approaches and he begins to suspect that
mempool shenanigans are preventing his refund transaction from
confirming, he takes a confirmed P2WPKH UTXO he's been saving for use in
CPFP fee bumps and spends part of its value (say 1 mBTC) to the
following scriptPubKey[1],
    OP_SHA256  OP_EQUAL
Assuming the feerate and the bribe amount are reasonable, any miner who
knows the preimage is incentivized to include Bob's transaction and a
child transation spending from it in their next block.  That child
transaction will include the preimage, which Bob will see when he
processes the block.
If any non-miner knows the preimage, they can also create that child
transaction.  The non-miner probably can't profit from this---miners can
just rewrite the child transaction to pay themselves since there's no
key-based security---but the non-miner can at least pat themselves on
the back for being a good Summaritan.  Again Bob will learn the preimage
once the child transaction is included in a block, or earlier if his
wallet is monitoring for relays of spends from his parent transaction.
Moreover, Bob can first create a bribe via LN and, in that case, things
are even better.  As Bob's deadline approaches, he uses one of his
still-working channels to send a bunch of max-length (20 hops?) probes
that reuse the earlier HTLC's .  If any hop along the path knows
the preimage, they can immediately claim the probe amount (and any
routing fees that were allocated to subsequent hops).  This not only
gives smaller miners with LN nodes an equal chance of claiming the
probe-bribe as larger miners, but it also allows non-miners to profit
from learning the preimage from miners.
That last part is useful because even if, as in your example, the
adversary is able to send one version of the transaction just to miners
(with the preimage) and another conflicting version to all relay nodes
(without the preimage), miners will naturally attempt to relay the
preimage version of the transaction to other users; if some of those
users run modified nodes that write all 32-byte witness data blobs to a
database---even if the transaction is ultimately rejected as a
conflict---then targetted relay to miners may not be effective at
preventing Bob from learning the preimage.
Obviously all of the above requires people run additional software to
keep track of potential preimages[2] and then compare them to hash
candidates, plus it requires additional complexity in LN clients, so I
can easily understand why it might be less desirable than the protocol
changes under discussion in other parts of this thread.  Still, with
lots of effort already being put into watchtowers and other
enforcement-assistance services, I wonder if this problem can be largely
addressed in the same general way.
[1] Requires a change to standard relay and mining policy.
[2] Pretty easy, e.g.
    bitcoin-cli getrawmempool \
 jq -r .[] \
 while read txid ; do
      bitcoin-cli getrawtransaction $txid true | jq .vout[].scriptPubKey.asm
    done \
 grep -o '\<[0-9a-f]\{64\}\>'

@_date: 2020-08-20 10:13:39
@_author: David A. Harding 
@_subject: [bitcoin-dev] Generalizing feature negotiation when new p2p 
I don't think it is.  The proposed BIP, as currently written, only tells
nodes to ignore unknown messages during peer negotiation.  The only case
where this will happen so far is BIP339, which says:
    The wtxidrelay message must be sent in response to a VERSION message
    from a peer whose protocol version is >= 70016, and prior to sending
    a VERACK
So unless you signal support for version >=70016, you'll never receive an
unknown message.  (And, if you do signal, you probably can't claim that
you were unaware of this new requirement, unless you were using a
non-BIP protocol like xthin[1]).
However, perhaps this new proposed BIP could be a bit clearer about its
expectations for future protocol upgrades by saying something like:
    Nodes implementing this BIP MUST also not send new negotiation
    message types to nodes whose protocol version is less than 70017.
That should promote backwards compatibility.  If you don't want to
ignore unknown negotiation messages between `version` and `verack`, you
can just set your protocol version to a max of 70016.
To be clear, the proposed requirement to ignore unknown messages is
limited in scope to the brief negotiation phase between `version` and
`verack`.  If you want to terminate connections (or do whatever) on
receipt of an unknown message, you can do that at any other time.
For whom?
That seems like a pretty significant limitation to decentralized
protocol development.
I think there are currently several people who want to run long-term
experiements for new protocol features using open source opt-in
codebases that anyone can run, and it would be advantageous to them to
have a flexible and lightweight feature negotiation system like this
proposed method.
I don't understand this.  How do two peers negotiate a set of two or
more optional features using only the exchange of single numbers?  For
- Node A supports Feature X (implemented in protocol version 70998) and Feature Y (version 70999).
- Node B does not support X but does want to use Y; what does it use for its
  protocol version number when establishing a connection with node A?
Overall, I like the proposed BIP and the negotiation method it
[1] This is not a recommendation for xthin, but I do think it's an example
    of the challenges of using a shared linear version number scheme for
    protocol negotiation in a decentralized system where different teams
    don't necessarily get along well with each other.

@_date: 2020-08-22 12:46:20
@_author: David A. Harding 
@_subject: [bitcoin-dev] reviving op_difficulty 
RIP, Tamas.
Subsequent to Blummer's post, I heard from Jeremy Rubin about a
scheme[1] that allows difficulty futures without requiring any changes
to Bitcoin.  In short, it takes advantage of the fact that changes in
difficulty also cause a difference in maturation time between timelocks
and height-locks.  As an simple example:
1. Alice and Bob create an unsigned transaction that deposits their
   money into a 2-of-2 multisig.
2. They cooperate to create and sign two conflicting spends from the multisig:
    a. Pays Alice with an nLockTime(height) of CURRENT_HEIGHT + 2016 blocks
    b. Pays Bob with an nLockTime(time) of CURRENT_TIME + 2016 * 10 * 60 seconds
3. After both conflicting spends are signed, Alice and Bob sign and
   broadcast the deposit transaction from 4. If hashrate increases during the subsequent period, the spend that
   pays Alice will mature first, so she broadcasts it and receives that
   money.  If hashrate decreases, the spend to Bob matures first, so he
   receives the money.
Of course, this basic formula can be tweaked to create other contracts,
e.g. a contract that only pays if hashrate goes down more than 25%.
As far as I can tell, this method should be compatible with offchain
commitments (e.g. payments within channels) and could be embedded in a
taproot commitment using OP_CLTV or OP_CSV instead of nLockTime.
[1]

@_date: 2020-12-06 08:04:53
@_author: David A. Harding 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
I don't think this is a good criteria to use for making a decision.  We
shouldn't deny users of working implementations the benefit of a feature
because some other developers didn't implement it correctly.
I disagreed with Rusty previously and he proposed we check to see how
disruptive an address format change would be by seeing how many wallets
already provide forward compatibility and how many would need to be
updated for taproot no matter what address format is used.  I think that
instead is a good criteria for making a decision.
I understand the results of that survey to be that only two wallets
correctly handled v1+ BIP173 addresses.  One of those wallets is Bitcoin
Core, which I personally believe will unhesitatingly update to a new
address format that's technically sound and which has widespread support
(doubly so if it's just a tweak to an already-implemented checksum
Given that, I also now agree with changing the checksum for v1+.

@_date: 2020-02-09 18:15:54
@_author: David A. Harding 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity (reflowed) 
When schnorr and taproot are done together, all of the following
transaction types can be part of the same set:
    - single-sig spends (similar to current use of P2PKH and P2WPKH)
    - n-of-n spends with musig or equivalent (similar to current use of
      P2SH and P2WSH 2-of-2 multisig without special features as used by
      Blockstream Green and LN mutual closes)
    - k-of-n (for low values of n) using the most common k signers
      (similar to BitGo-style 2-of-3 where the keys involved are
      alice_hot, alice_cold, and bob_hot and almost all transactions are
      expected to be signed by {alice_hot, bob_hot}; that common case
      can be the key-path spend and the alternatives {alice_hot,
      alice_cold} and {alice_cold, bob_hot} can be script-path spends)
    - contract protocols that can sometimes result in all parties
      agreeing on an outcome (similar to LN mutual closes, cross-chain
      atomic swaps, and same-chain coinswaps)
The four cases above represent an overwhelming percentage of the spends
seen on the block chain today and throughout Bitcoin's entire history to
date, so optimizing to include them in the anonymity set presents a huge
Earlier in y'alls email, you claim that the difference between the two
approaches for a particular example is 67 bytes.  I haven't checked that
calculation, but it seems you're talking entirely about bytes that could
appear in the witness data and so would only represent 16.75 vbytes.
Compare that to the size of the other elements which would need to be
part of a typical input:
- (36 vbytes) outpoint
- (1) scriptSig compactSize uint
- (4) nSequence - (16.25) schnorr signature (includes size byte)
That's 57.25 vbytes exclusive of your example data or 74.00 vbytes
inclusive.  That means the overhead you're concerned about adds only
about 23% to the size of the input (or 30% on an exclusive basis).
That's definitely worth considering optimizations for, but I'm
personally ok with requiring users of advanced scripts (who can't manage
to produce mutual closes) pay an extra 23% for their inputs in order to
allow the creation of the large anonymity set described above for all
the other cases.
If, subsequent to deployment, large numbers of users do end up using
taproot script-path spends and we want to make things more fair, we can
even out the weighting, perhaps by simply increasing the weight of
key-path spends by 16.75 vbytes (though that would, of course,
proportionally lower the capacity of the block chain).  As mentioned in
a separate email by Matt Corallo, it seems worthwhile to optimize for
the case where script-path spenders are encouraged to look for
mutually-agreed contract resolutions in order to both minimize block
chain use and increase the size of the anonymity set.
The evidence that current users of single-sig, n-of-n, and k-of-n (for
small n) with a default k-set, and mutual-agreed contract protocol
outcomes vastly outweigh all other transaction inputs today and for all
of Bitcoin's history to date.

@_date: 2020-02-14 16:36:42
@_author: David A. Harding 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity (reflowed) 
That's fair.  However, it's only true if everyone constructs their
merkle tree in the same way, with a single ` OP_CHECKSIG` as
one of the top leaves.   Taproot effectively standardizes the position
of the all-parties-agree condition and so its anonymity set may contain
spends from scripts whose creators buried or excluded the the all-agree
option because they didn't think it was likely to be used.
More importantly, there's no incentive for pure single-sig users to use a
merkle tree, since that would make both the scriptPubKey and the witness
data are larger for them than just continuing to use v0 segwit P2WPKH.
Given that single-sig users represent a majority of transactions at
present (see AJ Towns's previous email in this thread), I think we
really want to make it as convenient as possible for them to participate
in the anonymity set.
(To be fair, taproot scriptPubKeys are also larger than P2WPKH
scriptPubKeys, but its witness data is considerably smaller, giving
receivers an incentive to demand P2TR payments even if spenders don't
like paying the extra 12 vbytes per output.)
Rough sums:
- P2WPKH scriptpubkey (22.00 vbytes): `OP_0 PUSH20 `
- P2WPKH witness data (26.75): `size(72) , size(33) `
- P2TR scriptpubkey (34.00): `OP_1 PUSH32 `
- P2TR witness data (16.25): `size(64) `
- BIP116 MBV P2WSH scriptpubkey (34.00): `OP_0 PUSH32 `
- BIP116 MBV P2WSH witness data (42.00): `size(64) , size(32)
  , size(32) , size(36)  PUSH32
   OP_MBV>`
P.S. I think this branch of the thread is just rehashing points that
     were originally covered over two years ago and which haven't really
     changed since then.  E.g.:

@_date: 2020-01-31 15:01:29
@_author: David A. Harding 
@_subject: [bitcoin-dev] Onchain fee insurance mechanism 
Ingrid is able to rescind this series of pre-signed transactions at any
time before one of the transactions is confirmed by double spending her
UTXO (e.g. via a RBF fee bump).  If Alice needs to trust Ingrid to honor
the contract anyway, they might as well not include Ingrid's input or
output in the transaction and instead use an external accounting and
payment mechanism.  For example, Alice and Ingrid agree to a fee
Then they wait for whichever version of the transaction to confirm and
one of them remits to the other the appropriate amount (either 400, 200,
or 1 base unit to Ingrid, or 3,000 base units to Alice).  This
remittance can be done by whatever mechanism they both support (e.g. an
onchain transaction, an LN payment, or just credit on an exchange).
Since it's possible to achieve equivilent security (or lack thereof)
without the locktime mechanism, I don't think the locktime mechanism
adds anything to the idea of hedging fees---and, as you note, it suffers
from incompatibility with some cases where users would be especially
eager to obtain feerate insurance.

@_date: 2020-07-03 10:39:45
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP draft: BIP32 Path Templates 
Hi Dmitry,
How do path templates compare to key origin identification[1] in
output script descriptors?
Could you maybe give a specfic example of how path templates might be
used?  Are they for backups?  Multisig wallet coordination?  Managing
data between software transaction construction and hardware device
[1]     (See earlier in the doc for examples)

@_date: 2020-06-19 15:58:46
@_author: David A. Harding 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Quoted text below is from the gist:
I think you're assuming here that the attacker broadcast a particular
state.  However, in a channel which potentially had thousands of state
changes, you'd have to broadcast a blind child for each previous state
(or at least each previous state that pays the attacker more than the
latest state).  That's potentially thousands of transactions times
potentially dozens of peers---not impossible, but it seems messy.
I think there's a way to accomplish the same goal for less bandwidth and
zero fees.  The only way your Bitcoin peer will relay your blind child
is if it already has the parent transaction.  If it has the parent, you
can just request it using P2P getdata(type='tx', id=$txid).[1]  You can
batch multiple txid requests together (up to 50,000 IIRC) to minimize
overhead, making the average cost per txid a tiny bit over 36 bytes.
If you receive one of the transactions you request, you can extract the
preimage at no cost to yourself (except bandwidth).  If you don't
receive a transaction, then sending a blind child is hopeless
anyway---your peers won't relay it.
Overall, it's hard for me to guess how effective your proposal would be
at defeating the attack.  I think the strongman argument for the attack
would be that the attacker will be able to perform a targeted relay of
their outdated state to just miners---everyone else on the network
will receive the counterparty's honest final-state close.  Unless the
counterparty happens to have a connection to a miner's node, the
counterparty will neither be able to CPFP fee bump nor use getdata to
retrieve the preimage.
It seems to me it's practical for a motivated attacker to research which
IP addresses belong to miners so that they can target them, whereas
honest users won't practically be able to do that research (and, even if
they could, it would create a centralizing barrier to new miners
entering the market if users focused on maintaining connections to
previously-known miners).
[1] You'd have to be careful to not attempt the getdata too soon after
    you think the attacker broadcast their old state, but I think that
    only means waiting a single block, which you have to do anyway to
    see if the honest final-commitment transaction confirmed.  See

@_date: 2020-06-19 16:52:20
@_author: David A. Harding 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
Whoops, I managed to confuse myself despite looking at Bastien's
excellent explainer.  The attacker would be broadcasting the latest
state, so the honest counterparty would only need to send one blind
child.  However, the blind child will only be relayed by a Bitcoin peer
if the peer also has the parent transaction (the latest state) and, if
it has the parent transaction, you should be able to just getdata('tx',
$txid) that transaction from the peer without CPFPing anything.  That
will give you the preimage and so you can immediately resolve the HTLC
with the upstream channel.
Revising my conclusion from the previous post:
I think the strongman argument for the attack would be that the attacker
will be able to perform a targeted relay of the low-feerate
preimage-containing transaction to just miners---everyone else on the
network will receive the honest user's higher-feerate expired-timelock
transaction.  Unless the honest user happens to have a connection to a
miner's node, the user will neither be able to CPFP fee bump nor use
getdata to retrieve the preimage.
Sorry for the confusion.

@_date: 2020-06-20 06:36:47
@_author: David A. Harding 
@_subject: [bitcoin-dev] [Lightning-dev] RBF Pinning with Counterparties 
The problem I have with that approach is that the incentive is to
connect to the highest hashrate pools and ignore the long tail of
smaller pools and solo miners.  If miners realize people are doing this,
they may begin to charge for information about their mempool and the
largest miners will likely be able to charge more money per hashrate
than smaller miners, creating a centralization force by increasing
existing economies of scale.
Worse, information about a node's mempool is partly trusted.  A node can
easily prove what transactions it has, but it can't prove that it
doesn't have a certain transaction.  This implies incumbent pools with a
long record of trustworthy behavior may be able to charge more per
hashrate than a newer pools, creating a reputation-based centralizing
force that pushes individual miners towards well-established pools.
This is one reason I suggested using independent pay-to-preimage
transactions[1].  Anyone who knows the preimage can mine the
transaction, so it doesn't provide reputational advantage or direct
economies of scale---pay-to-preimage is incentive equivalent to paying
normal onchain transaction fees.  There is an indirect economy of
scale---attackers are most likely to send the low-feerate
preimage-containing transaction to just the largest pools, so small
miners are unlikely to learn the preimage and thus unlikely to be able
to claim the payment.  However, if the defense is effective, the attack
should rarely happen and so this should not have a significant effect on
mining profitability---unlike monitoring miner mempools which would have
to be done continuously and forever.
ZmnSCPxj noted that pay-to-preimage doesn't work with PTLCs.[2]  I was
hoping one of Bitcoin's several inventive cryptographers would come
along and describe how someone with an adaptor signature could use that
information to create a pubkey that could be put into a transaction with
a second output that OP_RETURN included the serialized adaptor
signature.  The pubkey would be designed to be spendable by anyone with
the final signature in a way that revealed the hidden value to the
pubkey's creator, allowing them to resolve the PTLC.  But if that's
fundamentally not possible, I think we could advocate for making
pay-to-revealed-adaptor-signature possible using something like
[1] [2] [3] Ignoring my concerns about mining centralization and from the
perspective of just the Lightning Network, that doesn't sound
unreasonable to me.  But from the perspective of a single LN node, it
might make more sense to get the information and *not* share it,
increasing your security and allowing you to charge lower routing fees
compared to your competitors.  This effect would only be enhanced if
miners charged for their mempool contents (indeed, to maximize their
revenue, miners might require that their mempool subscribers don't share
the information---which they could trivially enforce by occasionally
sending subscribers a preimage specific to the subscriber and seeing if
it propagated to the public network).
I don't see how eltoo helps.  Eltoo helps ensure you reach the final
channel state, but this problem involves an abuse of that final state.

@_date: 2020-06-28 08:15:17
@_author: David A. Harding 
@_subject: [bitcoin-dev] MAD-HTLC 
Thank you for your interesting research!  Further quotes are from your
This is a good abstract description, but I think it might be useful for
readers of this list who are wondering about the impact of this attack
to put it in concrete terms.  I'm bad at statistics, but I think the
probability of bribery failing (even if Bob offers a bribe with an
appropriately high feerate) is 1-exp(-b*h) where `b` is the number of
blocks until timeout and `h` is a percentage of the hashrate controlled
by so-called myopic miners.  Given that, here's a table of attack
failure probabilities:
                     "Myopic" hashrate
     B          1%      10%     33%     50%
     l       +---------------------------------
     o  6    |  5.82%   45.12%  86.19%  95.02%
     c  36   |  30.23%  97.27%  100.00% 100.00%
     k  144  |  76.31%  100.00% 100.00% 100.00%
     s  288  |  94.39%  100.00% 100.00% 100.00%
So, if I understand correctly, even a small amount of "myopic" hashrate
and long timeouts---or modest amounts of hashrate and short
timeouts---makes this attack unlikely to succeed (and, even in the cases
where it does succeed, Bob will have to offer a very large bribe to
compensate "rational" miners for their high chance of losing out on
gaining any transaction fees).
Additionally, I think there's the problem of measuring the distribution
of "myopic" hashrate versus "rational" hashrate.  "Rational" miners need
to do this in order to ensure they only accept Bob's timelocked bribe if
it pays a sufficiently high fee.  However, different miners who try to
track what bribes were relayed versus what transactions got mined may
come to different conclusions about the relative hashrate of "myopic"
miners, leading some of them to require higher bribes, which may lead
those those who estimated a lower relative hash rate to assume the rate
of "myopic" mining in increasing, producing a feedback loop that makes
other miners think the rate of "myopic" miners is increasing.  (And that
assumes none of the miners is deliberately juking the stats to mislead
its competitors into leaving money on the table.)
By comparison, "myopic" miners don't need to know anything special about
the past.  They can just take the UTXO set, block height, difficulty
target, and last header hash and mine whatever available transactions
will give them the greatest next-block revenue.
In conclusion, I think: 1. Given that all known Bitcoin miners today are "myopic", there's no
   short-term issue (to be clear, you didn't claim there was).
2. A very large percentage of the hashrate would have to implement
   "rational" mining for the attack to become particularly effective.
   Hopefully, we'd learn about this as it was happening and could adapt
   before it became an issue.
3. So-called rational mining is probably a lot harder to implement
   effectively than just 150 loc in Python; it probably requires a lot
   more careful incentive analysis than just looking at HTLCs.[1]
4. Although I can't offer a proof, my intuition says that "myopic"
   mining is probably very close to optimal in the current subsidy-fee
   regime.  Optimizing transaction selection only for the next block has
   already proven to be quite challenging to both software and protocol
   developers[2] so I can't imagine how much work it would take to build
   something that effectively optimizes for an unbounded future.  In
   short, I think so-called myopic mining might actually be the most
   rational mining we're capable of.
Nevertheless, I think your results are interesting and that MAD-HTLC is
a useful tool that might be particularly desirable in contracts that
involve especially high value or especially short timeouts (perhaps
asset swaps or payment channels used by traders?).  Thank you again for
[1] For example, your paper says "[...] the bribing cost required to
    attack HTLC is independent in T, meaning that simply increasing the
    timeout does contribute to HTLC?s security."  This implies that
    Alice, after she sees Bob's attempted bribe, could offer a counter
    bribe that spends all output value to fees (the scorched earth
    policy ZmnSCPxj describes) with a timelock set to the maximum
    single-transaction value (block 500 million, due to be mined in
    about 10 millennia, give or take a few centuries) and miners would
    hold on to it until then, never mining Bob's lower-feerate bribe.
    That's ridiculous, but it's understandable in your paper because
    you're mainly analyzing time periods so short that you don't need to
    worry much about the time-value-of-money discount (also mentioned by
    ZmnSCPxj); however, your paper also says that your Python
    implementation uses the same formulas in your paper to determine
    whether or not a bribe will profitable, which would obviously be
    wrong for a 10,000-year timelock.
[2] See the never ending discussions on this list and Lightning-Dev
    about ancestor mining package size/depth limits and BIP125 opt-in
    RBF rule

@_date: 2020-06-28 12:41:32
@_author: David A. Harding 
@_subject: [bitcoin-dev] MAD-HTLC 
I'm not these are safe if your counterparty is a miner.  Imagine Bob
offers Alice a MAD-HTLC.  Alice knows the payment preimage ("preimage
A").  Bob knows the bond preimage ("preimage B") and he's the one making
the payment and offering the bond.
After receiving the HTLC, Alice takes no action on it, so the timelock
expires.  Bob publicly broadcasts the refund transaction with the bond
preimage.  Unbeknownst to Bob, Alice is actually a miner and she uses her
pre-existing knowledge of the payment preimage plus her received
knowledge of the bond preimage to privately attempt mining a transaction
that pays her both the payment ("deposit") and the bond ("collateral").
Assuming Alice is a non-majority miner, she isn't guaranteed to
succeed---her chance of success depends on her percentage of the network
hashrate and how much fee Bob paid to incentivize other miners to
confirm his refund transaction quickly.  However, as long as Alice has a
non-trivial amount of hashrate, she will succeed some percentage of the
time in executing this type of attack.  Any of her theft attempts that
fail will leave no public trace, perhaps lulling users into a false
sense of security.

@_date: 2020-03-22 03:54:15
@_author: David A. Harding 
@_subject: [bitcoin-dev] Block solving slowdown question/poll 
There are only two practical solutions I'm aware of:
1. Do nothing
2. Hard fork a difficulty reduction
If bitcoins retain even a small fraction of their value compared to the
previous retarget period and if most mining equipment is still available
for operation, then doing nothing is probably the best choice---as block
space becomes scarcer, transaction feerates will increase and miners
will be incentivized to increase their block production rate.
If the bitcoin price has plummeted more than, say, 99% in two weeks
with no hope of short-term recovery or if a large fraction of mining
equipment has become unusable (again, say, 99% in two weeks with no
hope of short-term recovery), then it's probably worth Bitcoin users
discussing a hard fork to reduce difficulty to a currently sustainable

@_date: 2020-03-31 06:35:08
@_author: David A. Harding 
@_subject: [bitcoin-dev] Statechain implementations 
Dr. Trevethan,
Would you be able to explain how your proposal to use statechains with
2P-ECDSA relates to your patent assigned to nChain Holdings for "Secure
off-chain blockchain transactions"?[1]      [1] Here are some excerpts from the application that caught my attention in
the context of statechains in general and your proposal to this list in
Thank you,

@_date: 2020-05-02 08:53:12
@_author: David A. Harding 
@_subject: [bitcoin-dev] BIP-341: Committing to all scriptPubKeys in the 
A wallet can easily check whether a scriptPubKey contais a specific
pubkey (as in P2PK/P2TR), but I think it's impractical for most wallets
to check whether a scriptPubKey contains any of the possible ~two
billion keys available in a specific BIP32 derivation path (and many
wallets natively support multiple paths).
It would seem to me that checking a list of scriptPubKeys for wallet
matches would require obtaining the BIP32 derivation paths for the
corresponding keys, which would have to be provided by a trusted data
source.  If you trust that source, you could just trust them to tell you
that none of the other inputs belong to your wallet.
Alternatively, there's the scheme described in the email you linked by
Greg Saunders (with the scheme co-attributed to Andrew Poelstra), which
seems reasonable to me.[1]  It's only downside (AFAICT) is that it
requires an extra one-way communication from a signing device to a
coordinator.  For a true offline signer, that can be annoying, but for
an automated hardware wallet participating in coinjoins or LN, that
doesn't seem too burdensome to me.
[1] The scheme could be trivially tweaked to be compatible with BIP322
    generic signed messages, which is something that could become widely
    adopted (I hope) and so make supporting the scheme easier.

@_date: 2020-10-08 10:59:38
@_author: David A. Harding 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
Excellent summary!
I don't think the second option forces upgrades.  It just creates
another opt-in address format that means we'll spend another several
years with every wallet having two address buttons, one for a "segwit
address" (v0) and one for a "taproot address" (v1).  Or maybe three
buttons, with the third being a "taproot-in-a-segwit-address" (v1
witness program using the original bech32 encoding).
It took a lot of community effort to get widespread support for bech32
addresses.  Rather than go through that again, I'd prefer we use the
backwards compatible proposal from BIPs PR and, if we want to
maximize safety, consensus restrict v1 witness program size, e.g. reject
transactions with scriptPubKeys paying v1 witness programs that aren't
exactly 32 bytes.
Hopefully by the time we want to use segwit v2, most software will have
implemented length limits and so we won't need any additional consensus
restrictions from then on forward.

@_date: 2020-10-20 06:29:52
@_author: David A. Harding 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
A while ago, around the Bitcoin Core 0.19.0 release that enabled
relaying v1+ segwit addresses, Mike Schmidt was working on the Optech
Compatibility Matrix[1] and tested a variety of software and services
with a v1 address using the original BIP341 specification (33 byte
pubkeys; we now use 32 byte keys).  Here's a summary of his results,
posted with his permission:
- abra: Bech32 not supported.
- binance: Does not pass front end javascript validation
- bitgo: Error occurs during sending process, after validation.
- bitmex: Bech32 not supported.
- bitrefill: Address does not pass validation.
- bitstamp: Address text input doesn?t allow bech32 addresses due to
  character limits.
- blockchain.info: Error occurs during sending process, after
  validation.
- brd: Allows sending workflow to complete in the UI. Transaction stays
  as pending in the transaction list.
- casa: Fails on signing attempt.
- coinbase: Fails address validation client side in the UI.
- conio: Server error 500 while attemping to send.
- copay: Allows v1 address to be entered in the UI. Fails during
  broadcast.
- edge: Allows sending workflow to complete. Transaction stays in
  pending state. Appears to causes issues with the balance calculation
  as well as ability to send subsequent transactions.
- electrum: Error message during broadcasting of transaction.
- green: Fails on validation of the address.
- jaxx: Fails on validation of the address.
- ledger live: Fails when transaction is sent to the hardwave device for
  signing.
- mycelium: Fails during address validation.
- purse: Transaction can be created and broadcast, relayed by peers
  compatible with Bitcoin Core v0.19.0.1 or above.
- river: Transaction can be created and broadcast, relayed by peers
  compatible with Bitcoin Core v0.19.0.1 or above.
- samourai: Fails on broadcast of transaction to the network.
- trezor: Fails on validation of the address.
- wasabi: Fails on validation of the address.
- xapo: Xapo allows users to create segwit v1 transactions in the UI.
  However, the transaction gets stuck as pending for an indeterminate
  period of time
I would guess that some of the failures / stuck transactions might now
be successes if the backend infrastructure has upgraded to Bitcoin Core
[1]

@_date: 2020-09-19 09:37:16
@_author: David A. Harding 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Interesting idea!  This is going to take a while to think about, but I
have one immediate question:
Is this in the reference implementation?  I don't see it and I'm
confused by this text.  I think it could mean either:
1. Sponsor Tx A can be replaced by Sponsor Tx B if A and B have at least
   one input in common (which is part of the "normal replacement policies")
2. A can be replaced by B even if they don't have any inputs in common
   as long as they do have a Sponsor Vector in common (while otherwise
   using the "normal replacement policies").
In the first case, I think Mallory can prevent Bob from
sponsor-fee-bumping (sponsor-bumping?) his transaction by submitting a
sponsor before he does; since Bob has no control over Mallory's inputs,
he can't replace Mallory's sponsor tx.
In the second case, I think Mallory can use an existing pinning
technique to make it expensive for Bob to fee bump.  The normal
replacement policies require a replacement to pay an absolute higher fee
than the original transaction, so Mallory can create a 100,000 vbyte
transaction with a single-vector sponsor at the end pointing to Bob's
transaction.  This sponsor transaction pays the same feerate as Bob's
transaction---let's say 50 nBTC/vbyte, so 5 mBTC total fee.  In order
for Bob to replace Mallory's sponsor transaction with his own sponsor
transaction, Bob needs to pay the incremental relay feerate (10
nBTC/vbyte) more, so 6 mBTC total ($66 at $11k/BTC).

@_date: 2020-09-19 13:24:17
@_author: David A. Harding 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
It's cheap if:
1. You were planning to consolidate all those UTXOs at roughly that
   feerate anyway.
2. After you no longer need your pinning transaction in the mempool, you
   make an out-of-band arrangement with a pool to mine a small
   conflicting transaction.
I think that works (as policy).
I think it'd be hard to find a constant relay fee bump amount that was
high enough to prevent abuse but low enough not to unduly hinder
legitimate users.

@_date: 2020-09-21 10:52:21
@_author: David A. Harding 
@_subject: [bitcoin-dev] A Replacement for RBF and CPFP: Non-Destructive 
Would it make sense that, instead of sponsor vectors
pointing to txids, they point to input outpoints?  E.g.:
1. Alice and Bob open a channel with funding transaction 0123...cdef,
   output 0.
2. After a bunch of state updates, Alice unilaterally broadcasts a
   commitment transaction, which has a minimal fee.
3. Bob doesn't immediately care whether or not Alice tried to close the
   channel in the latest state---he just wants the commitment
   transaction confirmed so that he either gets his money directly or he
   can send any necessary penalty transactions.  So Bob broadcasts a
   sponsor transaction with a vector of 0123...cdef:0
4. Miners can include that sponsor transaction in any block that has a
   transaction with an input of 0123...cdef:0.  Otherwise the sponsor
   transaction is consensus invalid.
(Note: alternatively, sponsor vectors could point to either txids OR
input outpoints.  This complicates the serialization of the vector but
seems otherwise fine to me.)
I don't think package relay based only on feerate solves RBF transaction
pinning (and maybe also doesn't solve ancestor/dependent limit pinning).
Though, certainly, package relay has the major advantage over this
proposal (IMO) in that it doesn't require any consensus changes.
Package relay is also very nice for fixing other protocol rough edges
that are needed anyway.

@_date: 2020-09-26 06:11:23
@_author: David A. Harding 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
FYI, I think this topic has been discussed on the list before (in
response to the selfish mining paper).  See this proposal:
  Of its responses, I thought these two stood out in particular:
    I think there may be some related contemporary discussion from
BitcoinTalk as well; here's a post that's not directly related to the
idea of using hash values but which does describe some of the challenges
in replacing first seen as the tip disambiguation method.  There may be
other useful posts in that thread---I didn't take the time to skim all
11 pages.
