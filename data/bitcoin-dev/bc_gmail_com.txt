
@_date: 2017-09-01 12:47:08
@_author: Cserveny Tamas 
@_subject: [bitcoin-dev] Horizontal scaling of blockchain 
I was thinking about how to scale the block-chain.
The fundamental problem is that if the miners add capacity it will only
increase (protect) their share of block reward, but does not increase the
speed of transactions. This will only raise the fee in the long run with
the block reward decreasing.
The throughput is limited by the block size and the complexity. Changing
any of variables in the above equation was raised already many times and
there was no consensus on them.
The current chain is effectively single threaded. If we look around in the
sw industry how single threaded applications could be scaled, one viable
option emerge: horizontal scaling. This is an option if the problem can be
partitioned, and transactions in partitions could be processed alongside.
Number of partitions would be start with a fairly low number, something
between 2-10, but nothing is against setting it to a higher number later on
according to a schedule.
Partitioning key alternatives:
*Ordering on inputs:*
1) In this case transactions would need to be mined per input address
2) TX have inputs in partition 1 and 2, then needs a confirmation in both
3) All partitioned chains have the same longest valid approach.
4) Only one chain needed to be considered for double spending, others are
invalid in case they contain that input.
This opens up questions like:
- how the fee will be shared? Fees per partition?
- Ensure a good hash function which spreads evenly, because the inputs
cannot be manipulated for load balancing
- What to do about half mined transactions (Maybe they should be two
transactions and then there is less effect about it, but payment won't be
atomic in both partitions)
*Ordering on transaction ids:*
1) Transactions would be partitioned by their TX-id. Maybe a field allowing
txid-s to match a given partition.
2) Creating blocks like this parallel would be safe for bonefide
transactions. A block will be created each 10 mins.
3) In order to get malicious/doublespent transactions out of the system
another layer must be introduced.
- This layer would be used to merge the parallel blocks. It would have to
refer all previous blocks considered for unspent inputs.
- Most of the blocks will merge just fine as normally block 1 and block 2
would not contain double spending. (of course malicious double spending
will revert speed to current levels, because the miner might have to drop a
block in the partition because it contains a spent input on another
stronger branch)
- The standard longest chain wins strategy would be used for validity on
the meta level
- Meta does not require mining, a branches can be added and they are valid
unless there are double spent inputs inside. Block inside this meta already
"paid for".
Generally both ways would have an effect on the block reward and
complexity, which is needs to be adjusted. (not to create more BTC at the
end, reduced hashing power on partitions.)
I think complexity is not an issue, the important thing is that we tune it
to 10mins / block rate per partition.
Activation could be done by creating the infrastructure first and using
only one partitions only, which is effectively the same as today. Then
activate partitions on a certain block according to a schedule. From that
block, partition enforcement will be active and the transactions will be
sorted to the right partition / chain.
It is easy to make new partitions, just need to activate them on branch
block number.
Closing partitions is a bit more complex in case of TX partitioned
transactions, but managed by the meta layer and activated at a certain
partition block. Maybe it is not even possible in case of input partitions.
I could imagine that it is too big change. Many cons and pros on partition
What is your opinion about it?

@_date: 2017-09-01 18:15:53
@_author: Cserveny Tamas 
@_subject: [bitcoin-dev] Horizontal scaling of blockchain 
Yes. I meant the single thread as an analogy, if a block is found, other
blocks are worthless. (more or less) Longest chain wins.
My line of though was, that currently the only way to scale with the
traffic (and lowering the fees) is increasing the block size (which is hard
as I learned from recent events), or reducing the complexity which is less
secure (most likely more controversial)
Splitting the chain is effectively increasing the block size, but without
the increased hashing and validation overhead.
The usage growth seems to be more of exponential rather than linear. Sooner
or later the block size will need to be 4 mb then 40 mb, then what is the
real limit? Otherwise waiting times and thus the fees will just grow
rapidly. I don't think that it is desirable.
With splitting the ledger, the block size can remain 1-2 mb for long time,
only new partitions needs to be added on a schedule. This would also make
better use of the hashing capacity.
On Fri, Sep 1, 2017 at 7:15 PM Lucas Clemente Vella
