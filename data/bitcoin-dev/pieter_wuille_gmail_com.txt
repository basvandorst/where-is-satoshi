
@_date: 2011-08-10 12:43:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Change to multiple executables? 
I do agree about splitting off bitcoincl - it's kinda confusing now how
the client behaves as a rpc daemon or UI when no RPC command-line
parameters are present, and as a command-line client otherwise.
I am less sure UI and RPC should be split (though being able to select
both independently from eachother at compile time would be nice). I
often run the UI and switch to RPC calls to inspect some details.
Not sure how common this usage pattern is, though.
The problem is that bitcoin-qt is built using qmake, and the rest using
makefiles... so it's more than just adding an additional makefile.
That said, it seems bitcoin-qt is mature enough to replace wxbitcoin
to me, and would definitely like to see it in mainline. How streamlined
is the process of building bitcoin-qt on windows and osx? Maybe we can
switch everything to qmake (for now, as long as no maintained autotools is present)?

@_date: 2011-08-10 20:57:53
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Roadmap/schedules 
No I didn't start writing anything - I've been quite busy the past few weeks,
and will be more so the coming weeks. Anyway, some ideas:
Either we try to make everything single threaded, and aim towards a bitcoin
library which you pass events (which can be network, rpc, UI, ...) and
it always processes in finite time, without any separate threads. That
would be a serious rewrite, and maybe a limitation on potential growth
(there *will* be a time where a full node doesn't run on anything but
a 16-core machine...).
The alternative is doing a very careful checking/rework of the locking
system. I think you want some per-object locking instead of per single
data structure. Making it so fine-grained forces careful checking of
the order in which things are locked. That is hard to keep track of,
and probably doesn't gain you very much (just a guess, experiments could
prove me wrong, obviously)
I would propose a system with one lock for the node-handling code
(mapTransactions, mapBlockIndex, mapOrphanBlocks, ...), one lock for
the wallet-handling code (mapWallet, CKeyStore), and one lock for
network-handling code. No access to any inner data structures of
these components is exposed, and everything goes through accessor
functions. All exposed functions of each component take the respective
lock upon entering the component. This includes functions that only
need read-only access (which currently often don't take a lock at
all, iirc).
However, I think we can move to reader-writer locks (boost's shared_mutex).
A lot of code does not need an exclusive lock on the data, as multiple
threads reading the internal data structures simultaneously is not a
problem. This would mean that all inspector functions are wrapped in a
lock_shared/unlock_shared blocks, all mutator functions are wrapped in
a lock_upgrade/unlock_upgrade block, and code that actually modifies
data structures is wrapped in a unlock_upgrade_and_lock/
unlock_and_lock_upgrade block. This is clearly part of a larger code-cleanup effort, as it would mean
moving all code in GUI and RPC that take locks on various things, to
the component they are taking locks on. That's immediately a nice step
towards "librarification" of the code...
I think it should be more or less finished by now in terms of
functionality, at least for dumpprivkey, importprivkey, removeprivkey.
I'm somewhat less sure about dumpwallet/importwallet, as some changes
to the json dump format might be useful still. It does require testing
I'd really like to see that - with or without autotools, if some degree
of consistent config/build architecture can be maintained for the
different platforms.
I was working on a draft for a reworked fee system. I didn't get to
write things out nicely, but the main idea was: assign a score to each
transaction group, in a way that scores always keep increasing over time.
Keep the memory pool sorted according to those scores, and drop the lowest
scoring ones when a configurable memory limit is reached (no limit on the
score itself). Finally, for mining, select the top N transaction groups
from the pool in such a way that an average configurable fee per byte
is maintained. As each mining node chooses a (hopefully more or less fixed, or at least
only slowly changing) cutoff score above which transactions are included,
the network should converge to a more or less fixed probability distribution
for the score at which transactions are included.
Nodes can measure and estimate this distribution, and calculate expected time
to inclusion for a given fee.
The devil is in the details, as it is kinda hard to define a scoring system
for transactions that is independent from the current exchange value of
bitcoins, from which kind of transactions are common on the network, but still
tries to mimic the cost for the network to handle that transaction.
Anyway, as said, I currently don't have the time to implement these ideas
right now. I do read the mailing list, though :)

@_date: 2011-08-11 15:08:45
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Change to multiple executables? 
I think there are a lot of misunderstandings here. Given your clarification
that you simply wanted to remove the RPC client from the gui/daemon executable,
I'm all for it. If I reread the answers, there is only "mild against" and a "no"
that was based on a partial misunderstanding.
Somehow it seems that many discussions in which some negative remarks were made
just die out, and the person originally suggesting it takes this as a "shot
down". Maybe (and that includes myself) we should be more outspoken about ideas
we do like.
As for the rest of this thread: i think some dev branch (either something
linux-next like, or a separate official branch, or something else) is indeed
very needed. The main tree should definitely be dealt with in a conservative
way, but it's hard to make progress if you know that every patch that does
some internal changes will need many rebasings and maintainance before it's
actually merged and finally tested by some larger user base.
Considering the issue of backward incompatible changes to the protocol: there is
no denying that there are some serious deficiencies now (double sha256 checksums,
the handling of the data being signed) and redundant things (magic bytes, verack).
Yes, it is true that we could change these in the future with a (nBlocks >= X)
test, but that would still mean you carry around both the old and the new code
until at least block height X. Additionally, if you get another (better) idea
that supercedes it somewhere between now and block X, you're still forced to
first switch to the intermediate one, as some clients may not have upgraded...
This is not to say this isn't an option we should consider, but for now, it just
doesn't seem worth the hassle to me. There may come a day where we absolutely
need a change to the protocol, and when we do, maybe it is time to fix all these
"known and agreed upon defficiencies". It's definitely useful to discuss these,
and in the context of "for when we do an incompatible change", no "breaks backward
compatibility!" argument is valid. I'm in favor of wiki page where these are listed,
together with pro and cons.

@_date: 2011-08-11 15:50:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Change to multiple executables? 
Back on topic:
I initially misunderstood your proposal. Let me reformulate, and
suggest some names:
* bitcoin-gui (or bitcoin-qt): always starts GUI, optionally starts
RPC server, no RPC client
* bitcoin-server: always starts RPC server, no RPC client, no GUI
* bitcoin-client: always runs RPC client, no RPC server, no GUI
Additionally, we could offer a script or symlinked executable with
names "bitcoin" and
"bitcoind" that detect whether RPC commands are present on the command line, and
based on this invoke either bitcoin-server/bitcoin-gui or
bitcoin-client (for backward

@_date: 2011-08-24 18:18:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New standard transaction types: time to 
What was the reason for disabling these opcodes in the first place? I can
understand wanting to prevent excessive signature-verification, or limitation
of arithmetic to a limited amount of bits, but completely disabling all
arithmetic beyond addition and subtraction, and all bitwise operations seems
very limiting to me. Thus, if we agree to do a future incompatible update,
i would vote to re-enable these, and maybe allow arithmetic up to 520 or
1024 bits numbers.
While we're at it, some additional opcodes could be useful. Either a custom
operator to do boolean evaluation, or a few more lowlevel operations. Given
the presence of bitwise operators, you could have scripts that process a
sequence of pubkey/signature pairs, build up a number in which each bit
corresponds to a valid signature, and then do some bitwise checks on this
number. I'd argue that a "count number of bits set in number" opcode would
be very useful for this.
In short: I'm in favor of re-enabling opcodes, and possibly adding an
OP_BITCOUNT operation.

@_date: 2011-08-25 22:14:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New standard transaction types: time to 
On the github pull request I posted a general scheme that can convert arbitrary
expressions over signature-checks (given in RPL notation) to bitcoin scripts.
Maybe we can define an address type that encodes an expression in RPL form (which
should be more compact and more easily parseable)?
That basically just means the usual bitcoin scripts, with two extra pseudo-
instructions: one that represents an address check, one that represents an
pubkey check.
For example (same example as on the pull req), the expression
a1 OR (a2 AND a3) OR COUNT(a4,a5,a6)>1 (with a1-a6 given addresses)
can be given in RPL form as
  ADDR ADDR ADDR BOOLAND BOOLOR ADDR ADDR
  ADD ADDR ADD 1 GREATERTHAN BOOLOR
Which is 13 bytes + 6*20 bytes, instead of the 54 bytes + 6*20 bytes for the
real bitcoin output script.

@_date: 2011-08-25 23:06:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New standard transaction types: time to 
Some extra estimates:
* single address: 21 bytes (36 characters), 27 bytes script
* 1-out-of-2: 43 bytes (66 characters), 56 bytes script
* 2-out-of-3: 67 bytes (98 characters), 88 bytes script * (a and b) or c: 65 bytes (96 characters), 85 bytes script

@_date: 2011-08-26 23:30:11
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New standard transaction types: time to 
You're quite right - currently addresses encode a particular output script,
and the client pattern matches to know how to deal with the incoming funds.
It's far from sure this will remain the case in the future. Maybe we'll have
an out of band protocol over which a request "i want to pay you for item X"
is sent, with the required transaction output as answer.
A generic way for encoding complex transaction scripts in a compact form may
be useful for "manual" playing with them - but I agree that we should
probably wait for a use case for this.
Independent from the question of complex-script-addresses are useful, I do
think it is useful (and possible, see pull req) to allow a class of scripts
that represent boolean expressions over signature checks to pass the
IsStandard() test - that way we make sure that whenever in the future we
want to support creating such an expression, there will at least be a to
encode it in a way that the network will accept it. The only question is
what possible problems there are with accepting them.

@_date: 2011-12-12 21:57:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bytes "2.0" 
It seems base58 is actually quite terrible for producing nice human-recognizable
addresses, even though base58 is specially intended for human usage. We'll just
have to deal with it, or completely overhaul it and move to a saner encoding.
Luke's proposal is somewhat more drastic than my original one, since it removes
the actual "version" notion from the version bytes, and changes testnet addresses.
However, I think it may be worth it. More data classes have been necessary
before, and new versions haven't. Furthermore, they are far more recognizable to
users, which is something that in particular for OP_EVAL addresses (script hashes)
will be a plus.
Therefore, I'm in favor of the proposal; the new versions would become:
0:   mainnet pubkey hashes ('1', as before)
192: testnet pubnet hashes ('2', instead of 111, 'm' and 'n')
5:   mainnet script hashes ('3'; for OP_EVAL)
196: testnet script hashes ('2', same as normal testnet addresses)
12:  mainnet private keys  ('Q', 'R' or 'S', instead of 128, '5')
204: testnet private keys  ('7', instead of 239, '8' and '9')

@_date: 2011-12-16 09:35:38
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP 15] Aliases 
Interesting discussion so far, with many nice ideas.
I'll try to give my opinion and comment on some in batch here.
First of all, I'm a big proponent of moving away from using base58 strings
as addresses. They are not flexible and not human-friendly. I did an own
proposal to improve the situation some time ago, see
  There was little reaction, and maybe the reason is we shouldn't try to solve/fix
everything at once.
a) IP transactions-like system with DNS resolution
Not only does this give you nice identifiers, but it also moves the
responsibility of getting the transaction accepted by the network from the
sender to the receiver - the one who actually cares about getting his
The authentication problem that was present in the original IP transactions
system can either be mitigated by trusting the existing SSL public-key
intrastructure (which not everyone may like) or (as Satoshi suggested) adding
bitcoin address-based authentication on top (separate from the address used in
the transaction itself). So you get an identifier like $, and
the communication to  would be authenticated using . This
is obviously not useful as human-typable alias, but is no problem for
clickable URLs on websites that want to provide the additional security.
I'm not sure about using the bitcoin p2p protocol here - i think there are
easier (or at least more widely deployed) protocols like HTTP. So maybe ...
b) HTTPS Web Service
we can just use an HTTPS web service, that provides the bitcoin address to
be used in the transaction to a client that queries a URL. This immediately
makes the identifier double as a clickable URL, and a merchant could add
metadata to the URL to make the transaction easily trackable.
As for the possibility for spoofing: relying on DNSSEC is currently
difficult i believe (though i'm not entirely up-to-date about its
deployment). Again, alternatives are the SSL PKI, or bitcoin address-based
authentication (basically doing SSL but using bitcoin pubkeys to
c) user at hostname-like identifiers
These look very good, and conveniently match the e-mail system's identifiers.
However, I believe they are only useful for one purpose: user-to-user
payments. For anything somewhat more business-y you probably want to use
a clickable URL, and hide all address information entirely from the user.
Still, for user-to-user payments they are nice.
I'm not convinced about the hardcoding of the " and
"/bitcoin-alias/?handle=" parts, though. These seem very arbitrarily
chosen to me, but if you consider an HTTPS-based variant of a bitcoin
ip-transactions-like system, the proposed "account" parameter to
checkorder would probably become a CGI parameter anyway...
d) DNS TXT lookups
I'm not entirely against this, but only allowing a fixed bitcoin address
to be returned would far too strongly encourage the use of fixed
addresses in transactions. If anything, it should be an identifier
for one of the other proposals (which do allow interaction, or at least
creation of a fresh bitcoin address) that is returned. To conclude: my suggestion would be to use URLs as address identifiers,
optionally suffixed with a bitcoin address for authentication.
This means my "address" would be either "sipa.be/pw.btc" or
"sipa.be/pw.btc$14TYdpodQQDKVgvUUcpaMzjJwhQ4KYsipa" (where "
is an implicit default. Initiating a payment to either of these would
result in a GET of  When a transaction is
constructed, it is POSTed back to that URL.
If we can agree on reasonable hardcoded mapping, pw at sipa.be could just
be a shorthand for either of these (though vulnerable to proofing...).

@_date: 2011-12-16 09:46:33
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Fwd: [BIP 15] Aliases 
Just replying to this one comment: yes, some interaction is always
necessary, but not necessarily directly with the entity hosting the wallet.
There are some EC crypto tricks to do this (often mentioned under
"deterministic wallets" before):
The wallet-hosting entity has a private key x, with public key X.
The address-generating entity knows X, and generates a fresh private
key y for each transaction. For each, it calculates Z=y*X, and asks
the client to pay to hash160(Z). Afterwards, it can send a bunch of
y's to the wallet hosting service, which can reconstruct z=y*x for
each. Alternatively, the y's can be generated according to a predefined
scheme instead.

@_date: 2011-12-16 17:17:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP 15] Aliases 
That's why you use URI + bitcoin address pairs, and use SSL communication
authenticated using the respective bitcoin pubkey. They may spoof your DNS
server, they can't fake having the requested corresponding private key.
Obviously, this moves the problem to getting the URL + address securely
to the client that wants to interact with it, but that is inevitable if
you're not going to rely on a pre-trusted certificate authority and PKI.
Also, the client software can cache the address corresponding to a particular
server or URL, making it similar to an ssh client that caches host keys and
warns when they change.

@_date: 2011-12-18 16:42:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Pubkey addresses 
Yes. The reason is that currently a send-to-address puts the address in the
output script, while redeeming requires the full pubkey plus the signature
to be placed in the input script. Overall, this requires more space than a
send-to-pubkey, where the output contains the pubkey, and the input the
There are several possible improvements however, and they may not all have
been explained in this thread. To summarize:
* compressed public keys (33 byte pubkeys instead of 65 bytes)
* compact signatures (66 bytes instead of 72, including hash type byte)
* pubkey recovery (allows the public key to be derived from a compact signature)
The first is very easy to implement (see pull  Compact signatures and pubkey recovery require a change to the scripting language (though are
already implemented, as they are used for message signing).
These result in several combinations that could be proposed:
1) send-to-pubkeys-hash
   - currently the default addres type
2) send-to-recovered-pubkeys-hash-with-compact-signature-inside-op_eval
   - extend the scripting language inside OP_EVAL, as described in
        - use compact signatures
   - use key recovery, and never put a pubkey in the blockchain data
3) send-to-pubkey
   - traditional transaction type
4) send-to-compressed-pubkey
   - what Luke proposes as new address type
5) send-to-compressed-pubkeys-hash
   - what pull  would bring
Gregory Maxwell made a small table to compare these options:
  If you don't consider pruning, everything is better than send-to-pubkeys-hash
as we have now. Both using pubkeys instead of hashes, using compressed pubkeys
instead of full ones improve the situation independently, and using key
recovery is even better.
If you do consider pruning, the advantages are smaller, but it is far from
clear to me how pruning will be implemented in the future (as a pruning
node cannot function as a NODE_NETWORK service anymore).

@_date: 2011-12-19 02:14:20
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP 15] Aliases 
Any DNS-based alias system is vulnerable to spoofing. If I can make people's
DNS server believe that mining.cz points to my IP, I'll receive payments to
If no trusted CA is used to authenticate the communication, there is no way
to be sure the one you are asking how to pay, is the person you want to pay.
Therefore, one solution is to put a bitcoin address in the identification
string itself, and requiring SSL communication authenticated using the
respective key.
This makes the identification strings obviously less useful as aliases,
but pure aliases in the sense of human-typable strings have imho
limited usefulness anyway - in most cases these identification strings
will be communicated through other electronic means anyway.
Furthermore, the embedded bitcoin address could be hidden from the user:
retrieved when first connecting, and stored together with the URI in
an address book. Like ssh, it could warn the user if the key changes
(which wil be ignored by most users anyway, but what do you do about

@_date: 2011-12-29 20:08:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Alternative to OP_EVAL 
If we're again brainstorming about alternatives for OP_EVAL, I'll do my own.
It is called OP_CHECKEDEVAL, and is specified as follows:
* It looks at the two elements most recently (in code position) pushed by a literal,
  and not yet consumed by another OP_CHECKEDEVAL. These are S (the serialized script),
  and H (its hash). This implies it defines its own literal-only stack, where all
  literals push to, and only OP_CHECKEDEVAL pops from. This special stack has the
  advantage of allowing static analysis - one does not need to execute any operations
  to find out which data will end up on it. Note that "skipped" code (inside the
  ignored part of an IF-THEN-ELSE) can still push to the literal stack.
* For the "outer script", it does not have any effect at all, except for:
  * 2 elements popped from the literal-only stack
  * potentially causing failure
  It does not touch the main stack, alt stack or any other part of the execution state
  not listed above.
* Failure is caused when either of these conditions hold:
  * No two elements remain on the literal-only stack
  * The Hash(S) != H
  * The inner script execution caused failure
* For the execution of the inner script:
  * It is executed in a completely new and independent execution environnement
  * It executes the deserialized S
  * It inherits the main stack and alt stack (without the serialized script and the hash
    themselves) from the outer execution.
This requires OP_CHECKEDEVAL to immediately follow the push of script and hash,
so the code in the pair <

@_date: 2011-12-29 22:00:26
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Alternative to OP_EVAL 
I realized this may have been needlessly complicated. All is required to achieve the
same properties (plus win half-verification by old clients) is a somewhat more
restricted OP_EVAL which:
* Does not touch the stack or altstack - it looks at the last (code-position wise)
  literal pushed (and not yet consumed by another OP_EVAL) on the stack and uses
  that as script to be executed.
* Executes its subscript in an independent environment, which inherits only the
  main stack (this allows the outer script to hide information from the
  inner script by moving it temporarily to the alt stack).
* OP_EVAL is an effective no-op for the execution state of the outer script,
  except for:
  * potentially causing failure (if the subscript doesn't parse or doesn't
    terminate succesfully)
  * popping an element from the literal-only stack
A pay-to-script-hash becomes:
  OP_EVAL OP_HASH160  OP_EQUAL
and is redeemed using
  [script input] <

@_date: 2011-07-01 10:00:43
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.3.24 
I've cleaned the commit up a bit, and created a pull request ( for it.
Nice, we definitely needed something like that. It wouldn't hurt to have multiple
people running such a seed, to prevent problems with occasional outages of DNS seeds,
once we move away from IRC entirely.
Given that there is no public outcry against these programs automatically
opening holes in firewalls, I assume it's safe for us to do the same.
It should be clearly explained in the release notes, though.
So: I'm in favor of an emergency release 0.3.24 with upnp default enabled,
dnsseed default enabled, block send limit, no-connect segfault bugfix.
Anything else?

@_date: 2011-07-03 12:44:17
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Reconsider build system change? 
I have no preference for any particular project build system. If a system
* is easy enough to set up (included in standard repositories, eg.)
* allows building of the bitcoin codebase on several linux distro's
* does cross-compilation to windows
* supports osx
* is easy to maintain
* it is not too hard to adapt other GUI's to use it (bitcoin-qt,
  maybe others as well, i hear about a cocoabitcoin?)
* gets implemented and tested to support all of the above
.. i have no problem with choosing that system for future versions.

@_date: 2011-07-07 10:49:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Reconsider build system change? 
Other opinions? Someone actually interested in writing a cmake configuration
for bitcoin?

@_date: 2011-07-07 13:15:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bytes 
Hello everyone,
after a discussion on IRC, we decided to try to standardize the version bytes
used by bitcoin for several applications.
There are 3 components that seem meaningful:
* network? (realnet, testnet, alternate chains?)
* data class? (address, private key, master key, ...?)
* version? (real version, per data class defined)
There is no technical reason why different network and different data classes
would need separate version bytes, but i think it is a good thing to keep
them from colliding. People will mix them up, and when things are well
defined, a nice warning message could help a lot ("Oops it seems you entered
a private key instead of an address!").
So, first of all, there is already one actually used alternate chain, namely
namecoin, using version byte 52 for addresses. For this reason, i'd like to
reserve bit 16 in the version byte for "alternate chain". When bit 16 is set,
everything is up to the network itself, and no further semantics are defined.
When bit 16 isn't set:
Then remains the rest of the network. The problem is that testnet already uses
version 111, which is not a single bit. We can use a trick though, namely
choosing bit 1 for testnet, and if bit 1 is set, XOR the rest of the version
number with 111. Otherwise, we could reset testnet (not actually reset, just
change its addresses a bit), and simply state odd=testnet, even=realnet.
That leaves use with 6 more bits to play with, namely 128,64,32 and 8,4,2.
As 128 is already used for private keys, let's use (128,64,32) for data classes,
and (8,4,2) for versions.
So, in full:
* Bits 128/64/32 define data class
** 0 = address
** 32,64,96,160,192 = reserved for future use
** 128 = private key
** 224 = extended data class, another "data class" byte follows
* Bit 16 defines "private"
** 0 = bitcoin
** 16 = alternate chain
* Bits 8/4/2 define version number
** 0 = only thing used for now
** 2,4,6,8,10,12 = reserved for future use
** 14 = extended version, another version byte follows
* Bit 1 defines testnet
** 0 = realnet
** 1 = testnet (possibly using XOR 111, if not reset)
This whole discussion started when Stefan wanted to define a format for master keys from which
to derive deterministic wallet keys, i suggest using data class 192 for that, leaving the
lower numbers for more basic data, like public keys.
Any comments?

@_date: 2011-07-07 21:40:08
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bytes 
I realize my mail may have been a bit unclear. This is about the version bytes
used in addresses and other base58-encoded data structures. I'd like to see some
convention adopted before everyone starts defining their own.
The proposal in the previous mail could be summarized by the following functions
(for non-alternate chains). It is compatible with all currently-used version bytes
that i know of (testnet, realnet, addresses, private keys, namecoin, multicoin):
enum dataclass_t     address = 0,
    privkey = 4,
    masterkey = 6,
    extended = 7
int EncodeVersionByte(dataclass_t class, int nVersion, bool fTestNet)
    return (class << 5 + nVersion << 1) ^ fTestNet*111;
void DecodeVersionByte(int nByte, dataclass_t& class, int& nVersion, bool& fTestNet)
    fTestNet = false;
    if (nByte & 1)
    {
        fTestNet = true;
        nByte ^= 111;
    }
    class = (nByte & 224) >> 5;
    nVersion = (nByte & 14) >> 1;

@_date: 2011-07-08 11:25:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bytes 
It does have another advantage: it makes testnet codes visually (after base58
encoding) different from realnet ones, which is probably the reason why the
relatively large number 111 was chosen.
The only small change that can cause the first base58 character to remain equal,
is a modification to nVersion of less than 5 in absolute value.
PS: +/- 111 is also possible, instead of XOR 111.

@_date: 2011-07-10 21:10:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Encrypted Wallet Backward Compatibility 
Though giving an mostly incomprehensible/unrelated error is never nice to
the user, i believe it's better than creating an empty wallet and letting
the user wonder where his wallet went. This way, we fail soon and don't
ever get a corrupt wallet.
ACK on newenc, and thanks for all the work you put in it already.

@_date: 2011-07-14 11:10:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Notifications from client/wallet 
I like this idea. Matt and I were considering a similar system for the internal
communication between net/wallet/blockdb/mempool, but weren't really aware of
boost::signal. I looked at it, and really seems to provide everything necessary.
Maybe even per-account?
Does that include more confirmations? I think we'd first need to define what
exactly is relevant information for transactions. It could be defined in
terms of a/some high-level information request functions for transactions, so
GUI/RPC don't inspect the wallet datastructures themselves anymore:
* GetTransactionState(): return state (immature, generated, unconfirmed,
                         rejected, confirmed), and number of confirmations.
                         (possibly using the negative number of confirmations
                         semantics as described here:
               * GetBroadcasts(): return either -1 (unknown) or some integer denoting how often
                   this tx was broadcast. The "0/offline" state is equal to                    unconfirmed + 0 broadcasts
* GetInputs(): return a list of pairs (uint256 txId, int nOffset, int64 nAmount)
* GetOutputsToMe(): return a list of pairs (address addr, string label, fBool isChange,
                int64 nAmount, bool fGenerated, bool fAvailable) describing all
                To-Me outputs
* GetOutputsToOthers(): return a list of pairs (address addr, string label,
                        int64 nAmount)
* GetFee(): get the fee paid
The only things that can change while the transaction is already in the wallet seems
to be GetTransactionState() and GetBroadcasts(), so those would cause a
transactionUpdated event?
Adding/removing private keys from the wallet may change the other outputs, so i suppose
those are also candidates for causing this event.
     - transactionRemoved(int256): transaction removed from wallet (can this
for now, that can't happen, but if something like unspending/rejecting/detection
of conflicting transactions is added, it may.
You simply mean the "Transaction requires fee of ..., agree?" message?
Regarding wallet encryption, we could use a
     - string askPassphrase()
Ok; those would need to be implemented as globals until a more modular structure
is implemented.

@_date: 2011-06-17 12:31:25
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Roadmap for autotools / Qt gui merge 
I'm in favor of merging autotools now, unless there are still known issues with
it, such as mingw32 compatibility?
If the Qt port is functionally complete and stable, I'd vote to have it merged as
well under these conditions:
* the changes to the 'core' are minimal/trivial
* marked as 'experimental', and not enabled by default
The only disadvantage of another GUI is that the (necessary) cleanup of RPC/GUI
code will now need makes changes in 3 places instead of 2, but as I understand
it, there are much more people willing to work on Qt code than on wx code?

@_date: 2011-06-19 02:30:49
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Roadmap for autotools / Qt gui merge 
The consensus seems to be to merge autotools and qtgui. However, autotools is
apparently not tested (enough) on native windows and osx platforms.
Anyone on this list with either development environnement, and willingness
to test?

@_date: 2011-06-22 16:18:37
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Roadmap for autotools / Qt gui merge 
Jaromil has his autotools branch rebased against git master, and included Mark's
NOSIGPIPE/NOSIGNAL patch (needs a commit summary, though).
Can people test this compiles+works on several systems (ubuntu, fedora(?), mingw
crosscompile, osx, ...)?
  See For me, it works on Ubuntu 10.10 amd64, including wx gui.

@_date: 2011-11-07 16:02:43
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Difficulty adjustment / time issues 
Here is an idea for an alternative (simple but hacky) solution:
* Keep all network rules as they are now.
* The timestamp value of mutliple-of-2016-blocks is set equal to
  the highest timestamp that occurred in the previous 11 blocks,
  instead of the current time. This will always obey the previous
  rules (it's always at least the median of the past 11 blocks,
  and never more in the future than them).
Initially, roll out software that only uses this new rule for
block creation, but doesn't enforce it. When enough miners have
upgraded, choose a point in the future where it becomes mandatory
(causing a block chain split only for those creating blocks using
old software).
If i understand the problem correctly, this will prevent an attacker
from introducing a time lapse in between the 2015-block windows.
One problem i do see, is that it prevents X-Roll-Time for miners.
Maybe a short interval (1 minute? 10 minutes?) instead of a fixed
value could be allowed for the multiple-of-2016 blocks.

@_date: 2011-11-07 16:43:21
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Difficulty adjustment / time issues 
In chains where no timejacking attack is going on, yes. In the common case
the timestamp is limited to a range of [5 blocks in the past ... 2 hours in
the future].
However, during a timejacking attack, the timestamps of multiple-of-2016 blocks
are essentially independent from the others. In such a case, most timestamps are
very low, and only those of multiple-of-2016-blocks correspond to the current time.
Each 2016*N-1 to 2016*N transition incurs an arbitrary large forward shift to the
present time, each 2016*N to 2016*N+1 transition does a time shift backwards again
that is allowed because the median allows single outliers. By fixing the timestamp
occasionally more tightly to the maximum, instead of the median, no such time
lapses are possible.
Updated proposed rule: limit the timestamp of multiple-of-2016-blocks to
[max(past 11 timestamps)+1 ... current_time+7200], essentially just using a maximum
instead of a median. I believe that is enough to prevent the attack.

@_date: 2011-11-21 03:34:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Compressed public keys 
Hello all,
I just submitted a pull request ( that enables the use of compressed
public keys in Bitcoin. As discovered by roconnor (IRC), this is possible
in such a way that old clients verify and relay them without problems.
They are supported by default by OpenSSL, and all alternative
implementations that use OpenSSL should support these keys just fine as
Compressed public keys are 33 bytes long, and contain the same information
as normal 65-byte keys. They only contain the X coordinate of the point,
while the value of the Y-coordinate is reconstructed upon use. This
requires some CPU, but only a fraction of the cost of verifying a
In theory, one private key now corresponds to two public keys, and thus
two different addresses. As this would probably cause confusion, this
implementation chooses only one of them (at key generation time). All
keys generated when -compressedpubkeys is active, are compressed, and
their reported address is that corresponding to its compressed pubkey.
Things that need attention:
* Do all client implementations support it?
* How to represent secrets for compressed pubkeys?
* send-to-pubkey transactions are untested, for now

@_date: 2011-11-21 12:48:20
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Compressed public keys 
To help in testing this: this address corresponds to a compressed
public key:

@_date: 2011-10-24 13:09:56
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Determine input addresses of a transaction 
Bitcoin transactions do not have input addresses - they optionally have addresses
the input coins were last sent to. I understand that being able to have a
'from' address on a transaction is useful in certain cases, but it encourages
using such 'from' addresses to identify transactions - which is imho the wrong
way to go.
As far as your green transactions idea is concerned, maybe we could provide an interface
to mark certain addresses as 'trusted', and have an RPC call to request all incoming
transaction that originate from trusted sources?

@_date: 2011-09-04 14:04:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.4rc1 known bugs] 
I've compiled bitcoind with Gavin's DEBUG_LOCKORDER, and fixed two potential
reported deadlock issues (see No deadlock warnings are given any more, but the issue remains. Just starting up
bitcoin with an empty addr.dat seems enough to cause it every few attempts. Is there is locked code that waits for an event that never occurs?

@_date: 2011-09-08 18:20:16
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Alert System 
Yes, Satoshi transferred the key to Gavin when he "left". I agree we should
keep it, btw. There have been suggestions before on this list to use the
alert system to ask people to upgrade to recent versions of the client (eg.
the disconnect issue 0.3.20-0.3.23 had). I feel there may come a moment when
we really need to use it for that purpose.

@_date: 2011-09-23 18:21:04
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Beyond IP transactions: towards a bitcoin 
Hello everyone,
here is an idea i've bean writing up: I hope it can start some discussion about moving away from static bitcoin addresses
as descriptions for transactions. I suppose it's a candidate for a BIP/BEPS/BFC/...,
but as things don't seem to have been decided completely about those, I put it in a
Please, comment.

@_date: 2011-09-24 01:15:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Beyond IP transactions: towards a bitcoin 
Well, I agree, this scheme is not (primarily) intended to be a solution
for people who want to accept anonymous donations; static addresses
work very well for that application (unless you want unlinkability
between different payments).
Let me try to explain what I do want to accomplish.
What current addresses are, is a reference to a public key. The way they
are used is as a template for a transaction. If you do not need complex
transactions, this suffices indeed, given that all other negotiation about
the payment occurs out-of-band already (e.g., a webshop interface that
after clicking 'pay' gives you a freshly generated bitcoin address and
stores it so it can track your payment).
What I want to do is to standardize part of that out-of-band communication
inside a protocol. The first observation is that if you want a freshly
negotiated key each time, some form of bidirectional communication is necessary anyway, and a static txout template does not suffice anymore.
If you're doing bidirectional communication, you are no longer limited
by the space constraints of something by-human-copy-pastable, and you can
just negotiate the txout directly, which transparently adds support for
anything that is possible through bitcoin scripts.
So far, the creation of transactions is "solved". However, by asking nodes
not to broadcast their transaction, but instead just send it back (we're
communicating with some other party already anyway, and this other party
is the one who cares about the tx being accepted), the receiver can track
it as well. Furthermore, by passing tags along, identification of
transactions becomes a lot easier. As a  extra advantage, this makes the
requirements for a client easier as well (it doesn't need to be a p2p
The third step is adding signatures to authenticate the whole process.
They are necessary to make sure the client is communicating with who he
thinks he is, but by using them for the submission of the transaction as
well, it gives the client a proof of payment acceptance too.
Summarized: addresses are a limited method for defining payments, and as
soon as you move to a protocol instead of a static template, a lot of
possibilities open up.

@_date: 2011-09-24 05:05:08
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Beyond IP transactions: towards a bitcoin 
I don't want to send a mail to you or chat with you when I'm buying something
in your webshop. Or do you mean my client does that automatically? Why not
through an HTTP connection like the one I'm already using anyway to view
the static address on your website?
They still require you to give me your public key root, and me to give
you the ephemeral private key I generated, optionally together with what
I'm paying you for. That's bidirectional communication to me. Agreed, your
scheme requires a few steps less, but I believe mine is far more flexible
and user-friendly.

@_date: 2012-04-02 17:23:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Network version increase 
Hello all,
Mike Hearn has submitted a pull request to add a pong message in reply to a ping.
This warrants an upgrade of the network protocol version number, which is since BIP14
independent from the version numbers of the reference client.
Any opinions about a numbering scheme? Currently the value 60000 is used. We could
simply start increasing the number one by one now for every change, or we could
do a "cleanup" to 100000 first, and start incrementing from there.

@_date: 2012-04-12 18:01:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Adding request/reply id in messages 
If there is a reasonable use for it, I have no objections.
However: the bitcoin P2P protocol is not fully request-reply based, and trying to use
it that may be be less intuitive than how it looks. For example, doing a second
identical "getblocks" request will not result in more "inv" replies, as the client
prevents retransmits. This is not a large problem, but maybe such an extension
should also include an extra "denied" message, which is sent if the client is
unwilling to answer (and may also be used to report transactions that are not
accepted into the memory pool, for example).

@_date: 2012-08-16 19:56:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 35: add mempool message 
I like the idea of being able to query the memory pool of a node; the
implementation is straightforward, which is good. Maybe effectively using the
command can be added? I suppose it is interesting in general for nodes to
get a memory pool refill at startup anyway.
I'm not sure about this last. What is it good for? inv packets can always be
sent, even not in response to others, so it is not that this gives you an
acknowledgement the mempool is updated?
This seems safe, although it forces other full implementations that want to
expose protocol version 60002 (or later) to also implement this. What do they
think about this?
I would like to suggest to allocate an extra service bit for this. We still
have 63 left, and this is a well-defined and useful extra service that was
not yet provided by any earlier node. Doing that would also mean that
mempool-providing survices may be discovered before connecting to them, as
the service bits are carried around in addr messages. Any opinions about that?

@_date: 2012-08-17 15:40:01
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 35: add mempool message 
I disagree. Returning an empty "inv" is a very strange way of replying
"empty mempool". Bitcoin P2P is not a request-response protocol, and
"inv" messages are sent where there are inventory items to send. The
reaction to a request (for example "getblocks") can be nothing, or one
or more "inv" messages if necessary. Special casing an empty "inv" to
mean empty mempool is trying to hack a request-response system on top
of the asynchronous system.
If there is need for confirming the transmission of the mempool is
complete, the proposal to use a MSG_MEMTX sounds good to me. No client
will ever receive such an inv without requesting the mempool, and
implementing handling MSG_MEMTX is trivial.

@_date: 2012-12-03 13:05:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
I've noticed this too, and it is a concern indeed.
If this were a proposal at the time Bitcoin was created, I would definitely
be in favor, but I feel we can't just change such a policy right now - it's
not what people signed up for when they started using the system. I also
see no way to implement this without a hard fork, which would require
planning at least 1-2 years in advance (imho). By that time, the economic
landscape of Bitcoin may be vastly different, and either dust spam will
have killed it, or we will have found another solution already.
Personally, I think the best solution is to change the mining policy to
prioritize (and perhaps favor for free relay/inclusion) transactions that
reduce the number of UTXO's.

@_date: 2012-12-03 13:33:09
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Chain dust mitigation: Demurrage based 
I disagree completely. The only power granted to miners is to decide the
order of otherwise valid transactions (up to postponing some indefinitely)
- they have no ability to control the rules for validity them self  In
particular, the rules that prevent double spending and (monetary) inflation
of the currency are deliberately NOT left to miners. If this were the case,
they could just as well vote to keep the 50 BTC block payout, and that
would certainly not be what people signed up for.

@_date: 2012-12-03 14:54:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 32 HD wallets, 
There is no way to iterate over all strings. The intention is that a wallet
application can detect a new account that becomes in use (e.g. during
disaster recovery), so the accounts get assigned incrementing numbers.
I wouldn't mind adding the ability to do "non-standard derivations" using
arbitrary strings, if this recoverability property is not desired.

@_date: 2012-12-03 21:44:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 32 HD wallets, 
I don't think there is a compelling reason to encourage uncompressed public
keys anymore on the network. They take more space in the block chain for no
additional value whatsoever. Software may of course continue supporting
uncompressed keys if they wish to provide compatibility, but for a new
standard, I think it makes sense to standardize on just compressed keys.
And since that software thus needs to support the compressed encoding,
there is no reason to use a different encoding inside the derivation scheme
Regarding the encoding itself, it is not hard: just 0x02 or 0x03 (depending
on whether Y is even or odd) followed by the 32-byte encoding of X. Decoding
is harder, but is never needed in the derivation. Software internally can use
any representation (and it will), which in almost all circumstances stores
both X and Y (and even more). Decoding compressed public keys is somewhat
harder, as Y must be reconstructed (but the algorithm isn't hard) - this is
only necessary when someone wants to import an extended public key though for
watch-only wallets.

@_date: 2012-02-01 11:02:08
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP16/17 replacement 
Op 1 feb. 2012 10:48 schreef "Andy Parkins"  het
rejected by
IsStandard() is for accepting transactions into the memory pool.
Non-standard transactions are verified just fine when they are in the block
BIP16/17 both create transactions that, when interpreted as old scripts,
are valid. The only change to the protocol is that previously-valid
transactions become invalid. As long as a supermajority of miners enforce
the new rules, everyone can happily keep using their old bitcoin client.
They won't create the new transaction type, and don't accept them as
payment, but they will accept the new block chain.
If we do a breaking change to the protocol - such as adding a new
transaction type - ALL users must upgrade. Those who don't will see a fork
of the chain from before the first new-style transaction. That is not the
case now.

@_date: 2012-02-03 01:19:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Announcement: libcoin 
the Satoshi client. (It was actually what got me to write libcoin in the
first place...). The Satoshi client HTTP server executes all rpc commands
in its own thread, but to do so, it needs to stop the thread of the Node,
even though the command executed is just a query (i.e. not a SendTo), you
hence have two threads blocking each other and when they wait, you wait...
In libcoin all the query methods access the blockChain as a const object
and they can hence safely query it without intervening the work of the Node
thread. The exception are the SendTo methods that first query if a
transaction can take place, then pushes it to the work-queue of the Node
thread and again exits immediately. The actual execution then follows once
the Node has finished its current tasks (e.g. validating a block).
Hello Michael,
I'm impressed by your refactorings, and hope some of them can make it into
the Satoshi codebase. I am however not sure what you've said above is safe.
In particular, how do you guarantee that no other thread modifies the
blockchain structure while you are performing your query on it? Does the
query code operate on a const copy of the structure, or is there guaranteed
only one thread accessing it?
I've been thinking about moving to read-write locks that allow multiple
threads reading the datastructure simultaneously, but removing the locking
all together sounds wrong to me.

@_date: 2012-02-28 17:48:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
Hello all,
as some of you may know, a vulnerability has been found in how the
Bitcoin reference client deals with duplicate transactions. Exploiting
it is rather complex, requires some hash power, and has no financial
benefit for the attacker. Still, it's a security hole, and we'd like
to fix this as soon as possible.
A simple way to fix this, is adding an extra protocol rule[1]:
  Do not allow blocks to contain a transaction whose hash is equal to
that of a former transaction which has not yet been completely spent.
I've written about it in BIP30[2]. There is a patch for the reference
client, which has been tested and verified to make the attack
impossible. The change is backward compatible in the same way BIP16
is: if a supermajority of mining power implements it, old clients can
continue to function without risk.
The purpose of this mail is asking for support for adding this rule to
the protocol rules. If there is consensus this rule is the solution, I
hope pools and miners can agree to update their nodes without lengthy
coinbase-flagging procedure that would only delay a solution. So, who
is in favor?
  [1]   [2]

@_date: 2012-02-28 18:18:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
As explained in the BIP, that would prevent pruning, as it would
require each full node to keep a database with all transaction hashes
It won't happen by accident. Duplicate coinbase transactions are
possible however (by badly written software, or malicious intent).
Transactions that spend duplcate coinbases can be made to have the
same hash as well.

@_date: 2012-02-28 21:24:15
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
Yes, he tried it on testnet against a patched node.
I prefer to avoid this if possible, as it increases the size of the patch
significantly. In particular, it would require the discouragement-system to
be backported to whatever versions pools are running. The current proposal
only requires adding 6 lines of code.

@_date: 2012-02-29 17:47:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
It's not exactly a secret anymore, as the patch also references it.
Russell O'Connor described the attack on his blog:

@_date: 2012-03-01 00:45:59
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
============================== START ==============================
Indeed; duplicate an old coinbase, fork chain without dupe, and spend the old coinbase.
The 100-blocks maturity will not help against is.
I'm not sure how you intend to fix DisconnectBlock() to prevent this in a backward-
compatible way, though.

@_date: 2012-01-01 23:43:33
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [ANN] Bitcoin-seeder v0.1 
Hello all,
I've just tagged v0.1.0 of my bitcoin-seeder program on github:
  This is the program powering the DNS seed on seed.bitcoin.sipa.be.
It is a crawler for the bitcoin network, with integrated DNS server.
It's not more than a preview release with only mininal functionality. Missing
features include:
* logging
* some configuration options
* daemonization
* ...
It has however been running without problem on my node for over a week now,
so I'm releasing it. Comments and questions are welcome.
The program regularly dumps its database in dnsseed.dat, allowing fast
reinitialization. As the program is far from finished, I do not guarantee that
the file format will remain compatible with future versions.

@_date: 2012-01-08 17:12:16
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Pull 748 pay to script hash 
Uhm, was it? I just added some unit tests though.
Very true; compressed public keys are 32 bytes smaller (so more keys
fit in a script), and are about 5% more CPU intensive to verify.

@_date: 2012-01-09 14:53:51
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Compressed public keys: import/export and 
Hello all,
pull  now also defines an import/export format for private keys
whose public key is compressed.
Rationale: even though a compressed and uncompressed public key share
the same actual 32-byte secret, the import/export format needs a
marker that states whether the corresponding compressed or
uncompressed public key should be used (since they have different
* uncompressed: 0x80 + [32-byte secret] + [4 bytes of Hash() of
previous 33 bytes], base58 encoded
* compressed: 0x80 + [32-byte secret] + 0x01 + [4 bytes of Hash()
previous 34 bytes], base58 encoded
Any comments or suggestions regarding this format? Below I've included
3 cases to test implementations with.
Test cases:
case 1:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5HwoXVkHoRM8sL2KmNRS217n1g8mPPBomrY7yehCuXC1115WWsh
    * pubkey (hex):
    * address (base58): 1MsHWS1BnwMc3tLE8G35UXsS58fKipzB7a
  * compressed:
    * secret (base58): KwntMbt59tTsj8xqpqYqRRWufyjGunvhSyeMo3NTYpFYzZbXJ5Hp
    * pubkey (hex):
    * address (base58): 1Q1pE5vPGEEMqRcVRMbtBK842Y6Pzo6nK9
case 2:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5KVzsHJiUxgvBBgtVS7qBTbbYZpwWM4WQNCCyNSiuFCJzYMxg8H
    * pubkey (hex):
    * address (base58): 1JyMKvPHkrCQd8jQrqTR1rBsAd1VpRhTiE
  * compressed:
    * secret (base58): L4ezQvyC6QoBhxB4GVs9fAPhUKtbaXYUn8YTqoeXwbevQq4U92vN
    * pubkey (hex):
    * address (base58): 1NKRhS7iYUGTaAfaR5z8BueAJesqaTyc4a
case 3:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5JMys7YfK72cRVTrbwkq5paxU7vgkMypB55KyXEtN5uSnjV7K8Y
    * pubkey (hex):
    * address (base58): 1PM35qz2uwCDzcUJtiqDSudAaaLrWRw41L
  * compressed:
    * secret (base58): KydbzBtk6uc7M6dXwEgTEH2sphZxSPbmDSz6kUUHi4eUpSQuhEbq
    * pubkey (hex):
    * address (base58): 19ck9VKC6KjGxR9LJg4DNMRc45qFrJguvV

@_date: 2012-01-09 14:53:51
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Compressed public keys: import/export and 
Hello all,
pull  now also defines an import/export format for private keys
whose public key is compressed.
Rationale: even though a compressed and uncompressed public key share
the same actual 32-byte secret, the import/export format needs a
marker that states whether the corresponding compressed or
uncompressed public key should be used (since they have different
* uncompressed: 0x80 + [32-byte secret] + [4 bytes of Hash() of
previous 33 bytes], base58 encoded
* compressed: 0x80 + [32-byte secret] + 0x01 + [4 bytes of Hash()
previous 34 bytes], base58 encoded
Any comments or suggestions regarding this format? Below I've included
3 cases to test implementations with.
Test cases:
case 1:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5HwoXVkHoRM8sL2KmNRS217n1g8mPPBomrY7yehCuXC1115WWsh
    * pubkey (hex):
    * address (base58): 1MsHWS1BnwMc3tLE8G35UXsS58fKipzB7a
  * compressed:
    * secret (base58): KwntMbt59tTsj8xqpqYqRRWufyjGunvhSyeMo3NTYpFYzZbXJ5Hp
    * pubkey (hex):
    * address (base58): 1Q1pE5vPGEEMqRcVRMbtBK842Y6Pzo6nK9
case 2:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5KVzsHJiUxgvBBgtVS7qBTbbYZpwWM4WQNCCyNSiuFCJzYMxg8H
    * pubkey (hex):
    * address (base58): 1JyMKvPHkrCQd8jQrqTR1rBsAd1VpRhTiE
  * compressed:
    * secret (base58): L4ezQvyC6QoBhxB4GVs9fAPhUKtbaXYUn8YTqoeXwbevQq4U92vN
    * pubkey (hex):
    * address (base58): 1NKRhS7iYUGTaAfaR5z8BueAJesqaTyc4a
case 3:
  * secret (hex):
  * uncompressed:
    * secret (base58): 5JMys7YfK72cRVTrbwkq5paxU7vgkMypB55KyXEtN5uSnjV7K8Y
    * pubkey (hex):
    * address (base58): 1PM35qz2uwCDzcUJtiqDSudAaaLrWRw41L
  * compressed:
    * secret (base58): KydbzBtk6uc7M6dXwEgTEH2sphZxSPbmDSz6kUUHi4eUpSQuhEbq
    * pubkey (hex):
    * address (base58): 19ck9VKC6KjGxR9LJg4DNMRc45qFrJguvV

@_date: 2012-01-15 16:22:27
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] some feedbacks 
This sounds like an easy improvement. Anyone interested in implementing
such a minimal "first-use wizard" ?
Please do.
I believe posting to the forum is restricted to certain boards for newbies.

@_date: 2012-01-30 03:31:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] CAddrMan: Stochastic IP address manager 
Hello all,
wanting to move to IPv6 support in the Satoshi bitcoin client
somewhere in the future, the way IP addresses were managed is not
really possible anymore. Right now, basically all addresses ever seen
are kept - both on-disk and in-memory, and sorted on last-seen time
with some randomization. For some people this lead to multi-megabyte
addr.dat files that took ages (well, seconds) to load.
After some discussion with Gregory Maxwell and others on IRC, I
decided to write a specialized address manager based on an entirely
different principle: only keep a limited number of addresses, keep and
index them in-memory, and only occasionally (and asynchronously) dump
them to disk. This of course leads to a weakness: attackers may try to
poison your entire address cache with addresses they control, in order
to perform a Sybil attack. This is especially dangerous in the context
of IPv6, where much more possible addresses exist.
To protect against this, we came up with this design: keep two tables:
one that keeps addresses we've had actual connections with, and one
that maintains untried/new addresses. Both are separated into several
limited-size buckets. Each tables provides a level of protection
against sybil attacks:
 * Addresses in the first table are placed in one of only a few
buckets chosen based on the address range (/16 for IPv4). This way, an
attacker cannot have tons of active nodes in the same /16 range, and
use those to fill the table.
 * Addresses in the second table are placed in one of a few buckets
chosen based on address range the information came from, instead of
the address itself. This way, an attacker spamming you with tons of
"addr" messages can only still have a limited effect.
 * All crucial decisions (selection of addresses, picking a place in a
bucket, which entry to evict if necessary, ...) are randomized with
biases to improve efficiency. Selection of buckets is based on a
cryptographic hash using a secret key to deterministically randomize
The implementation is available in pull request 787
( but there is certainly
need for testing, and room for improvements. Test reports, comments,
constructive criticism, suggestions and improvements are very welcome.

@_date: 2012-01-31 11:39:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 21 (modification BIP 20)] 
It is actually a "send of private key", not to. And I agree, it should be part
of a separate BIP.

@_date: 2012-01-31 11:44:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 21 (modification BIP 20) 
For merchant purposes, I believe URI's containing a static pubkeyhash-address
are only a temporary solution until more elaborate solutions that deal with
all concerns appear (tagging transactions, feedback to the merchant, making
the receiver responsible for inclusion, certificates that a payment was
accepted, authentication, ...). I believe static addresses are too limited
for this purpose, and we shouldn't be trying to extend them with too many
There have been discussions about more dynamic approaches (such as HTTP
communication to negotiate an address) here, and I've written my own
proposal as well ( The details are not
really relevant at this time, but these dynamic approaches seem a much
better way of dealing with what you're trying to add to the bitcoin URI
system now.
My 2 cents: keep bitcoin URI's simple for now.

@_date: 2012-07-06 18:52:04
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Pruning in the reference client: ultraprune 
Hello all,
I've implemented a new block/transaction validation system for the reference client, called "ultraprune".
Traditionally, pruning for bitcoin is considered to be the ability to delete full transactions from storage once all their outputs are spent, and they are buried deeply enough in the chain. That is not what this is about.
Given that almost all operations performed on the blockchain do not require full previous transactions, but only their unspent outputs, it seemed wasteful to use the fully indexed blockchain for everything. Instead, we keep a database with only the unspent transaction outputs. After some effort to write custom compact serializations for these, I could reduce the storage required for such a dataset to less than 70 MB. This is kept in a BDB database file (coins.dat), and with indexing and overhead, and takes around 130 MB.
Now, this is not enough. You cannot have a full node wit just these outputs. In particular, it cannot undo block connections, cannot rescan for wallet transactions, and cannot serve the chain to other nodes. These are, however, quite infrequent operations. To compensate, we keep non-indexed but entire blocks (just each block in a separate file, for now), plus "undo" information for connected blocks around in addition to coins.dat. We also need a block index with metadata about all stored blocks, which takes around 40 MB for now (though that could easily be reduced too). Note that this means we lose the ability to find an actual transaction in the chain from a txid, but this is not necessary for normal operation. Such an index could be re-added later, if Once you have this, the step to pruning is easily made: just delete block files and undo information for old blocks. This isn't implemented for now, but there shouldn't be a problem. It simply means you cannot rescan/reorg/server those old blocks, but once those are deep enough (say a few thousand blocks), we can tolerate that.
So, in summary, it allows one to run a full node (now) with:
* 130 MB coins.dat
* 40 MB chain.dat
* the size of the retained blocks
  * + +-12% of that for undo information.
Oh, it's also faster. I benchmarked a full import of the blockchain (187800 blocks) on my laptop (2.2GHz i7, 8 GiB RAM, 640 GB spinning harddisk) in 22 minutes. That was from a local disk, and not from network (which has extra overhead, and is limited by bandwidth If people want to experiment with it, see my "ultraprune" branch on github: Note that this is experimental, and has some disadvantages:
* you cannot find a (full) transaction from just its txid. * if you have transactions that depend on unconfirmed transactions,   those will not get rebroadcasted
* only block download and reorganization are somewhat tested; use at   your own risk
* less consistency checks are possible on the database, and even fewer   are implemented
Also note that this is not directly related to the recent pruning proposals that use an alt chain with an index of unspent coins (and addresses), merged mined with the main chain. This could be a step towards such a system, however.

@_date: 2012-07-27 11:59:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Scalability issues 
And now to the list...
I doubt the encryption is the problem. I have a much more recent machine
(i7 with 8 GiB RAM), and my blockchain is on a (userspace!) encrypted
filesystem. However, I do not notice any measurable slowdown from doing so.
You are however using a filesystem (ZFS) that is uses its own filesystem
caching implementation to reach some performance, and is known to be very
memory-hungry at that. Furthermore, I believe it is known to have
performance issues on 32-bit architectures. The bdb backend Bitcoin uses
does many I/O operations, and writes them synchronously to disk, killing
whatever caching your filesystem provides. For those who run a large
database on ZFS, I believe it is advised to put ZFS's intent log on a
separate SSD-backed device, to get fast synchronous writes.
That said, some improvememts may be coming. Mike has been working on
changing the backend from bdb to leveldb, which may (or may not) result in
a very different performance profile on your machine. Also, I've been
working on switching the bitcoin block verifier to use a different style
database layout ("ultraprune"), which is smaller, faster, and will support
pruning. A recent test on my own machine synced the blockchain up to the
latest checkpoint in 7 minutes (6 minutes when stored in RAM instead of

@_date: 2012-07-31 16:26:21
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] script tests - invalid script in 
Ultraprune changes the block validation mechanism to only use a set of
coins and the latest block pointer as state. This state is represented
by an abstract class with several implementations. It would be easy to
have a testset run with a memory-backed coin set as state, with the
list of coins and blocks loaded from a file.

@_date: 2012-06-10 01:10:55
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] getmemorypool 
Hello everyone,
Luke's getmemorypool/BIP22 pull request has been open for a
long time, and didn't receive too much discussion.
I think that having a stable and flexible API for negotiating
block generation is important to be standardized. The fact that
it allows moving block generation to specialized programs is a
step in the right direction. However, it seems to me that too
few people (myself included) understand all the details of
BIP22 (or don't care enough) to judge their necessity. I gave
up trying to follow all design decisions some time ago, and as
it seems I'm not alone, nobody liked merging support for it in
the Satoshi client. This is a pity, and I hope the situation
can be improved soon.
I'm sorry for being this late with these comments, but I think
it's essential that the standard is not more complex than
necessary (making it as easy as possible to write either
servers or clients for it), and perhaps even more important,
that its purpose and intended use cases are clear.
proposal and submit. The general idea is that   1) a miner requests a block template
  2) builds/modifies a block based on this, and optionally
     uses propose to check whether the server is willing to
     accept it before doing work
  3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least,
it too me way too long to extract this.
Regarding the block template: is there a particular reason
for sending the full transactions (serialized in hex) both in
templates and submissions? The server will always need to have
access to the transaction database anyway, and clients will
(afaics) rarely care about the actual transactions. My
suggestion would be to just transfer txids - if the client is
interested in the actual transactions, we have the
gettransaction RPC call already. This seems to be captured by
the several "submit/*" and "share/*" variations, but making
it optional seems way more complex than just limiting the API
to that way of working.
That's another thing that bothers me in the standard: too many
optional features. In particular, I understand the usefulness of
having some flexibility in what miner clients are allowed to
modify, but I'm unconvinced we need 10 individually selectable
variations. In particular:
* coinbase outputs: can we just add a list of required coinbase
  outputs (amount + scriptPubKey) to the template? If no
  generation+fee amount remains, nothing can be added.
* coinbase input: put the required part in the template;
  miners can always add whatever they like. Is there any known
  use case where a server would not allow a client to add
  stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not
  limited, there is certainly no reason to limit nonceranges.
  This adds unnecessary complexity to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template
  (i believe those are already there anyway). Anything in
  between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all
  transactions be removable (perhaps except those marked
  'required').
* prevblock: one getmemorypool per new block shouldn't be
  a problem imho, so do a longpoll instead of having the client
  able to modify prevblock themselves.
One more thing that I do not like is often several ways for
specifying the same behaviour. For example, txrequires specifies
that the first N transactions are mandatory, a 'required' field
in the transaction list itself specifies that that transaction is
mandatory, and the lack of transactions as variation means that
they must not be touched at all. Pick one way that is flexible
enough, and discard the others.
In summary, I'd like to see the standard simplified. I have
no problem merging code that makes getmemorypool compliant to
a standard that is agreed upon, but like to understand it first.
In my opinion - but I'm certainly open to discussion here - the
standard could be simplified to:
* getblocktemplate: create a new block template, and return it.
  The result contains:
  * bits, previousblockhash, version: as to be used in block
  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp
    between (curtime - local_time_at_receipt + local_time),
    decreased by mintimeoff and increased maxtimeoff
  * expires, sigoplimit, sizelimit: unchanged
  * subsidy: amount generated (5000000000 for now)
  * coinbaseaux: what generated coinbase's scriptSig must start
    with
  * coinbaseoutputs: list of objects, each specifying a required
    coinbase output. Each has fields:
    * amount: sent amount
    * scriptPubKey: hex serialized of the output script
  * transactions: list of object, each specifying a suggested
    transaction (except for the coinbase) in the generated block.
    Each has fields:
    * txid: transaction id
    * depends: list of dependencies (txids of earlier objects in
      this same transactions list).
    * fee: fee generated by this transaction, which increases the
      max output of the coinbase.
    * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized
  block header, hex serialized coinbase transaction, and a list of
  txids. Returns true or string describing the problem. Proof of
  work is checked last, so that error is only returned if there is
  no other problem with the suggested block (this allows it to
  replace both propose and submit).
Are there important use cases I'm missing?

@_date: 2012-06-10 11:03:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP22/getmemorypool 
Hello everyone,
Luke's getmemorypool/BIP22 pull request has been open for a long time, and didn't receive too much discussion.
I think that having a stable and flexible API for negotiating block generation is important to be standardized. The fact that it allows moving block generation to specialized programs is a step in the right direction. However, it seems to me that too few people (myself included) understand all the details of BIP22 (or don't care enough) to judge their necessity. I gave up trying to follow all design decisions some time ago, and as it seems I'm not alone, nobody liked merging support for it in the Satoshi client. This is a pity, and I hope the situation can be improved soon.
I'm sorry for being this late with these comments, but I think it's essential that the standard is not more complex than necessary (making it as easy as possible to write either servers or clients for it), and perhaps even more important, that its purpose and intended use cases are clear.
  1) a miner requests a block template
  2) builds/modifies a block based on this, and optionally uses propose to check whether the server is willing to accept it before doing work
  3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least, it too me way too long to extract this.
Regarding the block template: is there a particular reason for sending the full transactions (serialized in hex) both in templates and submissions? The server will always need to have access to the transaction database anyway, and clients will (afaics) rarely care about the actual transactions. My suggestion would be to just transfer txids - if the client is interested in the actual transactions, we have the gettransaction RPC call already. This seems to be captured by the several "submit/*" and "share/*" variations, but making it optional seems way more complex than just limiting the API to that way of working.
That's another thing that bothers me in the standard: too many optional features. In particular, I understand the usefulness of having some flexibility in what miner clients are allowed to modify, but I'm unconvinced we need 10 individually selectable variations. In particular: * coinbase outputs: can we just add a list of required coinbase outputs (amount + scriptPubKey) to the template? If no generation+fee amount remains, nothing can be added.
* coinbase input: put the required part in the template; miners can always add whatever they like. Is there any known use case where a server would not allow a client to add stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not limited, there is certainly no reason to limit nonceranges. This adds unnecessary complexity to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template (i believe those are already there anyway). Anything in between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all transactions be removable (perhaps except those marked 'required').
* prevblock: one getmemorypool per new block shouldn't be a problem imho, so do a longpoll instead of having the client able to modify prevblock themselves.
One more thing that I do not like is often several ways for specifying the same behaviour. For example, txrequires specifies that the first N transactions are mandatory, a 'required' field in the transaction list itself specifies that that transaction is mandatory, and the lack of transactions as variation means that they must not be touched at all. Pick one way that is flexible enough, and discard the others.
In summary, I'd like to see the standard simplified. I have no problem merging code that makes getmemorypool compliant to a standard that is agreed upon, but like to understand it first. In my opinion - but I'm certainly open to discussion here - the standard could be simplified to:
* getblocktemplate: create a new block template, and return it. The result contains:
  * bits, previousblockhash, version: as to be used in block
  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between (curtime - local_time_at_receipt + local_time), decreased by mintimeoff and increased maxtimeoff
  * expires, sigoplimit, sizelimit: unchanged
  * subsidy: amount generated (5000000000 for now)
  * coinbaseaux: what generated coinbase's scriptSig must start with
  * coinbaseoutputs: list of objects, each specifying a required coinbase output. Each has fields:
    * amount: sent amount
    * scriptPubKey: hex serialized of the output script
  * transactions: list of object, each specifying a suggested transaction (except for the coinbase) in the generated block. Each has fields:
    * txid: transaction id
    * depends: list of dependencies (txids of earlier objects in this same transactions list).
    * fee: fee generated by this transaction, which increases the max output of the coinbase.
    * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized block header, hex serialized coinbase transaction, and a list of txids. Returns true or string describing the problem. Proof of work is checked last, so that error is only returned if there is no other problem with the suggested block (this allows it to replace both propose and submit).
Are there important use cases I'm missing?

@_date: 2012-06-10 16:18:47
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP22/getmemorypool 
Hello everyone,
Luke's getmemorypool/BIP22 pull request has been open for a long time, and
didn't receive too much discussion.
I think that having a stable and flexible API for negotiating block
generation is important to be standardized. The fact that it allows moving
block generation to specialized programs is a step in the right direction.
However, it seems to me that too few people (myself included) understand
all the details of BIP22 (or don't care enough) to judge their necessity. I
gave up trying to follow all design decisions some time ago, and as it
seems I'm not alone, nobody liked merging support for it in the Satoshi
client. This is a pity, and I hope the situation can be improved soon.
I'm sorry for being this late with these comments, but I think it's
essential that the standard is not more complex than necessary (making it
as easy as possible to write either servers or clients for it), and perhaps
even more important, that its purpose and intended use cases are clear.
submit. The general idea is that
 1) a miner requests a block template
 2) builds/modifies a block based on this, and optionally uses propose to
check whether the server is willing to accept it before doing work
 3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least, it too me way
too long to extract this.
Regarding the block template: is there a particular reason for sending the
full transactions (serialized in hex) both in templates and submissions?
The server will always need to have access to the transaction database
anyway, and clients will (afaics) rarely care about the actual
transactions. My suggestion would be to just transfer txids - if the client
is interested in the actual transactions, we have the gettransaction RPC
call already. This seems to be captured by the several "submit/*" and
"share/*" variations, but making it optional seems way more complex than
just limiting the API to that way of working.
That's another thing that bothers me in the standard: too many optional
features. In particular, I understand the usefulness of having some
flexibility in what miner clients are allowed to modify, but I'm
unconvinced we need 10 individually selectable variations. In particular:
* coinbase outputs: can we just add a list of required coinbase outputs
(amount + scriptPubKey) to the template? If no generation+fee amount
remains, nothing can be added.
* coinbase input: put the required part in the template; miners can always
add whatever they like. Is there any known use case where a server would
not allow a client to add stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not limited, there is
certainly no reason to limit nonceranges. This adds unnecessary complexity
to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template (i believe
those are already there anyway). Anything in between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all transactions be
removable (perhaps except those marked 'required').
* prevblock: one getmemorypool per new block shouldn't be a problem imho,
so do a longpoll instead of having the client able to modify prevblock
One more thing that I do not like is often several ways for specifying the
same behaviour. For example, txrequires specifies that the first N
transactions are mandatory, a 'required' field in the transaction list
itself specifies that that transaction is mandatory, and the lack of
transactions as variation means that they must not be touched at all. Pick
one way that is flexible enough, and discard the others.
In summary, I'd like to see the standard simplified. I have no problem
merging code that makes getmemorypool compliant to a standard that is
agreed upon, but like to understand it first.
In my opinion - but I'm certainly open to discussion here - the standard
could be simplified to:
* getblocktemplate: create a new block template, and return it. The result
 * bits, previousblockhash, version: as to be used in block
 * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between
(curtime - local_time_at_receipt + local_time), decreased by mintimeoff and
increased maxtimeoff
 * expires, sigoplimit, sizelimit: unchanged
 * subsidy: amount generated (5000000000 for now)
 * coinbaseaux: what generated coinbase's scriptSig must start with
 * coinbaseoutputs: list of objects, each specifying a required coinbase
output. Each has fields:
   * amount: sent amount
   * scriptPubKey: hex serialized of the output script
 * transactions: list of object, each specifying a suggested transaction
(except for the coinbase) in the generated block. Each has fields:
   * txid: transaction id
   * depends: list of dependencies (txids of earlier objects in this same
transactions list).
   * fee: fee generated by this transaction, which increases the max output
of the coinbase.
   * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized block
header, hex serialized coinbase transaction, and a list of txids. Returns
true or string describing the problem. Proof of work is checked last, so
that error is only returned if there is no other problem with the suggested
block (this allows it to replace both propose and submit).
Are there important use cases I'm missing?

@_date: 2012-06-12 12:50:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP22/getmemorypool 
Please. This is not about whether getmemorypool is useful (at least I am
a big fan of BIP22's big picture). It's about whether it needs 20 optional

@_date: 2012-06-16 01:11:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Manual file cleanup on exit, 
Use -detachdb if you want to detach the blockchain database files from the
database environment at exit. This was turned off by default in 0.6.0 to
speed up the shutdown process very significantly, and few people have a need
to manually fiddle with their blockchain database files.

@_date: 2012-06-16 21:26:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys 
Hello all,
while OpenSSL's silent support for compressed public keys allowed us to
enable them in a fully backward-compatible way, it seems OpenSSL supports yet
another (and non-standard, and apparently useless) encoding for public keys.
As these are supported by (almost all?) fully validating clients on the
network, I believe alternative implementations should be willing to handle
them as well. No hybrid keys are used in the main chain, but I did test them
in testnet3, and they work as expected.
In total, the following encodings exist:
* 0x00: point at infinity; not a valid public key
* 0x02 [32-byte X coord]: compressed format for even Y coords
* 0x03 [32-byte X coord]: compressed format for odd Y coords
* 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format
* 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords
* 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords
Handling them is trivial: if you see a public key starting with a 0x06 or
0x07, use it as if there was a 0x04 instead.
I suppose we could decide to forbid these after a certain date/block height,
and try to get sufficient mining power to enforce that before that date.
Any opinions? Forbidding it certainly makes alternative implementation
slightly easier in the future, but I'm not sure the hassle of a network
rule change is worth it.

@_date: 2012-06-19 13:38:59
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] LevelDB benchmarking 
I'm all for moving away from BDB. It's a very good system for what it is
intended for, but that is not how we use it. The fact that it is tied to
a database environment (but people want to copy the files themselves
between systems), that is provides consistency in case of failures (but
because we remove old log files, we still see very frequent corrupted
systems), the fact that its environments are sometimes not even forward-
compatible, ...
Assuming LevelDB is an improvement in these areas as well as resulting in
a speed improvement, I like it.
How portable is LevelDB? How well tested is it? What compatibility
guarantees exist between versions of the system?
I don't mind including the source code; it doesn't seem particularly
large, and the 2-clause BSD license shouldn't be a problem.
Jeff was working on splitting the database into several files earlier, and
I'm working on the database/validation logic as well. Each of these will
require a rebuild of the databases anyway. If possible, we should try to
get them in a single release, so people only need to rebuild once. PS: can we see the code?

@_date: 2012-06-20 13:37:46
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] LevelDB benchmarking 
Two days ago on 21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?
21:01:31< sipa> maybe it predates the wallet logic
(read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)

@_date: 2012-06-26 16:11:29
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Tor hidden service support 
Hello everyone,
a few days ago we merged Tor hidden service support in mainline. This means
that it's now possible to run a hidden service bitcoin node, and connect to
other bitcoin hidden services (via a Tor proxy) when running git HEAD. See
doc/Tor.txt for more information. This is expected to be included in the 0.7
Additionally, such addresses are exchanged and relayed via the P2P network.
To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in this
80-bit range is mapped to an onion address, and treated as belonging to a
separate network. This network range is the same as used by the OnionCat
application (though we do not use OnionCat in any way), and is part of the
RFC4193 Unique Local IPv6 range, which is normally not globally routable.
Other clients that wish to implement similar functionality, can use this
test case: 5wyqrzbvrdsumnok.onion == FD87:D87E:EB43:edb1:8e4:3588:e546:35ca.
The conversion is simply decoding the base32 onion address, and storing the
resulting 80 bits of data as low-order bits of an IPv6 address, prefixed by
fd87:d87e:eb43:. As this range is not routable, there should be no
compatibility problems: any unaware IPv6-capable code will immediately fail
when trying to connect.

@_date: 2012-03-01 15:30:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
I don't understand this.
Just disallowing duplicate coinbases is possible, but it requires keeping a
set of all coinbases transaction around until infinity. That's not really a problem,
but it can be avoided. One very reasonable proposed solution is adding the block
height to the coinbase. However, as coinbases are used for all kinds of things
already, this is harder to roll out network-wide. Hence, first this "emergency"
solution that already prevents (afaik) all practical attacks, and in a later step
forcing unique coinbases, so that transactions can be assumed to be unique
identifiable by their hash again.

@_date: 2012-03-02 02:56:34
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
After some private discussion, Ben Reeves pointed out two potential
small weaknesses in the proposed patch, which seem viable to me.
First: disconnecting the same coinbase transaction twice would fail,
as EraseTxIndex will not find anything the second time. This is
extremely hard to pull off, as it requires reverting a chain of at
least 120 blocks long. Still, the fix is very easy imho: allow
EraseTxIndex to fail.
Second: assume the following order of events: block with coinbase A is
created, 120 blocks later, A:0 is spent in transaction B. Then, a dupe
of A is created, and another 120 blocks are waited. At this point, A:0
and B:0 are still spendable. Now a block is created with two
transactions: first C which spends B:0, followed by a dupe of B. This
dupe is accepted, as its former instance is completely spent now.
However, if this last block is disconnected again, B:0 is not
spendable anymore, causing a risk for chain split. Ben suggested
moving the check for dupes up, turning the new network rule into:
  Blocks are not allowed to contain transactions whose hash matches
that of an earlier transaction in the same chain, unless that
transaction was already completely spent before said block.
I've updated the patch, and will update the BIP soon.
What do you all think? Can we still move forward with deploying this?

@_date: 2012-03-03 17:41:03
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Duplicate transactions vulnerability 
After getting responses from Deepbit, bitcoin.cz (slush), MtRed, Bitlc
and BTCmine, it looks like march 15 is a reasonable deployment date
for the security update described in BIP 30.
I have created patches for:
* git master: * v0.4.0: * v0.3.24: * v0.3.24+vinced:
* v0.3.19: I've asked pool operators to upgrade, and confirm when they have done
so. If you are a miner or pool operator, and have the ability to
upgrade, please do so as well.

@_date: 2012-03-31 12:54:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.7 merge recommendations/status 
Nice summary, thanks.
I think that's right - for several reasons, the time between 0.5 and 0.6 was over 4 months. I prefer more frequent releases, as it slows down development
this way.
I'd rather see a decent encapsulation of wallet and blockchain data structures
that allow us to make their mutexes private, and let only the code from the
respective mutex take locks in it when necessary. That will automatically
lead to multithreaded RPC, but in a safe way, without needing guesswork about
which two calls may or may not be called simultaneously.
Of course, that requires a lot more work, but at some point that will be needed
anyway, imho.
I've used loadblocks often in my personal branches. At least on Linux it seems
to work fine. The data scanning code is mostly Cish though, and there may be
more preferrable to use boost or generic C++ solutions.
I've already had a fully functional IPv6 node based on 0.3.24. Most of the changes
there have since been incorported in netbase ( and because of a risk for DoS'es
based on the much larger number of addresses an attacker could have under his control,
addrman ( was necessary before IPv6 could be fully implemented. So, the technical
part of supporting IPv6 seems mostly finished - right now, it's mostly just removing
some (!IsIPv4()) checks and adding listen/connect code that is IPv6-compatible.
I'll do a pullreq for that soon.
There are a few other issues, though. For example: how will relaying work: will IPv4
nodes relay IPv6 addresses? If not, the IPv6 bitcoin network will be completely
separate from the IPv4 one, though both may overlap in some points. The opposite is
also possible: allowing all nodes to relay IPv6 addresses, but only use them in case
an IPv6-compatible interface is detected. Any opinions about this?
Something else was suggested by Jeff: what if a node accidentally connects to itself?
As we're moving towards multiple local addresses with IPv6, the chances for this
become larger. Finally, there are always extra risks involved, as we could unknowingly
be opening DoS or others vulnerabilities.
Finally, supporting IPv6 in a somewhat general way would pave the way for bitcoin
functioning for example as a Tor or I2P hidden service, by using onioncat-like
tor-encoded-in-IPv6 addresses. This way, two bitcoin nodes could connect to eachother
without the need for passing any exit node.

@_date: 2012-03-31 13:08:47
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.7 merge recommendations/status 
My mistake: I mean two nodes connecting twice to eachother. There is already protection
against a node connecting to itself.

@_date: 2012-03-31 14:28:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.7 merge recommendations/status 
============================== START ==============================
Thanks for that - without a license change it would not be possible to merge anything.
Yes, I like its design and refactorings a lot, but at the same time it's very large change to accept at once. In particular, I'm not entirely convinced yet about its thread-safety. For example, acceptblock is a public method, but it seems (i may be missing something) to grab no lock at all until setBestBlock or reorganize is called. Is it impossible to call acceptBlock twice simultaneously? One may start with a bestblockindex value that gets modified a split second later by a simultaneous call. It may be the case that there are indeed no possibilities for this to happen because of things I'm missing, but although I'm a big fan of well-encapsulated locks and the use of reader-writer locks, it's hard to verify whether you use them enough. My suggestion would be: make each publicly accessible method of BlockChain grab either a reader lock (if it's a const function) or an upgradable lock, and take a writer lock in each method that actually performs changes.
Not sure what you mean: the serialized combination of the 32-bit IPv4 address and 12 bytes padding in CAddress are exactly a bsd socket library in6_addr in network byte order. In 0.6.0, CAddress derives from CNetAddr, which encapsulates these 16 bytes.

@_date: 2012-05-26 15:14:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development]  IPv6 bitcoin support now live 
Since yesterday, my DNS seeder (running at seed.bitcoin.sipa.be) ?also
crawls the IPv6 network, and returns corresponding AAAA records.
Hopefully this helps IPv6 nodes to find eachother.

@_date: 2012-11-06 20:14:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
I've implemented code for efficient representation of partial merkle
trees: see The encoding/decoding algorithm uses a depth-first traversal of the tree, at
each node outputting whether anything relevant is beneath, and if not, storing
its hash and not descending into the children.
Furthermore, thanks to some properties of the tree, some hard upper bounds on
the size of the serialized structures are guaranteed. See the comments in the
code for details.
Unit tests are included to verify that
1) encoding and decoding a subset of transactions is an identity
2) the calculated merkle root matches the merkle root calculated by the existing merkle algorithm in the source code
3) the calculated merkle root actually depends on all hashes in the data structure.
4) serialization/deserialization is an identity
5) bounds on the size of the serialization are respected
Hope it is clear enough to port to other languages.

@_date: 2012-11-08 13:56:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] IRC meeting agenda, 18:00 UTC Thursday 
Right now, there seem to be little problems with allocation and viability of
proposed BIPs, with hardly any reviewing/formal allocation being done in
practice. In the past there have been collisions though, and there also have
been nonsensical proposals. I'm in favor of some moderate form of process,
but if the process becomes a burden more than a help, there is clearly a
It seems there is also little attractiveness to writing BIPs. If many proposals
do not result in useful discussion, there is little incentive to write one
except for those proposals that absolutely need to (p2p protocol, block
validity rules, ...). That's a pity in my opinion - I'd like to see non-core
proposals related to Bitcoin being discussed more often as well.
Agree, I think Bloom filtering should make it into 0.8 - it's a critical
step to make SPV clients more useful for end users.
Regarding ultraprune, there are a few TODOs left:
* Auto-migrate old data (depends on -reindex, for which there is a pullreq)
* UTXO set consistency checks at startup (-checklevel)

@_date: 2012-11-21 16:15:35
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Two comments I made on the pullreq page, which are probably better discussed here:
* The 0xFFFFFFFF/(n-1)*i seed value seems intended to result in large bit
  differences between the different hash function's seeds. Together with the tweak,
  however, the first and the last now get seeds tweak and tweak-1. I think
  something simpler like k1*i+k2*n+tweak is better (with k1 and k2 arbitrary large
  odd 32-bit integers).
* I feel uneasy about the arbitrary filter parameters used for the implicitly
  created filter when sending filteradd without filterload. The server cannot be
  expected to make a reasonable guess how the client is going to use the filter,
  and the client still has to track how large the server-side filter grows anyway.
  I'd prefer to make this simply illegal (DoS 100): filteradd always requires an
  active filter. Maybe the actual filter data in filterload can be made optional:
  if it is omitted, it's assumed to be all zeroes (though that would mean the size
  has to be specified).

@_date: 2012-11-27 12:36:13
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol Proposal: 
Gavin's proposal differs in this from my original proposal, where I
exactly *didn't* want to couple the receipt with the acceptance of
the Bitcoin transaction.
If a merchant/payment processor is willing to take the risk of zero or
low confirmation transactions (because they are insured against it,
for example), they were allowed to reply "accepted" immediately, and
this would be a permanent proof of payment, even if the actual Bitcoin
transaction that backs it gets reverted.
For that reason, I also had a separate "pending" state, which means the
receiver isn't willing to just accept the current state as irrevocably
paid. In this case, the sender was allowed to retry until the receipt
sayd "accepted" or "rejected".
The whole point was to avoid that customers/merchants would have to
deal with the uncertainty involved in Bitcoin transaction. At some
point, someone is going to accept the transaction (whether that is at
0 or at 120 confirmations), and acceptance will at the higher level
be considered a boolean anyway - not some "probably, unless reorg".

@_date: 2012-11-27 22:10:23
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Sure, it's nothing important, but it seems like it fails to do what it was intended for.
How about just this: tweak + i*0xFBA4C795 (number optimized to give large seed
differences for every tweak). If you want variation when changing the number of hash
functions, just choose a different seed. It's probably not worth it for something that is max 36 kilobytes. If ever
necessary, we can define a new message type that just lists a number of bits to
be set in the server-side filter.
For now, I agree that you should just send the filter as intended, and not expect to
do many filteradds (though you should take the implicitly-added txids into
accounted when computing the filter size).

@_date: 2012-10-20 19:55:43
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Public key and signature malleability 
Hello all,
as some may be aware, OpenSSL accepts several encodings for the same
public key or the same signature. It even accepts encodings that fail
to conform to the SEC and DER specification by which they are defined.
As it perfectly capable of parsing every standard-compliant encoding,
this is not a problem on itself. However, as near every full Bitcoin
node in existence uses OpenSSL, they will accept such non-standard
encodings in transactions, and even let them into blocks.
In order to make the Bitcoin network rules more well-defined, I'd like
to propose strict rules about what is acceptable, and which do not
depend on OpenSSL's implementation. This would make it easier for
alternative full node implementations, and prevent "malleability
For that, I've submitted a pull request some time ago (see
 The specific rules are
 * For public keys (SEC compressed or uncompressed EC points)
   * First byte is either 0x02, 0x03 or 0x04
   * If the first byte is 0x02 or 0x03, exactly 32 bytes follow
   * If the first byte is 0x04, exactly 64 bytes follow
 * For signatures (DER encoded integer pairs)
   * Format is 0x30  0x02   0x02      * R and S are MSB encoded integers, which encode a non-negative
integer without excessive zero bytes in front. This implies they do
not start with a 0x00 unless the next byte has its highest bit set
(>=0x80) or are 0, in which case exactly one 0x00 is required. Their
length is thus between 1 and 33 bytes.
   *  is a single byte that is either 0x01, 0x02, 0x03,
0x81, 0x82 or 0x83.
All these are rules that are followed by almost all clients already
(including Armory and Bitcoin-js, which until recently didn't).
Sergio Lerner recently discovered that one can take the
secp256k1-field-size complement of the S value in the signature
without invalidating it. The easiest solution to prevent this, would
be to require that the lowest bit of the S value is always even (as
taking the complement changes this). This would require some
coordination, as no client currently enforces this, but it is easy to
The reason malleability can be a problem, is that a malicious relayer
can modify transactions in-transit without invalidating them. This
will not cause any loss of coins, but a lot of wallet software
wouldn't deal gracefully with a modified version of their transactions
that gets accepted in a block.
What do you think about these rules? If people want these rules,
nothing would happen for now - just start try to find software that
doesn't produce complying data. In a second step, these could be
enabled as check similar to IsStandard() - making it hard for them to
get into blocks, but still be accepted when they aren't standard.
Finally, when no significant amount of non-standard transactions are
seen anymore, we can write a BIP and start enforcing this as a network

@_date: 2012-10-21 00:33:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Ultraprune merged in mainline 
Hello everyone,
I've just merged my "ultraprune" branch into mainline (including
Mike's LevelDB work). This is a very significant change, and all
testing is certainly welcome. As a result of this, many pull requests
probably don't apply cleanly anymore. If you need help rebasing them
on the new structure, ask me.
The idea behind ultraprune is to use an ultra-pruned copy (only
unspent transaction outputs in a custom compact format) of the block
chain for validation (as opposed to a transaction index into the block
chain). It still keeps all blocks around for serving them to other
nodes, for rescanning, and for reorganisations. As such, it is still a
full node. So, despite the name, it does not implement any actual
pruning yet, though pruning would be trivial to implement now. This
would have profound effects on the network though, so may still need
some discussion first.
A small summary of the changes:
 * Instead of blk000?.dat, we have blocks/blk000??.dat files of max
128 MiB, pre-allocated per 16 MiB
 * Instead of a Berklely DB blkindex.dat, we have a LevelDB directory
blktree/. This only contains a block index, no transaction index.
 * A new LevelDB directory coins/, which contains data about the
current unspent transaction output set.
 * New files blocks/rev000??.dat contain undo data for blocks
(necessary for reorganisation).
 * More information is kept about blocks and block files, so
facilitate pruning in the future, and to prepare for a headers-first
 * Two new RPC calls are added: gettxout and gettxoutsetinfo.
The most noticeable change should be performance: LevelDB deals much
better with slow I/O than BDB does, and the working set size for
validation is an order of magnitude smaller. In the longer run, I
think it is an evolution towards separation between validation nodes
and archive nodes, which is needed in my opinion.

@_date: 2012-10-24 18:22:56
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
Some questions:
* why limit the number of matching transactions to 255?
* what does "each hash and key in the output script" mean exactly? what about the output script in its entirety?
* is sharing parts of the merkle branches not worth it?
I'm not in favor of stuffing too much into the version message, it already seems overloaded.
A byte with some bit-flags is fine by me - higher bits can later be added for other boolean

@_date: 2012-10-24 19:11:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Draft BIP for Bloom filtering 
So all data push operations? Including or excluding 1-byte constants?
What about the entire output script? (if I want to match just one particular multisig output script)
I'm not sure. As soon as you have 129 transactions in a block (including coinbase), you need 8 path
entries for each included transaction, which requires more bytes than the transaction itself.
When you're including M out of N transactions of a block, you never need more than N-M path entries
in total to reconstruct the merkle root. With the proposed format, it requires M*ceil(log2(N)).
For a 1000-transaction block, when matching ~everything, you need >300 KiB of overhead, while almost
nothing is required.

@_date: 2012-09-13 20:59:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Segmented Block Relaying BIP draft. 
I'm quite sure Gregory thoroughly understands how Merkle trees work and why they are useful.
His question was about the use case. Let me try to answer his question, by making some assumptions about your intentions. Correct me if I'm wrong - I haven't read all details.
You want to parallellize block downloads, while at the same time preventing re-download of transactions that are already known.
To do so, a requesting node would first request (for example) the 8 level-3 hashes, then start 8 parallel threads to download the
transactions from presumably 8 different peers. Each thread then fetches the transaction id's that correspond to its own 1/8th of
the block, and requests the transactions whose txid is not yet known.
Comparing this with Gregory's own suggestion (just fetch the entire txid list at first, and then (again as parallellized as needed)
fetch the unknown transactions from several peers), this does indeed have an advantage: you need to download (relatively) far less
data before the threaded part can start. If this is what you propose, it is certainly meaningful, but the gains aren't very large,
in my opinion.
However, my impression while reading your proposal was at times that you intend to process the different layers of the
Merkle tree iteratively after starting the initial parallel segments. I don't think that is useful, as you'll need the actual
txids anyway before deciding whether they need to be downloaded at all, it adds several round-trips, and once you have downloaded
the intermediate merkle hashes for about 8 levels, you've already transferred more data than the transactions themselves. I think
Gregory also assumed something like this, making him question why it's useful. Anyway, this whole paragraph is one assumption, so
if it's not the case, ignore me.
Can you clarify what you mean? Preferably by giving a concrete sequence of exchanged messages, with a given purpose?
PS: the reason Satoshi used a Merkle tree for the transactions in a block was to allow a short piece of data (the hashes along a
path in the tree) to prove a transaction belongs to it - I doubt he intended it for parallellizing downloads (though it certainly
opens some opportunities here).

@_date: 2013-04-07 17:34:22
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Who is creating non-DER signatures? 
(cross-post from bitcointalk.org)
Hello all,
as some may know, Bitcoin uses DER-encoded signatures in its transactions.
However, OpenSSL (which is used to verify them) accepts more than just the
strict DER specification (it allows negative numbers, extra zero padding,
extra bytes at the end, and perhaps more). As we don't like the de-facto
specification of the Bitcoin block validity rules to depend on OpenSSL,
we're trying to introduce a rule to make such non-standard signatures
invalid. Obviously, that can't be done as long as any significant amount of
clients on the network is creating these.
I've monitored all transactions the past weeks (1.4M transactions), and it
seems 9641 of them contain at least one non-standard signature. See
 for a list of the top
addresses that had coins used as inputs in such transactions. If you
recognize any of these addresses, or have an idea of who owns them or what
software they are using, please let me know.

@_date: 2013-04-07 18:21:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Who is creating non-DER signatures? 
The majority (~90%) is negative R or S values (which are just interpreted as
unsigned by OpenSSL, but if the top byte has its highest bit set, it must be
preceeded by a 0x00 accordinging to DER). A smaller number uses excessively
padded R or S value (with a 0x00 in front when it's not necessary). Finally
there are 4 signatures with an incorrect length marker in the beginning
(which likely means they contain some garbage at the end).

@_date: 2013-04-13 23:43:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Who is creating non-DER signatures? 
Without significant effort, I don't think we're going to be able to get
that number down. I'd like to stress that without making these non-standard
encodings effectively invalid on the network, pretty much every full node
implementation needs to depend on OpenSSL to guarantee compatibility (and
that is hoping OpenSSL doesn't change its supported encodings), or make
assumptions about which deviations are allowed.
The next question, I guess, is at which transaction frequency it's
acceptable to move forward with this? The first step is definitely just not
accepting them into memory pools, but leaving them valid inside blocks.
Actual network rules will need to come later. However, even just not
accepting them into memory pools will it make very hard (if not impossible)
for the buggy clients that create transactions to get any confirmations.
I'm not sure... 0.6% isn't much, but 9600 transactions is.

@_date: 2013-04-15 13:51:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Who is creating non-DER signatures? 
A patch was just merged in git head to enforce strict encodings for accepting
transactions into the memory pool. As miners and other nodes don't upgrade
immediately (and 0.8.2 isn't even released yet), this means such transactions
will likely still make it into blocks, but will have an increasingly harder
time doing so.
When the rate of non-standard encodings in the block chain has dropped far
enough, we can attempt scheduling a soft forking change to make it required.
At that point, the network rules will no longer depend on OpenSSL's parsers.
As a summary, here are the rules now enforced for acceptance into the memory
* 0. These rules are only applied for _evaluated_ scripts, as there is no
     guaranteed way to know which data is supposed to be interpreted as a
     public key or signature before actually evaluating the script. This
     means that for example a 1-of-2 multisig can have an incorrectly-
     encoded public key, but still be redeemed if a valid (and correctly
     encoded) signature is given for the other key.
* 1. Public keys are either compressed (0x02 + 32 bytes, or 0x03 + 32 bytes)
     or uncompressed (0x04 + 64 bytes). The non-standard "hybrid" encoding
     supported by OpenSSL is not allowed.
* 2. Signatures are strictly DER-encoded (+ hashtype byte). The format is:
     0x30  0x02   0x02        * R and S are signed integers, encoded as a big-endian byte sequence.
       They are stored in as few bytes as possible (i.e., no 0x00 padding in
       front), except that a single 0x00 byte is needed and even required
       when the byte following it has its highest bit set, to prevent it
       from being interpreted as a negative number.
     * lenR and lenS are one byte, containing the length of the R and S
       records, respectively.
     * lenT is one byte, containing the length of the complete structure
       following it, starting from the 0x02, up to the S record. Thus, it
       must be equal to lenR + lenS + 4.
     * The hashtype is one byte, and is either 0x01, 0x02, 0x03, 0x81, 0x82
       or 0x83.
     * No padding is allowed before or after the hashtype byte, thus lenT
       is equal to the size of the whole signature minus 3.
* 3. These rules also apply to testnet.

@_date: 2013-04-28 17:51:55
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
Hello all,
I think it is time to move forward with pruning nodes, i.e. nodes that
fully validate and relay blocks and transactions, but which do not keep
(all) historic blocks around, and thus cannot be queried for these.
The biggest roadblock is making sure new and old nodes that start up are
able to find nodes to synchronize from. To help them find peers, I would
like to propose adding two extra service bits to the P2P protocol:
* NODE_VALIDATE: relay and validate blocks and transactions, but is only
guaranteed to answer getdata requests for (recently) relayed blocks and
transactions, and mempool transactions.
* NODE_BLOCKS_2016: can be queried for the last 2016 blocks, but without
guarantee for relaying/validating new blocks and transactions.
* NODE_NETWORK (which existed before) will imply NODE_VALIDATE and
guarantee availability of all historic blocks.
The idea is to separate the different responsibilities of network nodes
into separate bits, so they can - at some point - be
implemented independently. Perhaps we want more than just one degree (2016
blocks), maybe also 144 or 210000, but those can be added later if
necessary. I monitored the frequency of block depths requested from my
public node, and got this frequency distribution:
 so it seems 2016 nicely matches the
set of frequently-requested blocks (indicating that few nodes are offline
for more than 2 weeks consecutively.
I'll write a BIP to formalize this, but wanted to get an idea of how much
support there is for a change like this.

@_date: 2013-04-28 18:44:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
Sure, that's why eventually several levels may be useful.
Adding new fields to the addr message and relaying those fields to newer
That's a more flexible model, indeed. I'm not sure how important speed of
propagation will be though - it may be very slow, given that there are
100000s of IPs circulating, and only a few are relayed in one go between
nodes. Even then, I'd like to see the "relay/validation" responsibility
split off from the "serve historic data" one, and have separate service
bits for those.
Disconnecting in case something is requested that isn't served seems like
an acceptable behaviour, yes. A specific message indicating data is pruned
may be more flexible, but more complex to handle too.
What is the use case for NODE_VALIDATE? Nodes that throw away blocks almost
NODE_VALIDATE doesn't say anything about which blocks are available, it
just means it relays and validates (and thus is not an SPV node). It can be
combined with NODE_BLOCKS_2016 if those blocks are also served.
The reason for splitting them is that I think over time these may be
handled by different implementations. You could have stupid
storage/bandwidth nodes that just keep the blockchain around, and others
that validate it. Even if that doesn't happen implementation-wise, I think
these are sufficiently independent functions to start thinking about them
as such.

@_date: 2013-08-07 22:31:45
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
I don't like the wording for payment_uri "where the payment _may_ be
sent to obtain a paymentACK", or the fact that in the diagram it is
the client wallet broadcasting the transaction to the network.
In my opinion, it should ultimately become the responsibility of the
receiver to get the transaction confirmed. Of course, the sender may
help, and if the transaction does not confirm, no payment took place.
But one of the advantages direct negotiation offers, is that the
sender wallet does not need to remain online anymore to get the
transaction broadcast. I don't think it should be even required that
the sender wallet is connected to the P2P network at all. All they
need to do is construct a satisfactory transaction, and send it to the
merchant who cares about it.
I would suggest the wording, "if a payment_uri is specified, the
wallet application should try - and if necessary, retry - to submit
the transaction there, resulting in a paymentACK from the merchant.
Broadcasting the transaction on the P2P network is optional.". Perhaps
we should even discourage broadcasting before the paymentACK is
obtained, to make sure the merchant received it, together with the
metadata, to decrease the chances of money arriving at a merchant
without metadata (to minimize the cases where manual intervention is

@_date: 2013-08-07 23:36:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
My concerns here are:
* Making sure wallet applications can function without supporting the
P2P protocol (which drops a huge implementation overhead for simple -
perhaps hardware-based - wallets).
* Making sure the responsibility of confirming transactions is with
the receiver (while the responsibility of creating a confirmable
transaction is with the sender), which again simplifies wallet
* Making receiving of metadata reliable, by minimizing cases where a
transaction is accidentally broadcast without the receiver being told
about it. This is perhaps not possible entirely, but it should be
possible to reduce it to a point where the remaining cases can be
dealt with manually. This also means indeed being able to give a
bitcoin URI (or why not just a URL to a payment descriptor?) that does
not contain a static Bitcoin address. I understand the compatibility
concern here, but IMHO we should do all effort to get rid of static
addresses were possible - the public key should be negotiated be
sender and receiver.
I worry about the scenario where some evil hotspot owner observes a
payment request, and later sees a bitcoin P2P transaction crediting
that key, but without payment being sent to the payment_uri (because
he blocked it), thus allowing him to construct a payment message
himself with the request + transaction, and adding his own refund
address or delivery location, or ... To fix problems related to this
completely, we'd need transactions that commit to the payment message,
instead of the other way around. I believe the pay-to-contract scheme
presented by Timo Hanke at the San Jose conference solved this.

@_date: 2013-08-07 23:49:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
By connecting to some other client, presumably. Have a small hardware
client that is able to do payments via NFC/QR/... directly with a
merchant, and can get 'recharged' by connecting with your desktop
client, for example. Maybe too futuristic to be a concern, but it
nicely illustrates how doing direct sender-to-receiver negotiation can
help decoupling tasks.

@_date: 2013-08-07 23:54:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
I see payment URIs rather as a replacement for bitcoin: URI rather
than an extension. It solves everything they did in a much cleaner
way, Given that bitcoin: have gotten some traction, we probably want
to reuse the moniker, but replace the rest entirely. In other words,
if a request is specified, nothing else should be.
There is just no useful way to combine them either - payments
generalize destinations to arbitrary scripts, messages are handled
inline, amounts are defined inline. And if you want to rely on the
payment infrastructure to work, you cannot risk people using the
old-style static address in the URI.

@_date: 2013-08-08 16:13:17
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment Protocol: BIP 70, 71, 72 
Sounds good.
I'd still like to see some effort to avoid losing metadata and
reducing the responsibilities of the sender.
I see there's an implementation difficulty in avoiding to broadcast a
transaction, but for example, if a payment_uri is specified, and it
cannot be contacted (at all), the transaction should fail. As soon as
you manage to connect, and have at least attempted to submit the
transaction, the merchant may have received it, and you want to mark
the coins spent, so store it after that point. But without such
protection we'll likely see a unnecessary payments happening without
metadata, when the payment server cannot be contacted for some reason.
Also, the receiver most certainly needs a P2P implementation (and
probably a strongly validating one) to verify incoming transactions,
so having him broadcast it shouldn't be hard. I don't think the client
should be required to stay online to broadcast at all, after a
paymentACK is received. The transaction arrived safely at that point.

@_date: 2013-08-15 10:37:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version 0.9 goals 
Just to make sure nobody expects a magic bullet: this was on a hexacore Xeon
CPU, with several GB of -dbcache, libsecp256k1 for verification, and a very good
network connection. It is repeatable and from random network peers, though. The
code is here:
  It's usable and seems to be stable (including reindexing, which needs support for
block files with out-of-order blocks now), but I still want to clean some
things up before pullreq'in. There are probably some heuristic tweaks
possible as well - Gregory found that performance is reduced for the first
part of the chain on high-latency networks.
I believe we do need some wider support than just Bitcoin-Qt, indeed, as
the number of people actually using the reference client as a wallet is
quite low now. Ideally, several clients and merchants start support for it
in a short timeframe...

@_date: 2013-08-15 12:56:25
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version 0.9 goals 
As far as I can see, that state is gone, and is now passed in a
separate object to the transaction-creation methods.
I'd like to see it go in, as I believe it can be helpful in
understanding the difference between the high-level abstraction
(wallet) and the underlying implementation (individual coins) -
something that many people are confused about. I think that's even a
more important advantage than the ability for micro-management it
offers. Multiwallet would be more appropriate for avoiding linkage
between identities, but it seems there's little progress on that front

@_date: 2013-08-17 16:15:31
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Combining bloom filters? 
If both constructed bloom filters use the same seed and the same number of
hash functions, yes. Assuming the input filters were optimal for a given FP
rate, the resulting filter will be worse.

@_date: 2013-08-17 22:53:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] LevelDB in master 
Thanks for investigating this. I guess it's my fault for not checking
the diff before the final merge. I guess the simultaneous switch to a
git-subtree'd leveldb made it harder to review.
In any case, the changes seem harmless, but I think we should revert
to a codebase as close as possible to upstream LevelDB 1.12. The diff
you have between bitcoin head and bitcoin-up shows a few reverted
patches that we included during 0.9's merge window, a patch by ripple
to add a compaction delay (which they seem to have reverted now too)
and one weird ripple-specific commit (which just seems to remove
I've put a cleaned-up history of the LevelDB subtree in the
 repository (branch bitcoin-fork),
and then used git-subtree to create a pull request ( which
switches our src/leveldb directory to this tree. It correctly lists
the reverted (and sometimes re-applied) changes in the squashed commit
(please review!). The actual diff corresponds to the diff you
produced, with the reverted changes in our repository re-applied.

@_date: 2013-08-19 22:14:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposal: remove "getwork" RPC from 
They were addressed and fixed in a successor API, getblocktemplate.
It's even more decentralization-friendly, as it allows the caller to
see what transactions the daemon is trying to put into a block, and
even modify it.
The suggestion here is not to remove functionality - only to remove an
obsolete API for doing so.

@_date: 2013-12-16 12:31:11
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Fees UI warning 
Will that also mean no longer reusing (change) addresses?

@_date: 2013-01-27 17:18:43
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Testnet DNS seed 
It has information about all non-banned IPs that were ever connected to
succesfully. The connectability% is a decaying average, with a time constant of up to a month, though.
I have a script ready which combines one or more seeds.txt's data into a
pnSeed[] array C source code, by the way. I'll add it to the repository.

@_date: 2013-07-14 21:28:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Reward for P2SH IsStandard() patch. 
Small comment: the current implementation in the reference client uses a custom
script encoder for the UTXO database, which stores every (valid) send-to-pubkey
as 33 bytes and every send-to-pubkeyhash or send-to-scripthash as 21 bytes.
So for "standard" address payment, there is no storage impact of using P2SH

@_date: 2013-07-14 21:42:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] libzerocoin released, 
I don't think that's what John means.
If you have hash power for the parent chain, mining invalid blocks for the
merge-mined chain costs you nothing. Yes, they will be invalid, but you've
lost nothing.
The basic assumption underlying mining security is that it is more profitable
to collaborate with mining a chain (and profit from the block payout) than to
attack it. In the case of merged mining, this assumption is not valid.

@_date: 2013-07-21 17:55:14
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Introducing BitcoinKit.framework 
I'm currently working on headers-first sync, which I believe is generally
very useful (it fixes tons of edge-cases block synchronization currently
experiences), but it's also a first step towards SPV mode.
So headers-first sync means you first synchronize just the headers, and then,
when you already know (or have strong evidence for a guess on) the best chain,
start requesting blocks along that best chain - potentially in parallel from
different peers.
SPV mode is basically headers-first sync, but never do the full block sync
step, and replace it with a bloom/birthday/...-based fetching of blocks
interesting to the associated wallets. In SPV you'll also need to disable
the mempool though, and there will be more small changes, but I think
the separate headers-sync phase will be most of the work.

@_date: 2013-07-22 16:44:59
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] Standard for private keys with birth 
I should have brought up this suggestion before, as there seems to be relevant other work.
I'd like to propose encoding keys data (whatever type) with a birth timestamp as:
 * @
The reason for not incorporating this inside the key serialization (for example BIP32), is because
birth timestamps are more generally a property of an address, rather than the key it is derived from.
For one, it is useful for non-extended standard serialized private keys, but for P2SH addresses,
the "private key" is really the actual scriptPubKey, but birth data is equally useful for this.
Reason for choosing the ' character: it's not present in the base58, hex, or base64 encodings that
are typically used for key/script data.
One downside is that this means no checksum-protection for the timestamp, but the advantage is
increased genericity. It's also longer than using a binary encoding, but this is an optional
part anyway, and I think "human typing" is already fairly hard anyway.

@_date: 2013-07-23 11:37:59
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
Depends what you mean by expose.
Maintaining an address/script-indexed UTXO is generally useful, in
particular for things like sweeping addresses. I certainly have
less problems with 'exposing' this than exposing a fully-indexed
block chain history.
However, and I expect that's what your question is about, this isn't
really useful for SPV (or less) nodes, as there is no way to
authenticate this data. If you can fake a UTXO entry, you can make
a peer believe anything about their balance, potentially resulting
in creating a valid transaction that sends change it didn't know
was there as fee to miners. Other than for normal block chain data,
there is no way to detect this without at least partial validation.
The only way to do this safely at an SPV security assumption, is by
having an address-indexed committed merkle UTXO-set tree, like the
one proposed by Alan Reiner, and being implemented by Mark
Friedenback. I know Michael Gronager has something similar implemented,
but I don't know whether it is script-indexed. To be actually useful,
it likely needs to be enforced by miners - putting a significant
burden on validation nodes. Still, if it can be done efficiently,
I think this would be worth it, but more research is needed first in
any case.
Regarding sweeping keys in the first place - I think using those,
and relying on address-indexed UTXO sets or blockchains to import
them, is an idea that doesn't scale very well in the first place.
If it is for things like scratch card or physical coins, with a
pre-set value, the obvious solution IMHO is storing the crediting
transaction with its merkle path together with the key. If that's
not possible, just the txid:vout of the credit output can suffice.
Yes, that's more data than is necessary now, but it's so much more
trivial to use.

@_date: 2013-07-23 11:42:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
There is actually no such index being maintained by default, and doing so is an
unnecessary burden IMHO (you need to enable -txindex since 0.8 to get this).
Of course, if enabled, it can be exposed.

@_date: 2013-07-23 11:56:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
The block chain is not involved at all to verify transactions, it's
just a historical
record to serve to other nodes, and to do wallet rescans with.
For validation, a separate database with just unspent transaction
outputs is used (around 230 MB now).

@_date: 2013-07-23 12:06:24
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
No, not really.
The UTXO set is the state you need to validate blocks and
transactions. You can see blocks as authenticated patches to the UTXO
set (consumes some outputs, produces others). During validation, we
store "undo data", basically (non-authenticated) reverse patches to
the UTXO set, so we can walk back in case of a reorganization.

@_date: 2013-07-23 12:19:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
In what way?
A partial blockchain is quite useless, as you can't build an UTXO set from it.
If you're talking simply about the storage requirements for maintaining history,
perhaps, but why rely on SPV nodes for this? Right now, those don't store any
blocks at all, and there is no reason why they should.
The only requirement is that old blocks remain available for new full
nodes to be
able to bootstrap. It's certainly not required that everyone keeps
every block ever
created. That's how the software currently works, but as soon as we get to a few
protocol changes that would allow new full nodes to find specific
peers with the data
they need, we can have fully-verifying yet (partially) pruning nodes.
I think that's a
much better idea than conflating "maintaining a wallet" with
"maintaining a subset
of historical block data".
I disagree strongly here. The rules of the network are enforced by
full nodes, not by
miners - they are just trying to satisfy the rules demaned by the network.
And as far as I know, there is no way to do some "partial validation"
using just the blocks
you care about (and remember, SPV nodes currently have none at all).
One interesting
possibility here is fraud proofs, where the network can relay proofs
that certain blocks or
transactions are violating certain network rules. In this case, even
SPV nodes can just
communicate with eachother and get some "herd immunity". But storing some blocks
doesn't matter here - it is all about whether you can maintain the
UTXO set or not.
That's assuming there is no powerful enough attacker that can benefit from doing
a sybil attack on you. For SPV nodes currently, that risk is limited
to an attacker
that can spend enough on faking a chain with valid proof-of-work, to make you
accept a transaction that will be reversed.
If you go let SPV nodes rely on unauthenticated UTXO set data, there is no such
limitation, and they can let you believe *anything*. There are some safeguards,
like combining it with merkle paths for all outputs that credit you (which again
requires a more expensive index on the other side), but you can't ever guarantee
that a particular outputs isn't spent yet.

@_date: 2013-07-23 12:27:35
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
On Tue, Jul 23, 2013 at 12:17 PM, Andreas Schildbach
That means they value convenience more than the trust-freeness of a
decentralized solution. The only way to avoid that is by making sure
the decentralized one is convenient enough. But relying on
unauthenticated data itself is equally bad - it means you lose
whatever benefit the decentralization had.
The difference between script-indexed and address-indexed is
absolutely trivial compared to the effort needed to implement and
maintain such authenticated trees by all full nodes. Restricting
things at the network level (which doesn't even know about a thing
like an address) to address-based indexes is ridiculous IMHO.
Sure, once you introduce trust, a lot can be done. But it's not really
Bitcoin anymore in that case - it's relying on a third party to do the
heavy indexing for you. And if that is the best-scaling solution, sure
- but I don't think we should encourage that. Or at least, we should
first search for alternatives. And encourage infrastructure that
doesn't require it.
Yeah, those are inherent problems with how there are used today. But
today there is also little problem - the UTXO set is tiny.
Absolutely, though a slightly bigger one.

@_date: 2013-07-23 12:36:46
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] HTTP REST API for bitcoind 
On Tue, Jul 23, 2013 at 12:29 PM, Andreas Schildbach
I don't object to using a trusted server for this if people want that,
but I don't think the reference client should encourage this.
Apart from that, exposing this HTTP-based interface publicly has its
own problems, like security risks and potential DoS risks. If
anything, we should be reducing the attack surface rather than
increase it. IMHO, the only thing that should be exposed in the P2P
protocol, which is inevitable, and already has some DoS protections.
I like this HTTP interface, but it should really only be used for
trusted local applications and debugging.

@_date: 2013-07-24 00:33:47
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Linux packaging letter 
Just to make sure there are no misunderstandings, as far as I know there is
no reason to assume the reported problem (comment on  is:
1) a fork (it's an indeterministic and avoidable database corruption, it seems)
2) related to leveldb
3) reproducible by more than one person
4) debian's fault.
That said, I think reaching out to packagers to educate them about the risks
is a good idea - but let's not blame people before we understand our own

@_date: 2013-06-10 10:35:19
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposal: Vote on the blocksize limit 
This is perhaps the largest misconception that keeps being repeated.
Bitcoin is not a democracy; it is a zero-trust system. The rules are
set in stone, and every full node verifies all rules and must
independently come to the same result as everyone else. Obviously, if
everyone changes their software, anything can change, but from within
the system there is no way to change which blocks are considered
valid, and there is certainly no voting mechanism about that.
What is voted about, is the one single thing that cannot be decided by
each node individually: namely the order of otherwise valid and
non-conflicting transactions, and that's just because it's a
necessity. Because deciding the order includes delaying transaction
potentially indefinitely, a majority of miners can indeed choose the
enforce an additional rule about which transactions are considered
valid, but the rules implemented in full nodes do not change without
changing the software. For example, miners cannot decide to raise the
block subsidy, even if every single miner out there would want that.
They'd just end up being ignored by everyone else.
The problem is that at some point, you have to look at the system from
a higher level than just the technical part. And because ultimately
the possibility exists where everyone changes their software, and
there is an exceedingly high incentive for consensus (a deliberate
hard-fork where two groups of users decide to use different and
incompatible rules, aware of eachother, is suicide for the system, in
my opinion). This results in the fact that proposed changes can indeed
become new adopted hard rules in the system, and I don't think there's
anything that can be done about it. Bitcoin is a consensus system - at
the technical level - but also a consensus of the people using it, and
ultimately they decide the rules.
So you're saying that instead of a zero-trust system, we should move
to a system where miners can decide _everything_ - as opposed to just
being in charge of ordering transactions? I don't think you understand
the system at all, if that is what you're proposing.

@_date: 2013-06-11 16:12:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bitcoin addresses -- opaque or not 
I'm afraid this is the result of a misunderstanding.
Yesterday on IRC you were asking why the URI specification doesn't
include the semantics and encoding of addresses. Some people,
including me, argued that addresses should be considered opaque. That
doesn't mean they don't have well-specified definition, only that for
the purposes of URI parsing and handling, code shouldn't know or care
what they represent or how they are formatted. Addresses are specified
in one place, and the URI format simply passes addresses through.
The reason for keeping them independent is that the address format
could change (say, a new type is added, like P2SH (BIP13) before), and
there is no reason why this should break or even concern URI handling
code. Clearly, anything that actually interprets addresses in order to
construct transactions will need changing. It's just two separate
concerns, and they should be dealt with separately.

@_date: 2013-06-19 15:54:04
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Optional "wallet-linkable" address format 
Have you seen Timo Hanke's pay-to-contract presentation at the San Jose
conference? It seems very related:

@_date: 2013-06-20 11:06:50
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Missing fRelayTxes in version 
Actually, that is not the same issue. What is being argued for here is that
the version in the version message itself should indicate which fields are
present, so a parser doesn't need to look at the length of the message. That
seems like a minor but very reasonable request to me, and it's trivial to do.
That doesn't mean that you may receive versions higher than what you know of,
and thus messages with fields you don't know about. That doesn't matter, you
can just ignore them.
I see no problem with raising the protocol version number to indicate
"all fields up to fRelayTxes are required, if the announced nVersion is above N".
In fact, I believe (though haven't checked) all previous additions to the version
message were accompanied with a protocol version (then: client version) increase
as well.

@_date: 2013-06-20 12:52:51
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Missing fRelayTxes in version 
Let's just increase the version number and be done with this discussion.
It's a small benefit, but it simplifies things and it's trivial to do.
I don't understand how a policy of requiring version increases could limit
future extensions: after the version/verack exchange, the protocol version
is negotiated between peers, and there is no need for anything optional
Note thay this is just about parsing, not about relaying - you should still
relay parts of a message you haven't parsed. But that doesn't apply to the
version message anyway, which is the only place where this matters.

@_date: 2013-06-26 01:34:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bitcoin-Qt/bitcoind version 0.8.3 released 
Just like every release ever, and probably until we reach 1.0.

@_date: 2013-03-12 01:18:10
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large number 
Hello everyone,
?'ve just seen many reports of 0.7 nodes getting stuck around block 225430,
due to running out of lock entries in the BDB database. 0.8 nodes do not
seem to have a problem.
In any case, if you do not have this block:
  2013-03-12 00:00:10 SetBestChain: new
 height=225439  work=853779625563004076992  tx=14269257  date=2013-03-11
you're likely stuck. Check debug.log and db.log (look for 'Lock table is
out of available lock entries').
If this is a widespread problem, it is an emergency. We risk having
(several) forked chains with smaller blocks, which are accepted by 0.7
nodes. Can people contact pool operators to see which fork they are on?
Blockexplorer and blockchain.info seem to be stuck as well.
Immediate solution is upgrading to 0.8, or manually setting the number of
lock objects higher in your database. I'll follow up with more concrete
If you're unsure, please stop processing transactions.

@_date: 2013-03-12 02:01:35
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
Hello again,
block 000000000000015c50b165fcdd33556f8b44800c5298943ac70b112df480c023
(height=225430) seems indeed to have cause pre-0.8 and 0.8 nodes to fork
(at least mostly). Both chains are being mined on - the 0.8 one growing
After some emergency discussion on  it seems best to try to
get the majority mining power back on the "old" chain, that is, the one
which 0.7 accepts
(with 00000000000001c108384350f74090433e7fcf79a606b8e797f065b130575932 at
height 225430). That is the only chain every client out there will accept.
BTC Guild is switching to 0.7, so majority should abandon the 0.8 chain
Short advice: if you're a miner, please revert to 0.7 until we at least
understand exactly what causes this. If you're a merchant, and are on 0.8,
stop processing transactions until both sides have switched to the same
chain again. We'll see how to proceed afterwards.

@_date: 2013-03-12 12:44:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Warning: many 0.7 nodes break on large 
We're using "0.7" as a short moniker for all clients, but this was a limitation that all
BDB-based bitcoins ever had. The bug is simply a limit in the number of lock objects
that was reached.
It's ironic that 0.8 was supposed to solve all problems we had due to BDB (except the
wallet...), but now it seems it's still coming back to haunt us. I really hated telling
miners to go back to 0.7, given all efforts to make 0.8 signficantly more tolerable...
Right now, mempools are relatively small in memory usage, but with small block sizes,
it indeed risks going up. In 0.8, conflicting (=double spending) transactions in the
chain cause clearing the mempool of conflicts, so at least the mempool is bounded by
the size of the UTXO subset being spent. Dropping transactions from the memory pool
when they run out of space seems a correct solution. I'm less convinced about a
deterministic time-based rule, as that creates a double spending incentive at that
time, and a counter incentive to spam the network with your risking-to-be-cleared
transaction as well.
Regarding the block space, we've seen the pct% of one single block chain space consumer
grow simultaneously with the introduction of larger blocks, so I'm not actually convinced
there is right now a big need for larger blocks (note: right now). The competition for
block chain space is mostly an issue for client software which doesn't deal correctly
with non-confirming transactions, and misleading users. It's mostly a usability problem
now, but increasing block sizes isn't guaranteed to fix that; it may just make more
space for spam.
However, the presence of this bug, and the fact that a full solution is available (0.8),
probably helps achieving consensus fixing it (=a hardfork) is needed, and we should take
advantage of that. But please, let's not rush things...

@_date: 2013-03-13 18:58:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.8.1 ideas 
This is a hardfork: it means some nodes will have to accept blocks they formerly considered invalid.

@_date: 2013-03-13 19:28:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Blocksize and off-chain transactions 
The current behaviour in 0.7 and before is indeed broken, and we cannot afford to keep
that as an implicitly-defined rule on the network. I fully agree with that, but it will
require a hardfork.
But we cannot just drop support for old nodes. It is completely unreasonable to put the
_majority_ of the network on a fork, without even as much as a discussion about it.
"Oh, you didn't get the memo? The rules implemented in your client are outdated." - that
is not how Bitcoin works: the network defines the rules.
IMHO, the way to go is first get a 0.8.1 out that mimics the old behaviour - just as a
stopgap solution. That will allow miners to safely use 0.8-based code at least. We can
also get patches for 0.7 and previous branches that fix the lock limit issue, but enforce
the same limit as 0.8.1 does, simply as band-aid for those who do not yet wish to upgrade
to 0.8.
Finally, we'll have to schedule a hard fork to drop the 0.8.1 limit. This is something
that requires widespread community consensus - far more than just miners and developers -
but as this is about fixing a bug that would otherwise prevent most evolution, I hope it
can be a very non-controversial one. To that end, I would prefer to limit this hard fork
to only direct bug fixes, and leave the block limit issue for later. By now, I think it
is clear that a hard fork will be inevitable, and by doing one, I think we can learn a
lot that can make later ones easier.

@_date: 2013-03-13 19:38:25
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.8.1 ideas 
The protocol is whatever the network enforces - and that is some mix of versions of the
reference client right now, but doesn't need to remain that way.
I would very much like to have a text book of rules that is authorative, and every client
that follows it would be correct. Unfortunately, that is not how a consensus system works.
All (full) clients validate all rules, and all must independently come to the same
solution. Consensus is of utmost importance, more than some theoretical "correctness".
If we'd have a specification document, and it was discovered that a lot of
nodes on the network were doing something different than the document, those nodes would
be buggy, but it would be the specification that is wrong.
That is what happened: 0.7 and before had a bug, but 0.8 was wrong for not following the
rules of the network (which I hate to say, as I'm responsible for many changes in 0.8).
As said in another thread, the problem in the old versions needs fixing (this would even
be the case if no 0.8 existed at all, and no fork risk existed at all). But let's please
do it in a way we can all agree about, in a controlled fashion.

@_date: 2013-05-03 14:30:19
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
(generic comment on the discussion that spawned off: ideas about how to
allow additional protocols for block exchange are certainly interesting,
and in the long term we should certainly consider that. For now I'd like to
keep this about the more immediate way forward with making the P2P protocol
not break in the presence of pruning nodes)
Yes, I like that better than broadcasting the exact height starting at
which you serve (though I would put that information immediately in the
version announcement). I don't think we can rely on the addr broadcasting
mechanism for fast information exchange anyway. One more problem with this:
DNS seeds cannot convey this information (neither do they currently convey
service bits, but at least those can be indexed separately, and served
explicitly through asking for a specific subdomain or so).
So to summarize:
* Add a field to addr messages (after protocol number increase) that
maintains number of top blocks served)?
* Add a field to version message to announce the actual first block served?
* Add service bits to separately enable "relaying/verifying node" and
"serves (part of) the historic chain"? My original reason for suggesting
this was different, I think better compatibility with DNS seeds may be a
good reason for this. You could ask the seed first for a subset that at
least serves some part of the historic chain, until you hit a node that has
enough, and once caught up, ask for nodes that relay.
Disconnecting in case something is requested that isn't served seems like
I'm sure there will be cases where a new node connects based on outdated
information. I'm just stating that I agree with the generic policy of "if a
node requests something it should have known the peer doesn't serve, it is
fair to be disconnected."
Maybe it validates, maybe it doesn't. What matters is that it doesn't
guarantee relaying fresh blocks and transactions. Maybe it does validate,
maybe it just stores any blocks, and uses a validating node to know what to
announce as best chain, or it uses an SPV mechanism to determine that. Or
it only validates and relays blocks, but not transactions. My point is that
"serving historic data" and "relaying fresh data" are separate
responsibilities, and there's no need to require them to be combined.

@_date: 2013-05-06 15:13:55
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Service bits for pruned nodes 
I think John's actually has a point here. If we're judging the quality of a
protocol change by how compatible it is with DNS seeding, then we're clearly not
using DNS seeding as seeding anymore (=getting an entry point into the P2P
network), but as a mechanism for choosing (all) peers.
Eventually, I think it makes sense to move to a system where you get seeds from
a DNS (or other mechanism), connect to one or a few of the results, do a getaddr,
fill your peer IP database with it, and disconnect from the DNS seeded peer.
This probably means we need to look at ways to optimize current peer exchange,
but that's certainly welcome in any case.

@_date: 2013-05-07 15:19:50
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] minor bitcoin-qt gripes moving BTC off 
I think the general solution here is providing a feature-reach Python RPC client,
which can do things like remember passwords, command history/tab completion,
perhaps even batch lookups of compound commands (getblock $(getblockhash X, for
example, ...). The naive RPC client built into bitcoind is not a good fit for
many features, as they can much more efficiently be developed outside of the
core binary,
I'm quite opposed to any per-key fiddling in the GUI. This will inevitably lead
to (even more) people misunderstanding how wallets work and shooting themself in
the foot. I don't mind an expert mode ("coin control") that enables such features,
but in general, we should for entire-wallet export and import rather than
individual keys.
Import & sweep an address is something else, that sounds safe to.
This belongs in coin control, IMHO.

@_date: 2013-05-09 03:13:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
"Meh". I think it's highly unlikely we'll break the block header format, as it
pretty much means invalidating all mining hardware.
There's also no need: 32 bits is plenty of precision. Hell, even 16 bits would
do (assuming there's never more than a 65535s (about 18 hours) gap between two
blocks). Just assume the "full" 64-bit time is the smallest one that makes
sense, given its lower 32 bits.

@_date: 2013-05-09 13:12:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 32 vs 64-bit timestamp fields 
No, though that's also a possibility, but a backward-incompatible one.
What I mean is have a well-defined 64-bit timestamp for each block, but
only put the lowest 32 bit in the header. Under the condition:
* There is never a gap of more than 136 years between two blocks.
The actual 64-bit timestamp can be deterministically derived from the
header, by prefixing it with the lowest 32-bit value that does not
cause the result to violate the
at-least-above-the-median-of-the-previous-11-blocks rule.

@_date: 2013-05-21 03:56:57
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Double Spend Notification 
This has been suggested, but I know of no such decision having been made.

@_date: 2013-05-27 22:45:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP0032 
I think the current formulation in the BIP text is a bit confusing, as there
is both "public derivation" (namely: derivation that can be done using just
the public key), and the "public derivation function" (the one that takes
the public key as input). Any suggestion for better terminology is welcome.
One possibility is calling it type-1 and type-2 derivation, but that's only
enlightening if you of the origin of the concept.
There is current "test vector generation" code on the 'detwallet' branch on
my github repo, but this isn't useful for actual deterministic wallets.
I'm working on having an implementation that nicely integrates with the key
Also, there are already other implementations available, such as this Python
one  and Java code in Bits of Proof
(with whom the test vectors match, after finding a bug in mine...)
Of course, implementing a determinstic wallet is more than just key derivation.
There is dealing with detecting new keys/chains being used, lookahead, how to
use accounts (if at all), and internal/external subchains. I think this is
much more likely to differ more between different implementations, and perhaps
interesting applications

@_date: 2013-11-04 15:34:35
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Auto-generated miner backbone 
Doesn't that mean that by selective blocking these near-PoW headers,
you can bias peers into preferring to mine on those with near-PoW
headers, turning the attack around? Of course, because of their size,
headers are likely much harder to slow down (in propagation speed)
than full blocks...

@_date: 2013-11-07 16:00:01
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] On the optimal block size and why 
(I didn't have time to read your e-mail entirely yet, I'll do so later)
I believe that C. Decker's paper used measurements for propagation
delays for blocks 180000-190000, which happened between may and juli
2012. The latest bitcoind/bitcoin-qt release at the time was 0.6.3.
I'm sure the general patterns are valid, but if you're relying on
actual speed numbers, I believe they may be very different now. I
don't have numbers of course, but at least the changes 0.8 should
impact propagation significantly. Some changes merged in git head (to
become 0.9) could improve things further. If we're talking about
long-term scalability, we should base decisions on the best technology
available, at least.

@_date: 2013-11-07 16:19:24
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] On the optimal block size and why 
Correcting myself:
They did use data from blocks 20000-210000, september-november 2012.
That was still before the 0.8 release, however.

@_date: 2013-10-11 13:41:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 0.8.5 with libsecp256k1 
The current idea is to provide a compile-time flag to enable it, which
at the same time disables the wallet and mining RPCs. In that state,
it should be safe enough to provide test builds.
I'm pretty sure that libsecp256k1 supports every signature that
OpenSSL supports, so that direction is likely covered. The other
direction - the fact that libsecp256k1 potentially supports more than
OpenSSL - is only a problem if a majority of the hash power would be
running on it. However, with canonical encodings enforced by recent
relaying nodes, I hope that by then we're able to schedule a softfork
and require them inside blocks.
Apart from that, there is of course the issue that there may be actual
exploitable mistakes in the crypto code. There are unit tests,
including ones that create signatures with libsecp256k1 and verify
them using OpenSSL and the other way around, but errors are certainly
more likely to occur in edge cases that you don't hit with randomized
tests. The only way to catch those is review I suppose. I certainly
welcome people looking at it - even if just to get comments like "Can
you add an explanation for why this works?".

@_date: 2013-10-23 23:07:18
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Revisiting the BIPS process, a proposal 
Indeed, I know of few people who are familiar with the source code
that use the wiki.
I do think that is a pity. The openness and transparency of the
protocol is essential to trusting the system (and shouldn't be limited
to those digging through the source code), and for that reason alone I
think it needs to be well-documented.
I also do agree with earlier comments, that due to the nature of the
consensus problem Bitcoin solves, it will always be the network that
dictates what the actual rules are - anything else can result in
inresolvable forks. If a "formal" specification were written, and we
would find out that the majority of nodes on the network deviate from
it in a subtle way, those nodes would be buggy in the sense that they
aren't doing what was expected, but it would be the specification that
is incorrect for not following the rules of the network. In short,
consistency is more important than correctness, and for that reason,
writing alternate implementation will always be hard and dangerous.
However, I do not think that making it hard to find information about
the details of the system is the way to go. Alternate implementations
are likely inevitable, and in the long run probably a win for the
ecosystem. If effort is put into accurately describing the rules, it
should indeed carry a strong notice about it being descriptive rather
than normative.
If someone is willing to work on that, I am (and likely many people in
 are) available for any questions about the protocol and
its semantics.

@_date: 2013-10-24 21:23:51
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP0039 Mnemonic code for generating 
This is probably too late in the discussion, and I certainly don't
want to derail any standard being formed. But if it is controversial,
I want to offer my own suggestion.
This is a proposal I wrote a year ago, but never spent enough work to
push it as a standard:
It needs some work, but I believe it may be a base for a superior
system than what is being proposed here. As the scheme linked above
has built-in configurable difficulty and checksums, the word set being
used doesn't need to function for checking anymore. You could use any
dictionary/language/text generator, and feed it into the system - the
software on the other side doesn't need to use the same dictionary.
The disadvantage is of course that it cannot encode arbitrary data -
it can only be used to generate a random seed. It does have some
theoretical advantages, though (see link).

@_date: 2013-10-26 23:30:37
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposal to replace BIP0039 
Let's first try to agree on what we are solving.
It seems that Thomas wants - in addition to the cryptographic data -
to encode the tree structure, or at least version information about
what features are used in it, inside the seed.
I'm not sure whether we're ready to standardize on something like that
yet, not having established best practices regarding different wallet
structures. I do think we first need to see what possibilities and
developments are possible related to it.
In addition, information about the wallet structure is strictly less
secret than the key data - it is even less confidential than address
book data, transaction annotations, labels and comments and
bookkeeping information. It could be backed up anywhere and everywhere
without any repercussions, as far as I can see. I understand that in
the case of Electrum, there is a strong reason to want this
encapsulated together with the seed, but it's not really a requirement
for most wallets.
(if really needed, any key derivation scheme that starts from random
strings can be augmented with metadata by enforcing property-bits on a
hash of the string (so not of the output), as this data doesn't need
protection from brute-forcing).
Regarding other requirements, I wonder why we want the transformation
to be bidirectional? If it is just about generating master seeds, one
direction should be enough, and allows far nicer properties w.r.t.
security. If we do settle on a standard for 'brainwallets', I would
strongly prefer if it had at least some strengthening built-in, to
decrease the impact of worst-case situations.
If the reason is backward-compatibility, I think that any client that
supports seeds already can just keep supporting whatever they
supported before. Only if it matches both encoding schemes (as
mentioned before) there is a potential collision (and in that case,
the user could just be asked).

@_date: 2013-10-28 04:02:18
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Feedback requested: "reject" p2p message 
Categories that make sense to me:
1) protocol related problems
1.a) failed to deserialize transaction
2) core principle violations
2.a) script evaluation fail (only owner is allowed to spend)
2.b) outputs larger than inputs (no creation of new money)
2.c) outputs not found/already spent (no double spending)
3) policy rules
3.a) not standard
3.b) ...

@_date: 2014-04-01 21:00:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
Hi all,
I understand this is a controversial proposal, but bear with me please.
I believe we cannot accept the current subsidy schedule anymore, so I
wrote a small draft BIP with a proposal to turn Bitcoin into a
limited-supply currency. Dogecoin has already shown how easy such
changes are, so I consider this a worthwhile idea to be explored.
The text can be found here: Please comment!

@_date: 2014-04-01 22:53:37
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Finite monetary supply for Bitcoin 
Interesting idea, but perhaps we can keep that change for a future
hard fork, as Matt suggested? That means it could be implemented much
more concisely too.
Mike, I'm sad to hear you feel that way. I'll move your name in the
document from ACKnowledgements to NAKnowledgements.
As this is a relatively urgent matter - we risk forks within 250 years
otherwise, I'd like to move this forward quickly.
In case there are no further objections (excluding from people who
disagree with me), I'd like to request a BIP number for this. Any
number is fine, I guess, as long as it's finite.

@_date: 2014-04-07 14:26:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Why are we bleeding nodes? 
In my opinion, the number of full nodes doesn't matter (as long as
it's enough to satisfy demand by other nodes).
What matters is how hard it is to run one. If someone is interesting
in verifying that nobody is cheating on the network, can they, and can
they without significant investment? Whether they actually will
depends also no how interesting the currency and its digital transfers
My own network crawler (which feeds my DNS seeder) hasn't seen any
significant drop that I remember, but I don't have actual logs. It's
seeing around 6000 "well reachable nodes" currently, which is the
highest number I've ever seen (though it's been around 6000 for quite
a while now).

@_date: 2014-04-08 15:18:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
I still don't understand the purpose of cointype. If you don't want to
risk reusing the same keys across different currencies, just don't use
the same seed or the same account? That is purely a client-side issue.
If the consensus is to add the cointype anyway, can we fix it to be
equal to the 4-byte magic in the serialization (after setting the high
bit to true)? That way there aren't two 4-byte magic codes that need
to be defined for each, and at the same time make it obvious from the
serialized form what it is for.

@_date: 2014-04-08 15:53:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
I see the cause of our disagreement now.
You actually want to share a single BIP32 tree across different
currency types, but do it in a way that guarantees that they never use
the same keys.
I would have expected that different chains would use independent
chains, and have serializations encode which chain they belong to.
Let me offer an alternative suggestion, which is compatible with the
original default BIP32 structure:
* You can use one seed across different chains, but the master nodes
are separate.
* To derive the master node from the seed, the key string "Bitcoin
seed" is replaced by something chain-specific.
* Every encoded node (including master nodes) has a chain-specific
serialization magic.
This is in practice almost the same as your suggestion, except that
the m/cointype' in m/cointype'/account'/change/n is replaced by
different masters. The only disadvantage I see is that you do not have
a way to encode the "super master" that is the parent of all
chain-specific masters. You can - and with the same security
properties - encode the seed, though.

@_date: 2014-04-10 13:32:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bitcoind-in-background mode for SPV 
There were earlier discussions.
The two ideas were either using one or a few service bits to indicate
availability of blocks, or to extend addr messages with some flags to
indicate this information.
I wonder whether we can't have a hybrid: bits to indicate general
degree of availability of blocks (none, only recent, everything), but
indicate actual availability only upon actually connecting (through a
"version" extension, or - preferably - a separate message). Reason is
that the actual blocks available are likely to change frequently (if
you keep the last week of blocks, a 3-day old addr entry will have
quite outdated information), and not that important to actual peer
selection - only to drive the decision which blocks to ask after

@_date: 2014-04-10 18:59:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Chain pruning 
As this is a suggestion that I think I've seen come up once a month
for the past 3 years, let's try to answer it thoroughly.
The actual "state" of the blockchain is the UTXO set (stored in
chainstate/ by the reference client). It's the set of all unspent
transaction outputs at the currently active point in the block chain.
It is all you need for validating future blocks.
The problem is, you can't just give someone the UTXO set and expect
them to trust it, as there is no way to prove that it was the result
of processing the actual blocks.
As Bitcoin's full node uses a "zero trust" model, where (apart from
one detail: the order of otherwise valid transactions) it never
assumes any data received from the outside it valid, it HAS to see the
previous blocks in order to establish the validity of the current UTXO
set. This is what initial block syncing does. Nothing but the actual
blocks can provide this data, and it is why the actual blocks need to
be available. It does not require everyone to have all blocks, though
- they just need to have seen them during processing.
A related, but not identical evolution is merkle UTXO commitments.
This means that we shape the UTXO set as a merkle tree, compute its
root after every block, and require that the block commits to this
root hash (by putting it in the coinbase, for example). This means a
full node can copy the chain state from someone else, and check that
its hash matches what the block chain commits to. It's important to
note that this is a strict reduction in security: we're now trusting
that the longest chain (with most proof of work) commits to a valid
UTXO set (at some point in the past).
In essence, combining both ideas means you get "superblocks" (the UTXO
set is essentially the summary of the result of all past blocks), in a
way that is less-than-currently-but-perhaps-still-acceptably-validated.

@_date: 2014-04-10 20:32:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Chain pruning 
A 51% attack can make you believe you were paid, while you weren't.
Full node security right now validates everything - there is no way
you can ever be made to believe something invalid. The only attacks
against it are about which version of valid history eventually gets
If you trust hashrate for determining which UTXO set is valid, a 51%
attack becomes worse in that you can be made to believe a version of
history which is in fact invalid.

@_date: 2014-04-10 22:29:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Chain pruning 
I'm aware of fraud proofs, and they're a very cool idea. They allow
you to leverage some "herd immunity" in the system (assuming you'll be
told about invalid data you received without actually validating it).
However, they are certainly not the same thing as zero trust security
a fully validating node offers.
For example, a sybil attack that hides the actual best chain + fraud
proofs from you, plus being fed a chain that commits to an invalid
UTXO set.
There are many ideas that make attacks harder, and they're probably
good ideas to deploy, but there is little that achieves the security
of a full node. (well, perhaps a zero-knowledge proof of having run
the validation code against the claimed chain tip to produce the known
UTXO set...).

@_date: 2014-04-15 17:11:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bug in 2-of-3 transaction signing in 
The first input seems to be already spent by another transaction
(which looks very similar).
0.9 should report a more detailed reason for rejection, by the way.

@_date: 2014-04-16 17:20:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Warning message when running wallet in 
Windows XP is no longer maintained. Don't use such a system for
protecting your money.
Microsoft: The suggestion here is to make Bitcoin Core detect when it's running
on Windows XP, and warn the user (they are likely unaware of the

@_date: 2014-04-17 00:00:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Warning message when running wallet in 
Or, even accidentally, cause a hard forking bug to be rolled out (or
worsen one).

@_date: 2014-04-20 16:53:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] "bits": Unit of account 
I told him specifically to bring it here (on a pull request for
Bitcoin Core), as there is no point in making such convention changes
to just one client.
I wasn't aware of any discussion about the "bits" proposal here before.

@_date: 2014-04-21 07:41:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] "bits": Unit of account 
usage I.e. bit.
What units will be called colloquially is not something developers will
determine. It will vary, depend on language and culture, and is not
relevant to this discussion in my opinion.
It may well be that people in some geographic or language area will end up
(or for a while) calling 1e-06 BTC "bits". That's fine, but using that as
"official" name in software would be very strange and potentially confusing
in my opinion. As mentioned by others, that would seem to me like calling
dollars "bucks" in bank software. Nobody seems to have a problem with
having colloquial names, but "US dollar" or "euro" are far less ambiguous
than "bit". I think we need a more distinctive name.

@_date: 2014-04-23 17:41:26
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Coinbase reallocation to discourage 
We have this awesome technology that solves the double-spending
problem. It's called a blockchain. Of course, it only works when
transactions are actually in a block.
This issue is about double-spending preventing before they're
confirmed. This is (and has always been) just a best-effort mechanism
in the network.
That is what is being proposed here, by introducing a mechanism where
miners can vote to penalize other miners if they seem to allow (too
many?) double spends.
Not very relevant.

@_date: 2014-04-23 19:42:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
Storing the seed is superior to storing the master node already
(whether coin specific or not), as it is smaller.
All this changes is making the seed the "super master" which allows
generating the coin-specific masters (which get an actual useful
function: revealing your entire-tree, but only one coin's subset of
Fair enough, it would break strictly BIP32. Then again, BIP32 is a
*Bitcoin* improvement proposal, and not something that necessarily
applies to other coins (they can adopt it of course, I don't care).
What I dislike is that this removes the ability of using the magic in
the serialization to prevent importing a chain from the wrong coin.
The standard could just say that instead of "Bitcoin seed", you'd use
"Coin seed: " + magic, so you don't need an extra mapping from
cointype to seed strings.

@_date: 2014-04-23 22:08:18
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
That's the point. BIP64 specifies such a structure, and you have to scan
all that it defines.
If you want to write wallet software that does not have the complexity to
deal with just one account, it is not BIP64 compliant. It could try to
define its own purpose system, with a hierarchy without accounts in it. I'm
not sure this is a very interesting use case, but I like how strict it is.
Construction of related chains for multisig addresses is perhaps a better
example of a different purpose structure.

@_date: 2014-04-23 22:12:37
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
I believe Luke means scanning chains as defined by the structure, but not
handing out addresses from other accounts than the first one.
That's certainly a possibly way to compatibly implement BIP64, but it
doesn't reduce all that much complexity. I hope people would choose that
over defining their own accountless structure though.

@_date: 2014-04-23 22:54:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
Would you consider software which scans all accounts as specified by
BIP64, but has no user interface option to distinguish them in any
way, view them independently, and has no ability to keep the coins
apart... compatible with BIP64?
According to the argument here mentioned earlier ("all or nothing"),
it is, as it will not break interoperability with other BIP64
software. Still, it doesn't support the accounts feature, and perhaps
that's fine?

@_date: 2014-04-23 23:42:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
In that case, maybe it makes sense to define another purpose id
without accounts as well already.
I believe many simple wallets will find multiple subwallets too
burdening for the user experience, or not worth the technical

@_date: 2014-04-24 09:10:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
To clarify:
BIP64 has a much stricter definition for accounts than BIP32.
In BIP32, it is not well specified what accounts are used for. They
can be used for "subwallets", "receive accounts" (as in bitcoind's
account feature), "recurring payments", part of a chain used as
multisig addresses, ... determined individually for each index.
In BIP64, they are strictly used for subwallets, and can't be used by
anything else.

@_date: 2014-04-26 15:41:56
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP32 "wallet structure" in use? Remove 
I agree. Especially when multiple chains are combined (multisig) for
P2SH usage, defining things like a gap limit becomes impossible
without knowing some metadata.
However, perhaps it is possible to define something like "BIP44
import-compatible", meaning that the application doesn't actually
support all of BIP44 features, but does guarantee not losing any funds
when imported? Similar things could be done for other purpose types.

@_date: 2014-08-10 20:07:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Synchronization: 19.5 % orphaned blocks 
This is actually an independent problem (though something to be aware
of). Flaky hardware can make synchronization fail completely - as it
relies on being able to exactly assess the validity of everything in
the blockchain.
Orphan blocks during synchronization are unfortunately very common,
and the result of a mostly broken download logic in the client. They
are blocks that are further ahead in the chain than the point where
you're currently synchronized to, and thus can't be validated yet.
Note that 'orphan' here means 'we do not know the parent'; it doesn't
just mean 'not in the main chain'. They are blocks that are received
out of order.
As Jeff mentions, headers-first synchronization fixes this problem
(and many other download-logic related things), by first verifying the
headers in the chain (thus already having partially validated
everything), and then downloading the blocks (in not necessarily the
right order) anymore, from multiple peers in parallel. There is
currently a pull request for it, but it's not production ready
Yes and no. While you're still synchronization, and don't actually
know the best chain, a peer could send you stale branches (with valid
proof of work), which you would accept, store and process. But it has
to be done very early, as once you learn of a good-enough chain, a
branch with more proof of work would be requires due to some
heuristics designed to exactly prevent such an attack.

@_date: 2014-08-18 19:35:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Outbound connections rotation 
Yes, I believe peer rotation is useful, but not for privacy - just for
improving the network's internal knowledge.
I haven't looked at the implementation yet, but how I imagined it would be
every X minutes you attempt a new outgoing connection, even if you're
already at the outbound limit. Then, if a connection attempt succeeds,
another connection (according to some scoring system) is replaced by it.
Given such a mechanism, plus reasonable assurances that better connections
survive for a longer time, I have no problem with rotating every few

@_date: 2014-08-23 13:38:09
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Reconsidering github 
Note that we're generally aiming (though not yet enforcing) to have
merges done through the github-merge tool, which performs the merge
locally, shows the resulting diff, compares it with the merge done by
github, and GnuPG signs it.
That allows using github as easy-access mechanism for people to
contribute and inspect, while having a higher security standard for
the actual changes done to master.

@_date: 2014-12-15 19:10:08
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Recent EvalScript() changes mean 
I agree, and I was thinking earlier that some rebasing would be needed
for CLTV when the change was made. I think this is a good thing
though:  introduced a clear separation between the script
evaluation code and what it can access out of its environment (the
transaction being verified). As CLTV changes the amount available out
of the environment, this indeed requires changing the interface.
Done. See  for a rebased
version of the BIP65 code on top of 0.10 and master. I haven't ported
any tests you may have that are not in the BIP, to avoid doing double
work. Those should apply cleanly. There is a less clean version (IMHO)
with smaller code changes wrt to the BIP code in my 'cltv' branch too.
I fully agree that we shouldn't be taking unnecessary risks when
changing consensus code. For example, I closed  (which I would
very much have liked as a code improvement) when realizing the risks.
That said, I don't believe we are at a point where we can just freeze
anything that touches consensus-related, and sometimes refactorings
are necessary. In particular,  introduced separation between a
very fundamental part of consensus logic (script logic) and an
optional optimization for it (caching). If we ever want to get to a
separate consensus code tree or repository, possibly with more strict
reviews, I think changes like this are inevitable.
I'm sorry to hear that that, and I do understand that many code
movements make this harder. If this is a concern shared by many
people, we can always decide to roll back some refactorings in the
0.10 branch. On the other hand, we don't even have release candidates
yet (which are a pretty important part of the testing and reviewing
process), and doing so would delay things further. 0.10 has many very
significant improvements which are beneficial to the network too,
which I'm sure you're aware of.
It's perfectly reasonable that not everyone has the same bandwidth
available to keep up with changes, and perhaps that means slowing
things down. Again, I don't want to say "this was reviewed before, we
can't go back to this" - but did you really need 3 months to realize
this change? I also see that elsewhere you're complaining about of yours which hasn't made it in yet - after less than 2 weeks. Yes, I
like the change, and I will review it. Surely you are not arguing it
can be merged without decent review?
I have been very much in favor of a libconsensus library, and for
several reasons. It's a step towards separating out the
consensus-critical parts from optional pieces of the codebase, and it
is a step towards avoiding the "reimplementing consensus code is very
dangerous! ... but we really don't have a way to allow you to reuse
the existing code either" argument. It does not fully accomplish
either of those goals, but gradual steps with time to let changes
mature in between are nice.
If at this point the consensus code was nicely separated, I would
argue to *copy* it (despite all normal engineering practices that
argue against code duplication), into a frozen separate directory or
even repository, and have all validation related code use the
consensus version, while everything else can use a normal tree
version, which then can evolve at a normal pace, and undergo cleanups,
API changes and feature improvements along with the rest of the code.
Unfortunately, it is not. The consensus code is mixed all over the
place, and this forces unnecessary strain on all development of the
codebase. I hope people agree that getting to a point where this is no
longer the case is important.

@_date: 2014-02-10 00:33:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with malleability 
Hello all,
it was something I planned to do since a long time, but with the
recent related issues popping up, I finally got around to writing a
BIP about how we can get rid of transaction malleability over time.
The proposed document is here: I expect most rules to not be controversial. Maybe rules 1 and 3, as
they require modifications to wallet software (Bitcoin Core 0.9 and
BitcoinJ already implement it, though) and potentially invalidate some
script functionality. However, these new rules remain optional and
controlled by an nVersion increase.
Comments please!

@_date: 2014-02-10 13:28:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Malleability and MtGox's announcement 
Hi all,
I was a bit surprised to see MtGox's announcement. The malleability of
transactions was known for years already (see for example the wiki
article on it,  it,
or mails on this list from 2012 and 2013). I don't consider it a very
big problem, but it does make it harder for infrastructure to interact
with Bitcoin. If we'd design Bitcoin today, I'm sure we would try to
avoid it altogether to make life easier for everyone.
But we can't just change all infrastructure that exists today. We're
slowly working towards making malleability harder (and hopefully
impossible someday), but this will take a long time. For example, 0.8
not supporting non-DER encoded signatures was a step in that direction
(and ironically, the trigger that caused MtGox's initial problems
here). In any case, this will take years, and nobody should wait for
There seem to be two more direct problems here.
* Wallets which deal badly with modified txids.
* Services that use the transaction id to detect unconfirming transactions.
The first is something that needs to be done correctly in software -
it just needs to be aware of malleability.
The second is something I was unaware of and would have advised
against. If you plan on reissuing a transaction because on old version
doesn't confirm, make sure to make it a double spend of the first one
- so that not both can confirm.
I certainly don't like press making this sound like a problem in the
Bitcoin protocol or clients. I think this is an issue that needs to be
solved at the layer above - the infrastructure building on the Bitcoin
system. Despite that, I do think that we (as a community, not just
developers) can benefit from defining a standard way to identify
transactions unambiguously. This is something Mark Karpeles suggested
a few days ago, and my proposal is this:
We define the normalized transaction id as SHA256^2(normalized_tx +
0x01000000), where normalized_tx is the transaction with all input
scripts replaced by empty scripts. This is exactly what would be
signed inside transaction signatures using SIGHASH_ALL (except not
substituting the previous scriptPubKey to be signed, and not dealing
with the input being signed specially). An implementation is here:
Note that this is not a solution for all problems related to
malleability, but maybe it can make people more aware of it, in
tangible way.

@_date: 2014-02-12 18:21:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
It's also not necessary for wallet software - it's really just for
human consumption.
A wallet can easily detect inputs being respent in another
transaction. You don't need a static hash for that (which wouldn't
need with all hash types, non-malleability double spends, ...).

@_date: 2014-02-12 18:22:50
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
That would be a hard fork. Certainly something to be discussed if we
ever introduce a version-2 scripting language, but that's a long-term

@_date: 2014-02-19 15:38:19
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
I consider actively mutating other's transactions worse than not
relaying them. If we want people to make their software deal with
malleability, either will work.
Regarding deterministic hash: that's impossible. Some signature hash
types are inherently (and intentionally) malleable. I don't think we
should pretend to want to change that. The purpose is making
non-malleability a choice the sender of a transaction can make.
Most of the rules actually are enforced by IsStandard already now.
Only  and  aren't.  affects the majority of all transactions, so
changing it right now would be painful.  only affects multisig.
The problem in making these rules into consensus rule (affecting
tx/block validity) is that some rules (in particular  may not be
wanted by everyone, as they effectively limit the possibilities of the
script language further. As it is ultimately only about protecting
senders who care about non-malleability, introducing a new transaction
version is a very neat way of accomplishing that. The new block
version number is only there to coordinate the rollout, and choosing
an automatic forking point.

@_date: 2014-02-19 22:11:14
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [RFC] [BIP proposal] Dealing with 
Just to be clear: this change is not directly intended to avoid
"incidents". It will take way too long to deploy this. Software should
deal with malleability. This is a longer-term solution intended to
provide non-malleability guarantees for clients that a) are upgraded
to use them  b) willing to restrict their functionality. As there are
several intended use cases for malleable transactions (the sighash
flags pretty directly are a way to signify what malleabilities are
*wanted*), this is not about outlawing malleability.
While we could right now make all these rules non-standard, and
schedule a soft fork in a year or so to make them illegal, it would
mean removing potential functionality that can only be re-enabled
through a hard fork. This is significantly harder, so we should think
about it very well in advance.
About new transaction and block versions: this allows implementing and
automatically scheduling a softfork without waiting for wallets to
upgrade. The non-DER signature change was discussed for over two
years, and implemented almost a year ago, and we still notice wallets
that don't support it. We can't expect every wallet to be instantly
modified (what about hardware wallets like the Trezor, for example?
they may not just be able to be upgraded). Nor is it necessary: if
your software only spends confirmed change, and tracks all debits
correctly, there is no need.

@_date: 2014-02-24 17:16:32
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] On OP_RETURN in upcoming 0.9 release 
These two statements are in direct contradiction with each other.

@_date: 2014-01-13 17:43:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment protocol and reliable Payment messages 
Hi all,
while thinking about what use cases the stealth addresses covers, in
particular in addition to the payment protocol, I found it useful to
bring this up again.
currently, BIP70 says for "payment_url": Secure (usually https)
location where a Payment message (see below) may be sent to obtain a
The fact that this is optional makes the "memo" and "refund" and
"merchant_data" fields in the Payment message useless, as merchants
cannot rely on it, thus need to provide an alternative, thus nobody
would have a use for trying to use the in-Payment versions. If we
truly want the use of this Payment being sent be optional, I'd vote to
get rid of these and just send the transaction.
In particular in the case of more anonymous senders, if the Payment
message isn't sent, it may result in funds being transferred without a
way to know who to refund it to if something goes wrong.
That would be a pity. I think having bi-directional communication in
the protocol is one of the nicest things the payment protocol can add.
I would prefer to at least formulate it in the BIP as "location where
a Payment message must be attempted to be sent to". In case it fails,
it should probably be stored in the client and retried later.
As an optimization (and I believe this is what Mike plans to implement
in BitcoinJ), if a payment_url is present, it should be encouraged to
only send the payment there, and not broadcast the transaction at all
on the P2P network (minimizing the risk that the transaction confirms
without the payment being received; it can't be guaranteed however).

@_date: 2014-01-13 18:56:57
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Payment protocol and reliable Payment 
I want to avoid the case where a transaction confirms, but the
associated payment is not delivered. If there is a reasonable chance
that this case occurs in normal operation, it means the payment
transmission cannot be relied upon.
On the other hand, if the payment gets sent, but the transaction is
not broadcasted, it can be broadcasted by the receiver (who has much
more reason to do so; he wants to spend his money).
So, yes, sending on the P2P network is fine, as long as everything is
done to get the payment delivered. Not broadcasting on P2P is just an
optimization that makes failures of not getting the transaction out
and not getting the payment delivered coincide better. I say just
optimization, as you can't rely on the fact that if the payment fails,
the transaction will also fail (the merchant may be malicious, make
the submission of the payment fail, but broadcast the transaction
anyway), so wallets must still be able to deal with this. Nonetheless,
I think it can increase the reliability of "payment being received for
otherwise confirming transactions".
That's a different issue. I'm very aware that payments over HTTP can
fail. The point is that I prefer the entire transaction to fail in
that case, instead, and focus on making the payment submission more
I'm aware. What issues do you see?

@_date: 2014-01-26 11:09:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP 32 proposed changes 
Hello all,
based on some feedback, I've created a pull request with a rewritten
version of BIP 32, hopefully making it more readable:
* Don't reuse the terminology 'public' vs 'private' for the alternate
derivation scheme which doesn't allow computing child public keys from
parent public keys, but call them "hardened".
* Add explicit type conversion functions.
* Don't use the ' suffix to mean "alternate" in two very different
meanings (use the 'h' suffix to mean hardened, and use CKDpriv and
CKDpub for the derivation functions that operate on private and public
* Several smaller changes.
These are just documentation changes, the semantics are unchanged.
Comments are welcome, see

@_date: 2014-01-27 23:17:58
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
Agree, I think we need a way for client applications to interpret the response.
In my opinion, that should be the primary meaning of receiving an ACK:
acknowledgement that the receiver takes responsibility for getting the
transaction confirmed (to the extent possible, of course).

@_date: 2014-01-28 14:09:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
You cannot prevent transactions from being broadcasted, but an ACK can
still mean "You're now relieved of the responsibility of getting the
transaction confirmed". That's independent from being allowed to
broadcast it.
If a payment_url is unavailable, you should imho retry. If you
broadcasted, and the payment_url is unavailable, you should
*certainly* retry. Otherwise the recipient cannot rely on receiving
memo and refund address, which would imho make these fields completely
I still like suggesting not broadcasting if a payment_uri to minimize
that risk further, but as you say - there are enough cases where you
cannot enforce that anyway.

@_date: 2014-01-30 12:46:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70 message delivery reliability 
You don't even have to assume malicious intent. A payment message
could just fail to arrive because the server is unreachable. As the
specification currently doesn't even suggest retrying, there is no way
the merchant can rely at all on the memo and refund address being
delivered, which makes them in my opinion useless.
Your proposal makes the whole protocol more atomic, which may be a
step too far at this point (though I like the idea very much), but I
really think the specification should do everything possible to
prevent transactions confirming without the payment message ever being
delivered (i.e., store them in the sender's client, retry when
necessary, exponential backoff, ...).

@_date: 2014-01-30 13:02:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70 message delivery reliability 
That's one right way to do it imho, but not what is suggested or
required by the specification, and not what bitcoin core master
currently implements.

@_date: 2014-01-30 16:16:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
Currently, with the specification and implementation in Bitcoin Core,
if a merchant wants to use the refund or memo feature, they need to
provide an alternative route for delivering that information to them
*before* the transaction is made, as sending the transaction may
result in the transfer of funds without knowing what to do with it (if
their receive server is down at the right time) and potnetially no way
to contact the sender. This makes these fields utterly useless.
This is not a matter of letting wallets experiment with the best
behaviour. This is removing the ability to rely on the payment
protocol being bidirectional.
I don't care whether wallets broadcast the transactions or not (they
can experiment with that as they like). But we should take measures to
prevent a transaction for being broadcast without the payment being
delivered. One way is never broadcasting the transaction yourself.
Another is retrying to send the payment if delivery fails.
Here is what I would suggest to add to the specification:
* If a payment_uri is specified, the client must attempt to send the
payment there.
* If a transaction is broadcast (which is permitted even if sending
the payment fails), a client should make a reasonable attempt of
delivering the payment (remembering, retrying, ...).
* If a paymentACK has been received, the client is no longer
responsible for broadcasting the transaction (but still may).

@_date: 2014-01-30 15:58:56
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP70: PaymentACK semantics 
Confirmed is probably the wrong word. But IMHO (not how it's currently
worded), the merchant should take that responsibility after delivering
a PaymentACK. This means the client does not need to stay online
anymore. More importantly, it removes the requirement for the P2P
network to function as a reliable sender->receiver communication
channel (and reduces it to a broadcast medium to get transactions to

@_date: 2014-07-18 17:14:53
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
Hi all,
I've sent a pull request to make a small change to BIP 62 (my
anti-malleability proposal) which is still a draft; see:
*  (the request)
*  (the result)
It makes two of the 7 new rules mandatory in new blocks, even for
old-style transactions. Both are already non-standard since 0.8.0, and
have no use cases in my opinion.
The reason for this change is dropping the requirement for signature
verification engines to be bug-for-bug compatible with OpenSSL (which
supports many non-standard encodings for signatures). Requiring strict
DER compliance for signatures means any implementation just needs to
support DER.

@_date: 2014-07-18 17:45:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
Nothing really. If it's controversial in any way, I'm fine with
changing that. It's just one those things that nobody needs, nobody
uses, has never been standard, and shouldn't have been possible in the
first place IMHO. Given that, it's easier to just make it a consensus
I'm not comfortable with dropping OpenSSL-based signature parsing
until we have well-defined rules about which encodings are valid. At
this point I'm not even convinced we *know* about all possible ways to
modify signature encodings without invalidating them.
But perhaps we should investigate how many non-DER signatures still
make it into blocks first...

@_date: 2014-07-18 19:25:50
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
In the last 11 blocks (4148 transactions), apparently none.

@_date: 2014-07-18 20:10:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
Or even in the last 389 blocks (159466 transactions).

@_date: 2014-07-19 16:46:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
scriptSig invalidates it.
Well yes, but that is true for each of the rules and is already covered by
the previous specification in BIP62. Making it mandatory even for old
transaction does not really protect much against malleability as there are
several other sources of malleability that cannot be made mandatory in old
transactions left.
The reason for including  is just "allowing this does not benefit anyone".

@_date: 2014-07-30 15:57:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Abusive and broken bitcoin seeders 
At least my crawler (bitcoin-seeder:0.01) software shouldn't reconnect
more frequently than once every 15 minutes. But maybe the two
connections you saw were instances?

@_date: 2014-06-05 21:34:15
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Future Feature Proposal - getgist 
I don't understand. If you're using getheaders(), there is no need to
use getblocks() anymore. You just do a getdata() immediately for the
block hashes you have the headers but not the transactions for.
In general, I think we should aim for as much verifiability as
possible. Much of the reference client's design is built around doing
as much validation on received data as soon as possible, to avoid
being misled by a particular peer. Getheaders() provides this: you
receive a set of headers starting from a point you already know, in
order, and can validate them syntactically and for proof-of-work
immediately. That allows building a very-hard-to-beat tree structure
locally already, at which point you can start requesting blocks along
the good branches of that tree immediately - potentially in parallel
from multiple peers. In fact, that's the planned approach for the
headers-first synchronization.
The getgist() proposed here allows the peer to basically give you
bullshit headers at the end, and you won't notice until you've
downloaded every single block (or at least every single header) up to
that point. That risks wasting time, bandwidth and diskspace,
depending on implementation.
Based on earlier experimenting with my former experimental
headersfirst branch, it's quite possible to have 2 mostly independent
synchronization mechanisms going on; 1) asking and downloading headers
from every peer, and validating them, and 2) asking and downloading
blocks from multiple peers in parallel, for blocks corresponding to
validated headers. Downloading the headers succeeds within minutes,
and within seconds you have enough to start fetching blocks. After
that point, you can keep a "download window" full with outstanding
block requests, and as blocks go much slower than headers, the headers
process never becomes a blocker for blocks to download.
Unless we're talking about a system with billions of headers to
download, I don't think this is a worthwhile optimization.

@_date: 2014-06-06 10:40:20
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] # error "Bitcoin cannot be compiled 
There are a few examples of things that would classify as
expensive/redundant checks:
* addrman consistency checks (only enabled with -DDEBUG_ADDRMAN).
* mempool consistency checks (only enabled with -checkmempool).
* deadlock detection (only enabled with -DDEBUG_LOCKORDER).
I'm not sure all of these make sense to put under a single runtime
flag. For example, addrman consistency is unlikely to be affected
unless you're working on addrman code, and is pretty expensive.
Still, I do like the idea of optional consistency checks, that help
guarantee the software always has a consistency state.

@_date: 2014-06-07 00:27:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Possible attack: Keeping unconfirmed 
Whenever you do a reissuing of a transaction that didn't go through
earlier, you should make sure to reuse one of the inputs for it. That
guarantees that both cannot confirm simultaneously.

@_date: 2014-06-21 17:14:15
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bitcoin miner heads-up: "getwork" RPC 
As there doesn't seem to be any objection, we may go ahead and merge
 (which among other things
removes the getwork RPC).

@_date: 2014-06-24 15:37:59
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Plans to separate wallet from core 
I'd like to point out that there is quite a difference between "what
core nodes should be like" and "what the codebase core nodes are built
from must support".
Given sufficiently modularized code (which I think everyone seems to
be in favor of, regardless of the goals), you can likely build a
binary that does full verification and maintains some indexes of some
I still believe that what we push for to run as the core nodes of the
network should aim for purely verification and relay, and nothing
else, but people can and will do things differently if the source code
allows it. And that's fine.
IMHO, maintaining a correct view of the current state of the chain
(excluding blocks, just headers) is already sufficiently hard (I hope
that everyone who ever implemented an SPV wallet can agree). You
simplify things a bit by not needing to verify what the peer claims if
you trust them, but not much. You still need to support
reorganizations, counting confirmations, making sure you stay
up-to-date. These are functions the (SPV) P2P protocol has already
shown to do well, and there are several codebases out there that
implement it. No need to reinvent the wheel with a marginally more
efficient protocol, if it means starting over everything else.

@_date: 2014-03-05 13:56:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New side channel attack that can recover 
As far as I know, judging from the implementation, there is hardly any
effort to try to prevent timing attacks.

@_date: 2014-03-05 15:04:41
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New side channel attack that can recover 
I've done some preliminary work on making it leak less, but it's by no
means guaranteed to be constant time either (so better assume it is

@_date: 2014-03-27 17:14:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] New BIP32 structure 
Just chiming in...
I'm not opposed to a more generic default key tree, but we need to
standardize this soon I believe. There are already existing code bases
that implement BIP32 wallets (and more are popping up...); just using
a separate one will result in lots of incompibilities.
That said, I'm not convinced about the extra layers. The "cointype" in
my opinion isn't necessary inside the derivation. There is already
support (4 bytes!) for magic bytes in the serialized form. Inside
applications/p2p it should always be known to which chain it applies,
and outside of that you shouldn't transfer raw keys. Maybe seeds need
some marker, but that's a separate case anyway. Mainnet and testnet
have specified magics here already - alts can define what they want
A 'reserved' field for future extensions may be useful, but as already
suggested by Mike, I don't believe we can encode how key chains are to
be used inside the derivation structure anyway. The most basic case
(not losing money in a wallet without special structure) can perhaps
be supported with just "the blockchain is your wallet", but I don't
believe this principle can scale to more advanced uses anyway, and you
need metadata in the wallet to deal with it.
In my view, your wallet just has a bunch of chains, and each chain
gets used for a particular purpose, fixing how the derivation beneath
it works. Either that is as a wallet, as part of a pair of multisig
keys, as a recurring payment receiver, ... or more complex things.
Some of these will require extra layers beneath, but that is
application specific. You would import a chain into your (advanced)
wallet with a particular extpub/extpriv code, and some metadata on how
to use it. Serialization formats for such designated extra uses sounds
better to me than trying to fit it into the derivation structure.

@_date: 2014-05-06 10:25:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bug in key.cpp 
Values 2 and 3 are only needed in theory. They together shouldn't
occur more than once in 2**127 (when the signature value is between
the group size and the field size).
That said, this is indeed a bug.

@_date: 2014-05-09 17:50:33
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
I believe stealth addresses and the payment protocol both have their
use cases, and that they don't overlap.
If you do not want to communicate with the receiver, you typically do
not want them to know who is paying or for what (otherwise you're
already talking to them in some way, right?). That's perfect for
things like anonymous donations.
In pretty much every other case, communicating directly with the
receiver has benefits. Negotiation of the transaction details,
messages associated with the transaction, refund information, no need
to scan the blockchain for incoming transaction... and the ability to
cancel if either party doesn't agree.
Instead of adding stealth functionality to the payment protocol as a
last resort, I'd rather see the payment protocol improve its
atomicity. Either you want the data channel sender->receiver, or you
don't. If it isn't available and you want it, things should fail. If
you don't want it, you shouldn't try to use it in the first place.

@_date: 2014-05-09 20:38:22
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] ECDH in the payment protocol 
I haven't talked much about it except for on IRC, but my idea was this:
* PaymentACK messages become signed (with the same key as the payment
request, or using some delegation mechanism, so that the same key
doesn't need to remain online).
* Instead of a full Bitcoin transaction, a Payment message contains a
scriptSig-less Bitcoin transaction + a limit on its byte size (and
perhaps a limit on its sigop count).
* The sender sends such a Payment to the receiver before actually
signing the transaction (or at least, before revealing the signed
* The receiver only ACKs if the transaction satisfies the request, is
within time limits, isn't unlikely to confirm.
* If the sender likes the ACK (the refund and memo fields are intact,
the transaction isn't changed, the signature key is valid, ...), he
either sends the full transaction (with receiver taking responsibility
for broadcasting) or broadcasts it himself.
Together, this means that a paymentACK + a confirmed matching Bitcoin
transaction can count as proof of payment. Both parties have a chance
to disagree with the transaction, and are certain all communicated
data (apart from transaction signatures) in both directions happened
correctly before committing. It would completely remove the chance
that the Bitcoin transaction gets broadcast without the receiver
liking it (for legitimate or illegitimate reasons), or without the
backchannel functioning correctly.
It's also compatible with doing multiple payments in one Bitcoin
transaction - you can ask for ACKs from multiple parties before
signing the transaction.
Of course, the sender can still withhold the signed transaction (in
which case nothing happened, but we probably need a second timeout),
or the receiver can always claim to have never received the
transaction. The sender can broadcast the transaction himself in order
to prevent that, after obtaining an ACK.

@_date: 2014-05-14 15:10:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] statoshi.info is now live 
that I hope to implement soon, but you're right in that tracking per-peer
pings is a different type of metric than what I'm currently collecting. I
actually noted the lack of pong messages in a post I made a few weeks ago:
See pull request (Sent from my phone)

@_date: 2014-05-30 17:24:15
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is up 
I don't think it would be too hard to add support for a option to the
seeder "for non-matching requests, forward to other DNS server at
IP:PORT", so you could cascade them.

@_date: 2014-05-30 17:59:26
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] testnet-seed.bitcoin.petertodd.org is up 
On Fri, May 30, 2014 at 5:40 PM, Andreas Schildbach
That's what Matt's implementation is doing. You don't have to run mine :)
I chose not to do so, as I wanted to be able to serve a different
response to every query, but more diversity is a good thing.

@_date: 2014-11-04 05:29:46
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
Hi all,
one of the rules in BIP62 is the "clean stack" requirement, which
makes passing more inputs to a script than necessary illegal.
Unfortunately, this rule needs an exception for P2SH scripts: the test
can only be done after (and not before) the second stage evaluation.
Otherwise it would reject all spends from P2SH (which rely on
"superfluous" inputs to pass data to the second stage).
I submitted a Pull Request to clarify this in BIP62:
However, this also leads to the interesting observation that the
clean-stack rule is incompatible with future P2SH-like constructs -
which would be very useful if we'd ever want to deploy a "Script 2.0".
Any such upgrade would suffer from the same problem as P2SH, and
require an exception in the clean-stack rule, which - once deployed -
is no longer a softfork.
Luke suggested on the pull request to not apply this rule on every
transaction with nVersion >= 3, which indeed solves the problem. I
believe this can easily be generalized: make the (non mandatory) BIP62
rules only apply to transaction with strict nVersion==3, and not to
higher ones. The higher ones are non-standard anyway, and shouldn't be
used before there is a rule that applies to them anyway - which could
include some or all of BIP62 if wanted at that point still.

@_date: 2014-11-04 05:50:35
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
I agree.
I also agree that the desire for softforks sometimes lead to ugly hacks.
I also that they are not "nice" philosophically because they reduce
the security model of former full nodes to SPV wrt. the new rules
without their knowledge.
I also agree that hardforks should be possible when they're useful.
But in practice, hardforks have a much larger risk which just isn't
justified for everything. Especially when it's about introducing a new
transaction type that won't be used before the softfork takes place
And to keep the option for doing future softforks open, I believe we
need to be aware of the effects of changes like this.

@_date: 2014-11-04 12:00:43
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
I prefer to see consensus rules as one set of rules (especially
because they only really apply to blocks - the part for lone
transactions is just policy), and thus have a single numbering. Still,
I have no strong opinion about it and have now heard 3 'moderately
against' comments. I'm fine with using nVersion==2 for transactions.

@_date: 2014-11-04 23:53:03
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP62 and future script upgrades 
Ok, addressed these (and a few other things) in
* Better names for the rules.
* Clarify interaction of BIP62 with P2SH.
* Clarify that known hashtypes are required, despite not being part of DER.
* Use v2 transactions instead of v3 transactions.
* Apply the optional rules only to strict v2, and not higher or lower.

@_date: 2014-11-06 02:47:29
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] SCRIPT_VERIFY_STRICTENC and CHECKSIG NOT 
Yeah, there's even a comment in script/interpreter.h currently about
how STRICTENC is not softfork safe. I didn't realize that this would
lead to the mempool accepting invalid transactions (I thought there
was a second validity check with the actual consensus rules; if not,
maybe we need to add that).
Sounds good to me, I disliked those semantics too.

@_date: 2014-11-06 02:51:14
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] SCRIPT_VERIFY_STRICTENC and CHECKSIG NOT 
Of course: do we apply this rule to all pubkeys passed to
CHECKMULTISIG (my preference...), or just the ones that are otherwise
This will likely make existing outputs hard to spend as well (I don't
have numbers), are we okay with that? We probably can't make this a
consensus rule, as it may make existing P2SH outputs/addresses

@_date: 2014-11-17 11:35:24
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
You can still send the signature out of band (for example using the
payment protocol), and just have the transaction commit to a hash of
that signature (or message in general), either using an OP_RETURN
output to store the hash, or using the pay-to-contract scheme that
Jorge mentioned above. That has exactly the same timestamping
My main concern with OP_RETURN is that it seems to encourage people to
use the blockchain as a convenient transport channel, rather than just
for data that the world needs to see to validate it. I'd rather
encourage solutions that don't require additional data there, which in
many cases (but not all) is perfectly possible.

@_date: 2014-11-17 13:00:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
On Mon, Nov 17, 2014 at 12:43 PM, Flavien Charlon
It wasn't limited to stop them from using it. It was limited to avoid
giving others the impression that OP_RETURN was intended for data
storage. For the intended purpose (making a transaction commit to some
external data) a 32-byte hash + 8 byte id is more than sufficient.
Do you really need that data published to everyone? You're at the very
least exposing yourself to censorship, and (depending on the design)
potentially decreased privacy for your users. I would expect that for
most colored coin applications, just having the color transfer
information in external data sent directly to the receiver with
transactions committing to it should suffice.

@_date: 2014-11-17 13:39:26
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Increasing the OP_RETURN maximum payload 
That is inevitable for any wallet that offers any functionality beyond
just maintaining a balance and the ability to send coins. In
particular, anything that wishes to list previous transaction (with
timestamps, history, metadata, messages sent using the payment
protocol, ...) needs backups.
What HD wallets (or any type of deterministic derivation scheme) offer
is the fact that you can separate secret data and public data. You
only need one safe backup of the master secret key - all the rest can
at most result in privacy loss and not in lost coins.

@_date: 2014-10-11 16:34:15
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Request for review/testing: headers-first 
Hi all,
I believe that a large change that I've been working on for Bitcoin
Core is ready for review and testing: headers-first synchronization.
In short, it changes the way the best chain is discovered, downloaded
and verified, with several advantages:
* Parallel block downloading (much faster sync on typical network connections).
* No more stalled downloads.
* Much more robust against unresponsive or slow peers.
* Removes a class of DoS attacks related to peers feeding you
low-difficulty valid large blocks on a side branch.
* Reduces the need for checkpoints in the code.
* No orphan blocks stored in memory anymore (reducing memory usage during sync).
* A major step step towards an SPV mode using the reference codebase.
Historically, this mode of operation has been known for years (Greg
Maxwell wrote up a description of a very similar method in
in early 2012, but it was known before that), but it took a long time
to refactor these code enough to support it.
Technically, it works by replacing the single-peer blocks download by
a single-peer headers download (which typically takes seconds/minutes)
and verification, and simultaneously fetching blocks along the best
known headers chain from all peers that are known to have the relevant
blocks. Downloading is constrained to a moving window to avoid
unbounded unordering of blocks on disk (which would interfere with
pruning later).
At the protocol level, it increases the minimally supported version
for peers to 31800 (corresponding to bitcoin v3.18, released in
december 2010), as earlier versions did not support the getheaders P2P
So, the code is available as a github pull request
( or packaged on
 where you can also find
binaries to test with.
Known issues:
* At the very start of the sync, especially before all headers are
processed, downloading is very slow due to a limited number of blocks
that are requested per peer simultaneously. The policies around this
will need some experimentation can certainly be improved.
* Blocks will be stored on disk out of order (in the order they are
received, really), which makes it incompatible with some tools or
other programs. Reindexing using earlier versions will also not work
anymore as a result of this.
* The block index database will now hold headers for which no block is
stored on disk, which earlier versions won't support. If you are fully
synced, it may still be possible to go back to an earlier version.
Unknown issues:
* Who knows, maybe it will replace your familiy pictures with Nyan
Cat? Use at your own risk.
TL;DR: Review/test  or

@_date: 2014-10-13 19:34:16
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Malleable booleans 
Hi all,
while working on a BIP62 implementation I discovered yet another type
of malleability: the interpretation of booleans.
Any byte array with non-zero bytes in it (ignoring the highest bit of
the last byte, which is the sign bit when interpreting as a number) is
interpreted as true, anything else as false. Other than numbers,
they're not even restricted to 4 bytes. Worse, the code for dealing
with booleans is not very consistent: OP_BOOLAND and OP_BOOLOR first
interpret their arguments as numbers, and then compare them to 0 to
turn them into boolean values.
This means that scripts that use booleans as inputs will be inherently
malleable. Given that that seems actually useful (passing in booleans
to guide some OP_IF's during execution of several alternatives), I
would like to change BIP62 to also state that interpreted booleans
must be of minimal encoded size (in addition to numbers).
Any opinions for or against?

@_date: 2014-10-14 11:54:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Malleable booleans 
To be clear: I indeed meant to only allow 0 and 1 as booleans (or,
more precisely: [] and [0x01]). Evaluating any stack element as a
boolean that is not any of these would result in script failure.
The only places where this is relevant:
* Inputs to OP_IF and OP_NOTIF (which are currently allowed to be any
byte array).
* Inputs to OP_BOOLAND and OP_BOOLOR (which are currently allowed to
be any valid number).
* The resulting final element on the stack for validity.
The code for converting stack elements to booleans is also invoked for
all OP_*VERIFY operators, but for those it is always the output of a
previous operator, so it will not have any semantic impact.
I think my goal is to have the property that for every possible
script, there is an equivalent one that is non-malleable. There are
likely still holes in that idea, but at least for just standard
scripts I think BIP62 (as is) covers this. And as your example points
out (Greg and I discussed this, though we didn't come up with such a
concise one), it is already possible for boolean inputs too.
The real question is whether there are use cases for not having this
requirement. I can't come up with any, as that would imply a boolean
that is also interpretable as a hash, a pubkey or a signature - all of
which seems crpytographically impossible to ever result in false.

@_date: 2014-09-03 18:34:33
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
Good catch - I'll write an update soon.
Perhaps we can go further, and include 6 as well? I see zero use cases
for zero-padded numbers, as their interpretation is already identical
to the non-padded case. I wouldn't include 1 (as it would break a
large amount of wallets today), 3 (which may have a use case in more
complex scripts with conditionals) or 7 (the superfluous element
consumed by CHECKMULTISIG could potentially be used for something in
the future).
I'll try to reword.

@_date: 2014-09-08 01:31:45
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
I've sent out a new pull request
( that:
* Changes the order of the rules.
* Adds more reference documentation about minimal pushes and number encodings.
* Clarified that extra consensus rules cannot prevent someone from
creating outputs whose spending transactions will be malleable.
I haven't changed which rules are mandatory in v3, so this is a pure
clarification & reorganization of the text.
Any comments?

@_date: 2014-09-12 18:35:25
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
Changes: Gregory, Jeff: does this address your concerns?
Others: comments?

@_date: 2014-09-14 00:45:14
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Small update to BIP 62 
I've made another change in the PR, as language about strictly only
compressed or uncompressed public keys was missing; please have a

@_date: 2014-09-15 16:55:47
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Does anyone have anything at all signed 
WoT is a perfectly reasonable way to establish trust about the link between
an online identity and a real world identity.
In the case of a developer with an existing reputation for his online
identity, that link is just irrelevant.

@_date: 2015-04-12 07:26:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Deprecating Bitcoin Core's regtest-specific 
Hello everyone,
Bitcoin Core's `setgenerate` RPC call has had a special meaning for
-regtest (namely instantaneously mining a number of blocks, instead of
starting a background CPU miner).
We're planning to deprecate that overloaded behaviour, and replace it with
a separate RPC call `generate`. Is there any software or user who would
need compatibility with the old behaviour? We're generally very
conservative in changing RPC behaviour, but as this is not related to any
production functionality, we may as well just switch it.
Note that the bitcoin.org developer documentation will need to be updated.

@_date: 2015-04-15 22:22:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
What problem are you trying to solve?
The reason why BIP62 (as specified, it is just a draft) does not make v1
transactions invalid is because it is opt-in. The creator of a transaction
needs to agree to protect it from malleability, and this subjects him to
extra rules in the creation.
Forcing v3 transactions would require every piece of wallet software to be

@_date: 2015-04-17 02:02:19
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] 75%/95% threshold for transaction versions 
Don't assume that because it does not (frequently) happen, that it cannot
happen. Large amounts of malleated transactions have happened in the past.
Especially if you build a system depends on non-malleability for its
security, you may at some point have an attacker who has financial gain
from malleation.
In theory, yes, anyone can alter the txid without invalidating it, without
mining power and without access to the sender's private keys.
All it requires is seeing a transaction on the network, doing a trivial
modification to it, and rebroadcasting it quickly. If the modifies version
gets mined, you're out of luck. Having mining power helps of course.
After BIP62, you will, as a sender, optionally be able to protect others
from malleating. You're always able to re-sign yourself.

@_date: 2015-04-28 04:01:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Bitcoin core 0.11 planning 
As softforks almost certainly require backports to older releases and other
software anyway, I don't think they should necessarily be bound to Bitcoin
Core major releases. If they don't require large code changes, we can
easily do them in minor releases too.

@_date: 2015-08-01 22:45:24
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
This is fine, I think. I believe we shouldn't proceed with a hardfork
without having reasonable expectation that it will be deployed by everyone
in time, while we can only measure miner acceptance. Still, as a
belt-and-suspenders this won't hurt.
Given the time recent softforks have taken to deploy, I think that's too
2016-01-12 00:00 UTC is Monday evening in US and Tuesday morning in China.
That's an interesting thing to take into account.
I have considered suggesting a faster ramp-up in the beginning, but I don't
think there is indisputable evidence that we can currently deal with
significantly larger blocks. I don't think "painful" is the right criterion
either; I'm sure my equipment can "handle" 20 MB blocks too, but with a
huge impact on network propagation speed, and even more people choosing the
outsource their full nodes.
Regarding "reasonable", I have a theory. What if we would have had 8 MB
blocks from the start? My guess is that some more people would have decided
to run their high-transaction-rate use cases on chain, that we'd regularly
see 4-6 MB blocks, there would be more complaints about low full node
counts, maybe 90% instead of 60% of the hash rate would be have SPV mining
agreements with each other, we'd somehow have accepted that even worse
reality, but people would still be complaining about the measly 25
transactions per second that Bitcoin could handle on-chain, and be
demanding a faster rampup to a more "reasonable" 64 MB block size as well.
I know many technologies have had faster growth, but I believe that global
bandwidth accessibility is the bottleneck, so the growth rate in my
proposal is based on that.
I'd rather be conservative here. My primary purpose is trying to create an
uncontroversial proposal that introduces an expectation of growth with

@_date: 2015-08-01 23:05:10
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Sat, Aug 1, 2015 at 10:22 PM, John T. Winslow via bitcoin-dev <
I disagree with the notion of "needed". The blockchain provides utility at
every size, and perhaps more at some sizes than at other sizes, but any
finite size will permit some use cases and not others. This is already the
case and will remain the case. I think the "demand for payments" should be
considered infinite, and some of them will fit on a block chain and pay a
fee for it, and others will need to rely on more efficient, cheaper, but
higher trust systems. You can't use observed usage as a metric for demand
without fixing the cost.
I think available space should grow with technology, to keep the relative
costs to the ecosystem for maintaining a decentralized system constant.
That may or may not lead to a fee market, but I don't think the fee market
is a goal - only a healthy outcome. The goal is an ecosystem that accepts
that the limit to scalability is set by the requirements of a decentralized
system, and its demand - and certainly not perceived demand at potentially
near-zero fee/cost - can't change that.
That's trivially gamable by miners, by filling the blocks with their own

@_date: 2015-08-02 12:32:53
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A compromise between BIP101 and Pieter's proposal 
period will be longer than 30 days. Anyway, if all major exchanges and
merchants agree to upgrade, people are forced to upgrade immediately or
they will follow a worthless chain.
If we don't want it to go fast, why let them? A hardfork is a means for the
community to agree on the rules that different parties have to obey.
would create mega blocks unless the fee could cover the extra orphaning
risk. Blocks were not constantly full until recent months, and many miners
are still keeping the 750kB soft limit. This strongly suggests that we
won't have 4MB blocks now even Satoshi set a 8MB limit.
I disagree. I think "demand" is strongly influenced by the knowledge of
space that looks available. If you look at historic block sizes, you see it
follows a series of block functions, not nice organic growth. My theory is
that this is changed defaults in software, new services appearing suddenly,
and people reacting to it. Demand fills the available space.
Also, SPV mining has nearly zero orphaning risk, only brief chance of loss
of fees as income.
primarily due to the 1MB cap, but the raise in BTC/USD rate. Since minting
reward is a fixed value in BTC, the tx fee must also be valued in BTC as it
is primarily for compensating the extra orphaning risk. As the BTC/USD rate
increases, the tx fee measured in USD would also increase, making
micro-payment (measured in USD) unsustainable.
I agree, but how does that matter? I don't think high fees and full blocks
should be the goal, but I think it would be a healthier outcome than what
we have now.
first, most users would run network nodes, but as the network grows beyond
a certain point, it would be left more and more to specialists with server
farms of specialized hardware. A server farm would only need to have one
node on the network and the rest of the LAN connects with that one node."
Theoretically, we only require one honest full node to prove wrongdoing on
the blockchain and tell every SPV nodes to blacklist the invalid chain.
Theoretically, we also only need one central bank, then? Sorry, if the
outcome is one (or just a few) entities that keep the system in check, I
think it loses any benefit it has over other systems, while still keeping
its costs and disadvantages (confirmation speed, mining infrastructure,
I know that 8 MB blocks do not immediately mean such a dramatic outcome.
But I do believe that as a community setting the block size based on
observed demand (which you do by saying "8 is a more reasonable size than
2" as argument) is the wrong way. What do you do when your 8 MB starts to
look "full", before your schedule says it can increase?
The block size and its costs - bandwidth, processing, centralization
effects for miners, ... - are the things that should be constrained. Demand
will fill it.
don't think we could stop this trend by artificially suppressing the block
size. Miners should just do it properly, e.g. stop mining until the
grandparent block is verified, which would make sure an invalid fork won't
grow beyond 2 blocks.
That would be a huge reduction in security as a mechanism itself, and even
worse due to needing to trust miners to follow that protocol. Without
proper incentives, miners become a trusted party, and due to needed SPV
mining agreements, potentially a closed group too. That is not an
interesting future.
would be the very minimum as that is only 75% of current SWIFT traffic.
See my BIP text about advantages of Bitcoin, please. A future where it has
to compete with the existing system - using that system's own strengths -
is not interesting.

@_date: 2015-08-04 11:03:22
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
I would like to withdraw my proposal from your self-appointed vote.
If you want to let a majority decide about economic policy of a currency, I
suggest fiat currencies. They have been using this approach for quite a
while, I hear.
Bitcoin's consensus rules are a consensus system, not a democracy. Find a
solution that everyone agrees on, or don't.
On Aug 4, 2015 9:51 AM, "jl2012 via bitcoin-dev" <

@_date: 2015-08-04 13:27:16
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
I would say that things already demonstrately got terrible. The mining
landscape is very centralized, with apparently a majority depending on
agreements to trust each other's announced blocks without validation. Full
node count is at its historically lowest value in years, and outsourcing of
full validation keeps growing.
I believe that if the above would have happened overnight, people would
have cried wolf. But somehow it happened slow enough, and "things kept
I don't think that this is a good criterion. Bitcoin can "work" with
gigabyte blocks today, if everyone uses the same few blockchain validation
services, the same few online wallets, and mining is done by a cartel that
only allows joining after signing a contract so they can sue you if you
create an invalid block. Do you think people will then agree that "things
got demonstratebly worse"?
Don't turn Bitcoin into something uninteresting, please.

@_date: 2015-08-04 15:54:47
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
If miners need to form alliances of trusting each other's blocks without
validation to overcome the inefficiencies of slow block propagation, I
think we have a system that is in direct conflict with the word
"permissionless" that you use later.
Specialization is perfectly fine.
Why is what you, personally, find interesting relevant?
I find it interesting to build a system that has potential to bring about
I understand you want to build an extremely decentralized system, where
That is not true, I'm sorry if that is the impression I gave.
I see centralization and scalability as a trade-off, and for better or for
worse, the block chain only offers one trade-off. I want to see technology
built on top that introduces lower levels of trust than typical fully
centralized systems, while offering increased convenience, speed,
reliability, and scale. I just don't think that all of that can happen on
the lowest layer without hurting everything built on top. We need different
trade-offs, and the blockchain is just one, but a very fundamental one.
I think it is more interesting to build a system that works for hundreds of
That sounds amazing, but do you think that Bitcoin, as it exists today, can
scale to hundreds of millions of users, while retaining any glimpse of
permission-lessness and decentralization? I think we need low-trust
off-chain systems and other innovations to make that happen.
I'm happy for you, then.

@_date: 2015-08-06 16:06:03
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
But you seem to consider that a bad thing. Maybe saying that you're
claiming that this equals Bitcoin failing is an exaggeration, but you do
believe that evolving towards an ecosystem where there is competition for
block space is a bad thing, right?
I don't agree that "Not everyone is able to use the block chain for every
use case" is the same thing as "People stop using Bitcoin". People are
already not using it for every use case.
Here is what my proposed BIP says: "No hard forking change that relaxes the
block size limit can be guaranteed to provide enough space for every
possible demand - or even any particular demand - unless strong
centralization of the mining ecosystem is expected. Because of that, the
development of a fee market and the evolution towards an ecosystem that is
able to cope with block space competition should be considered healthy."

@_date: 2015-08-06 16:53:29
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
So if we would have 8 MB blocks, and there is a sudden influx of users (or
settlement systems, who serve much more users) who want to pay high fees
(let's say 20 transactions per second) making the block chain inaccessible
for low fee transactions, and unreliable for medium fee transactions (for
any value of low, medium, and high), would you be ok with that? If so, why
is 8 MB good but 1 MB not? To me, they're a small constant factor that does
not fundamentally improve the scale of the system. I dislike the outlook of
"being forever locked at the same scale" while technology evolves, so my
proposal tries to address that part. It intentionally does not try to
improve a small factor, because I don't think it is valuable.
What is bad is artificially limiting or centrally controlling the supply of
It's exactly as centrally limited as the finite supply of BTC - by
consensus. You and I may agree that a finite supply is a good thing, and
may disagree about whether a consensus rule about the block size is a good
idea (and if so, at what level), but it's a choice we make as a community
about the rules of the system we want to use.

@_date: 2015-08-06 17:26:11
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
The network can "handle" any size. I believe that if a majority of miners
forms SPV mining agreements, then they are no longer affected by the block
size, and benefit from making their blocks slow to validate for others (as
long as the fee is negligable compared to the subsidy). I'll try to find
the time to implement that in my simulator. Some hardware for full nodes
will always be able to validate and index the chain, so nobody needs to run
a pesky full node anymore and they can just use a web API to validate
Being able the "handle" a particular rate is not a boolean question. It's a
question of how much security, centralization, and risk for systemic error
we're willing to tolerate. These are not things you can just observe, so
let's keep talking about the risks, and find a solution that we agree on.
I don't believe there is a short-term problem. If there is one now, there
will be one too at 8 MB blocks (or whatever actual size blocks are
Maybe. But I believe that it is essential to not take unnecessary risks,
and find a non-controversial solution.

@_date: 2015-08-06 20:52:28
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
It's not about reliability. There are plenty of nodes currently for
synchronization and other network functions.
It's about reduction of trust. Running a full node and using it verify your
transactions is how you get personal assurance that everyone on the network
is following the rules. And if you don't do so yourself, the knowledge that
others are using full nodes and relying on them is valuable. Someone just
running 1000 nodes in a data center and not using them for anything does
not do anything for this, it's adding network capacity without use.
That doesn't mean that the full node count (or the reachable full node
count even) are meaningless numbers. They are an indication of how hard it
is (for various reasons) to run/use a full node, and thus provide feedback.
But they are not the goal, just an indicator.

@_date: 2015-08-06 22:01:53
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
On Aug 6, 2015 9:42 PM, "Gavin Andresen via bitcoin-dev" <
2. The "market minimum fee" should be determined by the market. It should
not be up to us to decide "when is a good time."
I partially agree. The community should decide what risks it is willing to
take, and set limits accordingly. Let the market decide how that space is
best used.
blocksize increase proposals should have to justify "why this size" or "why
this rate of increase." Part of my frustration with this whole debate is
we're talking about a sanity-check upper-limit; as long as it doesn't open
up some terrible new DoS possibility I don't think it really matters much
what the exact number is.
It is only a DoS protection limit if you want to rely on trusting miners. I
prefer a system where I don't have to do that.
But I agree the numbers don't matter much, for a different reason: the
market will fill up whatever space is available, and we'll have the same
discussion when the new limit doesn't seem enough anymore.

@_date: 2015-08-07 01:32:12
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Wrapping up the block size debate with voting 
"Miners can do this unilaterally" maybe, if they are a closed group, based
They don't need to use full nodes for propagation. Miners don't care when
other full nodes hear about their blocks, only whether they (eventually)
accept them.
And yes, full nodes can change what blocks they accept. That's called a
hard fork :)

@_date: 2015-08-07 17:16:34
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
That's only the case when the actual rate of transactions with a non-zero
fee is below what fits in blocks. If the total production rate is higher,
even without configured floor by miners, a free transaction won't ever be
mined, as there will always be some backlog of non-free transaction. Not
saying that this is a likely outcome - it would inevitably mean that people
are creating transactions without any guarantee that they'll be mined,
which may not be what anyone is interested in. But perhaps there is some
"use" for ultra-low-priority unreliable transactions (... despite DoS
Fair enough, I don't think anyone knows.
I guess my question (and perhaps that's what Jorge is after): do you feel
that blocks should be increased in response to (or for fear of) such a
scenario. And if so, if that is a reason for increase now, won't it be a
reason for an increase later as well? It is my impression that your answer
is yes, that this is why you want to increase the block size quickly and
significantly, but correct me if I'm wrong.

@_date: 2015-08-07 18:28:47
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
This is a fundamental disagreement then. I believe that the demand is
infinite if you don't set a fee minimum (and I don't think we should), and
it just takes time for the market to find a way to fill whatever is
available - the rest goes into off-chain systems anyway. You will run out
of capacity at any size, and acting out of fear of that reality does not
improve the system. Whatever size blocks are actually produced, I believe
the result will either be something people consider too small to be
competitive ("you mean Bitcoin can only do 24 transactions per second?"
sounds almost the same as "you mean Bitcoin can only do 3 transactions per
second?"), or something that is very centralized in practice, and likely
In general that sounds reasonable, but it's a dangerous precedent to make
technical decisions based on a fear of change of economics...

@_date: 2015-08-07 18:30:28
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
On Fri, Aug 7, 2015 at 6:06 PM, Thomas Zander via bitcoin-dev <
Yay, trust!
I never said it is the only factor that influences node count.
Or, in other words, without a need to run a node you can't judge the
If the incentives for running a node don't weight up against the
cost/difficulty using a full node yourself for a majority of people in the
ecosystem, I would argue that there is a problem. As Bitcoin's fundamental
improvement over other systems is the lack of need for trust, I believe
that with increased adoption should also come an increased (in absolute
terms) incentive for people to use a full node. I'm seeing the opposite
trend, and that is worrying IMHO.

@_date: 2015-08-07 19:09:30
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
On Fri, Aug 7, 2015 at 7:00 PM, Thomas Zander via bitcoin-dev <
Of course there is a need. It's the primary mechanism that keeps Bitcoin
secure and immune from malicious influence.
Of course not everyone needs to run a node. But that leaves the
responsibility on us - the community - to help the situation by not making
it too hard to run a node. And I see the block size as the primary way
through which we do that.
If the impact of the system goes us, so should the - joint - incentives to
keep it secure. And I think we're (slowly) failing at that.

@_date: 2015-08-07 20:10:48
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
cost/difficulty using a full node yourself for a majority of people in the
ecosystem, I would argue that there is a problem. As Bitcoin's fundamental
improvement over other systems is the lack of need for trust, I believe
that with increased adoption should also come an increased (in absolute
terms) incentive for people to use a full node. I'm seeing the opposite
trend, and that is worrying IMHO.
to trust nothing but the genesis block hash (decide to run a full node)
there is a problem?
I shouldn't have said majority, sorry. But I do believe that as the odds at
stake in the system go up, so should those who take an interest in
verifying. That doesn't seem to be the case, and is a problem, where that
is a result of the block chain size or not.
misunderstood how you think about trust/centralization/convenience
tradeoffs in the past.
tradeoffs, and I believe that is OK-- people should be free to make those
I agree. Though I believe that the blockchain itself cannot offer many
tradeoffs, and by trying to make it scale we hurt the whole system. The
place to introduce tradeoffs is in layers on top - there you can build
systems with various levels of trust without hurting others.
using a centralized service or an SPV-level-security wallet was better even
two or three years ago when blocks were tiny (I'd have to go back and dig
up number-of-full-nodes and number-of-active-wallets at the big web-wallet
providers, but I bet there were an order of magnitude more people using
centralized services than running full nodes even back then).
That's inevitable, I'm sure.
to run a full node or not.
Within certain limits, maybe not. Within certain limits, maybe it also does
not incentivize trusted miner setups like SPV mining (which hurt the
security of SPV nodes tremendously).
But if the reason for increasing is because you fear a change of economics,
then it means you prefer not dealing with that change. I believe you prefer
not dealing with it ever, and would rather have a system where as much as
possible happens on-chain, even when we unknowingly go beyond those limits.
I think we don't do the ecosystem a service by promising that such a future
is possible without compromises.
So, I think the block size should follow technological evolution, and not
reenforce the belief that the block size should follow demand. There will
always be demand, and we should learn to deal with it.

@_date: 2015-08-10 16:34:55
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I think I see your point of view. You see demand for on-chain transactions
as a single number that grows with adoption. Once the transaction creation
rate grows close to the capacity, transactions will become unreliable, and
you consider this a bad thing.
And if you see Bitcoin as a payment system where guaranteed time to
confirmation is a feature, I fully agree. But I think that is an
unrealistic dream. It only seems reliable because of lack of use. It costs
1.5 BTC per day to create enough transactions to fill the block chain at
the minimum relay fee, and a small multiple of that at actual fee levels.
Assuming that rate remains similar with an increased block size, that
remains cheap.
If you want transactions to be cheap, it will also be cheap to make them

@_date: 2015-08-10 19:14:26
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] What Lightning Is 
On Aug 10, 2015 7:03 PM, "odinn via bitcoin-dev" <
Note that I've
No offence, but I think that anyone who claims a block size limit change
can be done as a soft fork has some basic reading to do first.
Also, please keep this thread about Lightning.

@_date: 2015-08-10 23:01:05
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] If you had a single chance to double the 
On Aug 7, 2015 11:19 PM, "Sergio Demian Lerner via bitcoin-dev" <
with any of them: "SPV" mining and the relay network has shown that block
propagation is not an issue for such as small change. Mining centralization
won't radically change for a 2x adjustment.
I don't understand this. All problems that result from propagation delay
are literally doubled by doing so. Centralization pressure results from the
ratio between propagation time and interblock time. Efficient propagation
algorithms like the relay network make this presumably grow sublinear with
larger blocks, but changing the interblock time affects it exactly
All problems that result from propagation delay are literally doubled by
doing this. Doubling the block size has a smaller effect. You may argue
that these centralization effects are small, but reducing the interblock
time has a stronger effect on them than the block size.
Also, you seem to consider SPV mining a good thing? It requires trust
between miners that know eachother, and fundamentally breaks the security
assumption of SPV clients... and if the propagation/interblock ratio was
lower, SPV mining would have less effect. I'd say it is exactly a result of
the centralization pressure we're trying to avoid.

@_date: 2015-08-11 00:31:33
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] If you had a single chance to double the 
On Aug 11, 2015 12:11 AM, "Sergio Demian Lerner" began using the TheBlueMatt relay network, so deteriorating the ratio 2x
does not put miners in a unknown future, but in an future which is far
better than the state they were a year ago.
It's still worse than doubling the block size, which was your main argument.
pushed even faster, fit in a single network packet, and can do without
inv/getdata round-trips because they basically "pay" for the bandwidth
usage by its own proof of work).
the time it takes currently to propagate an empty block.
So yes, better relay protocols (whether you consider "SPV mining" a form of
that or not) reduce the effect of the block size. That does not give any
benefit for reduced interblock times.
Your argument seems to be "centralization pressure is not bad now, because
it already improved a lot... so we can make it worse again by reducing
interblock time"? I disagree that it is not bad, and shorter blocks have
other downsides which were already mentioned.
avoid. It's a real incentive. It must exists so Bitcoin is incentive
compatible. We can talk for hours and hours and we won't prevent miners
from doing it. I predicted it back in 2013, without even being a miner
myself. It's here to stay. Forever. It's a pity Greg chose that awful name
of "SPV" mining instead some cool name like "Instant" mining that we could
market as Bitcoin latest feature :)
breaks the security assumption of SPV clients...
The SPV security assumption is that no hashrate majority will collude in
order to make my transactions incorrectly look confirmed.
With validation-less mining, even a 0.1% hashrate that is part of a group
with 60% hashrate is enough to make that happen.
Of course they won't intentionally do that. No other miner would agree to
do validation-less mining with them again, making it harder for them to
compete. So it is not permissionless: you get higher profitability by
making an agreement with the largest hashrate. I think that is a much worse
centralization effect than having an optional centralized relay network
available... there could even be multiple such networks.
if I don't know you, I know you wouldn't waste 25 BTC to try to cheat me
for 25 BTC with a probability of 1/100, that's for sure. On average, you
loose 24.75 BTC per cheat attempt.
Per cheat attempt, or per bug.
incentives. SPV mining also must be there to prevent malicious actors from
DoS-ing the relay network. If it's there, then the DoS incentive disappears.
I thought about this too. Since headers-first it would be trivial to do: if
our best header is ahead of our best block, hand out an empty template in
createblocktemplate, and we're done.
Unfortunately, Greg Maxwell pointed out that this (even with a time limit)
amplifies selfish mining, since I can propagate headers before propagating
blocks, in order to make others temporarily work on top of my chain.
and that's good for miners. Last, as I already said, having a lower average
block interval strengthens Bitcoin value proposition, so miners would be
delighted that their bitcoins are more worthy.
Only a small constant factor, but yes.

@_date: 2015-08-11 00:52:23
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Aug 11, 2015 12:18 AM, "Thomas Zander via bitcoin-dev" <
That's an incorrect analogy. You choose the rate you pay, and get higher
priority when you pay more. Taxi drivers can't pick out higher-paying
customers in advance.
A better comparison is Uber, which charges more in places with high demand,
and you can accept or refuse in advance. And yes, it remains reliable if
you're among those with the highest willingness to pay.
If 2500 transactions fit in the block chain per day (assuming constant
size) and there are less than 2500 per hour that pay at least 0.001 BTC in
fee, then any transaction which pays more than 0.001 BTC will have a very
high chance of getting in a small multiple of one hour, since miners
prioritize by feerate.
If there are in addition to that 5000 transactions per hour which pay less,
then yes, they need to compete for the remaiming space and their
confirmation will be unreliable.
The whole point is that whether confirmation at a particular price point is
reliable depends on how much demand there is at that price point. And
increasing the block size out of fear of what might happen is failing to
recognize that it can always happen that there is a sudden change in demand
that outcompetes the rest.
The point is not that evolution towards a specific higher feerate needs to
happen, but an evolution to an ecosystem that accepts that there is never a
guarantee for reliability, unless you're willing to pay more than everyone
else - whatever that number is.

@_date: 2015-08-11 01:11:14
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
transport? They
may be
is more
priority when you pay more. Taxi drivers can't pick out higher-paying
customers in advance.
I'm sorry, I missed your "if everyone is paying it". This changes a lot. I
agree with you: if everyone wants to pay much then it becomes unreliable.
But I don't think that is something we can avoid with a small constant
factor block size increase, and we don't do the world a service by making
it look like it works for longer.
Let's grow within bounderies set by technology and centralization pressure
that we can agree on. Let the market decide whether how they will that will
low volume reliable transactions and/or high volume unreliable ones.

@_date: 2015-08-11 21:51:59
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Tue, Aug 11, 2015 at 9:37 PM, Michael Naber via bitcoin-dev <
The question is not what the technology can deliver. The question is what
price we're willing to pay for that. It is not a boolean "at this size,
things break, and below it, they work". A small constant factor increase
will unlikely break anything in the short term, but it will come with
higher centralization pressure of various forms. There is discussion about
whether these centralization pressures are significant, but citing that
it's artificially constrained under the limit is IMHO a misrepresentation.
It is constrained to aim for a certain balance between utility and risk,
and neither extreme is interesting, while possibly still "working".
Consensus rules are what keeps the system together. You can't simply switch
to new rules on your own, because the rest of the system will end up
ignoring you. These rules are there for a reason. You and I may agree about
whether the 21M limit is necessary, and disagree about whether we need a
block size limit, but we should be extremely careful with change. My
position as Bitcoin Core developer is that we should merge consensus
changes only when they are uncontroversial. Even when you believe a more
invasive change is worth it, others may disagree, and the risk from
disagreement is likely larger than the effect of a small block size
increase by itself: the risk that suddenly every transaction can be spent
twice (once on each side of the fork), the very thing that the block chain
was designed to prevent.
My personal opinion is that we should aim to do a block size increase for
the right reasons. I don't think fear of rising fees or unreliability
should be an issue: if fees are being paid, it means someone is willing to
pay them. If people are doing transactions despite being unreliable, there
must be a use for them. That may mean that some use cases don't fit
anymore, but that is already the case.

@_date: 2015-08-11 23:32:25
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
On Tue, Aug 11, 2015 at 11:30 PM, Angel Leon via bitcoin-dev <
Then they also don't need their transactions to be on the blockchain, right?

@_date: 2015-08-11 23:51:55
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Fees and the block-finding process 
If it is less expensive, it is harder to be reliable (because it's easier
for a sudden new use case to outbid the available space), which is less
useful for a payment mechanism.
If it has better scale (with the same technology), it will have higher
centralization pressure. The higher price you potentially pay (in fees) to
get your transactions on a smaller block chain is the price of higher
security and independence. Perhaps the compromise is not at the optimal
place, but please stop saying "below what the technology can do". The
technology can "do" gigabyte blocks I'm sure, If you accept that you need a
small cluster to keep up with validation, and all blocks are produced by a
single miner cartel.
IMHO, Bitcoin (or any cryptocurrency) on-chain as a payment system is:
* Expensive: there is a (known in advance and agreed upon) inflation that
we're using to pay miners. But by holding Bitcoin you're paying for the
security of the system, even if it is not in fees.
* Unreliable: you never know when suddenly there will be more higher-fee
transactions that outbid you.
* Slow, unless you already trust the sender to not double spend (in which
case you don't actually need the security of the blockchain).
I don't know the future, and I don't know what use cases will develop and
what they'll want to pay or what reliability they need. But let's please
not throw out the one quality that Bitcoin is still good at: lack of
centralized parties to trust.

@_date: 2015-12-09 22:43:22
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] "Subsidy fraud" ? 
I meant a miner claiming more in the coinbase's output than subsidy + fees

@_date: 2015-12-13 19:07:01
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
On Sun, Dec 13, 2015 at 4:25 PM, jl2012--- via bitcoin-dev
The use of a NOP opcode to indicate a witness script was something I
considered at first too, but it's not really needed. You wouldn't be
able to use that opcode in any place a normal opcode could occur, as
it needs to be able to inspect the full scriptSig (rather than just
its resulting stack) anyway. So both in practice and conceptually it
is only really working as a template that gets assigned a special
meaning (like P2SH did). We don't need an opcode for that, and instead
we could say that any scriptPubKey (or redeemscript) that consists of
a single push is a witness program.
What is that good for?
Do you mean the scriptPubKey itself, or the script that follows after
the version byte?
* The scriptPubKey itself: that's in contradiction with your rule 4,
as segwit scripts are by definition only a push (+ opcode), so they
can't be an OP_RETURN.
* The script after the version byte: agree - though it doesn't
actually need to be a script at all even (see further).
I don't think it's very useful to allow P2SH inside segwit, as we can
actually do better and allow segwit scripts to push the (perhaps 256
bit) hash of the redeemscript in the scriptPubKey, and have the full
redeemscript in the witness. See further for details. The numbers I
showed in the presentation were created using a simulation that used
that model already.
It is useful however to allow segwit inside P2SH (so the witness
program including version byte goes into the redeemscript, inside the
scriptSig). This allows old clients to send to new wallets without any
modifications (at slightly lower efficiency). The rules in this case
must say that the scriptSig is exactly a push of the redeemscript
(which itself contains the witness program), to provide both
compatibility with old consensus rules and malleability protection.
So let me summarize by giving an equivalent to your list above,
reflecting how my current prototype works:
A) A scriptPubKey or P2SH redeemscript that consists of a single push
of 2 to 41 bytes gets a new special meaning, and the byte vector
pushed by it is called the witness program.
A.1) In case the scriptPubKey pushes a witness program directly, the
scriptSig must be exactly empty.
A.2) In case the redeemscript pushes a witness program, the scriptSig
must be exactly the single push of the redeemscript.
B) The first byte of a witness program is the version byte.
B.1) If the witness version byte is 0, the rest of the witness program
is the actual script, which is executed after normal script evaluation
but with data from the witness rather than the scriptSig. The program
must not fail and result in a single TRUE on the stack, and nothing
else (to prevent stuffing the witness with pointless data during relay
of transactions).
B.2) if the witness version byte is 1, the rest of the witness program
must be 32 bytes, and a SHA256 hash of the actual script. The witness
must consist of an input stack to feed to the program, followed by the
serialized program itself (whose hash must match the hash pushed in
the witness program). It is executed after normal script evluation,
and must not fail and result in a single TRUE on the stack, and
nothing else.
B.3) if the witness version byte is 2 or higher, no further
interpretation of the data happens, but can be softforked in.
I'll write a separate mail on the block commitment structure.

@_date: 2015-12-16 19:34:32
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
You present this as if the Bitcoin Core development team is in charge
of deciding the network consensus rules, and is responsible for making
changes to it in order to satisfy economic demand. If that is the
case, Bitcoin has failed, in my opinion.
What the Bitcoin Core team should do, in my opinion, is merge any
consensus change that is uncontroversial. We can certainly -
individually or not - propose solutions, and express opinions, but as
far as maintainers of the software goes our responsibility is keeping
the system running, and risking either a fork or establishing
ourselves as the de-facto central bank that can make any change to the
system would greatly undermine the system's value.
Hard forking changes require that ultimately every participant in the
system adopts the new rules. I find it immoral and dangerous to merge
such a change without extremely widespread agreement. I am personally
fine with a short-term small block size bump to kick the can down the
road if that is what the ecosystem desires, but I can only agree with
merging it in Core if I'm convinced that there is no strong opposition
to it from others.
Soft forks on the other hand only require a majority of miners to
accept them, and everyone else can upgrade at their leisure or not at
all. Yes, old full nodes after a soft fork are not able to fully
validate the rules new miners enforce anymore, but they do still
verify the rules that their operators opted to enforce. Furthermore,
they can't be prevented. For that reason, I've proposed, and am
working hard, on an approach that includes Segregated Witness as a
first step. It shows the ecosystem that something is being done, it
kicks the can down the road, it solves/issues half a dozen other
issues at the same time, and it does not require the degree of
certainty needed for a hardfork.

@_date: 2015-12-16 21:59:41
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
Not correct. I propose defining the virtual_block_size as base_size +
witness_size * 0.25, and limiting virtual_block_size to 1M. This
creates a single variable to optimize for. If accepted, miners are
incentived to maximize fee per virtual_block_size instead of per size.
Wallet software can individually choose whether to upgrade or not.
Once they upgrade, they get to perform 1.75x as many transactions for
the same fee (assuming non-complex transactions), and this is
independent of whether anyone else upgrades.
Agree, however everyone can upgrade whenever they want, and get the
reduced fees as soon as they do. This is contrary to a hard fork,
which forces every full node to upgrade at once (though indeed, light
clients are not necessarily forced to upgrade).
It multiplies all current DoS vectors by a factor equal to the
capacity increase factor. SW increases capacity while leaving several
worst-case effects constant.
I believe you have misunderstood the proposal in that case.
Again, I think you misunderstood. The proposal includes a single new
formula for block size, and optimizes for it. In case the proposal is
accepted, the mining code is automatically as optimal as it was
I agree here. SW is not a replacement for a scale increase. However, I
think it can be adopted much more easily, as it doesn't require the
massively pervasive consensus that a hardfork requires to perform
But old clients may not care about the new rules, and they still
validate the old ones they chose to enforce.
Furthermore, soft forks cannot be prevented: miners can always choose
to enforce stronger rules than the network demands from them.

@_date: 2015-12-16 22:11:52
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
We are not avoiding a choice. We don't have the authority to make a choice.
I indeed think we can communicate much better that deciding consensus
rules is not within our power.

@_date: 2015-12-16 22:36:09
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Segregated Witness in the context of Scaling 
Maybe I haven't explained this properly, so consider this example:
A miner receives sets of 200 byte transactions with all identical
fees. Non-witness ones (whose virtual size is thus 200 bytes) and a
witness one (where 120 of the 200 bytes are witness data, so its
virtual size is 80 + 120*0.25 = 110 bytes).
The consensus rules would limit 1) the base size to 1000000 bytes 2)
the virtual size to 1000000 bytes. However, as the virtual size is
defined as the sum of the base size plus a non-negative number,
satisfying (2) always implies satisfying (1).
Thus, the miners' best strategy is to accept the witness transactions,
as it allows 1000000/110=9090 transactions rather than
In fact, the optimal fee maximizing strategy is always to maximize fee
per virtual size.

@_date: 2015-12-18 03:30:38
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] On the security of softforks 
Hello all,
For a long time, I was personally of the opinion that soft forks
constituted a mild security reduction for old full nodes, albeit one
that was preferable to hard forks due to being far less risky, easier,
and less forceful to deploy.
After thinking more about this, I'm not convinced that it is even that anymore.
Let's analyze all failure modes (and feel free to let me know whether
I've missed any specific ones):
1) The risk of an old full node wallet accepting a transaction that is
invalid to the new rules.
The receiver wallet chooses what address/script to accept coins on.
They'll upgrade to the new softfork rules before creating an address
that depends on the softfork's features.
So, not a problem.
2) The risk of an old full node wallet accepting a transaction whose
coins passed through a script that depends on the softforked rules.
It is reasonable that the receiver of a transaction places some trust
in the sender, and on the basis of that, decides to reduce the number
of confirmations before acceptance. In case the transaction indirectly
depends on a low-confirmation transaction using softforked rules, it
may be treated as an anyone-can-spend transaction. Obviously, no trust
can be placed in such a transactions not being reorged out and
replaced with an incompatible one.
However, this problem is common for all anyonecanspend transactions,
which are perfectly legal today in the blockchain. So, if this is a
worry, we can solve it by marking incoming transactions as "uncertain
history" in the wallet if they have an anyonecanspend transaction with
less than 6 confirmations in its history. In fact, the same problem to
a lesser extent exists if coins pass through a 1-of-N multisig or so,
because you're not only trusting the (indirect) senders, but also
their potential cosigners.
3) The risk of an SPV node wallet accepting an unconfirmed transaction
which is invalid to new nodes.
Defrauding an SPV wallet with an invalid unconfirmed transaction
doesn't change with the introduction of new consensus rules, as they
don't validate them anyway.
In the case the client trusts the full node peer(s) it is connected to
to do validation before relay, nodes can either indicate (service bit
or new p2p message) which softforks are accepted (as it only matters
to SPV wallets that wish to accept transactions using new style script
anyway), or wallets can rely on the new rules being non-standard even
to old full nodes (which is typically aimed for in softforks).
4) The risk of an SPV node wallet accepting a confirmed transaction
which is invalid to new nodes
Miners can of course construct an invalid block purely for defrauding
SPV nodes, without intending to get that block accepted by full nodes.
That is expensive (no subsidy/fee income for those blocks) and more
importantly it isn't in any way affected by softforks.
So the only place where this matters is where miners create a block
chain that violates the new rules, and still get it accepted. This
requires a hash rate majority, and sufficiently few economically
important full nodes that forking them off is a viable approach.
It's interesting that even though it requires forking off full nodes
(who will notice, there will be an invalid majority hash rate chain to
them), the attack only allows defrauding SPV nodes. It can't be used
to bypass any of the economic properties of the system (as subsidy and
other resource limits are still enforced by old nodes, and invalid
scripts will either not be accepted by old full nodes wallets, or are
as vulnerable as unrelated anyonecanspends).
Furthermore, it's easily preventable by not using the feature in SPV
wallets until a sufficient amount of economically relevant full nodes
are known to have upgraded, or by just waiting for enough
So, we'd of course prefer to have all full nodes enforce all rules,
but the security reduction is not large. On the other hand, there are
also security advantages that softforks offer:
A) Softforks do not require the pervasive consensus that hardforks
need. Soft forks can be deployed without knowing when all full nodes
will adopt the rule, or even whether they will ever adopt it at all.
B) Keeping up with hard forking changes puts load on full node
operators, who may choose to instead switch to delegating full
validation to third parties, which is worse than just validating the
old rules.
C) Hardfork coordination has a centralizing effect on development. As
hardforks can only be deployed with sufficient node deployment, they
can't just be triggered by miner votes. This requires central
coordination to determine flag times, which is incompatible with
having multiple independent consensus changes being proposed. For
softforks, something like BIP9 supports having multiple independent
softforks in flight, that nodes can individually chose to accept or
not, only requiring coordination to not choose clashing bit numbers.
For hardforks, there is effectively no choice but having every
codebase deployed at a particular point in time to support every
possible hard forks (there can still be an additional hashpower based
trigger conditions for hardforks, but all nodes need to support the
fork at the earliest time it can happen, or risk being forked off).
D) If you are concerned about the security degradation a soft fork
might bring, you can always configure your node to treat a (signalled)
softfork as a hardfork, and stop processing blocks if a sortfork
condition is detected. The other direction is not possible.

@_date: 2015-12-18 08:56:58
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
I don't see any plan, but will you say the same thing when the subsidy
dwindles, and mining income seems to become uncertain? It will equally
be an economic change, which equally well will have been predictable,
and it will equally well be treatable with a hardfork to increase the
Yes, I'm aware the argument above is a straw man, because there was
clear expectation that the subsidy would go down asymptotically, and
much less an expectation that the blocksize would remain fixed
But I am not against a block size increase hard fork. My talk on
segregated witness even included proposed pursuing a hard fork at a
slightly later stage.
But what you're arguing for is that - despite being completely
expected - blocks grew fuller, and people didn't adapt to block size
pressure and a fee market, so the Core committee now needs to kick the
can down the road, because we can't accept the risk of economic
change. That sounds very much like a bailout to me.
Again. I am not against growth, but increasing in response to fear of
economic change is the wrong approach. Economic change is inevitable.
That is an assumption. I expect demand for transactions at a given
feerate to stop growing at a certain contention level (and we've
reached some level of contention already, with mempools not being
cleared for significant amounts of time already).
That is assuming a hard fork consensus forming, deployment,
activation, ... goes faster than a softfork.
That's not required. Everyone who individually switches to new
transactions gets to do 1.75x more transactions for the same price
(and at the same time gets safer contracts, better script
upgradability, and more security models at their disposal), completely
independent of whether anyone else in the ecosystem does the same.
The only question is how many transactions for what price. Contention
always happens at a specific feerate level anyway.
Both SW and a short bump (which is apparently not what BIP102 does
anymore?) increase capacity available per price, and yes, they are
completely orthogonal.
My only disagreement is the motivation (avoiding economic change, as
opposed to aiming for safe growth) and the claim that a capacity
increase hardfork is easier and safe(r) to roll out quickly than
sortfork SW.
Apart from that, we clearly need to do both at some point.

@_date: 2015-12-18 16:48:43
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
I right?
However, SW immediately gives a 1.75 capacity increase for anyone who
adopts it, after the softfork, instantly. They don't need to wait for
anyone else.
A hard fork is an orthogonal improvement, which is also needed if we don't
want to be stuck with a constant maximum ultimately.
Hardforks can however only be deployed at a time when all full node
software can reasonably have agreed to upgrade, while a softfork can be
deployed much earlier.
They are independent improvements, and we need both. I am however of the
opinion that hard forks need a much clearer consensus and much longer
rollout timeframes to be safe (see my thread on the security of softforks).

@_date: 2015-12-21 05:33:16
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Better late than never, let me comment on why I believe pursuing this plan
is important.
For months, the block size debate, and the apparent need for agreement on a
hardfork has distracted from needed engineering work, fed the external
impression that nothing is being done, and generally created a toxic
environment to work in. It has affected my own productivity and health, and
I do not think I am alone.
I believe that soft-fork segwit can help us out of this deadlock and get us
going again. It does not require the pervasive assumption that the entire
world will simultaneously switch to new consensus rules like a hardfork
does, while at the same time:
* Give a short-term capacity bump
* Show the world that scalability is being worked on
* Actually improve scalability (as opposed to just scale) by reducing
bandwidth/storage and indirectly improving the effectiveness of systems
like Lightning.
* Solve several unrelated problems at the same time (fraud proofs, script
extensibility, malleability, ...).
So I'd like to ask the community that we work towards this plan, as it
allows to make progress without being forced to make a possibly divisive
choice for one hardfork or another yet.

@_date: 2015-12-26 17:44:41
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
That's exactly the point: a hard fork does not just affect miners, and
cannot just get decided by miners. All full nodes must have accepted the
new rules, or they will be forked off when the hashrate percentage triggers.
Furthermore, 75% is pretty terrible as a switchover point, as it guarantees
that old nodes will still see a 25% forked off chain temporarily.
My opinion is that the role of Bitcoin Core maintainers is judging whether
consensus for a hard fork exists, and is technically necessary and safe. We
don't need a hashpower vote to decide whether a hardfork is accepted or
not, we need to be sure that full noded will accept it, and adopt it in
time. A hashpower vote can still be used to be sure that miners _also_
Currently, a large amount of developers and others believe that the
treshhold for a hardfork is not reached, especially given the fact that we
scale in the short term, as well as make many changes that long-term
benefit scalability, with just a softfork (which does not require forking
off nodes that don't adopt the new rules, for whatever reason).

@_date: 2015-12-27 00:01:04
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
guarantees that old nodes will still see a 25% forked off chain temporarily.
4000 to 8000 blocks (1 to 2 months).
I think that's extremely short, even assuming there is no controversy about
changing the rules at all. Things like BIP65 and BIP66 already took
significantly longer than that, were uncontroversial, and only need miner
adoption. Full node adoption is even slower.
I think the shortest reasonable timeframe for an uncontroversial hardfork
is somewhere in the range between 6 and 12 months.
For a controversial one, not at all.

@_date: 2015-12-27 00:16:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size: It's economics & user preparation & 
hard fork to increase the blocksize for years, I do not think that
mobilizing people to upgrade their nodes is going to be hard.
their full nodes. We may want to request that miners not trigger the fork
until some percentage of visible full nodes have upgraded.
I am generally not interested in a system where we rely on miners to make
that judgement call to fork off nodes that don't pay attention and/or
disagree with the change. This is not because I don't trust them, but
because I believe one of the principle values of the system is that its
consensus system should be hard to change.
I can't tell you what code to run of course, but I can decide what system I
find interesting to build. And it seems many people have signed off on
working towards a plan that does not include a hard fork being scheduled
right now:

@_date: 2015-02-01 11:00:39
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] var_int ambiguous serialization 
Hashes are always computed by reserializing data structures, never by
hashing wire data directly. This has been the case in every version of the
reference client's code that I know of.
This even meant that for example a block of 999999 bytes with non-shortest
length for the transaction count could be over the mazimum block size, but
still be valid.
As Wladimir says, more recently we switched to just failing to deserialize
(by throwing an exception) whenever a non-shortest form is used.

@_date: 2015-02-02 16:44:37
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I would like to fix this at some point in any case.
If we want to do that, we must at least have signatures with too-long
R or S values as non-standard.
One way to do that is to just - right now - add a patch to 0.10 to
make those non-standard. This requires another validation flag, with a
bunch of switching logic.
The much simpler alternative is just adding this to BIP66's DERSIG
right now, which is a one-line change that's obviously softforking. Is
anyone opposed to doing so at this stage?

@_date: 2015-02-03 10:15:18
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I understand it's late, which is also why I ask for opinions. It's
also not a priority, but if we release 0.10 without, it will first
need a cycle of making this non-standard, and then in a further
release doing a second softfork to enforce it.
It's a 2-line change; see

@_date: 2015-02-03 15:38:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I'm retracting this proposed change.
Suhar Daftuas pointed out that there remain edge-cases which are not
covered (a 33-byte R or S whose first byte is not a zero). The intent
here is really making sure that signature validation and parsing can
be entirely separated, and that signature checking itself does not
need a third return value ("invalid encoding", in addition to "valid
signature" and "invalid signature"). If we don't want to make
assumptions about how that implementation works, the only guaranteed
way of doing that is requiring that R and S are in fact within the
range allowed by secp256k1, which would require an integer decoder
inside the signature encoding checker. I consider that to be
In addition, a much cleaner solution that covers this as well has
already been proposed: only allow 0 (the empty byte vector) as invalid
signature. That would 100% align signature validity with decoding, and
is much simpler to implement.

@_date: 2015-02-06 13:38:40
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Four implementations exist now:
* for master:  (merged)
* for 0.10.0:  (merged,
and included in 0.10.0rc4)
* for 0.9.4: * for 0.8.6: The 0.8 and 0.9 version have reduced test code, as many tests rely on
new test framework code in 0.10 and later, but the implementation code
is identical. Work to improve that is certainly welcome.

@_date: 2015-01-20 19:35:49
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Hello everyone,
We've been aware of the risk of depending on OpenSSL for consensus
rules for a while, and were trying to get rid of this as part of BIP
62 (malleability protection), which was however postponed due to
unforeseen complexities. The recent evens (see the thread titled
"OpenSSL 1.0.0p / 1.0.1k incompatible, causes blockchain rejection."
on this mailing list) have made it clear that the problem is very
real, however, and I would prefer to have a fundamental solution for
it sooner rather than later.
I therefore propose a softfork to make non-DER signatures illegal
(they've been non-standard since v0.8.0). A draft BIP text can be
found on:
    The document includes motivation and specification. In addition, an
implementation (including unit tests derived from the BIP text) can be
found on:
    Comments/criticisms are very welcome, but I'd prefer keeping the
discussion here on the mailinglist (which is more accessible than on
the gist).

@_date: 2015-01-21 15:30:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Thanks for the comments. I hope I have clarified the text a bit accordingly.
I've renamed the function to IsValidSignatureEncoding, as it is not
strictly about DER (it adds a Bitcoin-specific byte, and supports and
empty string too).
I've expanded the comments about it a bit.

@_date: 2015-01-21 23:20:25
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I agree that is more in the spirit of C++, but part of the motivation
for including C++ code that it mostly matches the exact code that has
been used in the past two major Bitcoin Core releases (to interpret
signatures as standard).

@_date: 2015-01-25 10:34:08
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Glad that you point this out; I believe that's a weakness with more
impact now that this function is used for consensus. Let me clarify.
This function was originally written for Bitcoin Core v0.8.0, where it
was only used to enforce non-standardness, not consensus. In that
setting, there was no need to require a maximum length for the R and S
arguments, as overly-long R or S values (which, because of a further
rule, do not have excessive padding) will always result in integers >=
2^256, which means the encoded signature would never be valid
according to the ECDSA specification. A restriction on the total
length is required however, as BER allows multi-byte length
descriptors, which this function cannot (and shouldn't, as it's not
DER) parse.
However, in the currently proposed soft fork, non-DER results in
immediate script failure, which is distinguishable from invalid
signatures (by negating the result of a CHECKSIG, for example using a
NOT after it). I must admit that having invalid signatures with
overly-long R or S but acceptable R+S size be distinguishable from
invalid signatures where R+S is too large is ugly, and unnecessary.
Adding individual R and S length restrictions (ideally: saying that no
more than 32 bytes, excluding the padding 0 byte in front, is invalid)
would be trivial, but it means deviating slightly from the
standardness rule implementation that has been deployed for a while.
There should not really be much risk in doing so, as there are still
no node implementation releases (apart from the v0.10.0 rc's) that
would mine a CHECKSIG whose result is negated.
So, I think there are two options:
* Just add this R/S length restriction rule as a standardness
requirement, but not make it part of the soft fork. A later softfork
can then add this easily. The same can be done for several other
changes if they are deemed useful, like only allowing 0 (the empty
array) as invalid signature (any other causes failure script
immediately), requiring correct encoding even for non-evaluated
signatures, ...
* Add it to the softfork now, and be done with it.

@_date: 2015-01-25 12:57:23
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
I've lately been updating the BIP text without updating the code in
the repository; I've synced them now. The sigsize=0 case was actually
already handled elsewhere already, so I removed the code and added a
comment about it now in the BIP text.
You didn't miss anything; that's correct. In fact, Peter Todd already
pointed out the possibility of making non-empty invalid signatures
illegal. The reason for not doing it yet is that I'd like this BIP to
be minimal and uncontroversial - it's a real problem we want to fix as
fast as is reasonable. It wouldn't be hard to make this a standardness
rule though, and perhaps later softfork it in as consensus rule if
there was sufficient agreement about it.
A significiant part of DERSIG behaviour (which didn't change, only the
cases in which it is enforced) was already tested, in fact. Some
branches remained untested however; I've added extra test cases in the
repository. They give 100% coverage for IsValidSignatureEncoding (the
new name for IsDERSignature) now (tested with gcov).
I agree, but that requires very significant changes to the codebase,
as we currently have no way to mine blocks with non-acceptable
transactions. Ideally, the RPC tests gain some means of
building/mining blocks from without the Python test framework. Things
like that would make the code changes also hard to backport, which we
definitely will need to do to roll this out quickly.

@_date: 2015-07-05 18:21:44
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 68 (Relative Locktime) bug 
I would say yes. Just putting a locktime in transaction may help against
fee sniping, even in transactions that are allowed to be mined at the same
time as some of their dependencies?

@_date: 2015-07-15 12:11:43
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Significant losses by double-spending unconfirmed 
On Wed, Jul 15, 2015 at 12:06 PM, Me via bitcoin-dev <
Running normal full nodes only provides extra service to nodes
synchronizing and lightweight clients. It does not "make the network
stronger" in the sense that it does not reduce the trust the participants
need to have in each other.
It's such a misconception that running many nodes somehow helps. It's much
better that you run and control one or a few full nodes which you actually
use to validate your transactions, than to run 1000s of nodes in third
party datacenters. The latter only looks more decentralized.

@_date: 2015-07-22 12:52:20
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Bitcoin Core and hard forks 
Hello all,
I'd like to talk a bit about my view on the relation between the Bitcoin
Core project, and the consensus rules of Bitcoin.
I believe it is the responsibility of the maintainers/developers of Bitcoin
Core to create software which helps guarantee the security and operation of
the Bitcoin network.
In addition to normal software maintenance, bug fixes and performance
improvements, this includes DoS protection mechanism deemed necessary to
keep the network operational. Sometimes, such (per-node configurable)
policies have had economic impact, for example the dust rule.
This also includes participating in discussions about consensus changes,
but not the responsibility to decide on them - only to implement them when
agreed upon. It would be irresponsible and dangerous to the network and
thus the users of the software to risk forks, or to take a leading role in
pushing dramatic changes. Bitcoin Core developers obviously have the
ability to make any changes to the codebase or its releases, but it is
still up to the community to choose to run that code.
Some people have called the prospect of limited block space and the
development of a fee market a change in policy compared to the past. I
respectfully disagree with that. Bitcoin Core is not running the Bitcoin
economy, and its developers have no authority to set its rules. Change in
economics is always happening, and should be expected. Worse, intervening
in consensus changes would make the ecosystem more dependent on the group
taking that decision, not less.
So to point out what I consider obvious: if Bitcoin requires central
control over its rules by a group of developers, it is completely
uninteresting to me. Consensus changes should be done using consensus, and
the default in case of controversy is no change.
My personal opinion is that we - as a community - should indeed let a fee
market develop, and rather sooner than later, and that "kicking the can
down the road" is an incredibly dangerous precedent: if we are willing to
go through the risk of a hard fork because of a fear of change of
economics, then I believe that community is not ready to deal with change
at all. And some change is inevitable, at any block size. Again, this does
not mean the block size needs to be fixed forever, but its intent should be
growing with the evolution of technology, not a panic reaction because a
fear of change.
But I am not in any position to force this view. I only hope that people
don't think a fear of economic change is reason to give up consensus.

@_date: 2015-07-28 16:27:18
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Disclosure: consensus bug indirectly solved by BIP66 
Hash: SHA1
Hello all,
I'd like to disclose a vulnerability I discovered in September 2014,
which became unexploitable when BIP66's 95% threshold was reached
earlier this month.
 Short description:
A specially-crafted transaction could have forked the blockchain
between nodes:
* using OpenSSL on a 32-bit systems and on 64-bit Windows systems
* using OpenSSL on non-Windows 64-bit systems (Linux, OSX, ...)
* using some non-OpenSSL codebases for parsing signatures
 Upgrade instructions:
None. Transactions that could trigger this problem have become invalid
on the network since BIP66's deployment of version 3 blocks reached 95%
on July 4th, 2015.
 Long description:
The problem is related to the signature encoding rules.
Bitcoin's signatures are ASN.1 BER encoded. BER is a complex standard
that allows many different encodings for the same data. Since Bitcoin
Core 0.8, a standardness rule has been in effect that only allowed
subset of encodings (DER) for relay and mining, even though any BER
remained valid in the blockchain - at least in theory.
In practice, BER has many weird edge cases, and I have not found a
single cryptographic codebase that can parse all of them correctly.
This includes OpenSSL, Crypto++, BouncyCastle, btcec, and our own
libsecp256k1 library.
This on itself would not be a problem, as full nodes on the network
currently use OpenSSL. However, while researching what was needed to
make libsecp256k1 compatible with it, I discovered that OpenSSL is even
inconsistent with itself across different platforms.
One of the features of BER is the ability for internal structures to
have a length descriptor whose size itself is up to 126 bytes (see
X.690-0207 8.1.3.5). A 1 terabyte data structure would for example use
a 5-byte length descriptor. However, there is no requirement to use the
shortest possible descriptor, so even a 70-byte ECDSA signature could
use a 5-byte length descriptor and be valid. Unfortunately, OpenSSL
supports length descriptors only as long as their size is at most that
of a C 'long int', a type whose size depends on the platform (Windows
and 32-bit Linux/OSX have a 4-byte long int, 64-bit Linux/OSX have an
8-byte long int). See
Some non-OpenSSL based signature validation
systems don't support such length descriptors at all, resulting in an
extra forking risk on top for them if used for blockchain validation.
This effectively means that a block chain containing a transaction with
a valid signature using such a 5-byte length descriptor would be
accepted by some systems and not by others, resulting in a fork if it
were mined into a block.
 Timeline:
* 2013-Feb-19: Bitcoin Core 0.8.0 was released, which made non-DER
signatures non-standard. No release since then would relay or mine
transactions that could trigger the vulnerability. However, such a
transaction was still valid inside blocks.
* 2014-Feb-10: I proposed BIP62 to deal with transaction malleability.
The BIP62 draft includes a rule that would make version 2 transactions
with non-DER signatures invalid.
* 2014-Jul-18: In order to make Bitcoin's signature encoding rules not
depend on OpenSSL's specific parser, I modified the BIP62 proposal to
have its strict DER signatures requirement also apply to version 1
transactions. No non-DER signatures were being mined into blocks
anymore at the time, so this was assumed to not have any impact. See
 and
Unknown at the time, but if deployed this would have solved the
* 2014-Sep-01: While analysing OpenSSL's source code for BER parsing, I
discovered the architecture dependency listed above and the associated
vulnerability. The best means to fix it at the time was by getting
BIP62 adopted.
* 2014-Sep-07, 2014-Oct-17, 2014-Oct-26, 2014-Dec-06, 2015-Jan-09:
Several proposed changes to BIP62. See
* 2015-Jan-10: OpenSSL releases versions 1.0.0p and 1.0.1k, with a fix
for CVE-2014-8275. The fix introduced a restriction on ECDSA signatures
to be strict DER, which would have solved all problems related to
signature encodings, except Bitcoin's consensus-critical nature
requires bug-for-bug compatibility between nodes. Worse, it seemed that
there was again a small (1%) number of blocks being created with
non-DER signatures in it, resulting in actual forks. The only immediate
solution that did not introduce more risk for forks was parsing and
re-encoding signatures using OpenSSL itself before verification to
bypass the restriction, making the problem persist. See
* 2015-Jan-21: The new attention to signature encoding might have
revealed the vulnerability, and the presence of miners not enforcing
strict DER might have made the vulnerability actually exploitable.
BIP62 was still a moving target, so we wanted a faster means to solve
this. Therefore, a new BIP was proposed with just the strict DER
requirement, numbered BIP66. This would both allow non-OpenSSL
verification, and solve the vulnerability, without needing to fix the
less urgent malleability problem that BIP62 wanted to address. See
* 2015-Feb-17: Bitcoin Core 0.10.0 was released, with support for
BIP66. See
* 2015-Jul-04: BIP66's 95% threshold was reached, enabling a consensus
rule for strict DER signatures in the blockchain. This solved the
vulnerability, and opens the door to using non-OpenSSL signature
verification in the near future.

@_date: 2015-07-30 14:50:46
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Thu, Jul 30, 2015 at 2:29 PM, Gavin via bitcoin-dev <
I think the benefit of an 8 MB over a 1 MB in terms of utility is marginal
(even assuming miners actually produce 8 MB blocks). There are very few use
cases that Bitcoin on-chain can support with a small extra factor. I think
the market will grow to adapt to whatever is offered anyway.
Bitcoin's advantage over other systems does not lie in scalability.
Well-designed centralized systems can trivially compete with Bitcoin's
on-chain transactions in terms of cost, speed, reliability, convenience,
and scale. Its power lies in transparency, lack of need for trust in
network peers, miners, and those who influence or control the system.
Wanting to increase the scale of the system is in conflict with all of
those. Attempting to buy time with a fast increase is not wanting to face
that reality, and treating the system as something whose scale trumps all
other concerns. A long term scalability plan should aim on decreasing the
need for trust required in off-chain systems, rather than increasing the
need for trust in Bitcoin.
Making controversial changes to the network, and not wanting to face the
reality that block chain space is a finite resource - whether enforced by a
consensus rule or by miner's capacity to process transactions - is a huge
treat to Bitcoin's usefulness in the long term.
I think the risks of trying to make a controversial change to the network
FAR outweighs the benefits of a small constant factor that "kicks the can
down the road".
Let's scale the block size gradually over time, according to technological

@_date: 2015-07-30 16:25:02
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
Hello all,
here is a proposal for long-term scalability I've been working on:
Some things are not included yet, such as a testnet whose size runs ahead
of the main chain, and the inclusion of Gavin's more accurate sigop
checking after the hard fork.

@_date: 2015-07-30 16:28:29
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Why Satoshi's temporary anti-spam measure isn't 
On Thu, Jul 30, 2015 at 4:05 PM, Gavin Andresen Oh come on. Immediately increasing to 8 MB while miners currently don't
even seem to bother validating blocks?
With the added belt&suspenders reality check of miners, who won't produce
Or a future where miners are even more centralized than now, which avoids
all problems relay and propagation speed has?
Lightning does not require a hard fork, except that larger blocks would be
very useful for its bulk settlements.
Or is the plan to avoid controversy by people voluntarily moving their
As I have said a dozen times now: sidechains are a mechanism for
experimentation. Maybe through them we will discover technology that allows
better on-chain and/or off-chain scalability, but people moving their coins
to a sidechain has far worse security tradeoffs than just increasing the
Bitcoin blockchain.
No plan for how to scale up is the worst of all possible worlds, and the
Ok, here is a proposal I was working on. I'd like to have had more time,
but I agree a direction/plan are needed to align expectations for the
future:  And I think that the reason that so many people care about this suddenly is
because of a fear that somehow the current block size "won't be enough".
Bitcoin has utility at any block size, and perhaps more at some values for
it than others. Talking about "not enough" is acknowledging that we really
believe the block size should scale to demand, while it is the other way

@_date: 2015-07-30 18:49:52
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Block size following technological growth 
ahead of the main chain, and the inclusion of Gavin's more accurate sigop
checking after the hard fork.
You're welcome.
conservative, and the most likely effect of being that conservative is that
the main blockchain becomes a settlement network, affordable only for
large-value transactions.
If there is demand for high-value settlements in Bitcoin, and this results
in a functioning economy where fees support the security of a transparent
network, I think that would be a much better outcome than what we have now.
centralization of payments (a future where only people running payment
hubs, big merchants, exchanges, and wallet providers settle on the
blockchain) and centralization of mining.
Well, centralization of mining is already terrible. I see no reason why we
should encourage making it worse. On the other hand, sure, not every
transaction fits on the blockchain. That is already the case, and will
remain the case with 2 MB or 8 MB or 100 MB blocks. Some use cases fit, and
others won't. We need to accept that, and all previous proposals I've seen
don't seem to do that.
Maybe the only ones that will fit are large settlements between layer-2
services, and maybe it will be something entirely different. But at least
we don't need to compromise the security of the main layer, or promise the
ecosystem unrealistic growth of space for on-chain transactions.
If Bitcoin needs to support a large scale, it already failed.
does monotonically increasing matter for max block size? I can't think of a
reason why a max block size of X bytes in block N followed by a max size of
X-something bytes in block N+1 would cause any problems.
I don't think it matters much, but it offers slightly easier transition for
the mempool (something Jorge convinced me of), and validation can benefit
from a single variable that can be set in a chain to indicate a switchover
happened, without needing to recalculate it every time.
I assume you're asking this because using raw nTime means you can check
what limits a p2p message should obey to by looking at just the first
bytes. I don't think this matters: your p2p protocol should deal with
whatever limits the system requires anyway. An attacker can take a valid
message of far in the chain, change a few bytes, and spam you with it at
zero extra effort anyway.

@_date: 2015-06-03 13:42:44
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bits proposal 
Thanks for all the comments.
I plan to address the feedback and work on an implementation next week.
On Tue, May 26, 2015 at 6:48 PM, Pieter Wuille

@_date: 2015-06-06 16:47:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
What do you gain by making PoPs actually valid transactions? You could for
example change the signature hashing algorithm (prepend a constant string,
or add a second hashing step) for signing, rendering the signatures in a
PoP unusable for actual transaction, while still committing to the same
actual transaction. That would also remove the need for the OP_RETURN to
catch fees.
Also, I would call it "proof of transaction intent", as it's a commitment
to a transaction and proof of its validity, but not a proof that an actual
transaction took place, nor a means to prevent it from being double spent.

@_date: 2015-06-06 17:13:46
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
Yes, of course. An alternative is adding a 21M BTC output at the end, or
bitflipping the txin prevout hashes, or another reversible transformation
on the transaction data that is guaranteed to invalidate it.
I think that the risk of asking people to sign something that is not an
actual transaction, but could be used as one, is very scary.
"Proof of Payment" indeed does make me think it's something that proves you
paid. But as described, that is not what a PoP does. It proves the ability
to create a particular transaction, and committing to it. There is no
actual payment involved (plus, payment makes me think you're talking about
BIP70 payments, not simple Bitcoin transactions).
I don't understand why something like "Proof of Transaction Intent" would
be incompatible with internet over telephone network either...

@_date: 2015-06-06 17:23:48
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
Using an invalid opcode would merely send funds into the void. It wouldn't
invalidate the transaction.

@_date: 2015-06-12 18:51:02
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Mining centralization pressure from 
Hello all,
I've created a simulator for Bitcoin mining which goes a bit further than
the one Gavin used for his blog post a while ago. The main difference is
support for links with different latency and bandwidth, because of the
clustered configuration described below. In addition, it supports different
block sizes, takes fees into account, does difficulty adjustments, and
takes processing and mining delays into account. It also simulates longer
periods of time, and averages the result of many simulations running in
parallel until the variance on the result is low enough.
The code is here: The configuration used in the code right now simulates two groups of miners
(one 80%=25%+25%+30%, one 20%=5%+5%+5%+5%), which are well-connected
internally, but are only connected to each other through a slow 2 Mbit/s
Here are some results.
This shows how the group of smaller miners loses around 8% of their
relative income (if they create larger blocks, their loss percentage goes
up slightly further):
  * Miner group 0: 80.000000% hashrate, blocksize 20000000.000000
  * Miner group 1: 20.000000% hashrate, blocksize 1000000.000000
  * Expected average block size: 16200000.000000
  * Average fee per block: 0.250000
  * Fee per byte: 0.0000000154
  * Miner group 0: 81.648985% income (factor 1.020612 with hashrate)
  * Miner group 1: 18.351015% income (factor 0.917551 with hashrate)
When fees become more important however, and half of a block's income is
due to fees, the effect becomes even stronger (a 15% loss), and the optimal
way to compete for small miners is to create larger blocks as well (smaller
blocks for them result in even less income):
  * Miner group 0: 80.000000% hashrate, blocksize 20000000.000000
  * Miner group 1: 20.000000% hashrate, blocksize 20000000.000000
  * Expected average block size: 20000000.000000
  * Average fee per block: 25.000000
  * Fee per byte: 0.0000012500
  * Miner group 0: 83.063545% income (factor 1.038294 with hashrate)
  * Miner group 1: 16.936455% income (factor 0.846823 with hashrate)
The simulator is not perfect. It doesn't take into account that multiple
blocks/relays can compete for the same bandwidth, or that nodes cannot
process multiple blocks at once.
The numbers used may be unrealistic, and I don't mean this as a prediction
for real-world events. However, it does very clearly show the effects of
larger blocks on centralization pressure of the system. Note that this also
does not make any assumption of destructive behavior on the network - just
simple profit maximalization.
Lastly, the code may be buggy; I only did some small sanity tests with
simple networks.

@_date: 2015-06-12 20:39:46
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Mining centralization pressure from 
If there is a benefit in producing larger more-fee blocks if they propagate
slowly, then there is also a benefit in including high-fee transactions
that are unlikely to propagate quickly through optimized relay protocols
(for example: very recent transactions, or out-of-band receoved ones). This
effect is likely an order of magnitude less important still, but the effect
is likely the same.

@_date: 2015-06-13 15:01:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Miners: You'll (very likely) need to 
Note that we've been above the 75% threshold since june 7th (before Peter's
main was sent).

@_date: 2015-06-13 16:39:04
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
In your proposal, transactions go to a chain based the addresses involved.
We can reasonably assume that different people's wallet will tend to be
distributed uniformly over several sidechains to hold their transactions
(if they're not, there is no scaling benefit anyway...). That means that
for an average transaction, you will need a cross-chain transfer in order
to get the money to the recipient (as their wallet will usually be
associated to a chain that is different from your own). Either you use an
atomic swap (which actually means you end up briefly with coins in the
destination chain, and require multiple transactions and a medium delay),
or you use the 2way peg transfer mechanism (which is very slow, and reduces
the security the recipient has to SPV).
Whatever you do, the result will be that most transactions are:
* Slower (a bit, or a lot, depending on what mechanism you use).
* More complex, with more failure modes.
* Require more and larger transactions (causing a total net extra load on
all verifiers together).
And either:
* Less secure (because you rely on a third party to do an atomic swap with,
or because of the 2 way peg transfer mechanism which has SPV security)
* Doesn't offer any scaling benefit (because the recipient needs to fully
validate both his own and the receiver chain).
In short, you have not added any scaling at all, or reduced the security of
the system significantly, as well as made it significantly less convenient
to use.
So no, sidechains are not a direct means for solving any of the scaling
problems Bitcoin has. What they offer is a mechanism for easier
experimentation, so that new technology can be built and tested without
needing to introduce a new currency first (with the related speculative and
network effect problems). That experimentation could eventually lead us to
discover mechanisms for better scaling, or for more scalability/security
tradeoffs (see for example the Witness Segregation that Elements Alpha has).

@_date: 2015-06-15 12:00:52
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
I did misunderstand that. That changes things significantly.
However, having paid is not the same as having had access to the input
coins. What about shared wallets or coinjoin?
Also, if I understand correctly, there is no commitment to anything you're
trying to say about the sender? So once I obtain a proof-of-payment from
you about something you paid, I can go claim that it's mine?
Why does anyone care who paid? This is like walking into a coffeshop,
noticing I don't have money with me, let me friend pay for me, and then
have the shop insist that I can't drink it because I'm not the buyer.
Track payments, don't try to assign identities to payers.

@_date: 2015-06-15 12:24:45
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] comments on BIP 100 
Since you keep bringing this up, I'll try to clarify this once again.
Since your patch was to enable querying spentness of particular outputs,
which is fundamentally unprovable data in Bitcoin as is (even your proposed
protocol that verifies scripts with amounts under sighash only proves
correctness of the txout data, not its spentness), I indeed don't see why
you would want anything else than querying such a service. I wish it were
different, but the choice is between querying a central service, or
trusting something a random peer on the internet tells you. At least with
the central service you can use an authenticated protocol with known keys
to detect MITM, and have someone to point to when they lie.
Not decentralized you say? Absolutely. But why do we want decentralization
in the first place? To remove central points of failure, and to reduce
trust. Bitcoin gets away with decentralization because it can validate (to
more or lesser extent) the data it received from its identityless peers. If
you can't do that, and are just aiming for removing central points of
failure, run a bunch of servers yourself, and let others run their own.
That sounds remarkably close to what you actually did, actually...
Do you want actually trustless querying of spentness in the future? Help
working on one of the several TXO commitment ideas to get them implemented.

@_date: 2015-06-15 12:40:51
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] comments on BIP 100 
The fact that using a centralized service is easier isn't a good reason
IMHO. It disregards the long-term, and introduces systemic risk.
But in cases where using a decentralized approach doesn't *add* anything, I
cannot reasonably promote it, and that's why I was against getutxos in the
P2P protocol.

@_date: 2015-06-15 19:09:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Scaling Bitcoin with Subchains 
If you are fine with the SPV security model, you are much better off by
just increasing the Bitcoin block size and using an SPV client, as those do
not care or even see the full block size by only downloading transactions
they care about. Infinite scalability!
The problem with scaling is that ultimately even SPV security relies on
others being able to validate. Both sidechains and larger block sizes make
that harder.
It's simple: either you care about validation, and you must validate
everything, or you don't, and you don't validate anything. Sidechains do
not offer you a useful compromise here, as well as adding huge delays and

@_date: 2015-06-15 22:50:36
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposal: Move Bitcoin Dev List to a 
That seems to be right. I just downloaded the entire archive of this list
(64 Mbyte, took a few seconds).

@_date: 2015-06-16 16:31:34
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
Not really. The problem is that you assume a transaction corresponds to a
single payment. This is true for simple wallet use cases, but not
compatible with CoinJoin, or with systems that for example would want to
combine multiple payments in a single transaction.
I owe you an apology here, for judging based on the summary you posted
rather than reading the actual text.
48 bits seems low to me, but it does indeed solve the problem. Why not 128
or 256 bits?
I think that is a mistake. You should not assume that the wallet who held
the coins is the payer/buyer. That's what I said earlier; you're implicitly
creating an identity (the one who holds these keys) based on the
transaction. This seems fundamentally wrong to me, and not necessary. The
receiver should not care who paid or how, he should care what was payed for.
The easiest solution to this IMHO would be an extension to the payment
protocol that gives you (or your wallet) a token in return for paying, and
that knowledge of that token is used to gain access to the services you

@_date: 2015-06-16 21:25:12
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
You can't avoid sharing the token, and you can't avoid sharing the private
keys used for signing either. If they are single use, you don't lose
anything by sharing them.
Also you are not creating a real transaction. Why does the OP_RETURN
limitation matter?

@_date: 2015-06-16 21:48:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] BIP for Proof of Payment 
I don't see why existing software could create a 40-byte OP_RETURN but not
larger? The limitation comes from a relay policy in full nodes, not a
limitation is wallet software... and PoPs are not relayed on the network.
Regarding sharing, I think you're talking about a different use case. Say
you want to pay for 1-week valid entrance to some venue. I thought the
purpose of the PoP was to be sure that only the person who paid for it, and
not anyone else can use it during that week.
My argument against that is that the original payer can also hand the
private keys in his wallet to someone else, who would then become able to
create PoPs for the service. He does not lose anything by this, assuming
the address is not reused.
So, using a token does not change anything, except it can be provided to
the payer - instead of relying on creating an implicit identity based on
who seems to have held particular private keys in the past.

@_date: 2015-06-18 14:29:42
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
On Thu, Jun 18, 2015 at 1:14 PM, Wladimir J. van der Laan The analogy goes further even. Even though I disagree with some of the
changes you're making, I respect Mike's (and anyone's) right to make a fork
of Bitcoin Core. That's how open source works: if people disagree with
changes made or not made, they can maintain their own version. However:
Consensus changes - in particular hardforks - are not about making a change
to the software. You are effectively asking users of the system to migrate
to a new system. Perhaps one which is a philosophical successor to the old
one, but a different system, with new rules that are incompatible with the
old one.
I believe this is something that can only be done if there is no
controversy about the change, for different reasons:
* Risk: no matter how you determine the switchover date, there is no way of
knowing when (and whether at all) everyone changes their full nodes (and
perhaps other software), and even very high hash power votes cannot prevent
an actual fork from appearing afterwards. At best, people lose the
guarantee that their confirmations are meaningful (because at some point it
becomes clear that the other side will get adopted, and they need to
switch). At worst, a fork persists, and two partitions appear, in each of
which you can spend every pre-existing coin. This defeats the primary
purpose Bitcoin was designed for: double spend protection.
* Philosophy: Bitcoin is not a democracy. The full node security model is
designed to minimize trust in other parties in the system. This works by
validating as much as possible according to the consensus rules. In
particular, there is no "majority vote" that can override things (contrary
to what some people think, it is not "longest chain wins, and a majority of
miners decide"; even a majority of miners cannot steal your coins or
produce more than the allowed subsidy, unless they convince others to
change their software). Changing the rules should be possible if there is
wide consensus, but nobody should feel forced to change their code against
their will.
* Governance: being able to push for a controversial change to the system
sets an incredibly dangerous precedent about who is in charge of the
system's rules. What if next time it is a change demanded by parties with
less good intentions (and yes, I believe people in this discussions all
have good intentions to improve the system in a way they think is most
useful)? I can promise you that I will say anything in mail to this list if
someone points a gun at me, and I think you should make the same assumption
about other people here. By avoiding controversial changes, you avoid
external and potentially invisible manipulation.
Of course, sometimes changes to the consensus rules may be wanted. The
presence of a bug is a good reason, and widespread agreement about one of
the system's limitation is too. As I said before, I think technological
growth in network bandwidth, processing power, and storage, are a good
reason why the system should be able to scale proportionally. I think there
are good technical and economic reasons why we should be cautious about
this, but the primary requirement is consensus, and aligning people's
expectation about what they can expect from network's evolution.

@_date: 2015-06-18 15:50:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Concerns Regarding Threats by a Developer 
Why do you keep talking about Bitcoin Core maintainers? The means for doing
a hard fork is convincing the network to run modified code, whether that is
a new version of Bitcoin Core or a fork of it, or something else entirely.
If I see consensus about a proposed network change, I will be in favor of
implementing it in Bitcoin Core. But we're not at that point. There is no
network change proposed with consensus. There is not even a patch to be
discussed. There are working proposals, and people are talking about them.
This is good.
I think maintainers of particular software should not be, and are not those
who decide the network's rules. People running the code are. Of course
maintainers have a large influence, but so do other people - like you.
Gavin were to do a pull request for the block size change and then merge
it, he would revert it. And I fully believe he would do so!
I believe so too, and I would do the same. Because I believe implementing a
consensus rule change without having very good expectations that the
network will adopt it, is reckless from the point of view of maintainers,
for all reasons I have mentioned before.

@_date: 2015-06-20 19:13:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Hard fork via miner vote 
Hello all,
I've seen ideas around hard fork proposals that involve a block version
vote (a la BIP34, BIP66, or my more recent versionbits BIP draft). I
believe this is a bad idea, independent of what the hard fork itself is.
Ultimately, the purpose of a hard fork is asking the whole community to
change their full nodes to new code. The purpose of the trigger mechanism
is to establish when that has happened.
Using a 95% threshold, implies the fork can happen when at least 5% of
miners have not upgraded, which implies some full nodes have not (as miners
are nodes), and in addition, means the old chain can keep growing too,
confusing old non-miner nodes as well.
Ideally, the fork should be scheduled when one is certain nodes will have
upgraded, and the risk for a fork will be gone. If everyone has upgraded,
no vote is necessary, and if nodes have not, it remains risky to fork them
I understand that, in order to keep humans in the loop, you want an
observable trigger mechanism, and a hashrate vote is an easy way to do
this. But at least, use a minimum timestamp you believe to be reasonable
for upgrade, and a 100% threshold afterwards. Anything else guarantees that
your forking change happens *knowingly* before the risk is gone.
You may argue that miners would be asked to - and have it in their best
interest - to not actually make blocks that violate the changed rule before
they are reasonably sure that everyone has upgraded. That is possible, but
it does not gain you anything over just using a 100% threshold, as how
would they be reasonably sure everyone has upgraded, while blocks creater
by non-upgraded miners are still being created?
TL;DR: use a timestamp switchover for a hard fork, or add a block voting
threshold as a means to keep humans in the loop, but if you do, use 100% as

@_date: 2015-06-20 20:11:53
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Hard fork via miner vote 
You can't observe the majority of nodes, only miners, and weighed by
hashrate. If you need a mechanism for protest, that should happen before
the hard fork change code is rolled out. I am assuming a completely
uncontroversial change, in order to not confuse this discussion with the
debate about what hard forks should be done.
So I am not talking about protest, just about deploying a change. And yes,
it is unreasonable to expect that every single node will upgrade. But there
is a difference between ignoring old unmaintained nodes that do not
influence anyone's behaviour, and ignoring the nodes that power miners
producing actual blocks. In addition, having no blocks on the old chain is
safer than producing a small number, as you want full nodes that have not
noticed the fork to fail rather than see a slow but otherwise functional

@_date: 2015-06-23 22:26:38
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Draft BIP : fixed-schedule block size increase 
If you believe we will need to go through this process once a year, we are
not talking about a one-time scaling to grant time for more decentralizing
solutions. It means you think we should keep scaling. I don't disagree
there - as long as we're talking about scaling as availability of
bandwidth, storage and processing power increase, there is no reason
Bitcoin's blockchain can't grow proportionally.
However, an initial bump 8 MB and the growth rate afterwards seem more like
a no-effectively-limit-ever to me.
I fear that the wish of not wanting to deal with - admittedly - a very hard
problem, resulted here in throwing away several protections we currently
have. And yes, I know you believe 8 MB won't be created immediately. I
truly, honestly, do not think so either. But I prefer a system where I
don't need to rely on anyone's guesses for the future.

@_date: 2015-06-26 16:09:18
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] The need for larger blocks 
Hello all,
here I'm going to try to address a part of the block size debate which has
been troubling me since the beginning: the reason why people seem to want
People say that larger blocks are necessary. In the long term, I agree - in
the sense that systems that do not evolve tend to be replaced by other
systems. This evolution can come in terms of layers on top of Bitcoin's
blockchain, in terms of the technology underlying various aspects of the
blockchain itself, and also in the scale that this technology supports.
I do, however, fundamentally disagree that a fear for a change in economics
should be considered to necessitate larger blocks. If it is, and there is
consensus that we should adapt to it, then there is effectively no limit
going forward. This is similar to how Congress voting to increase the
copyright term retroactively from time to time is really no different from
having an infinite copyright term in the first place. This scares me.
Here is how Gavin summarizes the future without increasing block sizes in
PR 6341:
rise; very-low-fee transactions will fail to get confirmed at all.
stop submitting transactions
growth and adoption
Is it fair to summarize this as "Some use cases won't fit any more, people
will decide to no longer use the blockchain for these purposes, and the
fees will adapt."?
I think that is already happening, and will happen at any scale. I believe
demand for payments in general is nearly infinite, and only a small portion
of it will eventually fit on a block chain (independent of whether its size
is limited by consensus rules or economic or technological means).
Furthermore, systems that compete with Bitcoin in this space already offer
orders of magnitude more capacity than we can reasonably achieve with any
blockchain technology at this point.
I don't know what subset of use cases Bitcoin will cater to in the long
term. They have already changed - you see way less betting transactions
these days than a few years ago for example - and they will keep changing,
independent of what effective block sizes we end up with. I don't think we
should be afraid of this change or try to stop it.
If you look at graphs of block sizes over time (for example,
 it seems to me that there is very little
"organic" growth, and a lot of sudden changes (which could correspond to
changing defaults in miner software, introduction of popular
sites/services, changes in the economy). I think these can be seen as the
economy changing to full up the available space, and I believe these will
keep happening at any size effectively available.
None of this is a reason why the size can't increase. However, in my
opinion, we should do it because we believe it increases utility and
understand the risks; not because we're afraid of what might happen if we
don't hurry up. And from that point of view, it seems silly to make a huge
increase at once...

@_date: 2015-06-26 17:24:21
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] The need for larger blocks 
I think you just proved my point by saying "when needed".

@_date: 2015-06-26 20:12:54
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] The need for larger blocks 
I am not saying that economic change is what we want. Only that it is
inevitable, independent of whether larger blocks happen or not.
I am saying that acting because of fear of economic change is a bad reason.
The reason for increase should be because of the higher utility. We need it
at some point, but there should be no rush.
I do understand that we want to avoid a *sudden* change in economic policy,
but I'm generally not too worried. Either fees increase and they get paid,
and we're good. But more likely is that some uses just move off-chain
because the block chain does not offer what they need. That's sad, but it
is inevitable at any size: some uses fit, some don't.

@_date: 2015-06-26 20:34:31
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] The need for larger blocks 
It is this sentence I disagree with. Why would there be a need? Bitcoin
provides utility at any block size, and potentially more with larger blocks.
But no matter what, I believe the economy will adapt to what is available.
And setting a precedent that increasing the size "because of a need" is
reasonable is to me essentially the same as saying the size should forever
scale to whatever people want.
I believe the most important effect of a limit block size - people deciding
not to use (on chain) Bitcoin transactions, is already happening, and it
will keep happening at any scale.
Either the resulting market is one which can live with high variability in
confirmation times, and blocks will end up being nearly full. Or maybe the
current fill level is what is acceptable, and we don't see much growth
beyond this, only a change in what it is used for.

@_date: 2015-07-01 01:45:09
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A possible solution for the block size limit: 
============================== START ==============================
The problem with this approach is that you need 100% exact behaviour for
every node on the network in their decision to reject a particular block.
So we need a 100% mempool synchronization across all nodes - otherwise just
an attempted double spend could result in a fork in the network because
some nodes saw it and some didn't. And actually, if we had 100% mempool
synchronization, we wouldn't need a blockchain in the first place, because
we could just use "first to enter mempool" as validity criterion.

@_date: 2015-05-07 05:47:16
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Block Size Increase 
Thanks for bringing this up. I'll try to keep my arguments brief, to avoid
a long wall of text. I may be re-iterating some things that have been said
before, though.
I am - in general - in favor of increasing the size blocks: as technology
grows, there is no reason why the systems built on them can't scale
proportionally. I have so far not commented much about this, in a hope to
avoid getting into a public debate, but the way seems to be going now,
worries me greatly.
* Controversial hard forks. I hope the mailing list here today already
proves it is a controversial issue. Independent of personal opinions pro or
against, I don't think we can do a hard fork that is controversial in
nature. Either the result is effectively a fork, and pre-existing coins can
be spent once on both sides (effectively failing Bitcoin's primary
purpose), or the result is one side forced to upgrade to something they
dislike - effectively giving a power to developers they should never have.
Quoting someone: "I did not sign up to be part of a central banker's
* The reason for increasing is "need". If "we need more space in blocks" is
the reason to do an upgrade, it won't stop after 20 MB. There is nothing
fundamental possible with 20 MB blocks that isn't with 1 MB blocks.
Changetip does not put their microtransactions on the chain, not with 1 MB,
and I doubt they would with 20 MB blocks. The reason for increase should be
"because we choose to accept the trade-offs".
* Misrepresentation of the trade-offs. You can argue all you want that none
of the effects of larger blocks are particularly damaging, so everything is
fine. They will damage something (see below for details), and we should
analyze these effects, and be honest about them, and present them as a
trade-off made we choose to make to scale the system better. If you just
ask people if they want more transactions, of course you'll hear yes. If
you ask people if they want to pay less taxes, I'm sure the vast majority
will agree as well.
* Miner centralization. There is currently, as far as I know, no technology
that can relay and validate 20 MB blocks across the planet, in a manner
fast enough to avoid very significant costs to mining. There is work in
progress on this (including Gavin's IBLT-based relay, or Greg's block
network coding), but I don't think we should be basing the future of the
economics of the system on undemonstrated ideas. Without those (or even
with), the result may be that miners self-limit the size of their blocks to
propagate faster, but if this happens, larger, better-connected, and more
centrally-located groups of miners gain a competitive advantage by being
able to produce larger blocks. I would like to point out that there is
nothing evil about this - a simple feedback to determine an optimal block
size for an individual miner will result in larger blocks for better
connected hash power. If we do not want miners to have this ability, "we"
(as in: those using full nodes) should demand limitations that prevent it.
One such limitation is a block size limit (whatever it is).
* Ability to use a full node. I very much dislike the trend of people
saying "we need to encourage people to run full nodes, in order to make the
network more decentralized". Running 1000 nodes which are otherwise unused
only gives some better ability for full nodes to download the block chain,
or for SPV nodes to learn about transactions (or be Sybil-attacked...).
However, *using* a full node for validating your business (or personal!)
transactions empowers you to using a financial system that requires less
trust in *anyone* (not even in a decentralized group of peers) than
anything else. Moreover, using a full node is what given you power of the
systems' rules, as anyone who wants to change it now needs to convince you
to upgrade. And yes, 20 MB blocks will change people's ability to use full
nodes, even if the costs are small.
* Skewed incentives for improvements. I think I can personally say that I'm
responsible for most of the past years' performance improvements in Bitcoin
Core. And there is a lot of room for improvement left there - things like
silly waiting loops, single-threaded network processing, huge memory sinks,
lock contention, ... which in my opinion don't nearly get the attention
they deserve. This is in addition to more pervasive changes like optimizing
the block transfer protocol, support for orthogonal systems with a
different security/scalability trade-off like Lightning, making full
validation optional, ... Call me cynical, but without actual pressure to
work on these, I doubt much will change. Increasing the size of blocks now
will simply make it cheap enough to continue business as usual for a while
- while forcing a massive cost increase (and not just a monetary one) on
the entire ecosystem.
* Fees and long-term incentives. I put this last, not because I don't think
it is not serious, but because I don't understand nearly enough about it.
I'll let others comment.
I don't think 1 MB is optimal. Block size is a compromise between
scalability of transactions and verifiability of the system. A system with
10 transactions per day that is verifiable by a pocket calculator is not
useful, as it would only serve a few large bank's settlements. A system
which can deal with every coffee bought on the planet, but requires a
Google-scale data center to verify is also not useful, as it would be
trivially out-competed by a VISA-like design. The usefulness needs in a
balance, and there is no optimal choice for everyone. We can choose where
that balance lies, but we must accept that this is done as a trade-off, and
that that trade-off will have costs such as hardware costs, decreasing
anonymity, less independence, smaller target audience for people able to
fully validate, ...
Choose wisely.
Thanks for reading this,

@_date: 2015-05-07 23:49:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Mechanics of a hard fork 
I would not modify my node if the change introduced a perpetual 100 BTC
subsidy per block, even if 99% of miners went along with it.
A hardfork is safe when 100% of (economically relevant) users upgrade. If
miners don't upgrade at that point, they just lose money.
This is why a hashrate-triggered hardfork does not make sense. Either you
believe everyone will upgrade anyway, and the hashrate doesn't matter. Or
you are not certain, and the fork is risky, independent of what hashrate
And the march 2013 fork showed that miners upgrade at a different schedule
than the rest of the network.

@_date: 2015-05-08 04:35:00
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Mechanics of a hard fork 
Yes, indeed, Bitcoin would be dead if this actually happens. But that is
still where the power lies: before anyone (miners or others) would think
about trying such a change, they would need to convince people and be sure
they will effectively modify their code.

@_date: 2015-05-08 07:52:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Removing transaction data from blocks 
So, there are several ideas about how to reduce the size of blocks being
sent on the network:
* Matt Corallo's relay network, which internally works by remembering the
last 5000 (i believe?) transactions sent by the peer, and allowing the peer
to backreference those rather than retransmit them inside block data. This
exists and works today.
* Gavin Andresen's IBLT based set reconciliation for blocks based on what a
peer expects the new block to contain.
* Greg Maxwell's network block coding, which is based on erasure coding,
and also supports sharding (everyone sends some block data to everyone,
rather fetching from one peer).
However, the primary purpose is not to reduce bandwidth (though that is a
nice side advantage). The purpose is reducing propagation delay. Larger
propagation delays across the network (relative to the inter-block period)
result in higher forking rates. If the forking rate gets very high, the
network may fail to converge entirely, but even long before that point, the
higher the forking rate is, the higher the advantage of larger (and better
connected) pools over smaller ones. This is why, in my opinion,
guaranteeing fast propagation is one of the most essential responsibility
of full nodes to avoid centralization pressure.
Also, none of this would let us "get rid of the block size" at all. All
transactions still have to be transferred and processed, and due to
inherent latencies of communication across the globe, the higher the
transaction rate is, the higher the number of transactions in blocks will
be that peers have not yet heard about. You can institute a policy to not
include too recent transactions in blocks, but again, this favors larger
miners over smaller ones.
Also, if the end goal is propagation delay, just minimizing the amount of
data transferred is not enough. You also need to make sure the
communication mechanism does not add huge processing overheads or adds
unnecessary roundtrips. In fact, this is the key difference between the 3
techniques listed above, and several people are working on refining and
optimizing these mechanisms to make them practically usable.

@_date: 2015-05-09 12:06:13
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
It's a very complex trade-off, which is hard to optimize for all use cases.
Using more UTXOs requires larger transactions, and thus more fees in
general. In addition, it results in more linkage between coins/addresses
used, so lower privacy.
The only way you can guarantee an economical reason to keep the UTXO set
small is by actually having a consensus rule that punishes increasing its

@_date: 2015-05-09 13:38:56
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
Miners do not care about the age of a UTXO entry, apart for two exceptions.
It is also economically irrelevant.
* There is a free transaction policy, which sets a small portion of block
space aside for transactions which do not pay sufficient fee. This is
mostly an altruistic way of encouraging Bitcoin adoption. As a DoS
prevention mechanism, there is a requirement that these free transactions
are of sufficient priority (computed as BTC-days-destroyed per byte),
essentially requiring these transactions to consume another scarce
resource, even if not money.
* Coinbase transaction outputs can, as a consensus rule, only be spent
after 100 confirmations. This is to prevent random reorganisations from
invalidating transactions that spend young coinbase transactions (which
can't move to the new chain). In addition, wallets also select more
confirmed outputs first to consume, for the same reason.

@_date: 2015-05-12 12:23:22
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
I have no strong opinion, but a slight preference for separate opcodes.
Reason: given the current progress, they'll likely be deployed
independently, and maybe the end result is not something that cleanly fits
the current CLTV argument structure.

@_date: 2015-05-13 10:14:07
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
Normalized transaction ids are only effectively non-malleable when all
inputs they refer to are also non-malleable (or you can have malleability
in 2nd level dependencies), so I do not believe it makes sense to allow
mixed usage of the txids at all. They do not provide the actual benefit of
guaranteed non-malleability before it becomes disallowed to use the old
mechanism. That, together with the +- resource doubling needed for the UTXO
set (as earlier mentioned) and the fact that an alternative which is only a
softfork are available, makes this a bad idea IMHO.
Unsure to what extent this has been presented on the mailinglist, but the
softfork idea is this:
* Transactions get 2 txids, one used to reference them (computed as
before), and one used in an (extended) sighash.
* The txins keep using the normal txid, so not structural changes to
* The ntxid is computed by replacing the scriptSigs in inputs by the empty
string, and by replacing the txids in txins by their corresponding ntxids.
* A new checksig operator is softforked in, which uses the ntxids in its
sighashes rather than the full txid.
* To support efficiently computing ntxids, every tx in the utxo set
(currently around 6M) stores the ntxid, but only supports lookup bu txid
This does result in a system where a changed dependency indeed invalidates
the spending transaction, but the fix is trivial and can be done without
access to the private key.
On May 13, 2015 5:50 AM, "Christian Decker"

@_date: 2015-05-13 11:40:34
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
Sufficient confirmations help of course, but make systems like this less
useful for more complex interactions where you have multiple unconfirmed
transactions waiting on each other. I think being able to rely on this
problem being solved unconditionally is what makes the proposal attractive.
For the simple cases, see BIP62.
I remember reading about the SIGHASH proposal somewhere. It feels really
I think you misunderstand the idea. This is related, but orthogonal to the
ideas about extended the sighash flags that have been discussed here before.
All it's doing is adding a new CHECKSIG operator to script, which, in its
internally used signature hash, 1) removes the scriptSigs from transactions
before hashing 2) replaces the txids in txins by their ntxid. It does not
add any data to transactions, and it is a softfork, because it only impacts
scripts which actually use the new CHECKSIG operator. Wallets that don't
support signing with this new operator would not give out addresses that
use it.
OP_*SIG* semantics don't change here either, we're just adding a superior
opcode (which in most ways behaves the same as the existing operators). I
agree with the advantage of not needing to monitor transactions afterwards
for malleated inputs, but I think you underestimate the deployment costs.
If you want to upgrade the world (eventually, after the old index is
dropped, which is IMHO the only point where this proposal becomes superior
to the alternatives) to this, you're changing *every single piece of
Bitcoin software on the planet*. This is not just changing some validation
rules that are opt-in to use, you're fundamentally changing how
transactions refer to each other.
Also, what do blocks commit to? Do you keep using the old transaction ids
for this? Because if you don't, any relayer on the network can invalidate a
block (and have the receiver mark it as invalid) by changing the txids. You
need to somehow commit to the scriptSig data in blocks still so the POW of
a block is invalidated by changing a scriptSig.
There certainly are merits using the SIGHASH approach in the short term (it
It requires a hard fork, but more importantly, it requires the whole world
to change their software (not just validation code) to effectively use it.
That, plus large up-front deployment costs (doubling the cache size for
every full node for the same propagation speed is not a small thing) which
may not end up being effective.

@_date: 2015-05-13 12:40:54
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
That's correct as long as you stay within your contract, but you likely
want compatibility with other software, without waiting an age before and
after your contract settles on the chain. It's a weaker argument, though, I
I remember reading about the SIGHASH proposal somewhere. It feels really
There would just be an OP_CHECKAWESOMESIG, which can do anything. It can
identical to how OP_CHECKSIG works now, but has a changed algorithm for its
signature hash algorithm. Optionally (and likely in practice, I think), it
can do various other proposed improvements, like using Schnorr signatures,
having a smaller signature encoding, supporting batch validation, have
extended sighash flags, ...
It wouldn't fetch A and normalize it; that's impossible as you would need
to go fetch all of A's dependencies too and recurse until you hit the
coinbases that produced them. Instead, your UTXO set contains the
normalized txid for every normal txid (which adds around 26% to the UTXO
set size now), but lookups in it remain only by txid.
You don't need a ntxid->txid mapping, as transactions and blocks keep
referring to transactions by txid. Only the OP_CHECKAWESOMESIG operator
would do the conversion, and at most once.
A client that did not update still would have no clue on how to handle
As for every softfork, it works by redefining an OP_NOP operator, so old
nodes simply consider these checksigs unconditionally valid. That does mean
you don't want to use them before the consensus rule is forked in
(=enforced by a majority of the hashrate), and that you suffer from the
temporary security reduction that an old full node is unknowingly reduced
to SPV security for these opcodes. However, as full node wallet, this
problem does not affect you, as your wallet would simply not give out
addresses using the new opcode (and thus, wouldn't receive coins using it),
unless it was upgraded to support it.
Could you provide an example of how this works?
Fair enough, I definitely agree the end result is superior in this case.
Also, what do blocks commit to? Do you keep using the old transaction ids
If the merkle tree of a block only commits to a transaction's normalized
hash, that means that the block hash does not change when the scriptSig is
altered. So, anyone on the network can take a random valid block, and
modify its scriptSig, and the block will become invalid _without_
invalidating the block header. This means that nodes on the network will
now classify that block header as having invalid transactions, and reject
it. Not having the ability anymore to mark blocks as invalid opens
significant DoS risks.
So yes, seeing a block with valid scriptSigs is indeed a proof the
transaction was legitimately authored. But the oppose is no longer true,
and we need that. The correct solution is to either keep using the old full
transaction ids in blocks, but ntxids everywhere else, or having some
alternative means to commit to the scriptSigs inside the block (for example
in the coinbase or using one of the more efficient block commitment
proposals), and have that enforced as consensus rule.

@_date: 2015-05-13 13:31:06
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
This was what I was suggesting all along, sorry if I wasn't clear.

@_date: 2015-05-13 17:37:30
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] [BIP] Normalized Transaction IDs 
Yes. So to be clear, I think there are 2 desirable end-goal proposals
(ignoring difficulty of changing things for a minute):
* Transactions and blocks keep referring to other transactions by full
txid, but signature hashes are computed off normalized txids (which are
recursively defined to use normalized txids all the way back to coinbases).
Is this what you are suggesting now as well?
* Blocks commit to full transaction data, but transactions and signature
hashes use normalized txids.
The benefit of the latter solution is that it doesn't need "fixing up"
transactions whose inputs have been malleated, but comes at the cost of
doing a very invasive hard fork.

@_date: 2015-05-13 17:58:28
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Long-term mining incentives 
You may find that the most economical solution, but I can't understand how
you can call it conservative.
Suggesting a hard fork is betting the survival of the entire ecosystem on
the bet that everyone will agree with and upgrade to new suggested software
before a flag date.

@_date: 2015-05-13 18:19:45
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Long-term mining incentives 
Transactions are already being dropped, in a more indirect way: by people
and businesses deciding to not use on-chain settlement. That is very sad,
but it's completely inevitable that there is space for some use cases and
not for others (at whatever block size). It's only a "things don't fit
anymore" when you see on-chain transactions as the only means for doing
payments, and that is already not the case. Increasing the block size
allows for more utility on-chain, but it does not fundamentally add more
use cases - only more growth space for people already invested in being
able to do things on-chain while externalizing the costs to others.
That only measures miner adoption, which is the least relevant. The
question is whether people using full nodes will upgrade. If they do, then
miners are forced to upgrade too, or become irrelevant. If they don't, the
upgrade is risky with or without miner adoption.

@_date: 2015-05-26 14:27:04
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] First-Seen-Safe Replace-by-Fee 
It's just a mempool policy rule.
Allowing the contents of blocks to change (other than by mining a competing
chain) would be pretty much the largest possible change to Bitcoin's

@_date: 2015-05-26 18:48:05
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Version bits proposal 
Hello everyone,
here is a proposal for how to coordinate future soft-forking consensus
changes: It supports multiple parallel changes, as well as changes that get
permanently rejected without obstructing the rollout of others.
Feel free to comment. As the gist does not support notifying participants
of new comments, I would suggest using the mailing list instead.
This is joint work with Peter Todd and Greg Maxwell.

@_date: 2015-05-28 10:34:32
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
I don't really believe that is possible. I'll argue why below. To be clear,
this is not an argument against increasing the block size, only against
using the assumption of size-independent propagation.
There are several significant improvements likely possible to various
aspects of block propagation, but I don't believe you can make any part
completely size-independent. Perhaps the remaining aspects result in terms
in the total time that vanish compared to the link latencies for 1 MB
blocks, but there will be some block sizes for which this is no longer the
case, and we need to know where that is the case.
* You can't assume that every transaction is pre-relayed and pre-validated.
This can happen due to non-uniform relay policies (different codebases, and
future things like size-limited mempools), double spend attempts, and
transactions generated before a block had time to propagate. You've
previously argued for a policy of not including too recent transactions,
but that requires a bound on network diameter, and if these late
transactions are profitable, it has exactly the same problem as making
larger blocks non-proportionally more economic for larger pools groups if
propagation time is size dependent).
  * This results in extra bandwidth usage for efficient relay protocols,
and if discrepancy estimation mispredicts the size of IBLT or error
correction data needed, extra roundtrips.
  * Signature validation for unrelayed transactions will be needed at block
relay time.
  * Database lookups for the inputs of unrelayed transactions cannot be
cached in advance.
* Block validation with 100% known and pre-validated transactions is not
constant time, due to updates that need to be made to the UTXO set (and
future ideas like UTXO commitments would make this effect an order of
magnitude worse).
* More efficient relay protocols also have higher CPU cost for
Again, none of this is a reason why the block size can't increase. If
availability of hardware with higher bandwidth, faster disk/ram access
times, and faster CPUs increases, we should be able to have larger blocks
with the same propagation profile as smaller blocks with earlier technology.
But we should know how technology scales with larger blocks, and I don't
believe we do, apart from microbenchmarks in laboratory conditions.

@_date: 2015-05-28 10:59:11
@_author: Pieter Wuille 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB 
dangerous for Bitcoin and the core developers themselves to become such a
central point of attack for those wishing to disrupt Bitcoin.
I could not agree more that developers should not be in charge of the
network rules.
Which is why - in my opinion - hard forks cannot be controversial things. A
controversial change to the software, forced to be adopted by the public
because the only alternative is a permanent chain fork, is a use of power
that developers (or anyone) should not have, and an incredibly dangerous
precedent for other changes that only a subset of participants would want.
The block size is also not just an economic policy. It is the compromise
the _network_ chooses to make between utility and various forms of
centralization pressure, and we should treat it as a compromise, and not as
some limit that is inferior to scaling demands.
I personally think the block size should increase, by the way, but only if
we can do it under a policy of doing it after technological growth has been
shown to be sufficient to support it without increased risk.

@_date: 2016-08-10 19:38:26
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
Worse. By revealing a public seed, anyone who has seen it (= anyone
who ever pays you through it) can identity all payments to _any_
address derived from that seed.

@_date: 2016-08-17 00:30:52
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
On Aug 17, 2016 00:23, "Russell O'Connor via bitcoin-dev" <
mined, it is more effective to just not relay the transaction rather than
to mess with the witness.  Given two transactions with the same txid and
different witness data, miners and good nodes ought to mine/relay the
version with the lower cost (smaller?) witness data.
That implies that everyone will see both versions and be able to make that
choice. Unfortunately, those two versions will be definition be in conflict
with each other, and thus only one will end up paying a fee. We're can't
relay two transactions for the price of one, or we'd expose the p2p network
to a very cheap DDoS attack: just send increasingly small versions of the
same transaction.
Segwit's third party mallebility protection makes it not an issue for
dependent contracts if transactions are mauled (=apparently the verb
related to malleability), but there are still good reasons for senders not
to gratuitously make their transactions extensible in size or other

@_date: 2016-08-17 00:39:08
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
limits on that?
BIP125 and mempool eviction both require the replacing transaction to have
higher fee, to compensate for the cost of relaying the replaced

@_date: 2016-08-25 11:02:27
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP Status updates (including to Active/Final 
This is not the place to discuss the merits and/or issues of these BIPs,
only whether they should be treated as final.
On Aug 25, 2016 10:51, "Marek Palatinus via bitcoin-dev" <

@_date: 2016-08-31 16:29:53
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 151 
Hello Eric,
I felt like I still owed you a response to the points below.
I think that's a false dichotomy. There is no reason why the P2P
network consists of purely servers (full nodes) and clients
(lightweight nodes). Where does a client fit that is SPV at startup,
but upgrades in the background to a full node? It seems strange that
such a client would use a 'client protocol' for initial connections,
but the P2P protocol for syncing with history, when both come from the
same peers, and transmit the same kind of information.
What would make sense IMHO is a protocol split between the different
kinds of transmission:
1) Historical block download
2) Block synchronization at the tip
3) Transaction relay
(1) prefers high bandwidth, has no connectivity concerns, and does not
care about latency and has no privacy concerns. (2) needs
partition-resistance, low latency and has also no privacy concerns.
(3) needs moderate latency, reliability of propagation and privacy.
If there were to be separate protocols for these, I would argue that
(3) should use opportunistic encryption always to increase transaction
source privacy, and (2) and (3) need authentication when one of the
peers is not fully validating.
BIP 150/151 give the tools to construct these.
Maybe, but I'm very unconvinced that that will happen more than how
today IP and DNS-based "authentication" is used already (in very
inadvisable ways).
I believe the risk is only in misunderstanding what it is good for,
and there significant benefits to a network that encrypts connections
by default, as it excludes purely passive attackers.
Preventing bad nodes from participating is a very hard problem, if not
impossible. That doesn't mean we can't improve the current situation:
people are relying on node identity already, and doing so in ways that
have unclear attack vectors (IP spoofing, DNS poisoning, BGP routing
attacks). Adding optional and non-discoverable cryptographic
identities can improve this.

@_date: 2016-12-10 09:39:57
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Managing block size the same way we do difficulty 
Models can predict orphan rate given block size and network/hashrate
topology, but you can't control the topology (and things like FIBRE hide
the effect of block size on this as well). The result is that if you're
purely optimizing for minimal orphan rate, you can end up with a single
(conglomerate of) pools producing all the blocks. Such a setup has no
propagation delay at all, and as a result can always achieve 0 orphans.

@_date: 2016-02-01 17:55:03
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Segwit Upgrade Procedures & Block Extension Data 
Agree, I've merged the changes to switch to a service bit instead.
We'll need further changes to prefer connecting to segwit nodes.
Those are good arguments for making the witness data more extensible.
I don't expect that changes that add more data to be relayed with
blocks will be frequent, though I certainly agree there may be some.
This will need a backward-incompatible change to the current segwit
change anyway, so at the risk of more bikeshedding, let me propose
going a bit further:
* The coinbase scriptSig gets a second number push (similar to the
current BIP34 height push), which pushes a number O. O is a byte
offset inside the coinbase transaction (excluding its witness data)
that points to a 32-byte hash H. This is more flexible and more
compact than what we have now (a suggestion by jl2012).
* H is the root of a Merkle tree, whose leaves are the hashes of the
coinbase witness's stack items.
* Item 0 of the coinbase witness stack must be 32 bytes, and must be
equal to the witness tree root.
* No further restrictions on the rest of the stack items; these can be
used for future commitments.
I agree with the concern, but I don't really understand how this idea solves it.

@_date: 2016-02-02 18:16:30
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] [BIP Draft] Allow zero value OP_RETURN in Payment 
On Feb 2, 2016 18:04, "Peter Todd via bitcoin-dev" <
I'll go further: whatever people have commented here and elsewhere about
this feature (myself included) are personal opinions on the feature itself,
in the hope you take the concerns into account.
These comments are not a judgement on whether this should be accepted as a
BIP. Specifically, the BIP editor should accept a BIP even if he personally
dislikes the ideas in it, when the criteria are satisfied.
Beyond that, having a BIP accepted does not mean wallets have to implement
it. That's up to the individual wallet authors/maintainers.

@_date: 2016-01-08 00:52:27
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
script, which you accept as being a payment to yourself. An attacker could
use a collision attack to construct scripts with identical hashes, only one
of which does have the property you want, and steal coins.
something we should encourage for that. Normal pubkey hashes don't have
that problem, as they can't be constructed to pay to you."
attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off, just
ignore that arbitrary data" a wallet can just refuse. Even more likely, a
contract wallet won't even recognize that as a pay-to-gavin transaction.
somebody_else_pubkey CHECKMULTISIG ... with the attacker using
somebody_else_pubkey to force the collision, but, again, trivial contract
protocol tweaks ("send along a proof you have the private key corresponding
to the public key" or "everybody pre-commits pubkeys they'll use at
protocol start") would protect against that.
Yes, this is what I worry about. We're constructing a 2-of-2 multisig
escrow in a contract. I reveal my public key A, you do a 80-bit search for
B and C such that H(A and B) = H(B and C). You tell me your keys B, and I
happily send to H(A and B), which you steal with H(B and C).
Sending along a proof does not help, you can't prove that you do not know
of a collision. Pre-committing does help, but is a very non-obvious
security requirement, something I strongly believe is far riskier in
Bitcoin does have parts that rely on economic arguments for security or
privacy, but can we please stick to using cryptography that is up to par
for parts where we can? It's a small constant factor of data, and it
categorically removes the worry about security levels.

@_date: 2016-01-08 18:38:34
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks or 
On Fri, Jan 8, 2016 at 2:54 AM, Gavin Andresen via bitcoin-dev <
Ok, just having one witness program version now is a somewhat different
proposal. It would be simpler for sure. The reasoning was that you'd need
this to not add significant overhead to small scripts, but that may not be
the case anymore. I wouldn't mind seeing numbers.
I don't think that is wise. Bitcoin has a 128-bit security target for
everything else. We did not know that P2SH and similar constructs were
vulnerable to a collision attack at the time, but now we do, so the obvious
choice is to pick a size that is sufficiently large to maintain the 128-bit
security target. This is a no brainer to me; we're not proposing switching
to a 160-bit EC curve either, right?
It is a proposal and we are discussing it. You first brought up some
criticisms in private, and I agreed with several things you said.
But it remains the proposal of a few people including me, and I do not
agree with the specific suggestion of reducing the security target for
witness scripts to 80 bits.
We are not deciding what the system will be. We're making a proposal, and
hope that due to its technical merit, the ecosystem will adopt it. You're
free to participate in that discussion.

@_date: 2016-06-12 16:40:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP141 segwit consensus rule update: extension of 
On Jun 8, 2016 18:46, "Luke Dashjr via bitcoin-dev" <
No strong opinion, but I'd rather not change it anymore, as I don't see the
point. Any data you would want to encode there can be moved to the witness
at 1/4 the cost and replaced by a 256-bit hash. If the data is 43 bytes or
higher, that is even cheaper. The only thing that cannot be in the hash is
metadata to indicate what hashing/rule scheme itself is used. I think 68
bits (OP_n + 8 bytes) for that is plenty.

@_date: 2016-06-15 13:00:42
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] RFC for BIP: Derivation scheme for 
On Jun 15, 2016 12:53, "Daniel Weigl via bitcoin-dev" <
its visible
send-to-single-address transactions
is able to recognize it)
Indeed, and you can go even further. When there are multiple "sending"
outputs, pick one at random, and mimic it for the change output. This means
that if you have a P2PKH and 3 P2SH sends, you'll have 25% chance for a
P2PKH change output, and 75% chance for a P2SH output.
You can go even further of course, if you want privacy that remains after
those sends get spent. In that case, you also need to match the template of
the redeemscript/witnessscript. For example, if the send you are mimicking
is a 2-of-3, the change output should also use 2-of-3.

@_date: 2016-06-23 13:30:45
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
On Jun 23, 2016 12:56, "Peter Todd via bitcoin-dev" <
Tor to
I hope you're not seriously suggesting to censor a BIP because you feel it
is a bad idea.

@_date: 2016-06-23 14:01:10
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
We? I don't feel like I have any authority to say what goes into that
repository, and neither do you. We just give technical opinion on
proposals. The fact that it's under the bitcoin organization on github
is a historical artifact.
Editorial control is inevitable to some extent, but I think that's
more a matter of process than of opinion. Things like "Was there
community discussion?", "Is it relevant?", "Is there a reference
implementation?". I don't think that you objecting for moral reasons
to an otherwise technically sound idea is a reason for removal of a
BIP. You are of course free to propose alternatives, or recommend
against its usage.

@_date: 2016-06-23 14:16:48
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Even more proposed BIP extensions to BIP 0070 
No, I do not. I am saying that some degree of editorial control will
inevitably exist, simply because there is some human making the choice of
assigning a BIP number and merging. My opinion is that we should try to
restrict that editorial control to only be subject to objective process,
and not be dependent on personal opinions.
I think that you are free to express dislike of BIP75. Suggesting to remove
it for that reason is utterly ridiculous to me, whatever you want to call

@_date: 2016-06-29 08:58:21
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 151 use of HMAC_SHA512 
On Jun 29, 2016 07:05, "Ethan Heilman via bitcoin-dev" <
SHA256(key|cipher-type|mesg).  But that's probably just my crypto
This property does technically not apply here, as the output of the hash is
kept secret, and the possible messages are constants (which are presumably
chosen in such a way that one is never an extension of another).
However, this is a good example of why you can't generically use a hash
function in places where you want a MAC (aka "a hash with a shared
secret"). Furthermore, if you already have a hash function anyway, HMAC is
very easy construct on top of it.

@_date: 2016-06-30 15:03:18
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 151 
On Thu, Jun 30, 2016 at 11:57 AM, Eric Voskuil via bitcoin-dev
I think this is a reasonable concern.
However, node identity is already being used widely, and in a very
inadvisable way:
* Since forever there have been lists of 'good nodes' to pass in
addnode= configuration options.
* Various people run multiple nodes in different geographic locations,
peering with each other.
* Various pieces of infrastructure exist that relies on connecting to
well-behaving nodes (miner relay networks, large players peering
directly with each other, ...)
* Several lightweight clients support configuring a trusted host to connect to.
Perhaps you deplore that fact, but I believe it is inevitable that
different pieces of the network will make different choices here. You
can'tg prevent people from create connections along preexisting trust
lines. That does not mean that the network as a whole relies on first
establishing trust everywhere.
And I do think there are advantages.
BIP 151 on its own gives you opportunistic encryption. You're very
right to point out that this does not give you protection from active
attackers, and that active attacking is relatively easy through sybil
attacks. I still prefer my attacker to actually do that over just
listening in on my connection. And yes, we should also work on
improving the privacy nodes and wallets have orthogonal to encryption,
but nothing will make everything perfectly private.
BIP 151 plus a simple optional pre-shared-secret authentication
extension can improve upon pure IP-based authentication, as well as
simplify things like SSL tunnels, and onion addresses purely used as
identity. This will still require explicit configuration, but not more
than now.
BIP 151 plus a non-leaking public key authentication scheme (where
peers can query "are you the peer with pubkey X?" but don't learn
anything if the answer is no) with keys specific to the IP addresses
can give a TOFU-like security. Nodes already remember IP addresses
they've succesfully interacted with in the past, and ban IP addresses
that misbehave. Being able to tell whether a node you connect to is
the same as one you've connected to before is a natural extension of
this, and does not require establishing any real-world identity beyond
what we're already implicitly relying on.
Perhaps these use cases and their security assumptions should be
spelled out more clearly in the BIP. If there is a misunderstanding,
it should be clearly stated that BIP 151 is only a building block for
further improvements
This is a widespread problem, but it exists far outside the scope of
this proposal. The privacy properties of Bitcoin are often
misrepresented and even used as advertizements. The solution is
education, not avoiding improvements because they may be
I really think this is an exaggeration. It's a diffie-hellman
handshake and a stream cipher (both very common constructions), that
apply to individual connections. There are no consensus risks nor a
requirement for coordinated change through the network. The
cryptographic code can be directly reused from a well-known project
(OpenSSH), and is very small in size.

@_date: 2016-05-09 19:06:06
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Hi Matt,
thank you for working on this!
This is a not, but I think it's a bit strange to have two separate
variable length integers in the same specification. I understand is one
is already the default for variable-length integers currently, and there
are reasons to use the other one for efficiency reasons in some places,
but perhaps we should aim to get everything using the latter?
Maybe it's worth mentioning that it is based on ASN.1 BER's compressed
integer format (see
section 8.1.3.5), though with a small modification to make every integer
have a single unique encoding.
I tried to derive what length of short ids is actually necessary (some
write-up is on
 but it's
For any reasonable numbers I can come up with (in a very wide range),
the number of bits needed is very well approximated by:
  log2( *  /
For example, with 20000 mempool transactions, 2500 transactions in a
block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to
reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =
34.54, or 5 byte txids would suffice.
Note that 1 in 10000 failures may sound like a lot, but this is for each
individual connection, and since every transmission uses separately
salted identifiers, occasional failures should not affect global
propagation. Given that transmission failures due to timeouts, network
connectivity, ... already occur much more frequently than once every few
gigabytes (what 10000 blocks corresponds to), that's probably already
more than enough.
In short: I believe 5 or 6 byte txids should be enough, but perhaps it
makes sense to allow the sender to choose (so he can weigh trying
multiple nonces against increasing the short txid length).
An alternative would be using SipHash-1-3 (a form of SipHash with
reduced iteration counts; the default is SipHash-2-4). SipHash was
designed as a Message Authentication Code, where the security
requirements are much stronger than in our case (in particular, we don't
care about observers being able to finding the key, as the key is just
public knowledge here). One of the designers of SipHash has commented
that SipHash-1-3 for collision resistance in hash tables may be enough:
Using SipHash-1-3 on modern hardware would take ~32 CPU cycles per txid.
There are a few more heuristics that MAY be used to improve performance:
* Receivers should treat short txids in blocks that match multiple
mempool transactions as non-matches, and request the transactions. This
significantly reduces the failure to reconstruct.
* When constructing a compact block to send, the sender can verify it
against its own mempool to check for collisions, and if so, choose to
either try another nonce, or increase the short txid length.

@_date: 2016-11-16 17:50:53
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
On Wed, Nov 16, 2016 at 5:58 AM, Eric Voskuil via bitcoin-dev <
So far in this discussion, and in a thread that has forked off, I think 3
cases of implementation aspects have been mentioned that under certain
circumstances result in the validity of chains changing:
* Buried softforks (by simplifying the activation rules for certain rules)
* Not verifying BIP30 after BIP34 is active (since only under a SHA256^2
collision a duplicate txid can occur)
* The existence (and/or removal) of checkpoints (in one form or another).
None of these will influence the accepted main chain, however. If they ever
do, Bitcoin has far worse things to worry about (years-deep reorgs, or
SHA256 collisions).
If you were trying to point out that buried softforks are similar to
checkpoints in this regard, I agree. So are checkpoints good now? I believe
we should get rid of checkpoints because they seem to be misunderstood as a
security feature rather than as an optimization. I don't think buried
softforks have that problem.

@_date: 2016-11-16 18:47:33
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] [BIP Proposal] Buried Deployments 
Yes, they can be used to control the "true chain", and this has happened
with various forks. But developers inevitably have this possibility, if you
ignore review. If review is good enough to catch unintended consensus
changes, it is certainly enough to catch the introduction of an invalid
checkpoint. The risk you point out is real, but the way to deal with it is
good review and release practices.
I wish we had never used checkpoints the way we did, but here we are.
Because of this, I want to get rid of them. However, It's not because I
think they offer an excessive power to developers - but because they're
often perceived this way (partially as a result of how they've been abused
in other systems).
Having users with the discipline you suggest would be wonderful to have. I
don't think it's very realistic, though, and I fear that the result would
be that everyone copies their config from one or a few websites "because
that's what everyone uses".
I do not consider the practice of "buried softforks" to be a fork at all.
It is a change that modifies the validity of a theoretically construable
chain from invalid to valid. However, a reorganization to that theoretical
chain itself is likely already impossible due to the vast number of blocks
to rewind, and economic damage that is far greater than chain divergence
It is clearly not a security feature, agreed. But how would you propose to
avoid the ISM checks for BIP34 and BIP66 all the time? I feel this approach
is a perfectly reasonable choice for code that likely won't ever affect the
valid chain again.

@_date: 2016-10-16 16:31:55
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Start time for BIP141 (segwit) 
Hello all,
We're getting ready for Bitcoin Core's 0.13.1 release - the first one
to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
mainnet, after being extensively tested on testnet and in other
software. Following the BIP9 recommendation [1] to set the versionbits
start time a month in the future and discussion in the last IRC
meeting [2], I propose we set BIP 141's start time to November 15,
2016, 0:00 UTC (unix time 1479168000).
Note that this is just a lower bound on the time when the versionbits
signalling can begin. Activation on the network requires:
(1) This date to pass
(2) A full retarget window of 2016 blocks with 95% signalling the
version bit (bit 1 for BIP141)
(3) A fallow period consisting of another 2016 blocks.
  [1]   [2]

@_date: 2017-12-11 10:26:40
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP - Dead Man's Switch 
On Dec 11, 2017 10:23, "Nick Pudar via bitcoin-dev" <
This topic has come up several times in recent years. While it is well
intentioned, it can have devastating outcomes for people that want to save
long term. If such a system were implemented, it would force people to move
funds around in order to not get nullified. In that process, it introduces
multiple opportunities for errors. Cold storage should be able to stay
cold. I personally would be apprehensive about implementing this kind of a
Furthermore, if it rewards miners with funds that are expired, it creates
terrible incentives. Miners in their best interest could choose to censor
transactions that move funds close to their expiration time, to increase
their own future rewards.

@_date: 2017-02-13 00:47:38
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP151 protocol incompatibility 
On Feb 12, 2017 23:58, "Eric Voskuil via bitcoin-dev" <
The BIP151 proposal states:
the encinit messages.
This statement is incorrect. Sending content that existing nodes do not
expect is clearly an incompatibility. An implementation that ignores
invalid content leaves itself wide open to DOS attacks. The version
handshake must be complete before the protocol level can be determined.
While it may be desirable for this change to precede the version
handshake it cannot be described as backward compatible.
The worst possible effect of ignoring unknown messages is a waste of
downstream bandwidth. The same is already possible by being sent addr
Using the protocol level requires a strict linear progression of (allowed)
network protocol features, which I expect to become harder and harder to
Using otherwise ignored messages for determining optional features is
elegant, simple and opens no new attack vectors. I think it's very much
preferable over continued increments of the protocol version.

@_date: 2017-02-25 14:14:44
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
On Feb 25, 2017 14:09, "Steve Davis via bitcoin-dev" <
Hi Peter,
I really, really don?t want to get into it but segwit has many aspects that
are less appealing, not least of which being the amount of time it would
take to reach the critical mass.
Surely there's a number of alternative approaches which could be explored,
even if only to make a fair assessment of a best response?
Any alternative to move us away from RIPEMD160 would require:
* A drafting of a softfork proposal, implementation, testing, review.
* A new address format
* Miners accepting the new consensus rules
* Wallets adopting the new address format, both on the sender side and
receiver side (which requires new signatures).
I.e., exactly the same as segwit, for which most of these are already done.
And it would still only apply to wallets adopting it.

@_date: 2017-02-25 22:36:25
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by 
Hi Pieter,
?Any alternative?? What about reverting to:
[, OP_CHECKSIG]
Could that be the alternative?
Ok, fair enough, that is an alternative that avoids the 160-bit hash
function, but not where it matters. The 80-bit collision attack only
applies to jointly constructed addresses like multisig P2SH, not single-key
ones. As far as I know for those we only rely preimage security, and
RIPEMD160 has 160 bit security there, which is even more than our ECDSA
signatures offer.

@_date: 2017-02-28 15:24:28
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A Better MMR Definition 
On Feb 28, 2017 15:10, "Bram Cohen via bitcoin-dev" <
On Tue, Feb 28, 2017 at 8:43 AM, G. Andrew Stone You have to have a lookup table going from prevouts to txo index. Lookups
on that are relatively fast because looking up things in a hashtable is a
single cache miss, while looking up things in a tree is logarithmic cache
I'm wondering if there is some confusion here.
Yes, someone needs to have a lookup table from prevouts to TXO tree
positions. But because an insertion-ordered TXO tree does not rebalance,
that table can be maintained by wallets or service providers for just their
own coins, instead of by every full node and miner individually for
everyone's coins.
In the simplest committed TXO model, full nodes simply maintain the TXO
root hash, and every transaction/block comes with a proof that its inputs
are in the TXO tree, and the necessary information to update the root after
spending the inputs and adding the outputs.

@_date: 2017-07-11 13:01:10
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
On Jul 11, 2017 09:18, "Chris Stewart via bitcoin-dev" <
Concept ACK.
If drivechains are successful they should be viewed as the way we scale
I strongly disagree with that statement.
Drivechains, and several earlier sidechains ideas, are not a scalability
improvement, but merely enabling users to opt-in for another security model.
While obviously any future with wider adoption will need different
technologies that have different trade-offs, and anyone is free to choose
their security model, I don't think this particular one is interesting. In
terms of validation cost to auditors, it is as bad as just a capacity
increase on chain, while simultaneously adding the extra risk of miners
being able to vote to steal your money.

@_date: 2017-07-11 14:40:36
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
I apologize here; I didn't mean to misrepresent his viewpoint.
If you're talking about the extreme case where every full node in the
increased capacity single chain model corresponds to a node that
validates both chains and all transfers between them in the
drivechains, I agree. At that point they become nearly equivalent in
terms of ease of adoption, resource costs, and capacity.
However, I don't think that is a realistic expectation. When
considering drivechains as a capacity increase, I believe most people
think about a situation where there are many chains that give an
increased capacity combined, but not everyone verifies all of them.
This is what I meant with uninteresting security model, as it requires
increased miner trust for preventing the other chains' coins from
being illegally transferred to the chain you're operating on.
Regardless, people are free experiment and adopt such an approach. The
nice thing about it not being a hardfork is that it does not require
network-wide consensus to deploy. However, I don't think they offer a
security model that should be encouraged, and thus doesn't have a
place on a roadmap.
Am I right in summarizing your point here as "This approach cannot
hurt, because if it were insecure, people can choose to not use it."?
I'm not sure I agree with that, as network effects or misinformation
may push users beyond what is reasonable.

@_date: 2017-03-08 15:12:01
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Unique node identifiers 
On Wed, Mar 8, 2017 at 1:20 PM, Jonas Schnelli via bitcoin-dev
I believe this discussion is getting sidetracked.
There is a difference between identification/fingerprinting (who are
you?) and proving identity (prove that you are who I think you are?).
BIP150 only facilitates the second, not the first. I don't think you
disagree about that, but I want to make it clear for anyone else
following the discussion.
The question is whether it encourages people to establish known and
pre-shared identities for nodes. Perhaps, but not in any way that
IP/onion addresses don't already. Think about it:
* If you know an IP/onion address, you can verify whether some node
has it. If you know an IP/onion address + BIP150 PSK, you can verify
whether some node has it.
* If you know 2 IP/onion addresses, you cannot figure out whether they
correspond to the same node (and if you can, that is a bug, not by
design). If you know 2 (IP/onion addresses, BIP150 PSK) pairs, you
cannot figure out whether they correspond to the same node (and if you
can, that is a bug, not by design).
* If you receive a connection from a node, you cannot know what their
onion address is. If you receive a connection from a node, you cannot
figure out what their PSK is.
In that way, I see BIP150 as an extension of IP addresses, except more
secure against network-level attackers. If you believe the concept of
people establishing links along existing trust lines is a problem, you
should be arguing against features in Bitcoin software that allows
configuring preferred IP addresses to connect to as well (-addnode and
-connect in Bitcoin Core, for example).

@_date: 2017-03-08 17:55:32
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Unique node identifiers 
So you're saying that a -onlyacceptconnectionsfrom=IP option wouldn't
be a concern to you because it can't exclude people? Of course it can
exclude people - just not your ISP or a state-level attacker.
Please, Eric. I think I understand your concern, but this argument
isn't constructive either.
The proposal here is to introduce visible node identities on the
network. I think that's misguided as node count is irrelevant and
trivial to fake anyway. But you bringing up BIP150 here isn't useful
either. I know that you equate the concept of having verifiable
identity keys in the P2P with a step towards making every node
identifiable, but they are not the same. It's just a cryptographic
tool to keep a certain class of attackers from bypassing restrictions
that people can already make.

@_date: 2017-03-20 14:35:08
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
Hello everyone,
I'd like to propose a new BIP for native segwit addresses to replace
BIP 142. These addresses are not required for segwit, but are more
efficient, flexible, and nicer to use.
The format is base 32 and uses a simple checksum algorithm with strong
error detection properties. Reference code in several languages as
well as a website demonstrating it are included.
You can find the text here:

@_date: 2017-03-23 17:20:32
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Issolated Bitcoin Nodes 
Hello Juan,
this is expected behaviour. Nodes with segwit active only download
blocks from other segwit peers, as old peers cannot provide the
witness data they need to verify the blocks.

@_date: 2017-03-28 13:16:03
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Bitcoin Core's (nor any other software's) maintainers can already not
decide on a hard fork, and I keep being confused by the focus on Core
in this topic. Even if a hard forking change (or lack thereof) was
included into a new release, it is still up to the community to choose
to run the new software. Bitcoin Core has very intentionally no
auto-update feature, as the choice for what network rules to implement
must come from node operators, not developers. Ask yourself this: if a
new Bitcoin Core release would include a new rule that blacklists
's coins. What do you think would happen? I hope
that people would refuse to update, and choose to run different full
node software.
Core is not special. It is one of many pieces of software that
implement today's Bitcoin consensus rules. If a hardfork is to take
place in a way that does not result in two currencies, it must be
clear that the entire ecosystem will adopt it. Bitcoin Core will not
merge any consensus changes that do not clearly satisfy that

@_date: 2017-05-07 14:39:14
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] A BIP proposal for segwit addresses 
Responding to a few comments:
By Andreas Schildbach:
usecase to target at
I think it should be. It's certainly not the most important way through
which addresses are communicated or verified, but I am trying to address
all places where humans interact with addresses. I have certainly tried to
verify addresses a few times through voice, when dealing with significant
Regarding your QR code comments: it is certainly possible to find a more
compact QR code representation. That is not the goal of the BIP though -
it's trying to introduce one commonly recognizable format that has good
properties for all use cases, even if that means being suboptimal in
certain aspects for some.
Obviously they cannot voice-communicate at all with only-english-speaking
users, so there is no need to communicate voice-communicate addresses
between them.
I assume that Peter Todd is talking about cases where English speakers are
interacting with non-native English speakers, who may know how to pronounce
numbers or alphabetical characters, but not all special characters.
*bigger* because due to the characters used for URL parameters (?&=) those
QR codes are locked to binary mode.
I believe that is incorrect. Data in QR codes can switch from one mode to
another on a per-character basis (with an overhead of a few bits). I don't
know to what extent common QR encoders make intelligent decisions about
this, but it does not seem very hard.
By Lucas Ontivero:
operations over big numbers. This is not very fast and most of the
programming languages don't provide support for big numbers OOB.
It's not that hard to emulate the bignum logic in languages that don't
support it. See for example this code in Bitcoin Core:
 So I
think it's not necessary to go into all the possible ways Base58 can be
implemented in the document, and the existing language ("Base58 decoding is
complicated and relatively slow.") is sufficient.
could lead to some confusions but what if in the future there is a new type
of address that can also be encoded with bech32? Don't we need a address
type anyway?
I believe that it's likely that new types of outputs that may be introduced
in the future will most likely not be a simple constant byte sequence that
can be computed directly from addresses, but need some processing by the
sender. This is the case for example for Reusable/Stealth addresses and
Confidential Transactions addresses. Such outputs, if ever introduced on a
wide scale, should ideally not be representable as existing address types,
as that could not only lead to confusion, but also to lost privacy and
And, If there ever is a need for introducing a "constant scriptPubKey" type
address again, the encoding proposed in this document can be reused.
Currently, the header value can be at most 17. In the future new proposals
could give a meaning to values 18 through 31.
In general:
In the past weeks people have contributed two new reference implementations
(Haskell and Rust), and a C++ and Go one are underway (see
I'd like to move forward and request a BIP number assignment for this

@_date: 2017-05-15 13:01:14
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
Hello all,
I would like to discuss a way of computing a UTXO set hash that is
very efficient to update, but does not support any compact proofs of
existence or non-existence.
Much has been written on the topic of various data structures and
derived hashes for the UTXO/TXO set before (including Alan Reiner's
trust-free lite nodes [1], Peter Todd's TXO MMR commitments [2] [3],
or Bram Cohen's TXO bitfield [4]). They all provide interesting extra
functionality or tradeoffs, but require invasive changes to the P2P
protocol or how wallets work, or force nodes to maintain their
database in a normative fashion. Instead, here I focus on an efficient
hash that supports nothing but comparing two UTXO sets. However, it is
not incompatible with any of those other approaches, so we can gain
some of the advantages of a UTXO hash without adopting something that
may be incompatible with future protocol enhancements.
1. Incremental hashing
Computing a hash of the UTXO set is easy when it does not need
efficient updates, and when we can assume a fixed serialization with a
normative ordering for the data in it - just serialize the whole thing
and hash it. As different software or releases may use different
database models for the UTXO set, a solution that is order-independent
would seem preferable.
This brings us to the problem of computing a hash of unordered data.
Several approaches that accomplish this through incremental hashing
were suggested in [5], including XHASH, AdHash, and MuHash. XHASH
consists of first hashing all the set elements independently, and
XORing all those hashes together. This is insecure, as Gaussian
elimination can easily find a subset of random hashes that XOR to a
given value. AdHash/MuHash are similar, except addition/multiplication
modulo a large prime are used instead of XOR. Wagner [6] showed that
attacking XHASH or AdHash is an instance of a generalized birthday
problem (called the k-sum problem in his paper, with unrestricted k),
and gives a O(2^(2*sqrt(n)-1)) algorithm to attack it (for n-bit
hashes). As a result, AdHash with 256-bit hashes only has 31 bits of
Thankfully, [6] also shows that the k-sum problem cannot be
efficiently solved in groups in which the discrete logarithm problem
is hard, as an efficient k-sum solver can be used to compute discrete
logarithms. As a result, MuHash modulo a sufficiently large safe prime
is provably secure under the DL assumption. Common guidelines on
security parameters [7] say that 3072-bit DL has about 128 bits of
security. A final 256-bit hash can be applied to the 3072-bit result
without loss of security to reduce the final size.
An alternative to multiplication modulo a prime is using an elliptic
curve group. Due to the ECDLP assumption, which the security of
Bitcoin signatures already relies on, this also results in security
against k-sum solving. This approach is used in the Elliptic Curve
Multiset Hash (ECMH) in [8]. For this to work, we must "hash onto a
curve point" in a way that results in points without known discrete
logarithm. The paper suggests using (controversial) binary elliptic
curves to make that operation efficient. If we only consider
secp256k1, one approach is just reading potential X coordinates from a
PRNG until one is found that has a corresponding Y coordinate
according to the curve equation. On average, 2 iterations are needed.
A constant time algorithm to hash onto the curve exists as well [9],
but it is only slightly faster and is much more complicated to
AdHash-like constructions with a sufficiently large intermediate hash
can be made secure against Wagner's algorithm, as suggested in [10].
4160-bit hashes would be needed for 128 bits of security. When
repetition is allowed, [8] gives a stronger attack against AdHash,
suggesting that as much as 400000 bits are needed. While repetition is
not directly an issue for our use case, it would be nice if
verification software would not be required to check for duplicated
2. Efficient addition and deletion
Interestingly, both ECMH and MuHash not only support adding set
elements in any order but also deleting in any order. As a result, we
can simply maintain a running sum for the UTXO set as a whole, and
add/subtract when creating/spending an output in it. In the case of
MuHash it is slightly more complicated, as computing an inverse is
relatively expensive. This can be solved by representing the running
value as a fraction, and multiplying created elements into the
numerator and spent elements into the denominator. Only when the final
hash is desired, a single modular inverse and multiplication is needed
to combine the two.
As the update operations are also associative, H(a)+H(b)+H(c)+H(d) can
in fact be computed as (H(a)+H(b)) + (H(c)+H(d)). This implies that
all of this is perfectly parallellizable: each thread can process an
arbitrary subset of the update operations, allowing them to be
efficiently combined later.
3. Comparison of approaches
Numbers below are based on preliminary benchmarks on a single thread
of a i7-6820HQ CPU running at 3.4GHz.
(1) (MuHash) Multiplying 3072-bit hashes mod 2^3072 - 1103717 (the
largest 3072-bit safe prime).
    * Needs a fast modular multiplication/inverse implementation.
    * Using SHA512 + ChaCha20 for generating the hashes takes 1.2us per element.
    * Modular multiplication using GMP takes 1.5us per element (2.5us
with a 60-line C+asm implementation).
    * 768 bytes for maintaining a running sum (384 for numerator, 384
for denominator)
    * Very common security assumption. Even if the DL assumption would
be broken (but no k-sum algorithm faster than Wagner's is found), this
still maintains 110 bits of security.
(2) (ECMH) Adding secp256k1 EC points
    * Much more complicated than the previous approaches when
implementing from scratch, but almost no extra complexity when ECDSA
secp256k1 signature validation is already implemented.
    * Using SHA512 + libsecp256k1's point decompression for generating
the points takes 11us per element on average.
    * Addition/subtracting of N points takes 5.25us + 0.25us*N.
    * 64 bytes for a running sum.
    * Identical security assumption as Bitcoin's signatures.
Using the numbers above, we find that:
* Computing the hash from just the UTXO set takes (1) 2m15s (2) 9m20s
* Processing all creations and spends in an average block takes (1)
24ms (2) 100ms
* Processing precomputed per-transaction aggregates in an average
block takes (1) 3ms (2) 0.5ms
Note that while (2) has higher CPU usage than (1) in general, it has
lower latency when using precomputed per-transaction aggregates. Using
such aggregates is also more feasible as they're only 64 bytes rather
than 768. Because of simplicity, (1) has my preference.
Overall, these numbers are sufficiently low (note that they can be
parallellized) that it would be reasonable for full nodes and/or other
software to always maintain one of them, and effectively have a
rolling cryptographical checksum of the UTXO set at all times.
4. Use cases
* Replacement for Bitcoin Core's gettxoutsetinfo RPC's hash
computation. This currently requires minutes of I/O and CPU, as it
serializes and hashes the entire UTXO set. A rolling set hash would
make this instant, making the whole RPC much more usable for sanity
* Assisting in implementation of fast sync methods with known good
blocks/UTXO sets.
* Database consistency checking: by remembering the UTXO set hash of
the past few blocks (computed on the fly), a consistency check can be
done that recomputes it based on the database.
  [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9]   [10]

@_date: 2017-05-16 11:17:19
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
On Tue, May 16, 2017 at 4:01 AM, Peter Todd via bitcoin-dev
I'm aware, I agree, and I even referenced that mail in my original post.
However, all of those approaches still require a network wide choice
to be useful. A validating node that does not maintain a UTXO X must
get a proof of its unspentness from somewhere for at least the block
which contains a spend of X. In a world where such a model is deployed
network-wide, that proof information is generated by the wallet and
relayed wherever needed. In a partial deployment however, you need
nodes that can produce the proof for other nodes, and the ability to
produce a proof is significantly more expensive than running either an
old or a new full node.
This ability to produce proofs becomes even harder when there are
different models deployed at once. Even just having a different
criterion for which UTXOs need a proof (eg. "only outputs created more
than 1000 blocks ago") may already cause compatibility issues. Combine
that with the multitude of ideas about this (insertion-ordered TXO
trees, txid-ordered UTXO Patricia tries, AVL+ trees, append-only
bitfield, ...) with different trade-offs (in CPU, RAM for validators,
complexity for wallets/index services, ...), I don't think we're quite
ready to make that choice.
To be clear: I'm very much in favor of moving to a model where the
responsibilities of full nodes are reduced in the long term. But
before that can happen there will need to be implementations,
experiments, analysis, ...
Because of that, I think it is worthwhile to investigate solutions to
the "how can we efficiently compare UTXO sets" problem separately from
the "how do we reduce full node costs by sending proofs instead of it
maintaining the data". And rolling UTXO set hashes are a solution for
just the first - and one that has very low costs and no normative
datastructures at all.

@_date: 2017-05-23 13:43:45
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
Oh, I didn't mean it that way at all. (1) is simpler to get decent
performance out of. Implementing (1) using any language that has big
integer support or can link against GMP is likely going to be faster
than the fastest possible implementation of (2).
That seems to completely defeat the purpose... if I want to give you a
UTXO set, and prove its correctness wrt the hash you know... I need to
remember the full transactions those outputs came from?
That's an interesting idea, but I believe you're forgetting:
* The size of txin prevout/nsequence, which is typically larger than
txouts (even when excluding scriptSig/witness data).
* The size of spent txouts for transactions with unspent outputs left.
* The fact that you can deduplicate the txids for txn that have
multiple unspent outputs in the UTXO set serialization, even if that
txid is repeated in the rolling hash computation.
The construction I was considering and benchmarking is using 256-bit
truncated SHA512(256bit txid || 32bit voutindex || 1bit coinbase ||
31bit height || CTxOut output) as secp256k1 X coordinate, or as key to
seed a ChaCha20 PRNG whose outputs is the 3072-bit MuHash number. The
reason for using SHA512 is that it can process most UTXOs in a single
transformation (as opposed to SHA256 which will almost always need 2).
The reason for using ChaCha20 is that it's incredibly fast for
producing much data when a key is already known. An alternative is
using SHAKE256 for the whole construction (as it both takes an
arbitrary amount of data, and produces an arbitrary length hash) - but
it's a bit slower.
That just seems scary to me...

@_date: 2017-10-30 07:26:29
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Visually Differentiable - Bitcoin Addresses 
On Oct 30, 2017 15:21, "shiva sitamraju via bitcoin-dev" <
For example bc1qeklep85ntjz4605drds6aww9u0qr46qzrv5xswd35uhjuj8ahfcqgf6hak
in 461e8a4aa0a0e75c06602c505bd7aa06e7116ba5cd98fd6e046e8cbeb00379d6 is 62
bytes !
While I get the error/checksum capabilities Bech32 brings, any user would
prefer a 20 byte address with a checksum  over an address that would wrap
several lines !!
That's an unfair comparison. You're pasting a P2WSH address which contains
a 256-bit hash.
A P2WPKH address (which only contains a 160-bit hash, just like P2PKH and
P2SH) in Bech32 is only 42 characters, not 62.

@_date: 2017-09-16 19:29:41
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] proposal: extend WIF format for segwit 
On Sep 15, 2017 01:56, "Thomas Voegtlin via bitcoin-dev" <
Note 3: we could also use a bech32 format for the private key, if it is
going to be used with a bech32 address. I am not sure if such a format
has been proposed already.
I've been working on an "extended bech32" format with 12 character checksum
rather than 6, for private keys and other things that need stronger
protection. It would guarantee correcting 4 errors, where normal bech32 can
only detect (but not correct) 4.
The rationale is that in the case of an address, if an error is detected,
you can ask the receiver for a corrected version. As that option doesn't
exist for private keys you want something stronger.
This has been a low-priority thing for me, though, and the computation work
to find a good checksum is significant.

@_date: 2017-09-22 15:09:07
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] cleanstack alt stack & softfork improvements 
MAST)
On Fri, Sep 22, 2017 at 2:54 PM, Sergio Demian Lerner via bitcoin-dev <
Or (my preference);
- Get rid of DER encoding alltogether and switch to fixed size signatures.

@_date: 2018-08-13 11:56:41
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Witness serialization in PSBT non-witness UTXOs 
Hello all,
BIP174 currently specifies that non-witness UTXOs (the transactions
being spent by non-witness inputs) should be serialized in network
I believe there are two issues with this.
1. Even in case the transaction whose output being spent itself has a
witness, this witness is immaterial to PSBT. It's only there to be
able to verify the txid commits to the output/amount being spent,
which can be done without witness.
2. "Network format" is a bit ambiguous. We can imagine a future
softfork that introduces a new type of witness. Network format could
be interpreted as including that new witness type, which is clearly
unnecessary (by the above argument), and would gratuitously break
compatibility with existing signers if implemented pedantically.
So my suggestion is to update the specification to state that
non-witness UTXOs must be serialized without witness. If it's too late
for that, it should instead be updated to explicitly specify with or
witnout witness, but it's safe to drop the witness.

@_date: 2018-01-09 06:21:08
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 117 Feedback 
On Jan 9, 2018 13:41, "Mark Friedenbach via bitcoin-dev" <
The use of the alt stack is a hack for segwit script version 0 which has
the clean stack rule. Anticipated future improvements here are to switch to
a witness script version, and a new segwit output version which supports
native MAST to save an additional 40 or so witness bytes. Either approach
would allow getting rid of the alt stack hack. They are not part of the
proposal now because it is better to do things incrementally, and because
we anticipate usage of MAST to better inform these less generic changes.
If the goal is to introduce a native MAST output type later, what is gained
by first having the tailcall semantics?
As far as I can see, this proposal does not have any benefits over Johnson
Lau's MAST idea [1]:
* It is more compact, already giving us the space savings a native MAST
version of the tail call semantics would bring.
* It does not need to work around limitations of push size limits or
cleanstack rules.
* The implementation (of code I've seen) is easier to reason about, as it's
just another case in VerifyScript (which you'd need for a native MAST
output later too) without introducing jumps or loops inside EvalScript.
* It can express the same, as even though the MBV opcode supports proving
multiple elements simultaneously, I don't see a way to use that in the tail
call. Every scenario that consists of some logic before deciding what the
tail call is going to be can be rewritten to have that logic inside each of
the branches, I believe.
* It does not interfere with static analysis (see further).
* Tail call semantics may be easier to extend in the future to enable
semantics that are not compactly representable in either proposal right
now, by allowing a single top-level script may invoke multiple subscripts,
or recursion. However, those sound even riskier and harder to analyse to
me, and I don't think there is sufficient evidence they're needed.
Native MAST outputs would need a new witness script version, which your
current proposal does indeed not need. However, I believe a new script
version will be desirable for other reasons regardless (returning invalid
opcodes to the pool of NOPs available for softforks, for example).
I will make a strong assertion: static analyzing the number of opcodes and
sigops gets us absolutely nothing. It is cargo cult safety engineering. No
need to perpetuate it when it is now in the way.
I'm not sure I agree here. While I'd like to see the separate execution
limits go away, removing them entirely and complicating future ability to
introduce unified costing towards weight of execution cost seems the wrong
way to go.
My reasoning is this: perhaps you can currently make an argument that the
current weight limit is sufficient in preventing overly expensive block
validation costs, due to a minimal ratio between script sizes and their
execution cost. But such an argument needs to rely on assumptions about
sighash caching and low per-opcode CPU time, which may change in the
future. In my view, tail call semantics gratuitously remove or complicate
the ability to reason about the executed code.
One suggestion to reduce the impact of this is limiting the per-script
execution to something proportional to the script size. However, I don't
think that addresses all potential concerns about static analysis (for
example, it doesn't easily let you prove all possible execution paths to a
participant in a smart contract).
Another idea that has been suggested on this list is to mark pushes of
potentially executable code on the stack/witness explicitly. This would
retain all ability to analyse, while still leaving the flexibility of
future extensions to tail call execution. If tail call semantics are
adopted, I strongly prefer an approach like that to blindly throwing out
all limits and analysis.
  [1]

@_date: 2018-07-04 12:09:29
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Outlawing conflicting values would imply forcing all Signers to
implement fixed deterministic nonce generation, which I don't think it
very desirable. Otherwise PSBTs that got copied and signed and
combined again may fail. So I think we should see it the other way: we
choose the keys in such a way that picking arbitrarily is safe. If
there really is a future extension for which it would not be the case
that picking arbitrarily is acceptable, more data can be moved to the
keys, and leave the actual resolution strategy to the Finalizer. That
way Combiners can remain dumb and not need script-specific logic in
every interaction.
An alternative would be to have a fixed resolution strategy (for
example, when combining multiple PSBTs, pick the value from the first
one that has a particular key set), but I don't think this adds very
much - if picking the first is fine, picking a arbitrary one should be
fine too.
Can you envision a situation in which this is needed? In every
scenario I can come up with, the worst that can happen is that the
resulting signature is just invalid. For example, if PSBT existed
before segwit, and then was later extended to support it, a pre-segwit
signer would not recognize that BIP143 would need to be used for
segwit inputs, and produce signatures using the old sighashing
algorithm. The result is just an invalid signature.
I believe that what you're trying to accomplish is preventing signing
something you don't understand, but that's an independent issue.
Signers generally will want to inspect the transaction they're
signing, or ask for confirmation w.r.t. fees or payment destinations
involved. The case where unknown fields are present for a reason you'd
want to withhold signing for will generally also just be the situation
where you don't understand the transaction you're signing.
Here is (perhaps far fetched) example of why it may not be desirable
to reject unknown fields when signing. Imagine an extension is defined
which adds pay-to-contract derivation for keys (Q = P + H(Q||C)G);
this would be a field similar to the current BIP32 derivation one, but
instead give a base key P and a contract C. Now say there is a 2-of-2
multisig in which you're one signer, and the other signer is (unknown
to you) using P2C. After the other party Updating, the input would
contain a P2C field which you don't understand - but it also isn't
something you care about or affects you.
I would not be opposed to having fields with an explicit flag bit that
says "Don't sign if you don't understand this", but I expect that that
can also be left for future extensions.
Completely agree here. Any implementation that understands a
particular field must enforce whatever structure the field is known to
Agree, just because Combiners are expected to work correctly on
unknown fields doesn't mean they can't enforce extra consistency
checks on known fields.

@_date: 2018-07-05 15:06:53
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
So consider two possible topologies for a multiparty signing:
A) Creator and Updater produce a PSBT T. T is sent to signer 1 who
turns it into PSBT T1. T1 is then forwarded to Signer 2 who turns it
into T12. A Finalizer extracts the transaction.
B) Creator and Updater produce a PSBT T. T is sent to signer 1 and 2
simultaneously, who then produce T1 and T2 respectively. A Combiner
combines those into T12. A Finalizer extracts the transaction.
The only case where "malicious" conflicting values can occur is when
one of the Signers produces an invalid signature, or modifies any of
the other fields already present in the PSBT for consumption by
others. If this were an issue, it would be an issue regardless of the
Combiner's operation, as in topology A no Combiner is even present.
This is generally true I think - Combiners can always be replaced with
just a different (and possibly less parallel) topology of data flow.
So the question is independent of Combiners IMHO, and really about how
we deal with roles that intentionally or unintentionally produce
invalid values. I believe this is mostly not an issue. Let's go over
the cases:
* If a partial signature is invalid, the resulting transaction will be invalid.
* if a non-witness UTXO is incorrect, you'll fail to sign because the
txid mismatches the input's prevout (which you do have to check)
* If a witness UTXO is incorrect, the resulting signature will be invalid.
* If a derivation path is incorrect, the signer will fail to find the
key, or sign with the wrong key resulting in an invalid transaction.
* If a witnessscript or redeemscript is incorrect, the resulting
signature will be invalid (as those go into the scriptCode of the
sighash, and affect the type of sighashing)
* If a sighash type is incorrect, the resulting transaction may be
useless for its intended purpose (but still something every signer
agreed to sign).
So all of this boils down to dealing with the possibility that there
can be roles which intentionally or unintentionally create incorrect
fields in a PSBT, and the solution is (a) checking that prevout txids
match non-witness utxos (b) checking that the transaction you're
signing is one you want to sign (including sighash type) (c) worst
case accepting that the resulting transaction may be invalid.
Now, (c) can sometimes be caught early, by implementing additional
sanity checks for known fields. For example, rejecting PSBTs with
partial signatures that are invalid (feed them through a verifier).
This is something a Combiner can of course optionally do, but so can a
Signer or any other role.
The bottom line is that a Combiner which picks arbitrarily in case of
conflicts will never end up with something worse than what you already
need to deal with. If you disregard the case of invalid fields
(because the result will just be an invalid transaction), then any
choice the Combiner makes is fine, because all the values it can pick
from are valid.
If you're worried about attack surface, I don't believe rejecting
invalid fields ever matters. An attacker can always drop the fields
you don't understand before giving you the PSBT, making your behavior
identical to one where you'd have ignore those fields in the first
At best, you can make it protect against accidental mistakes that
would result in invalid transactions anyway.
If there is a way to sign a message in a way that can be
misinterpreted as a signature on a different message with a different
meaning, then that is a huge flaw in Bitcoin itself, and not going to
be solved by rejecting to sign unknown fields.
With regard to defense in depth:
There could be some rule like "if the highest bit of the field type is
set, don't sign", but I don't think there is any current field where
such a flag would be necessary right now.

@_date: 2018-07-06 11:08:34
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Hello everyone,
Here is a proposed BIP for 64-byte elliptic curve Schnorr signatures,
over the same curve as is currently used in ECDSA:
It is simply a draft specification of the signature scheme itself. It
does not concern consensus rules, aggregation, or any other
integration into Bitcoin - those things are left for other proposals,
which can refer to this scheme if desirable. Standardizing the
signature scheme is a first step towards that, and as it may be useful
in other contexts to have a common Schnorr scheme available, it is its
own informational BIP.
If accepted, we'll work on more production-ready reference
implementations and tests.
This is joint work with several people listed in the document.

@_date: 2018-07-08 19:29:19
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Multiparty signatures 
This is a very vague description. Is there some paper you can reference, or
a more detailed explanation of the algorithm?
This would allow m of n devices to sign a transaction without any of them
IE: each device can roll a random number as a share and the interpolation
At no point was the private key anywhere.
All of this sounds like a threshold signature scheme, which as Tim pointed
out is already possible with Schnorr.
What are the advantages of what you're describing?

@_date: 2018-07-08 19:33:06
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Multiparty signatures 
Schnorr signatures are provably secure in the random oracle model assuming
the discrete logarithm problem is hard in the used group.
What does "more secure" mean? Is your construction secure with weaker

@_date: 2018-07-08 21:39:56
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Multiparty signatures 
That sounds very useful if true, but I don't think we should include novel
cryptography in Bitcoin based on your not seeing an obvious problem with it.
I'm looking forward to seeing a more complete writeup though.

@_date: 2018-07-11 11:27:11
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Adding that sounds like a good idea, indeed.
Sounds good.
But the point is: you are not signing an input with unknown data. You
are signing your own interpretation (since you compute the sighash
yourself), which doesn't include what you don't understand. If that
interpretation doesn't match reality, the signature is at worst
useless. Who cares that someone added information about a transaction
that doesn't affect what you sign?
I don't think that's a particularly useful policy, but certainly
Signers are allowed to implement any policy they like about what they
accept in signing.

@_date: 2018-07-14 14:20:48
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
Following the design decision to use key-prefixed Schnorr, the
signature must commit to the entire public key, including its Y
It would be possible to only permit public keys whose Y coordinates
are even, or quadratic residues (like the signature internally uses
for the R point), but that would mean changing what public keys are
acceptable. Not doing so has significant practical advantages, like
not breaking existing key generation mechanisms (like BIP32 and
So if we're going to serialize the public key into the hash, in full,
the easiest choice seems to be to use the encoding everyone already
uses for public keys.
This is mostly a historical argument. When Schnorr is applied to an
integer multiplication group rather than an elliptic curve group,
serializing a group element is many times larger than serializing a
hash. For elliptic curve based Schnorr, there is hardly any benefit
for choosing the (e,s) form over (R,s).
Randomly picked by the verifier, yes. The randomization factors are
there so that an attacker cannot choose signatures which cancel out
other invalid signatures within the same batch.
What would "mandatory" mean? To follow the BIP, signers must sign
using nonces generated deterministically following the provided
method. That's as far as mandatory can go.
However, it is not possible to enforce (by a verifier) than nonces
were generated in a specific way. To do so, the verifier would need to
know the nonce, which implies learning the private key. So the nonce
choosing algorithm cannot be enforced by the verifier. This implies
that it is possible to generate valid (and secure) nonces in a way
that does not follow the BIP.
There are two documents on the site linked to. One describes the ECDSA
signing algorithm and serializations, the other specifies the curve
parameter. I could link to both.
Thanks for your comments, will review.

@_date: 2018-05-31 17:25:04
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
While you may be right in this situation, I'm not sure that conclusion
follows from your argument. Whether or not a construction is safe does
not just depend on the consensus rules, but also on how it is used.
Otherwise you could as well argue that since OP_TRUE is possible right
now which is obviously insecure, nothing more dangerous can be
accomplished through any soft fork.
The best argument for why Graftroot does not need to be optional I
think was how Greg put it: "since the signer(s) could have signed an
arbitrary transaction instead, being able to delegate is strictly less

@_date: 2018-06-02 23:11:56
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
On Sat, Jun 2, 2018, 22:56 Tamas Blummer via bitcoin-dev <
I don't think that's the point of discussion here. Of course, in order to
have filters that verifiably don't lie by omission, the filters need to be
committed to by blocks.
The question is what data that filter should contain.
There are two suggestions:
(a) The scriptPubKeys of the block's outputs, and prevouts of the block's
(b) The scriptPubKeys of the block's outputs, and scriptPubKeys of outputs
being spent by the block's inputs.
The advantage of (a) is that it can be verified against a full block
without access to the outputs being spent by it. This allows light clients
to ban nodes that give them incorrect filters, but they do need to actually
see the blocks (partially defeating the purpose of having filters in the
first place).
The advantage of (b) is that it is more compact (scriot reuse, and outputs
spent within the same block as they are created). It also had the advantage
of being more easily usable for scanning of a wallet's transactions. Using
(a) for that in some cases may need to restart and refetch when an output
is discovered, to go test for its spending (whose outpoint is not known
ahead of time). Especially when fetching multiple filters at a time this
may be an issue.
I think both of these potentially good arguments. However, once a committed
filter exists, the advantage of (a) goes away completely - validation of
committed filters is trivial and can be done without needing the full
blocks in the first place.
So I think the question is do we aim for an uncommitted (a) first and a
committed (b) later, or go for (b) immediately?

@_date: 2018-06-03 12:23:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
First of all, thanks for working on this.
I have some concerns about the use of Bech32. It is designed for
detecting 3 errors up to length 1023 (but is then picked specifically
to support 4 errors up to length 89). However, for error correction
this translates to just being able to efficiently correct 1 error
(3/2, rounded down) up to length 1023. You can of course always try
all combinations of up to N changes to the input (for any N), testing
the checksum, and comparing the results against the UTXO set or other
wallet information that may have been recovered. However, the checksum
at best gives you a small constant speedup here, not a fundamentally
improved way for recovery.
However, we can design other base32 BCH codes easily with different
properties. As we mostly care about efficient algorithms for recovery
(and not just error detection properties), it seems more important to
have good design strength (as opposed to picking a code from a large
set which happens to have better properties, but without efficient
algorithm, like Bech32).
This is what I find for codes designed for length 93 (the first length
for which efficient codes exist with length long enough to support 256
bits of data):
* correct 1 error = 3 checksum characters
* correct 2 errors = 6 checksum characters
* correct 3 errors = 10 checksum characters
* correct 4 errors = 13 checksum characters
* correct 5 errors = 16 checksum characters
* ...
* correct 8 errors = 26 checksum characters (~ length * 1.5)
* correct 11 errors = 36 checksum characters (~ maximum length without
pushing checksum + data over 93 characters)
For codes designed for length 341 (the first length enough to support
512 bits of data):
* correct 1 error = 3 checksum characters
* correct 2 errors = 7 checksum characters
* correct 3 errors = 11 checksum characters
* correct 4 errors = 15 checksum characters
* correct 5 errors = 19 checksum characters
* ...
* correct 7 errors = 26 checksum characters (~ length * 1.25)
* correct 13 errors = 51 checksum characters (~ length * 1.5)
* correct 28 errors = 102 checksum characters (~ length * 2)
So it really boils down to a trade-off between length of the code, and
recovery properties.
These two sets of codes are distinct (a code designed for length 93
has zero error correction properties when going above 93), so either
we can pick a separate code for the two purposes, or be limited to the
second set.
If there is interest, I can construct a code + implementation for any
of these in a few days probably, once the requirements are clear.

@_date: 2018-06-05 21:01:00
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving 
Hi Bradley,
thank you for working on this and going as far as implementing the
entire protocol. It looks like a very well-worked out idea already,
and its semantics can probably be adopted pretty much as-is. It would
be very exciting to bring these kinds of privacy improvements to
Bitcoin's P2P protocol.
I do have a number of comments on the specification and suggested
implementation in Bitcoin Core. I'm dumping my thoughts here, though
at this stage the specification is probably more important. The
implementation can be discussed more thoroughly when there is a PR
* Overall, I think it would be worthwhile to describe the intended
node behavior in the BIP, at a higher level than Bitcoin Core
patchsets, but more detailed than what is in the BIP now. The
patch-based descriptions are both hard to read for developers working
on different systems who are unfamiliar with the Core codebase, and
don't make it clear to what extent implementation decisions are local
policy (which can be changed without network coordination), and which
follow from security or privacy arguments for the protocol.
* Interaction with feefilter (BIP 133) and Bloom filter (BIP 37). When
peers have given us filters on what transactions they will accept,
should Dandelion transactions be subject to the same? Should it
influence the choice of route? One simple possibility is perhaps to
avoid choosing BIP37 peers as Dandelion routes, and treat transactions
that do not pass the feefilter for its
would-be-outgoing-Dandelion-route as an automatic fluff - justified by
noting that relaying a transaction close to what fee is acceptable to
the network's mempools is already less likely to get good privacy due
to reduced chances of propagation.
* Mempool dependant transactions. It looks like the current
implementation accepts Dandelion transactions which are dependant on
other Dandelion (stempool) transactions and on confirmed blockchain
transactions, but not ones that are dependant on other unconfirmed
normal mempool transactions. Is this intentional, or resulting from a
difficulty in implementing this? Should the correct behaviour be
specified, or left free for nodes to decide?
* Orphan transactions. It looks like the current implementation
assumes no orphan transactions, but in a dynamic network (especially
with occasionally shuffling of Dandelion routes), I expect that
sometimes a dependent transaction will go on a different route than
its parent. Do you have any thoughts about that (even if not addressed
in a very implementation). Could we have a Dandelion-orphan-pool of
transactions, similar to the normal mempool has a set of orphan
* Preferred connections. Should we bias the outgoing connection peer
selection code to prefer Dandelion-capable peers when the number is
too low?
* How do we control the size of the stempool? Should acceptance of a
transaction to the normal mempool and/or blockchain result in eviction
of it (and conflicts) from the stempool? The existing code
intentionally has an upper bound on the size of the mempool to assure
predictable resource usage - the introduction of the stempool
shouldn't change that.
* I don't think you need to fully materialize all the routes. Instead,
you can just maintain a vector of 2 selected Dandelion-supporting
peers (and if one disconnects, replace just that one with another
one). To map incoming peers to an index in that list of peers, you can
use deterministic randomness (see SipHasher in the source code) with
the incoming node_id as data and a single global secret nonce (chosen
at startup, and reset on reshuffle).
* setDandelionInventoryKnown looks like it can grow unboundedly. A
rolling Bloom filter (like used for filterInventoryKnown) is perhaps
easier to guarantee predictable memory usage for.
* Use a scheduler job instead of a separate thread for shuffling the
routes (extra threads use unnecessarily large amounts of memory).
* (nit) coding style: doc/developer-notes.md has a number of
guidelines on coding style you may want to check out.

@_date: 2018-06-06 10:04:23
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Interesting point; I don't see any relevant implications to this
either, but it's indeed good to point out this as a distinction.
So you're saying: the Graftroot signature data could be made identical
to the signature hash of an implicit 1-input-1-output transaction
spending the coin and creating a new output with the delegated script
as sPK, and the same amount.
I like that idea, but I don't think it can be *exactly* that. If it's
possible to take a Graftroot signature and instead construct a
transaction with it, you have inherently introduced a malleability.
The created outpoint will be different in both cases (different txid),
meaning that a chain of dependent unconfirmed transactions may be
broken by giving one participant the ability to choose between
Graftroot delegation or actual spending.
Two points here: (1) the implicit transaction would be 0 fee (unless
we somehow assign a portion of the fee to the delegation itself for
purposes of sighash computing), and (2) this sounds very similar to
the issue SIGHASH_NOINPUT is intended to solve. About that...
You're right when you're comparing with existing transaction sighash
semantics, but not when SIGHASH_NOINPUT would exist. If that were the
case, the only real difference is your point above of not being able
to commit the implicit transaction separately. In other words, we're
back to something Johnson pointed out earlier: some of the perceived
problems with Graftroot are also issues with SIGHASH_NOINPUT.
I wonder if we can make this explicit: Graftroot spending becomes a
special sighash flag (which possibly is only allowed at the top level)
- it builds an implicit transaction which moves all the coins to a
newly provided script, computes the sighash of that transaction
(taking all of the Graftroot signature's sighash flags into account -
including potentially SIGHASH_NOINPUT), and requires a signature with
that. The delegated script is then evaluated in the context of that
implicit transaction.
However, in order to avoid the malleability issue I think the actual
signature should still be different - possibly by simply passing
through the Graftroot sighash flag into the sighash being computed.

@_date: 2018-06-11 18:05:14
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving 
I understand the argument about not making routing decisions based on
self-reported features, but I would expect it to only matter if done
selectively? Allowing a node to opt out of Dandelion entirely should always
be possible regardless - as they can always indicate not supporting it.
The reason for my suggestion was that most full nodes on the network use
feefilter, while only (from the perspective of Dandelion uninteresting)
light nodes and blocksonly nodes generally use Bloom filters.
Just dropping stem transactions that would otherwise be sent to a Dandelion
peer which fails its filter, and relying on embargo seems fine. But perhaps
this option is something to describe in the BIP ("Nodes MAY choose to
either drop stem transactions or immediately start diffusion when a
transaction would otherwise be sent to a Dandelion node whose filter is not
satisfied for that transaction. A node SHOULD NOT make any routing
decisions based on the transaction itself, and thus SHOULD NOT try to find
an alternative Dandelion node to forward to" for example).
Regarding mempool-dependent transactions, the reference implementation adds
Oh, I see! I was just judging based on the spec code you published, but I
must have missed this. Yes, that makes perfect sense. There may be some
issues with this having a significant impact on stempool memory usage, but
let's discuss this later on implementation.
Orphans: stem orphans can occur when a node on the stem shuffles its route
Another option (just brainstorming, I may be missing something here), is to
remember which peer each stempool transaction was forwarded to. When a
dependent stem transaction arrives, it is always sent to (one of?) the
peers its dependencies were sent to, even if a reshuffle happened in
Thinking more about it, relying on embargo is probably fine - it'll just
result in slightly lowered average stem length, and perhaps multiple
simultaneous fluffs starting?
Regarding preferred connections, we have found that making Dandelion
Oh, I don't mean routing decisions, but connections in general.
On the implementation side:
Let's discuss these later.
I think that's the primary thing to focus on at this point, but perhaps
others on this list feel different.

@_date: 2018-06-12 19:44:47
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
Here is an example BCH code for base32 data which adds 27 checksum
characters, and can correct up to 7 errors occurring in strings up to
length 1023 (including the checksum characters themselves):
It can encode sequences of integers (between 0 and 31):
ref.py encode 13 7 22 23 11 29 21 15 3 26 20 26 4 7 6 11 19 1 6 8 31 13 4 19
Decode it again:
ref.py decode d8khta40r656y8xtnpxgldyne96vsfr83uch908se82g98rmnaa
Or correct errors:
ref.py decode d8khta50r656y8xtmpxhlcyne96vsfr84udh908se82g98rmnat
The code above is just a randomly picked BCH code, and has no special
properties beyond the ones it is designed for.
I can easily generate similar code for BCH codes with different properties.

@_date: 2018-06-15 16:34:40
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Hello all,
given some recent work and discussions around BIP 174 (Partially
Signed Bitcoin Transaction Format) I'd like to bring up a few ideas.
First of all, it's unclear to me to what extent projects have already
worked on implementations, and thus to what extent the specification
is still subject to change. A response of "this is way too late" is
perfectly fine.
So here goes:
* Key-value map model or set model.
This was suggested in this thread:
The motivation behind using a key-value model rather than a simple
list of records was that PSBTs can be duplicated (given to multiple
people for signing, for example), and merged back together after
signing. With a generic key-value model, any implementation can remove
the duplication even if they don't understand fields that may have
been added in future extensions.
However, almost the same can be accomplished by using the simpler set
model (the file consists of a set of records, with no duplication
allowed). This would mean that it would technically be legal to have
two partial signatures with the same key for the same input, if a
non-deterministic signer is used.
On the other hand, this means that certain data currently encoded
inside keys can be dropped, reducing the PSBT size. This is
particularly true for redeemscripts and witnessscripts, as they can
just be computed by the client when deserializing. The two types could
even be merged into just "scripts" records - as they don't need to be
separated based on the way they're looked up (Hash160 for P2SH, SHA256
for P2WSH). The same could be done for the BIP32 derivation paths,
though this may be expensive, as the client would need to derive all
keys before being able to figure out which one(s) it needs.
One exception is the "transaction" record, which needs to be unique.
That can either be done by adding an exception ("there can only be one
transaction record"), or by encoding it separately outside the normal
records (that may also be useful to make it clear that it is always
* Ability for Combiners to verify two PSBT are for the same transaction
Clearly two PSBTs for incompatible transactions cannot be combined,
and this should not be allowed.
It may be easier to enforce this if the "transaction" record inside a
PSBT was required to be in a canonical form, meaning with empty
scriptSigs and witnesses. In order to do so, there could be per-input
records for "finalized scriptSig" and "finalized witness". Actually
placing those inside the transaction itself would only be allowed when
all inputs are finalized.
* Optional signing
I think all operation for the Signer responsibility should be
optional. This will inevitably lead to incompatibilities, but with the
intent of being forward compatible with future developments, I don't
think it is possible to require every implementation to support the
same set of scripts or contracts. For example, some signers may only
implement single-key P2PKH, or may only support signing SegWit inputs.
It's the user's responsibility to find compatible signers (which is
generally not an issue, as the different participants in a setup
necessarily have this figured out before being able to create an
address). This does mean that there can't be an unconditional test
vector that specifies the produced signature in certain circumstances,
but there could be "For implementations that choose to implement
signing for P2PKH inputs using RFC6979, the expected output given
input X and access to key Y is Z".
On the other hand, the Combiner and Finalizer roles can probably be
specified much more accurately than they are now.
* Derivation from xpub or fingerprint
For BIP32 derivation paths, the spec currently only encodes the 32-bit
fingerprint of the parent or master xpub. When the Signer only has a
single xprv from which everything is derived, this is obviously
sufficient. When there are many xprv, or when they're not available
indexed by fingerprint, this may be less convenient for the signer.
Furthermore, it violates the "PSBT contains all information necessary
for signing, excluding private keys" idea - at least if we don't treat
the chaincode as part of the private key.
For that reason I would suggest that the derivation paths include the
full public key and chaincode of the parent or master things are
derived from. This does mean that the Creator needs to know the full
xpub which things are derived from, rather than just its fingerprint.
* Generic key offset derivation
Whenever a BIP32 derivation path does not include any hardened steps,
the entirety of the derivation can be conveyed as "The private key for
P is equal to the private key for Q plus x", with P and Q points and x
a scalar. This representation is more flexible (it also supports
pay-to-contract derivations), more efficient, and more compact. The
downside is that it requires the Signer to support such derivation,
which I don't believe any current hardware devices do.
Would it make sense to add this as an additional derivation method?
* Hex encoding?
This is a very minor thing. But presumably there will be some standard
way to store PSBTs as text for copy-pasting - either prescribed by the
spec, or de facto. These structures may become pretty large, so
perhaps it's worth choosing something more compact than hexadecimal -
for example Base64 or even Z85 (

@_date: 2018-06-19 10:16:51
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Thanks for your comments so far. I'm very happy to see people dig into
the details, and consider alternative approaches.
Yes, the reason is address reuse. It may be discouraged, but it still
happens in practice (and unfortunately it's very hard to prevent
people from sending to the same address twice).
It's certainly possible to make them per-input (and even per-output as
suggested below), but I don't think it gains you much. At least when a
signer supports any kind of multisig, it needs to match up public keys
with derivation paths. If several can be provided, looking them up
from a global table or a per-input table shouldn't fundamentally
change anything.
However, perhaps it makes sense to get rid of the global section
entirely, and make the whole format a transaction plus per-input and
per-output extra fields. This would result in duplication in case of
key reuse, but perhaps that's worth the complexity reduction.
I think here it makes sense because there can actually only be (up to)
one redeemscript and (up to) one witnessscript. So if we made those
per-input and per-output, it may simplify signers as they don't need a
table lookup to find the correct one. That would also mean we can drop
their hashes, even if we keep a key-value model.
Perhaps, yes.
Originally there was at least this intuition for why it shouldn't be
necessary: the resulting signature for an input is either valid or
invalid. Adding information to a PSBT (which is what signers do)
either helps with that or not. The worst case is that they simply
don't have enough information to produce a signature together. But an
ignored unknown field being present should never result in signing the
wrong thing (they can always see the transaction being signed), or
failing to sign if signing was possible in the first place. Another
way of looking at it, the operation of a signer is driven by queries:
it looks at the scriptPubKey of the output being spent, sees it is
P2SH, looks for the redeemscript, sees it is P2WSH, looks for the
witnessscript, sees it is multisig, looks for other signers'
signatures, finds enough for the threshold, and proceeds to sign and
create a full transaction. If at any point one of those things is
missing or not comprehensible to the signer, he simply fails and
doesn't modify the PSBT.
However, if the sighash request type becomes mandatory, perhaps this
is not the case anymore, as misinterpreting something like this could
indeed result in an incorrect signature.
If we go down this route, if a field is marked as mandatory, can you
still act as a combiner for it? Future extensions should always
maintain the invariant that a simple combiner which just merges all
the fields and deduplicates should always be correct, I think. So such
a mandatory field should only apply to signers?
I wouldn't say it's trying very hard to be space-conservative. The
design train of thought started from "what information does a signer
need", and found a signer would need information on the transaction to
sign, and on scripts to descend into, information on keys to derive,
and information on signatures provided by other participants. Given
that some of this information was global (at least the transaction),
and some of this information was per-input (at least the signatures),
separate scopes were needed for those. Once you have a global scope,
and you envision a signer which looks up scripts and keys in a map of
known ones (like the signing code in Bitcoin Core), there is basically
no downside to make the keys and scripts global - while giving space
savings for free to deduplication.
However, perhaps that's not the right way to think about things, and
the result is simpler if we only keep the transaction itself global,
and everything else per-input (and per-output).
I think there are good reasons to not be gratuitously large (I expect
that at least while experimenting, people will copy-paste these things
a lot and page-long copy pastes become unwieldy quickly), but maybe
not at the cost of structural complexity.
On Tue, Jun 19, 2018 at 7:22 AM, matejcik via bitcoin-dev
Agree, but this doesn't actually need to be specified right now. As
the key's (and or value's) interpretation (including the type) is
completely unspecified, an extension can just start using 2-byte keys
(as long as the first byte of those 2 isn't used by another field
Hmm, I wouldn't say so. Perhaps the transaction's inputs and outputs
are chosen by one entity, and then sent to another entity which has
access to the UTXOs or previous transactions. So while the UTXOs must
be present before signing, I wouldn't say the file format itself must
enforce that the UTXOs are present.
However, perhaps we do want to enforce at-most one UTXO per input. If
there are more potential extensions like this, perhaps a key-value
model is better, as it's much easier to enforce no duplicate keys than
it is to add field-specific logic to combiners (especially for
extensions they don't know about yet).
If we go with the "not put signatures/witnesses inside the transaction
until all of them are finalized" suggestion, perhaps the number of
inputs field can be dropped. There would be always one exactly for
each input (but some may have the "final script/witness" field and
others won't).
Let me elaborate.
Right now, the BIP32 fields are of the form ...
Instead, I suggest fields of the form ...
The fingerprint in this case is identical to the first 32 bit of the
Hash160 of , so certainly no information is lost by
making this change.
This may be advantageous for three reasons:
* It permits signers to have ~thousands of master keys (at which point
32-bit fingerprints would start having reasonable chance for
collisions, meaning multiple derivation attempts would be needed to
figure out which one to use).
* It permits signers to index their master keys by whatever they like
(for example, SHA256 rather than Hash160 or prefix thereof).
* It permits signers who don't store a chaincode at all, and just
protect a single private key.

@_date: 2018-06-21 10:39:57
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Ah, you're thinking of an even simpler signer than I was imagining. I
don't think this works in general, because the hash being signed
depends on the structure of the script. For example, if it is P2SH, it
is the redeemscript that goes into the scriptCode serialization rather
than the scriptPubKey. If it is segwit, BIP143 serialization needs to
be used, etc. It may work if your signing is restricted to just one of
those structures, though.
Right - but I think anything but the simplest signer must do this,
just to be able to distinguish between different kinds of signature
But you're right, having per-input redeemscript/witnessscript
simplifies things still - instead of needing to look a script hash in
a map, you can just compare it with *the* redeemscript/witnessscript.
I understand your point now. I hadn't considered the possibility of
just signing with all BIP32 derivation paths given for which the
master matches, instead of extracting pubkeys/pkhs from the script.
That's a major simplification for signers indeed. I do think you need
some conditions before to determine the script structure (see above),
but this is a good point in favour of making the derivation paths
It's more a side-effect of focusing on forward compatibility. I expect
that we will have transactions with inputs spending different kinds of
outputs, and some signers may not be able to understand all of them.
However, as long as the design goal of having Combiners function
correctly for things they don't understand, everything should be able
to work together fine.
That makes sense - I think we've already touched this when discussing
the requirement for UTXOs to be added. Perhaps those aren't added by
the Creator, but by some index server. The same could be true for the
scripts or derivations paths.
And indeed, most of the information in the derivation paths is
effectively opaque to the Creator - it's just some data given out by
the Signer about its keys that gets passed back to it so it can
identify the key. There is benefit in keeping it in a fixed structure
(like xpub/chaincode, or fingerprint + derivation indexes), to
guarantee compatibility between multiple signer implementations with
access to the same key.
On Tue, Jun 19, 2018 at 5:39 PM, Jason Les via bitcoin-dev
Both Base64 and Z85 can be stored in JSON strings without quoting
(neither uses quotation characters or backslashes), but Z85 is
slightly more compact (Z85 is 5 characters for 4 bytes, Base64 is 4
characters for 3 bytes). Both use non-alphanumeric characters, so I
don't think there is much difference w.r.t. copy-pastability either.
Z85 is far less common though.
On Thu, Jun 21, 2018 at 4:44 AM, Tomas Susanka via bitcoin-dev
Up to one per output, and up to one per input - indeed.
On Thu, Jun 21, 2018 at 7:32 AM, Tomas Susanka via bitcoin-dev
Perhaps you're missing the reason for having output scripts? It is so
that signers that wish to known the amounts transferred can be told
which outputs of the to-be transaction are change, and thus shouldn't
be counted towards the balance. By providing the scripts and
derivation paths in a PSBT, the Creator can prove to the Signer that
certain outputs do not actually move funds to some other entity.
Based on the points before, my preference is having everything
per-input and per-output except the transaction (with empty
scriptSig/witness) itself, and having exactly one set/map per input
and output (which may include a "finalized scriptSig/witness field"
for finalized inputs). The overhead of having at least one separator
byte for every input and output in the transaction is at most a few
percent compared to the data in the transaction itself. If size is
really an issue (but I think we've already established that small size
gains aren't worth much extra complexity), we could also serialize the
transaction without scriptSigs/witnesses (which are at least one byte
each, and guaranteed to be empty).
I'm unsure about typed record vs. key-value model. If we'd go with a
per-input script approach, the key would just be a single byte ("the
redeemscript" and "the witnessscript"), so the advantage of being able
to drop the script hashes applies equally to both models. After that,
it seems the only difference seems to be that a well-defined prefix of
the records is enforced to be unique as opposed to the entire record
being enforced to be unique. I don't think there is much difference in
complexity, as Combiners and Signers still need to enforce some kind
of uniqueness even in a typed records model.

@_date: 2018-06-22 12:10:15
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
On Thu, Jun 21, 2018 at 12:56 PM, Peter D. Gray via bitcoin-dev
This is awesome to hear. We need to hear from people who have comments
or issues they encounter while implementing, but also cases where
things are fine as is.
I understand you find the suggestions being brought up in this thread
to be bikeshedding over details, and I certainly agree that "changing
X will gratuitously cause us more work" is a good reason not to make
breaking changes to minutiae. However, at least abstractly speaking,
it would be highly unfortunate if the fact that someone implemented a
draft specification results in a vested interest against changes which
may materially improve the standard.
In practice, the process surrounding BIPs' production readiness is not
nearly as clear as it could be, and there are plenty of BIPs actually
deployed in production which are still marked as draft. So in reality,
truth is that this thread is "late", and also why I started the
discussion by asking what the state of implementations was. As a
result, the discussion should be "which changes are worth the hassle",
and not "what other ideas can we throw in" - and some of the things
brought up are certainly the latter.
So to get back to the question what changes are worth the hassle - I
believe the per-input derivation paths suggested by matejcik may be
one. As is written right now, I believe BIP174 requires Signers to
pretty much always parse or template match the scripts involved. This
means it is relatively hard to implement a Signer which is compatible
with many types of scripts - including ones that haven't been
considered yet. However, if derivation paths are per-input, a signer
can just produce partial signatures for all keys it has the master
for. As long as the Finalizer understands the script type, this would
mean that Signers will work with any script. My guess is that this
would be especially relevant to devices where the Signer
implementation is hard to change, like when it is implemented in a
hardware signer directly.
What do you think?

@_date: 2018-06-23 12:49:54
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
Here are some more numbers then. It's important to note that the
number of correctable errors includes errors inside the checksum
characters themselves. So if you want to aim for a certain percentage
of correctable characters, the numbers go up much more dramatically.
For codes restricted to 341 characters total (including the checksum
characters), and assuming 103 data characters (enough for 512 bits):
* With 26 checksum characters (adding 25%, 20% of overall string), 7
errors can be corrected (5% of overall string)
* With 62 checksum characters (adding 60%, 38% of overall string), 17
errors can be corrected (10% of overall string)
* With 116 checksum characters (adding 113%, 53% of overall string),
33 errors can be corrected (15% of overall string)
* With 195 checksum characters (adding 189%, 65% of overall string),
60 errors can be corrected (20% of overall string)
For codes restricted to 1023 characters total (including the checksum
characters), and assuming 103 data characters (enough for 512 bits):
* With 27 checksum characters (adding 26%, 21% of overall string), 7
errors can be corrected (5% of overall string)
* With 64 checksum characters (adding 62%, 38% of overall string), 17
errors can be corrected (10% of overall string)
* With 127 checksum characters (adding 123%, 57% of overall string),
36 errors can be corrected (15% of overall string)
* With 294 checksum characters (adding 285%, 74% of overall string),
80 errors can be corrected (20% of overall string)
* With 920 checksum characters (adding 893%, 90% of overall string),
255 errors can be corrected (25% of overall string)
I'll gladly construct reference source code for any of these.

@_date: 2018-06-26 13:30:04
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
On Tue, Jun 26, 2018 at 8:33 AM, matejcik via bitcoin-dev
I understand this is a philosophical point, but to me it's the
opposite. The file conveys "the script is X", "the signature for key X
is Y", "the derivation for key X is Y" - all extra metadata added to
inputs of the form "the X is Y". In a typed record model, you still
have Xes, but they are restricted to a single number (the record
type). In cases where that is insufficient, your solution is adding a
repeatable flag to switch from "the first byte needs to be unique" to
"the entire record needs to be unique". Why just those two? It seems
much more natural to have a length that directly tells you how many of
the first bytes need to be unique (which brings you back to the
key-value model).
Since the redundant script hashes were removed by making the scripts
per-input, I think the most compelling reason (size advantages) for a
record based model is gone.
Forward compatibility with new script types. A transaction may spend
inputs from different outputs, with different script types. Perhaps
some of these are highly specialized things only implemented by some
software (say HTLCs of a particular structure), in non-overlapping
ways where no piece of software can handle all scripts involved in a
single transaction. If Combiners cannot deal with unknown fields, they
won't be able to deal with unknown scripts. That would mean that
combining must be done independently by Combiner implementations for
each script type involved. As this is easily avoided by adding a
slight bit of structure (parts of the fields that need to be unique -
"keys"), this seems the preferable option.
No, a Combiner can pick any of the values in case different PSBTs have
different values for the same key. That's the point: by having a
key-value structure the choice of fields can be made such that
Combiners don't need to care about the contents. Finalizers do need to
understand the contents, but they only operate once at the end.
Combiners may be involved in any PSBT passing from one entity to
In case of BIP32 derivation, computing the pubkeys is possibly
expensive. A simple signer can choose to just sign with whatever keys
are present, but they're not the only way to implement a signer, and
even less the only software interacting with this format. Others may
want to use a matching approach to find keys that are relevant;
without pubkeys in the format, they're forced to perform derivations
for all keys present.
And yes, it's simple enough to make the key part of the value
everywhere, but in that case it becomes legal for a PSBT to contain
multiple signatures for a key, for example, and all software needs to
deal with that possibility. With a stronger uniqueness constraint,
only Combiners need to deal with repetitions.
If you take the records model, and then additionally drop the
whole-record uniqueness constraint, yes, though that seems pushing it
a bit by moving even more guarantees from the file format to
application level code. I'd like to hear opinions of other people who
have worked on implementations about changing this.

@_date: 2018-06-27 08:06:39
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 174 thoughts 
Yes, I wasn't claiming otherwise. This was just a response to your question
why it is important that Combiners can process unknown fields. It is not an
argument in favor of one model or the other.
No, I'm exactly arguing against smartness in the Combiner. It should always
be possible to implement a Combiner without any script specific logic.
That's because PSBTs can be copied, signed, and combined back together. A
Combiner which does not deduplicate (at all) would end up having every
original record present N times, one for each copy, a possibly large blowup.
For all fields I can think of right now, that type of deduplication can be
done through whole-record uniqueness.
The question whether you need whole-record uniqueness or specified-length
uniqueness (=what is offered by a key-value model) is a philosophical one
(as I mentioned before). I have a preference for stronger invariants on the
file format, so that it becomes illegal for a PSBT to contain multiple
signatures for the same key for example, and implementations do not need to
deal with the case where multiple are present.
It seems that you consider the latter PSBT "invalid". But it is well
It's not about considering. We're writing a specification. Either it is
made invalid, or not.
In a key-value model you can have dumb combiners that must pick one of the
keys in case of duplication, and remove the necessity of dealing with
duplication from all other implementations (which I consider to be a good
thing). In a record-based model you cannot guarantee deduplication of
records that permit repetition per type, because a dumb combiner cannot
understand what part is supposed to be unique. As a result, a record-based
model forces you to let all implementations deal with e.g. multiple partial
signatures for a single key. This is a minor issue, but in my view shows
how records are a less than perfect match for the problem at hand.
To repeat and restate my central question:
Again, because otherwise you may need a separate Combiner for each type of
script involved. That would be unfortunate, and is very easily avoided.
Actually, I can imagine the opposite: having fields with same "key"
That can always be avoided by using different identifying information as
key for these fields. In your example, assuming you're talking about some
form of threshold signature scheme, every party has their own "shard" of
the key, which still uniquely identifies the participant. If they have no
data that is unique to the participant, they are clones, and don't need to
interact regardless.
Perhaps you want to avoid signing with keys that are already signed with?
If you need to derive all the keys before even knowing what was already
signed with, you've already performed 80% of the work.
Of course a file format can make guarantees. If certain combinations of
data in it do not satsify the specification, the file is illegal, and
implementations do not need to deal with it. Stricter file formats are
easier to deal with, because there are less edge cases to consider.
To your point: proto v2 afaik has no way to declare "whole record
uniqueness", so either you drop that (which I think is unacceptable - see
the copy/sign/combine argument above), or you deal with it in your
application code.

@_date: 2018-03-26 01:53:23
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] {sign|verify}message replacement 
Thanks for starting a discussion about this idea.
A few comments inline:
On Wed, Mar 14, 2018 at 1:09 AM, Karl Johan Alm via bitcoin-dev <
You need a bit more logic to deal with softforks and compatibility. The
question is which script validation flags you verify with:
* If you make them fixed, it means signatures can't evolve with new address
types being introduced that rely on new features.
* If you make it just consensus flags (following mainnet), it means that
people with old software will see future invalid signatures as always
valid; this is probably not acceptable.
* If you make it standardness flags, you will get future valid signatures
that fail to verify.
One solution is to include a version number in the signature, which
explicitly corresponds to a set of validation flags. When the version
number is something a verifier doesn't know, it can be reported as
inconclusive (it's relying on features you don't know about).
An solution is to verify twice; once with all consensus rules you know
about, and once with standardness rules. If they're both valid, the
signature is valid. If they're both invalid, the signature is invalid. If
they're different (consensus valid but standardness invalid), you report
the signature validation as inconclusive (it's relying on features you
don't know about). This approach works as long as new features only use
previous standardness-invalid scripts, but perhaps a version number is
still needed to indicate the standardness flags.
RPC commands:
Why not extend the existing signmessage/verifymessage RPC? For legacy
addresses it can fall back to the existing signature algorithm, while using
the script-based approach for all others.
(**) If  is true,  is the sighash, otherwise
That's very dangerous I'm afraid. It could be used to trick someone into
signing off on an actual transaction, if you get them to sign a "random
looking" prehashed message. Even if you have a prehashed message, there is
no problem with treating it as hex input to a second hashing step, so I
think the prehashed option isn't needed. It's why the existing message
signing functionality always forcibly prefixes "Bitcoin signed message:",
to avoid signing something that unintentionally corresponds to a message
intended for another goal.

@_date: 2018-05-18 20:06:10
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I was wondering about that too, but it turns out that isn't necessary. At
least in Bitcoin Core, all the data needed for such a filter is in the
block + undo files (the latter contain the scriptPubKeys of the outputs
being spent).
I have a script running to compare the filter sizes assuming the regular
That's very helpful, thank you.

@_date: 2018-05-22 11:17:42
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Hello all,
Given the recent discussions about Taproot [1] and Graftroot [2], I
was wondering if a practical deployment needs a way to explicitly
enable or disable the Graftroot spending path. I have no strong
reasons why this would be necessary, but I'd like to hear other
people's thoughts.
As a refresher, the idea is that a script type could exists which
looks like a pubkey Q, but can be spent either:
* By signing the spending transaction directly using Q (key spending)
* By proving Q was derived as Q = P + H(P,S)*G, with a script S and
its inputs (Taproot script spending).
* By signing a script S using Q, and providing S's inputs (Graftroot
script spending).
Overall, these constructions let us create combined
pay-to-pubkey-or-script outputs that are indistinguishable, and don't
even reveal a script spending path existed in the first place when the
key spending path is used. The two approaches ((T)aproot and
(G)raftroot) for the script spending path have different trade-offs:
* T outputs can be derived noninteractively from key and scripts; G
outputs need an interaction phase where the key owner(s) sign off on
the potential script spending paths.
* T lets you prove what all the different spending paths are.
* T without any other technology only needs to reveal an additional
point when spending a single script; G needs half-aggregated
signatures [3] to achieve the same, which complicates design (see
* G is more compact when dealing with many spending paths (O(1) in the
number of spending paths), while T spends need to be combined with
Merkle branches to deal with large number of spends (and even then is
still O(log n)).
* G spending paths can be added after the output is created; T
requires them be fixed at output creation time.
My question is whether it is safe to always permit both types of
script spending paths, or an explicit commitment to whether Graftroot
is permitted is necessary. In theory, it seems that this shouldn't be
needed: the key owners are always capable of spending the funds
anyway, so them choosing to delegate to others shouldn't enable
anything that isn't
possible by the key owners already.
There are a few concerns, however:
* Accidentally (participating in) signing a script may have more broad
consequences. Without Graftroot, that effect is limited to a single
transaction with specific inputs and outputs, and only as long as all
those inputs are unspent. A similar but weaker concern exists for
* In a multisignature setting (where the top level key is an aggregate
of multiple participants), the above translates to the ability for a
(threshold satsisfying) subset of participants being able to (possibly
permanently) remove others from the set of signers (rather than for a
single output).
* In a situation where private keys are stored in an HSM, without
Graftroot an attacker needs access to the device and convince it to
sign for every output he wants to steal (assuming the HSM prevents
leaking private keys). With Graftroot, the HSM may be tricked into
signing a script that does not include itself. Arguably, in a
Graftroot setting such an HSM would need a degree of protection
similar to not leaking private keys applied to not signing scripts,
but this may be less obvious.
Overall, none of these are convincing points, but they do make me
uncomfortable about the effect the Graftroot spending path may have on
some use cases. Given that Taproot/Graftroot's primary advantage is
increasing fungibility by making all outputs look identical, it seems
good to discuss potential reasons such outputs couldn't or wouldn't be
adopted in certain applications.
  [1]   [2]   [3]   [4]

@_date: 2018-05-23 18:58:11
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
Thanks everyone who commented so far, but let me clarify the context
of this question first a bit more to avoid getting into the weeds too
If it turns out to be necessary to explicitly commit to permitting
Graftroot spending, there are a number of approaches:
* Use a different witness version (or other marker directly in the
scriptPubKey) to enable Graftroot.
* Signal the permission to spend through Graftroot inside the Taproot
script as suggested by ZmnSCPxj.
* Make "Spend through Graftroot" a special script (possibly indirectly
with a Merkle tree in Taproot).
* Implement Graftroot as an opcode/feature inside the scripting
language (which may be more generically useful as a delegation
* Postpone Graftroot.
All of these are worse in either efficiency or privacy than always
permitting Graftroot spends directly. Because of that, I think we
should first focus on reasons why a lack of commitment to enabling
Graftroot may result in it being incompatible with certain use cases,
or other reasons why it could interfere with applications adopting
such outputs.
 all of these concerns only apply to a new hypothetical
Taproot/Graftroot output type, which combines pay-to-pubkey and
pay-to-script in a single scriptPubKey that just contains a public
key. It doesn't apply to existing P2SH like constructions.
Also, the concern of making Graftroot optional does not apply to
Taproot, as the Taproot spending path's script is committed to (using
scriptPubKey = P + H(P,script)*G), allowing the script to be
explicitly chosen to be a non-spendable script, which the author could
prove is the case (without revealing it to the entire world).
It is also always possible to create a "script only" Taproot output
(without key that can unconditionally spend), by picking a pubkey that
is provably unspendable (hashing onto a curve point, in particular),
or through pubkey recovery.

@_date: 2018-05-25 10:54:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Minimizing the redundancy in Golomb Coded Sets 
Hi all,
I spent some time working out the optimal parameter selection for the
Golomb Coded Sets that are proposed in BIP158:
TL;DR: if we really want an FP rate of exactly 1 in 2^20, the Rice
parameter should be 19, not 20. If we don't, we should pick an FP rate
of 1 in a 1.4971*2^B. So for example M=784931 B=19 or M=1569861 B=20.

@_date: 2018-11-19 14:37:57
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Hello everyone,
For future segwit versions, I think it would be good add a few things
to the sighash by default that were overlooked in BIP143:
* Committing to the absolute transaction fee (in addition to just the
amount being spent in each input) would categorically remove concerns
about wallets lying about fees to HW devices or airgapped signers.
* Committing to the scriptPubKey (in addition to the scriptCode) would
prevent lying to devices about the type of output being spent, even
when the scriptCode is correct. As a reminder, the scriptCode is the
actually executed script (which is the redeemscript in non-segwit
P2SH, and the witnesscript in P2WSH/P2WPKH).
As this implies additional information that may not be desirable to
commit to in all circumstances, it makes sense to make these optional.
This obviously interacts with SIGHASH_NOINPUT, which really adds two
different ways of rebinding signatures to inputs:
* Changing the prevout (so that the txid doesn't need to be known when
the signature is created)
* Changing the script (so that the exact scriptPubKey/redeemScript/...
doesn't need to be known when the signature is created)
Of course, the second implies the first, but do all use cases require
both being able to change the prevout and (arbitrarily) changing the
scriptPubKey? While BIP118 correctly points out this is secure if the
same keys are only used in scripts with which binding is to be
permitted, I feel it would be preferable if signatures/scripts would
explicitly state what can change. One way to accomplish this is by
indicating exactly what in a script is subject to change.
Here is a combined proposal:
* Three new sighash flags are added: SIGHASH_NOINPUT, SIGHASH_NOFEE,
and SIGHASH_SCRIPTMASK.
* A new opcode OP_MASK is added, which acts as a NOP during execution.
* The sighash is computed like in BIP143, but:
  * If SIGHASH_SCRIPTMASK is present, for every OP_MASK in scriptCode
the subsequent opcode/push is removed.
  * The scriptPubKey being spent is added to the sighash, unless
SIGHASH_SCRIPTMASK is set.
  * The transaction fee is added to the sighash, unless SIGHASH_NOFEE is set.
  * hashPrevouts, hashSequence, and outpoint are set to null when
SIGHASH_NOINPUT is set (like BIP118, but not for scriptCode).
So my question is whether anyone can see ways in which this introduces
redundant flexibility, or misses obvious use cases?

@_date: 2018-11-27 19:41:02
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Thanks for all the input so far. Going over the suggestions and other ideas:
* OP_MASK should be required to be followed by a push, as suggested by
Anthony Towns. The alternative would permit substituting arbitrary
opcodes for masked pushes, which is at least very hard to reason
about. This would effectively turn it into a multi-byte OP_MASKEDPUSH
* It's probably better to sign the amounts of all inputs, as suggested
by Johnson Lau. As that would cause default sighashes to sign all
input and output amounts, is there still a need to sign the tx fee
explicitly? Or in other words, are there situations where changing the
set of inputs or outputs after signing is desired, but the net
difference between them cannot change? If not, that would remove the
need for NOFEE.
* Do we need to keep the rule that sequence values of other inputs are
only signed with default sighash? It feels cleaner to always sign the
sequence values of all inputs that are included in the sighash anyway
(so all of them, unless ANYONECANPAY or NOINPUT, which would make it
sign only the current input's sequence value). If NOINPUT also blanks
the sequence values (as currently specified by BIP118), and all input
amounts are signed, that would make amounts/sequence values always be
treated identically.
* If MASK implies NOINPUT, and NOINPUT implies ANYONECANPAY, the 3 of
them can be encoded in just 2 bits using the
PARTIALSCRIPT/KNOWNSCRIPT/KNOWNTX/ALL_INPUTS encoding Anthony Towns
* Regarding the discussion about preventing signatures from being
rebound to a different script(path)/checksig:
  * With MAST there is indeed less need for this, but at least
single-tree MAST constructions cannot replace all script branches (a
script with 40 IF/THEN/ELSE constructions may have 2^40 different
execution paths, for which computing a Merkle tree is intractable).
  * Just signing the opcode position of the CHECKSIG operator isn't
enough for all cases either. For example, you could have a complex
nested set of branches that puts a number of pubkeys on the stack, and
then a CHECKMULTISIG after the last ENDIF to verify all of them. In
such a situation, if the same key can occur in multiple combinations,
you still may want to prevent a signature generated for one
combination from being rebindable to the same key in another
combination. I believe that signing the opcode position plus the
true/false condition of all previous(?) IF statements is probably
sufficient to achieve that, but it would also introduce unnecessary
complexity for signers in most cases (see next point).
  * Thinking about signing code, adding these sort of execution trace
commitments to the sighash means they need to know which checksig
operator etc. they are signing for. I believe that in practice for
example HW devices will just whatever position the wallet indicated,
rather than verifying it corresponds with a particular intended code
path. Preventing rebinding isn't very useful if an attacker can make
you bind to the wrong thing regardless, so I'm not convinced this is
even worth having by default.
  * An alternative (not sure who suggested it) is to simply make every
CHECKSIG sign the opcode position of the last executed CODESEPARATOR
(and remove the earlier cut-of-scriptCode effect of CODESEPARATOR).
This gives a simple (but somewhat limited) way for scripts that need
to prevent certain kinds of cross-execution-trace rebinding.
A few misc ideas:
* (Taken from For a default sign-everything sighash, the sighash byte can be
* For the commitments to the scriptPubKey and scriptCode, an
intermediary hash should be used (so the data included in the sighash
includes a hash of those, rather than the script directly). This
prevents a blow up in hashing time for large scripts with many
different sighash types in its signatures.
* When masking the scriptCode, the push opcode immediately following
OP_MASKEDPUSH can be replaced by OP_VERIF (which will never collide
with any real script, as OP_VERIF makes a script invalid even when
occurring in an unexecuted branch).
* Sighashes (and really all new hashes that are introduced) should be
prefixed with a fixed 64-byte array as "tag", chosen to not collide
with any existing use of SHA256 in Bitcoin, to prevent signatures from
being re-interpretable as something else. Picking 64 bytes as tag size
means it can be efficiently implemented as just a modified SHA256 IV.
So a combined proposal:
* All existing sighash flags, plus NOINPUT and MASK
(ANYONECANPAY/NOINPUT/MASK are encoded in 2 bits).
* A new opcode called OP_MASKEDPUSH, whose only runtime behaviour is
failing if not immediately followed by a push, or when appearing as
last opcode in the script.
* Signatures are 64 plus an optional sighash byte. A missing sighash
byte implies ALL, and ALL cannot be specified explicitly.
* The sighash is computed from the following:
  * A 64-byte constant tag
  * Data about the spending transaction:
    * The transaction version number
    * The hash of txins' prevouts+amounts+sequences (or nothing if ANYONECANPAY)
    * The hash of all txouts (or just the corresponding txout if
SINGLE; nothing if NONE)
    * The transaction locktime
  * Data about the output being spent:
    * The prevout (or nothing if NOINPUT)
    * The amount
    * The sequence number
    * The hash of the scriptPubKey (or nothing if MASK)
  * Data about the script being executed:
    * The hash of the scriptCode (after masking out, if MASK is set)
    * The opcode number of the last executed OP_CODESEPARATOR (or
0xFFFFFFFF if none)
  * The sighash mode

@_date: 2018-10-23 19:22:24
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Generalised taproot 
On Thu, Jul 12, 2018 at 6:52 PM Anthony Towns via bitcoin-dev
I think this is a very neat construction, and has advantages beyond
solving the recursive-taproot-without-revealing-intermediary-scripts problem
(which is useful, but I would consider a stretch goal at best).
To summarize, this is my understanding of g'root:
* A spending condition is a policy of the form "sign with public key A
  and additionally satsify script S". Such a condition is associated with
  the point P = A + s*G2 (where G2 is a second independent generator for the
  curve, and s=H(S)). To satisfy such a condition, you reveal S, provide
  inputs that satisfy S, together with a signature for public key (P - s*G2).
  We'll call A the companion key of spending condition P (as opposed to other
  public keys which may appear in the script S).
* A scriptPubKey (or redeemScript in case of P2SH) can either be a spending
  condition P directly, or a P2C derivation (using P + H(P,Q)G) of a spending
  condition and an alternative. That alternative can either be another P2C
  derivation ("recursive Taproot"), or a Merkle tree of disjunct spending
  conditions.
This is elegant in that it removes the distinction between pay-to-pubkey and
pay-to-script constructions; every point becomes the representation of both.
As long as every script(branch) requires at least one pubkey check, it
comes at no cost (neither witness size or computational).
However, I think it also offers an easy way to construct a softfork-safe
cross-input aggregation system (discussed here before:
Essentially what's done here is extracting one key out of every spending
condition, given it a special place (the companion key) in the execution
structure - rather than being part of freeform script opcodes - and made it
cheaper to satisfy (as no pubkey needs to be revealed for it). This makes sense,
as we can assume that every (secure) script contains at least one CHECKSIG or
semantically equivalent operation, and with Schnorr multisignatures, can often
expect that to be just one key representing the set of all those who have to
However, it also means we could simply restrict a future cross-input signature
aggregation system to only apply to the set of these companion keys (one per
input). They are not subject to potential changes to the scripting language, as
they're outside of that. Under the assumption that most spending policies can be
encoded s a tractably-sized set of disjunct conditions, each with just a single
fixed set of public keys, the companion keys actually embody all public keys
involved in a transaction.
Sebastian Geisler, Glenn Willen, and I had an hour long discussion to come up
with a name for the privileged key in g'root, but unfortunately had to resort
to the Valve universe instead to find "companion key"...

@_date: 2018-09-04 09:57:28
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Extending BIP174 for HTLCs 
I've been thinking about this as well.
A useful way to look at it IMHO is to see a hash as the analogue of a
public key, and the preimage as the analogue of a signature.
That would suggest adding two fields to PSBT:
* A request for the preimage of a given hash (similar to the pubkey/path
field currently)
* A revealed preimage for a given hash (similar to the partial signature
field currently).
The workflow would in this case would be:
* An updater recognizes an output/script as being one that requires a
preimage, and adds a preimage request field to the input (along with pubkey
fields for all involved keys).
* A "signer" who knows the preimage sees the request field, verifies he's
willing to divulge the secret, and adds a preimage field (along with any
signatures he may be able to create).
* A finalizer who is compatible with the type of hashlock script combines
all signatures and preimages into a final scriptSig/witness stack.
An obvious difficulty is having updaters and finalizers which are
compatible with all possible variations of hashlocks and other scripts.
Not sure on the best format for this, but what I have been thinking
That's one approach to reducing the complexity of the finalizer: adding
information about the composition of the scriptSig to the PSBT itself.
However, I don't think that approach scales very well (you'd need new
fields for all kinds of new script constructions). In particular, dealing
with multiple possible satisfactions may complicate things, especially when
the number of combinations is intractable.
I've been working on another approach that doesn't involve changes to PSBT,
but instead uses an easily-parsable subset of script (which includes
and/or/threshold/pubkey/locktimes/hashlocks). I hope to publish something
soon about it.

@_date: 2018-09-18 17:06:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Bitcoin Core update notice 
Hash: SHA512
Hello all,
Bitcoin Core 0.16.3 was just released with a fix for
We urge all network participants to upgrade to 0.16.3[*] as soon
as possible.
[*] For those who build from source, the 0.14, 0.15, 0.16, 0.17,
and master branches on GitHub (
are fixed as well.

@_date: 2019-08-09 11:16:29
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] 32-byte public keys in Schnorr and Taproot 
Hello all,
It has been suggested [1] to drop the Y oddness bit in the witness
program for Taproot outputs. This seems like a worthwhile change, as:
* The bit doesn't actually contribute to security.
* It avoids Taproot outputs from being more expensive to create than v0 P2WSH.
* It doesn't preclude future changes that would still need the
additional byte anyway.
In exploring that option, Jonas Nick found that it seems cleanest [2]
to actually introduce a type of 32-byte public keys (which implicitly
have an even Y coordinate) in bip-schnorr, with associated signing and
verification logic that are distinct from the 33-byte variant.
This makes me wonder if we need 33-byte public keys at all.
So I'd like to hear opinions about modifying bip-schnorr to only
define 32-byte public keys. The implications of that would be:
* bip-schnorr public keys wouldn't be exactly the same as ECDSA public
keys, however all derivation logic would still apply (BIP32,
mnemonics, xpubs, ... would still exist and be compatible - just the
first pubkey byte would be dropped at the end).
* The Q point in bip-taproot (the one going in the scriptPubKey) would
just follow the 32-byte pubkey encoding, rather than needing a 33rd
* The P point in bip-taproot (the internal key revealed during script
path) would become just a 32-byte public key (and we can drop the one
bit in the control block to transmit the oddness of the Y coordinate
of P).
* In order to permit batch verification of the P to Q tweaking for
script-path spending, another control block bit is now needed, namely
one that indicates whether a negation was needed to make P+H(P||m)*G's
Y coordinate even.
* All public keys inside bip-tapscript would also become 32-bytes. If
desired, the "upgradable public key" construction in bip-tapscript can
be kept, by triggering based on the length of public keys (rather than
based on their first byte).
One question is whether it's worth such a change to bip-schnorr at
this point. We've purposefully never progressed it past draft since
publishing [3], but it has been a while. If necessary, it's possible
to keep verification compatible by still hashing the implied "even"
byte inside the scheme (which commits to the pubkey), but if we're
going to change things, it's perhaps best to do it as cleanly as
possible and also drop that byte.
  [1]   [2]   [3]

@_date: 2019-08-09 11:29:55
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot proposal 
Since the idea of implicitly even pubkeys has potentially more general
implications, I've started a separate thread [1] about that idea.
That's unfortunately not correct. If we want to maintain
batch-verifiability of the taproot tweaking (the Q = P + H(P,m)G
relation), we still need a bit in the control block to convey whether
a negation was necessary to make P+H(P,m)G even, even if P and Q both
have implied-even Y coordinates. Not doing that would require
exploring 2^n combinations to batch verify n relations, obviously
destroying any performance savings the batch verification had in the
first place.
If we keep the leaf version idea (it's possible to instead just rely
entirely on OP_SUCCESSx, and drop leaf versions), my preference is to
still keep it separate from script, though just for a fairly banal
reason: that way the script consists entirely of opcodes and can be
treated uniformly by debug tools, rather than needing to treat the
first byte special. I do understand your preference too, but I don't
know how it weighs up.
  [1]

@_date: 2019-08-19 16:17:21
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Miniscript 
Hi all,
Miniscript is a project we've been working on for the past year or so,
and is now at a stage where I'd like to get it some more attention. It is joint
work with Andrew Poelstra and Sanket Sanjalkar.
It's a language for writing (a subset of) Bitcoin Scripts in a structured way,
enabling analysis, composition, generic signing and more.
For example the script
   OP_CHECKSIG OP_IFDUP OP_NOTIF OP_DUP OP_HASH160   OP_EQUALVERIFY OP_CHECKSIGVERIFY <144> OP_CSV OP_ENDIF
in Miniscript notation would be
  or_d(c:pk(A),and_v(vc:pk_h(B),older(144)))
making it human (engineer?) readable that this is a script that permits A to
take the coins at any time, and B after 1 day. A full description of the
language can be found on the project website Using Miniscript it's possible to:
* Write descriptors for addresses for scripts that implement things more
  complicated than multisig.
* Make software that can deal with composition of policies (e.g. have funds
  in a 2-of-3 setup where one of the 3 "keys" is itself a policy that involves
  perhaps multiple devices and timeouts).
* Compile complex spending policies to efficient scripts.
* Figure out under what necessary and/or sufficient conditions a script can be
  satisfied.
* Given signatures for a sufficient set of keys (and hash preimages, if needed),
  generically construct a witness for arbitrary scripts, without metadata
  apart from the script itself and public keys appearing in it. This means
  generic PSBT signers are possible for this class of scripts.
* Compute the bounds on the size of a witness for arbitrary scripts.
* Perform static analysis to see if any of Script's resource limitations
  (ops limit, stack size, ...) might interfere with the ability to spend.
* Who knows what else...
We have two implementations:
* a C++ one (
* a Rust library (
The implementations are a work in progress, but through large scale randomized
tests we have confidence that the language design and associated witnesses are
compatible with the existing consensus and standardness rules.
To be clear: Miniscript is designed for Bitcoin as it exists today (primarily
P2WSH), and does not need any consensus changes. That said, we plan to extend
the design to support future script changes Bitcoin may include.

@_date: 2019-08-21 11:33:03
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] testing bitcoin nodes 
============================== START ==============================
On Tue, 6 Aug 2019 at 09:57, Niels Thijssen via bitcoin-dev
Hi Niels,
You're probably not getting many answers because this isn't the right
place to ask. The mailinglist is about development of the Bitcoin
protocol and conventions about its usage across multiple applications.
If you want to learn about the Bitcoin Core software and its testing
infrastructure, see

@_date: 2019-12-09 14:31:13
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Analysis of Bech32 swap/insert/delete detection and 
Hi all,
I've made a writeup on Bech32's detection abilities, analysing how it
behaves in the presence of not just substitution errors, but also
swapping of characters, and insertions and deletions:
It shows that the "insert or delete a 'q' right before a final 'p'" is
in fact the only deviation from the expected at-most-1-in-a-billion
failure to detect chance, at least when restricted to the classes of
errors analyzed with various uniformity assumptions. There is some
future work left, such as analyzing combinations of insertions and
substitutions, but I would be surprising if additional weaknesses
exist there.
It also shows that changing one constant in Bech32 would resolve this
issue, while not affecting the error detection properties for other
classes of errors.
So my suggestion for the next steps are:
* Update BIP173 to include the insertion weakness as an erratum, and
the results of this analysis.
* Amend segwit addresses (either by amending BIP173, or by writing a
short updated BIP to modify it) to be restricted to only length 20 or
32 (as fixed-length strings are unaffected by the insertion issue, and
I don't think inserting 20 characters is an interesting error class).
* Define a variant of Bech32 with the modified constant, so that
non-BIP173 uses of Bech32 can choose a non-impacted version if they
worry about this class of errors.
* Later, if and when we expect a need for non-32-byte witness programs
in the medium term, define an updated segwit address scheme that uses
the modified Bech32 variant.
I believe that the impact on production systems will be minimal using
the above, as many wallets already do not accept unknown witness
versions in outputs, and it gives us probably years to adapt.
What do people think?

@_date: 2019-12-09 22:36:20
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Analysis of Bech32 swap/insert/delete detection 
Correct - just documenting the issue.
Right, for v0 there is an inherent restriction to size 20 or 32
already, so this would only semantically change anything for version 1
through 16 (not 15).
It seems BOLT11 already doesn't care very much about the error
detection properties, as it's using Bech32 outside its design
parameters (max length 90 characters).
I think it depends on the application and their tolerance to this sort
of errors. In some cases, these insertions may be detected already
with high probability (e.g. because of length restrictions, or the
fact that it adds random unstructured symbols at the end of the data

@_date: 2019-02-07 12:36:25
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Interrogating a BIP157 server, 
On Thu, 7 Feb 2019 at 12:19, Tamas Blummer via bitcoin-dev
Hi Tamas,
I think you're confusing the lack of sign of imminent commitment for a
sign it isn't the end goal. Changes in consensus rules take a while,
and I think adoption of BIP157 in a limited setting where offered by
trusted nodes is necessary before we will see a big push for that.
In my personal view (and I respect other opinions in this regard),
BIP157 as a public network-facing service offered by untrusted full
nodes is fair uninteresting. If the goal wasn't to have it eventually
as a commitment, I don't think I would be interested in helping
improving it. There are certainly heuristics that reduce the risk of
using it without, but they come at the cost of software complexity,
extra bandwidth, and a number of assumptions on the types of scripts
involved in the transactions. I appreciate work in exploring more
possibilities, but for a BIP157-that-eventually-becomes-a-commitment,
I think they're a distraction. Unless you feel that changes actually
benefit that end goal, I think the current BIP157 filter definition
should be kept.
There is no problem however in optionally supporting other filters,
which make different trade-offs, which are intended to be offered by
(semi) trusted nodes. Still, for the reasons above I would very much
like to keep those discussions separate from the

@_date: 2019-02-08 16:39:54
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
On Wed, 19 Dec 2018 at 18:06, Rusty Russell via bitcoin-dev
Having had some more time to consider this and seeing discussions
about alternatives, I agree. It doesn't seem that OP_MASK protects
against any likely failure modes. I do think that there are realistic
risks around NOINPUT, but output tagging (as suggested in another ML
thread) seems to match those much better than masking does.

@_date: 2019-07-19 12:17:38
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] PubRef - Script OP Code For Public Data References 
Furthermore, right now block validation does not require access to the
whole historical chain (only to the set of unspent outputs), so a change
like this would massively increase storage requirements for validation.

@_date: 2019-05-02 16:35:06
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] IsStandard 
On Thu, 2 May 2019 at 16:28, Aymeric Vitte via bitcoin-dev
Generally, all spends of P2SH/P2WSH is standard, with the following exceptions:
* Non-push operations in the scriptSig
* Resource limitations (too large scripts or items on the stack)
* Protections against known attack vectors (low s rule, cleanstack
rule, minimally encoded numbers rule, codesep usage, ...)
* Usage of unconditionally spendable constructions intended for future
extensions, such as spending future segwit versions.

@_date: 2019-05-06 10:57:57
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot proposal 
Hello everyone,
Here are two BIP drafts that specify a proposal for a Taproot
softfork. A number of ideas are included:
* Taproot to make all outputs and cooperative spends indistinguishable
from eachother.
* Merkle branches to hide the unexecuted branches in scripts.
* Schnorr signatures enable wallet software to use key
aggregation/thresholds within one input.
* Improvements to the signature hashing algorithm (including signing
all input amounts).
* Replacing OP_CHECKMULTISIG(VERIFY) with OP_CHECKSIGADD, to support
batch validation.
* Tagged hashing for domain separation (avoiding issues like
CVE-2012-2459 in Merkle trees).
* Extensibility through leaf versions, OP_SUCCESS opcodes, and
upgradable pubkey types.
The BIP drafts can be found here:
* specifies the transaction input spending rules.
* specifies the changes to Script inside such spends.
* is the Schnorr signature proposal that was discussed earlier on this
list (See An initial reference implementation of the consensus changes, plus
preliminary construction/signing tests in the Python framework can be
found on  All
together, excluding the Schnorr signature module in libsecp256k1, the
consensus changes are around 520 LoC.
While many other ideas exist, not everything is incorporated. This
includes several ideas that can be implemented separately without loss
of effectiveness. One such idea is a way to integrate SIGHASH_NOINPUT,
which we're working on as an independent proposal.
The document explains basic wallet operations, such as constructing
outputs and signing. However, a wide variety of more complex
constructions exist. Standardizing these is useful, but out of scope
for now. It is likely also desirable to define extensions to PSBT
(BIP174) for interacting with Taproot. That too is not included here.

@_date: 2019-05-08 16:06:51
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot proposal 
Thanks for the comments so far!
I'm going to respond to some of the comments here. Things which I plan
to address with changes in the BIP I'll leave for later.
It's true you can't cache/precompute things across tags, but I also
think there is no need. The type of data hashed in a sighash vs a
merkle branch/leaf vs a tweak is fundamentally different. I think this
is perhaps a good guidance to include about when separate tags are
warranted vs. simply making sure the input does not collide: there
shouldn't be much or any shared data with things that are expected to
be inputs under other tags.
Yes, absolutely - you can use a point with unknown discrete logarithm
as internal key. This will result in only script path spends being
available. For the specific goal you're stating an alternative may be
using a valid known private key, using it to pre-sign a timelocked
transaction, and destroying the key.
If you don't reuse public keys, effectively every branch is
automatically salted (and the position in the tree gets randomized
automatically when doing so, providing a small additional privacy
If you're talking about the ability to sign over the ability to spend
to another script ("delegation"), there are lots of interesting
applications and ways to implement it. But it overlaps with Graftroot,
and doing it efficiently in general has some interesting and
non-obvious engineering challenges (for example, signing over to a
script that enforces "f(tx)=y" for some f can be done by only storing
f and then including y in the sighash).
For the specific example of BIP115's functionality, that seems like a
reasonable thing that could be dealt with using the annex construction
in the proposed BIP. A consensus rule could define a region inside the
annex that functions as a height-blockhash assertion. The annex is
included in all sighashes, so it can't be removed from the input;
later opcodes could include the ability to inspect that assertion
Indeed, though as I suggested above, you can also use timelocked
transactions (but using only CLTV branches is more flexible
Yes, that's a TODO that's left in the draft, but this is absolutely
possible (using a hash-to-curve operation). As ZmnSCPxj already
suggested, there can even be a fixed known constant you can use for
this. However, you get better privacy by taking this fixed known
constant (call it C) and using as internal key a blinded version of it
(C+rG, for some random value r, and G the normal secp256k1 generator);
as long as the DL between G and C is unknown, this is safe (and does
not reveal to the world that in fact no key-path was permitted when
Spending using the internal key always uses a single Schnorr signature
and nothing else. When you spend using a script path, you must reveal
both the script and its leaf version. If that leaf version is 0xc0,
the script is interpreted as a tapscript (in which only Schnorr
opcodes exist). If that leaf version is not 0xc0, the script is
undefined, and is unconditionally valid. This is one of the included
extension mechanisms, allowing replacing the whole script language
with something else, but without revealing it unless a branch using it
is actually used (different Merkle tree leaves can have a distinct
leaf versions).
So the reason that tapscript is not mandatory is because other leaf
versions are undefined, and left for future extensions (similar to how
future segwit versions at the output level are undefined).

@_date: 2019-05-22 19:06:42
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot proposal 
IIRC I looked into this a few months ago, and found that the spending
cost (script size + expected witness size) of the optimal script for
every Miniscript policy at most changes by 1 WU (in either direction)
by requiring an empty stack rather than a true value, though in a
(admittedly very arbitrarily biased) distribution, more policies were
improved by it than degraded. This is not taking Taproot into account
(as many of those policies in a Taproot-supporting world should
optimally make use of the internal key and Merkle tree, rather than
turning everything into a monolithic script). I expect that this may
make the impact somewhat larger, but still never more than a 1 WU
I don't think the spending cost changes justify this change, so the
remaining criteria are complexity ones. In my view, the main benefit
would be to authors of hand-written scripts where the consistency
benefits matter, but this needs to be weighed against the mental cost
of learning the new semantics. For Miniscript itself, this doesn't
make much difference - the top level calling convention would become
'V' instead of 'T', but 'T' would still exist as a calling convention
that may be used internally; it's a few lines change.
So overall this feels like something with marginal costs, but also at
most marginal benefits. Perhaps other people have stronger opinions.
This feels like the right thing to do; as we're making MINIMALIF part
of consensus for Tapscript it would make sense to apply the same rule
to the "return" value of the script. There is a downside though,
namely that in some places where you'd use "
OP_CHECKSEQUENCEVERIFY" or " OP_CHECKLOCKTIMEVERIFY" you now need
to add an additional OP_0NOTEQUAL to convert the left-over element n
into an exact 0x01. I also can't come up with any practical benefits
that this would have; if the top stack element in a particular code
path comes directly from the input, it's insecure regardless; if there
isn't, it'll generally be a a boolean (or an intentional non-boolean
true value) computed by the script.
That's a good point; without security benefit there's no reason to
make pay-to-taproots more expensive. Making them the same cost as
P2WSH is nice in any case.
I prefer (4). There is a slight complexity in needing a conditional
sign swap when signing (to make sure the corresponding key is even),
but I think it's minimal compared to the other changes needed here
already. I'll try to amend the reference code soon to see what impact
this idea has.
I'm not sure there is much to gain here. There is perhaps a minimal
fungibility improvement by not having another bit (P2SH or not) that
can leak some information about the software you're using. On the
other hand, until native taproot outputs are common, choosing P2SH
wrapped ones leak less information at output creation time. Apart from
that, I think it would only minimally impact implementation
complexity. Are there other advantages I'm missing?

@_date: 2019-05-23 12:45:37
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] OP_DIFFICULTY to enable difficulty hedges (bets) 
On Thu, 23 May 2019 at 11:33, Tamas Blummer via bitcoin-dev
If the difficulty can be directly observed by the script language, you
would need to re-evaluate all scripts in unconfirmed transactions
whenever the difficulty changes. This complicates implementation of
mempools, but it also makes reasoning about validity of (chains of)
unconfirmed transactions harder, as an unconfirmed predecessor may
have conditions that change over time.
For things like block time/height, this is solved by not having the
script itself observe the context state directly, but instead having
an assertion on the state outside of script (nLockTime for absolute
time/height and nSequence for relative), and then having opcodes
inside script that observe the assertion (OP_CLTV and OP_CSV). By
doing so, script validity is a single context-free yes or not that can
be evaluated once, and the variable part is just transaction-level
reasoning that doesn't involve a full script interpreter.
Additionally, the supported assertions are restricted in such a way
that if they are true within a particular block, they're also true in
any descendant, removing the complexity of reasoning about validity
(apart from the inevitable reasoning about possible double-spend
before confirmation).
I feel a similar construction is needed for observing block
difficulty. This can be done by either having an opcode that as a side
effect of execution "posts" an assertion (e.g. "difficulty at block
height X is at least Y"), instead of putting the difficulty on the
stack. An alternative is having the assertion be part of the
transaction structure (for example in the annex we propose in
bip-taproot), and having an opcode that observes the difficulty
assertion inside script.
I don't have a strong opinion either way on the usefulness of having
difficulty-dependent transaction/scripts.

@_date: 2019-05-26 10:54:08
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Two questions about segwit implementation 
This is a question that belongs on  not
this list.

@_date: 2019-11-07 14:35:42
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot addresses 
Hello all,
A while ago it was discovered that bech32 has a mutation weakness (see
 for details). Specifically,
when a bech32 string ends with a "p", inserting or erasing "q"s right
before that "p" does not invalidate it. While insertion/erasure
robustness was not an explicit goal (BCH codes in general only have
guarantees about substitution errors), this is very much not by
design, and this specific issue could have been made much less
impactful with a slightly different approach. I'm sorry it wasn't
caught earlier.
This has little effect on the security of P2WPKH/P2WSH addresses, as
those are only valid (per BIP173) for specific lengths (42 and 62
characters respectively). Inserting 20 consecutive "p"s in a typo
seems highly improbable.
I'm making this post because this property may unfortunately influence
design decisions around bip-taproot, as was brought up in the review
session (
past tuesday. In the current draft, witness v1 outputs of length other
than 32 remain unencumbered, which means that for now such an
insertion or erasure would result in an output that can be spent by
anyone. If that is considered unacceptable, it could be prevented by
for example outlawing v1 witness outputs of length 31 and 33.

@_date: 2019-11-10 13:51:48
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
Admittedly, this affecting development of consensus or standardness rules
would feel unnatural. In addition, it also has the potential downside of
breaking batched transactions in some settings (ask an exchange for a
withdrawal to a invalid/nonstandard version, which they batch with other
outputs that then get stuck because the transaction does not go through).
So, Ideally this is indeed solved entirely on the bech32/address encoding
side of things. I did not initially expect the discussion here to go in
that direction, as that could come with all problems that rolling out a new
address scheme in the first place has. However, there may be a way to
mostly avoid those problems for the time being, while also not having any
impact on consensus or standardness rules.
I believe that most new witness programs we'd want to introduce anyway will
be 32 bytes in the future, if the option exists. It's enough for a 256-bit
hash (which has up to 128-bit collision security, and more than 128 bits is
hard to achieve in Bitcoin anyway), or for X coordinates directly. Either
of those, plus a small version number to indicate the commitment structure
should be enough to encode any spendability condition we'd want with any
achievable security level.
With that observation, I propose the following. We amend BIP173 to be
restricted to witness programs of length 20 or 32 (but still support
versions other than 0). This seems like it may be sufficient for several
years, until version numbers run out. I believe that some wallet
implementations already restrict sending to known versions only, which
means effectively no change for them in addition to normal deployment.
In the mean time we develop a variant of bech32 with better
insertion/erasure detecting properties, which will be used for witness
programs of length different from 20 or 32. If we make sure that there are
never two distinct valid checksum algorithms for the same output, I don't
believe there is any need for a new address scheme or a different HRP. The
latter is something I'd strongly try to avoid anyway, as it would mean
additional cognitive load on users because of another visually distinct
address style, plus more logistical overhead (coordination and keeping
track of 2 HRPs per chain).
I believe improving bech32 itself is preferable over changing the way
segwit addresses use bech32, as that can be done without making addresses
even longer. Furthermore, the root of the issue is in bech32, and it is
simplest to fix things there. The easiest solution is to simply change the
constant 1 that is xor'ed into the checksum before encoding it to a 30-bit
number. This has the advantage that a single checksum is never valid for
both algoritgms simultaneously. Another approach is to implicitly including
the length into the checksummed data.
What do people think?

@_date: 2019-11-12 22:30:18
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Bech32 weakness and impact on bip-taproot 
On Tue, Nov 12, 2019, 21:33 ZmnSCPxj via bitcoin-dev <
This has the following properties:
That's very close to what I was suggesting: create an improved bech32
algorithm and use that for future addresses, rather than working around the
problem in the address encoding while keeping the existing bech32 checksum.
Sorry if that wasn't clear from my previous email.
In this case, there is no need to even implicitly include the length in the
checksum algorithm. Replacing the "xor 1" at the end of the algorithm to
"xor (2^30 - 1)" would reduce the occurrence of this weakness from 1/32 to
1/2^30, and have no downsides otherwise. I'd like to do some analysis to
ascertain it actually will catch any other kind of insertion/deletion
errors with high probability as well before actually proposing it, though.
There are other solutions which do include the length in some fashion
directly in the checksum calculation, which may be preferable (I need to
analyse things...). It's also possible to do this in such a way that for
33-symbol and 53-symbol data parts (corresponding to P2WPKH and P2WSH
lengths) the new algorithm is defined as identical to the old one. That
would simplify upstream users of a bech32 library (which would then
effectively need no changes at all, apart from updating the
checksum/decoder code).
That brings me to Matt's point: there is no need to do this right now. We
can simply amend BIP173 to only permit length 20 and length 32 (and only
length 20 for v0, if you like; but they're so far apart that permitting
both shouldn't hurt), for now. Introducing the "new" address format (the
one using an improved checksum algorithm) only needs to be there in time
for when a non-32-byte-witness-program would come in sight.
Of course, I should update BIP173 to indicate the issue, and have a
suggested improvement for other users of bech32, if they feel this issue is
significant enough.

@_date: 2019-10-09 14:34:32
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot updates 
Hi all,
I wanted to give an update on some of the changes we've made to the
bip-schnorr/taproot/tapscript drafts following discussions on this
* The original post:
and follow-ups
* Using 2 or 4 byte indexes:
* 32-byte public keys:
* Resource limits:
* P2SH support or not:
We've made the following semantical changes to the proposal:
* 32-byte public keys everywhere instead of 33-byte ones: dropping one
byte that provably does not contribute to security, while remaining
compatible with existing BIP32 and other key generation algorithms.
* No more P2SH support: more efficient chain usage, no gratuitous
fungibility loss from having 2 versions, no mode limited to 80-bit
security for non-interactive multiuser constructs; however senders
will need bech32 support to send to Taproot outputs.
* 32-bit txin position and codesep position indexes instead of 16-bits ones.
* Tagged hashes also in bip-schnorr: the signature and nonce
generation now also use tagged hashes, rather than direct SHA256
(previously tagged hashes were only used in bip-taproot and
* Dropping the 10000 byte script limit and 201 non-push opcode limit:
as no operations remain whose validation performance depends on the
size of scripts or number of executed opcodes, these limits serve no
purpose, but complicate creation of Scripts.
* Increased the limit on the depth of Merkle trees from 32 to 128: a
limit of 32 would necessitate suboptimal trees in some cases, but more
than 128 levels are only necessary when dealing with leaves that have
a chance of ~1/2^128 of being executed, which our security level
treats as impossible anyway.
See the updated documents:
* * * In addition, a lot of clarifications and rationales were added. The
reference implementation on
 was also updated to
reflect these changes, has a cleaner commit history now, and improved
tests (though those can still use a lot of work).

@_date: 2019-09-18 13:50:48
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] bip-tapscript resource limits 
Hi all,
In the draft for bip-tapscript (see [1], current version [2]), we
propose removing the per-block sigops limit for tapscript scripts, and
replacing it with a "every script gets a budget of sigops based on its
witness size (one per 50 WU)". Since signatures (plus pubkeys) take
more WU than that, this is not a restriction for anything but
pathologically constructed scripts. Simultaneously, it removes the
multi-dimensional optimization problem that theoretically needs to be
solved to maximize revenue in block template construction.
With our recent work on Miniscript (see [3]), we discovered that the
variety of other script resource limits also introduce (weaker)
complex optimization requirements, but for script constructors instead
of miners. An overview:
1) Scripts are limited to 10000 bytes (and 3600 by standardness currently)
2) The total number of non-push opcodes in a script + the number of
keys participating in executed OP_CHECKMULTISIG(VERIFY) opcodes must
not exceed 201.
3) The size of the stack + altstack combined cannot exceed 1000
elements during execution (and the initial stack is limited to 100
elements by standardness currently)
4) The maximum size of elements on the stack is 520 bytes (and 80
bytes in the initial stack by standardness)
In a discussion about this with Andrew Poelstra we wondered whether
all these limits are still necessary in bip-tapscript. I believe the
only relevant ones are those that reduce total memory usage, or
verification CPU usage per witness byte. Total script verification CPU
usage isn't relevant I believe, because the same effect can be
accomplished by having a transaction (or block) with multiple inputs.
So let's go over the above resource limits, and see how they help with
limiting memory usage or CPu usage per byte.
# Script size limit
Memory usage for validation can grow with larger scripts, but only
indirectly by constructing extra stack data. Since those are
independently limited by (3), we don't need to consider those here.
There used to be a way through which larger scripts would cause larger
per byte verification cost, but it no longer applies, I believe. Due
to the scriptCode being replaced with a pre-hashed tapleaf hash, the
per-sigop hashing cost is now easily made independent of the size of
the script in implementations.
My suggestion is to drop the script size limit in tapscript, and
instead have it only be implicitly limited by transaction size limits.
# The 201 non-push opcodes limit
Ignoring how more opcodes can grow the stack and altstack (which are
already restricted by 3), I believe there is only one way that
additional (executed) opcodes can increase per-opcode execution time
in the current Bitcoin Core implementation [4], namely the "vfExec"
stack that keeps track of what sides of IF/NOTIF/ELSE/ENDIF execution
is currently passing through. As pointed out by Sergio Demian Lerner
[5], an O(1) algorithm can do this just as well (a variant of which is
implemented in PR 16902 [6]).
Taking such a patch into account, I don't think there are any problems
with removing the 201 ops limit for bip-tapscript scripts. Especially
given its strange semantics around OP_CHECKMULTISIG(VERIFY) (the keys
participating in those are each counted as 1 towards the 201 limit,
but only when executed, while all non-push opcodes are counted as 1
even when not executed), I think this is a nice simplification.
# The 1000 element limit for stack + altstack
A limit for the number of elements on the stack/altstack directly
affects memory usage. In a naive implementation without deduplication
as is used in Bitcoin Core now, every OP_3DUP can add 120 bytes of
memory usage plus the size of the data in the created elements
themselves (which can be a multiple of that number), leading to
several GB of memory usage for executing a maximal 4 MB script
(multiplied by the number of parallel executions). Even when using
reference-counting techniques to reduce duplication, 100 MB memory
usage is not unreasonable. I don't think those are acceptable numbers.
The stack size can also directly affect per-opcode execution time for
OP_ROLL, again shown by [5]. A block full of the most tightly packed
OP_ROLLS (which I believe is a repetition of OP_3DUP OP_ROLL OP_ROLL
OP_ROLL) operating on a stack of 1000 elements for me takes around 4.3
s of CPU time to verify. That's significant, but it's surprisingly
close to what a block packed with OP_CHECKSIGs (taking the 1 sigop /
50 WU limit into account) takes to verify on the same machine (3.8 s).
Even more remarkably, that time is also very close to how long a block
full of most tightly packed OP_HASH256s on 520 byte inputs take to
verify when disabling SHA256 hardware acceleration (3.6 s).
I believe we should keep this 1000 element stack limit for these
reasons. The 100 limit on input stacks can be increased to 1000 for
uniformity with the during-execution limit.
# The 520 byte stack element size limit
Given that there are no known use cases for stack elements larger than
65 bytes (and no opcodes apart from hashes that can even operate on
them), plus their impact on memory usage the execution time of
pathologically constructed scripts full of hashes (see above), I think
we should keep this limit.
Note that this limit can be changed using the OP_SUCCESSx mechanism, if need be.
# Summary
I propose the following changes to resource limits in bip-tapscript
(compared to segwit v0 scripts):
* Replace the separate sigops counter with a "executed sigops must not
exceed (witness size / 50 WU) + 1" rule (already in the BIP).
* Drop the 10000 byte limit for script size (and 3600 byte standardness limit)
* Drop the 201 non-push ops limit per script.
* Drop the 100 input stack elements standardness limit, and replace
with a (consensus) 1000 limit.
The rules limiting the stack + altstack number of elements during
execution to 1000 remains, as well as the 520 byte limit for elements
on the stack.
# References
  [1]   [2]   [3]   [4]   [5]   [6]

@_date: 2019-09-18 14:21:56
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot proposal 
I'm starting to lean towards not allowing P2SH wrapped Taproot as well.
Given the progress bech32 adoption has made in the past year or so, I
don't think adding P2SH support would result in many more software
authors deciding to implement receive-to-taproot functionality. And
without that advantage, having the option of supporting P2SH wrapping
actually risks degrading the privacy goals it aims for (see ZmnSCPxj's
argument above).
My main intuition for keeping P2SH is that Segwit was really designed
to support both, and I expect that disallowing P2SH would actually
require (very slightly) more complex validation code. I don't think
this is a sufficiently strong reason, especially as keeping P2SH
support does increase the number of combinations software needs to
test (both in consensus code and wallets).

@_date: 2020-02-18 15:29:21
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] Taproot (and graftroot) complexity (reflowed) 
Right, I think we shouldn't just see Taproot as adding a possibility
of a cheap single-key branch to Merkle tree. It is actively choosing
to incentivize protocols and implementations that can use the key
path, making sure that the cheapest way spending of spending is also
the most private one - as we can assume that it indeed will be the
most frequent one. I don't believe having a separate MAST-no-Taproot
spending type (through whatever mechanism) is beneficial to that.
Taproot effectively gives everyone a "key path spend is included in
the price", making it maximally appealing even to those who don't care
about privacy.
I don't think this is an unreasonable angle. There are plenty of other
options that exists if we just want to make verification constructions
cheap but disregard incentives for privacy. For example, why don't we
research account-based state/payments? Being able to avoid change
would make simple payments significantly cheaper (both in terms of
block space and computation). Of course, the reason (or at least one
of them) is that it would result in a discount for those willing to
reduce their privacy (use accounts = reuse address = don't pay for
change), and this hurts everyone (and indirectly the fungibility of
the system as a whole). This is true even when there are use cases
that would legitimately benefit from having this option.
This is of course a much weaker case, but I think it is similar.
Having no-Taproot available would reduce the incentives for those who
don't care about spending policy privacy to put the engineering effort
into building key-path-spendable protocols and implementations - and I
think such protocols, wherever possible, should be the goal. There
probably are some use cases where key path spending is truly not an
option, but I suspect they're rare, or sufficiently high value that 8
vbyte differences don't matter to them. If that turns out to be wrong,
it remains possible to add a new output type/witness version that does
support them. This does mean distinguishable outputs, but only for
things that would be distinguishable at spending time anyway, and
that's a cost we'll have to pay anyway for future structural script
improvements (like cross-input aggregation or graftroot).

@_date: 2020-02-23 20:26:17
@_author: Pieter Wuille 
@_subject: [bitcoin-dev] BIP 340 updates: even pubkeys, 
Hello list,
Despite saying earlier that I expected no further semantical changes
to BIP 340-342, I've just opened
 to make a number of small
changes that I believe are still worth making.
1. Even public keys
Only one change affects the validation rules: the Y coordinate of
32-byte public keys is changed from implicitly square to implicitly
even. This makes signing slightly faster (in the microsecond range),
though also verification negligibly slower (in the nanosecond range).
It also simplifies integration with existing key generation
infrastructure. For example BIP32 produces public keys with known
even/oddness, but squaredness would need to be computed separately.
Similar arguments hold for PSBT and probably many other things.
Note that the Y coordinate of the internal R point in the signature
remains implicitly square: for R the squaredness gives an actual
performance gain at validation time, but this is not true for public
keys. Conversely, for public keys integration with existing
infrastructure matters, but R points are purely internal.
This affects BIP 340 and 341.
2. Nonce generation
All other semantical changes are around more secure nonce generation
in BIP 340, dealing with various failure cases:
* Since the public key signed for is included in the signature
challenge hash, implementers will likely be eager to use precomputed
values for these (otherwise an additional EC multiplication is
necessary at signing time). If that public key data happens to be
gathered from untrusted sources, it can lead to trivial leakage of the
private key - something that Greg Maxwell started a discussion about
on the moderncrypto curves list:
 We
believe it should therefore be best practice to include the public key
also in the nonce generation, which largely mitigates this problem.
* To protect against fault injection attacks it is recommended to
include actual signing-time randomness into the nonce generation
process. This was mentioned already, but the update elaborates much
more about this, and integrates this randomness into the standard
signing process.
* To protect against differential power analysis, a different way of
mixing in this randomness is used (masking the private key completely
with randomness before continuing, rather than hashing them together,
which is known in the literature to be vulnerable to DPA in some
3. New tagged hash tags
To make sure that any code written for the earlier BIP text fails
consistently, the tags used in the tagged hashes in BIP 340 are
changed as well.
What do people think?
