
@_date: 2015-08-21 23:35:18
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Dynamically Controlled Bitcoin Block Size Max C 
"I ran some simulations, and if blocks take 20 seconds to propagate, a
network with a miner that has 30% of the hashing power will get 30.3% of
the blocks."
Peter_R's analysis of fee markets in the absence of blocksize limits [1]
shows that the hashrate advantage of a large miner is a side-effect of
coinbase subsidization. As the block rewards get smaller, so will large
miner advantages. An easy way to think about this is as follows:
Currently, the main critique of larger blocksizes is that we'll connected
miners can cut out smaller miners by gratuitously filling up blocks with
self-paying transactions. This only works because block subsidies exist.
The moment block rewards become comparable to block TX fees, this exploit
ceases to be functional.
Basically, large miners will still be forced to move full blocks, but it
will go against their interest to fill them with spam since their main
source of income is the fees themselves. As a result, large miners (unlike
smaller ones) will lose the incentive to mine an un full block this evening
the playing field.
In this context, large blocksizes as proposed by BIP100-101 hope to
stimulate the increase of TX fees by augmenting the network's capacity. The
sooner block rewards become comparable to block fees, the sooner we will
get rid of mine centralization.

@_date: 2015-08-27 00:22:42
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Unlimited Max Blocksize (reprise) 
I don't get how it's very risky to have the Mike and Gavin redirect the
course of the bitcoin protocol but it's totally fine to consider complex
miner voting protocols as a hard fork option.
I believe that this community has not given due weight to the analysis
proposed by Peter__R on the existence of fee markets with  uncapped max
blocksizes. The critiques made toward his work were in no way definitive
and discussion just stopped. Is it the math that bothers people?
If his work stands the test of scrutiny, then a controlled raising of the
max blocksize in the interim to ease into the fee market dynamic described
is the best option. Possibly a stepwise BIP101 where the community
hardforks every two years until we all trust the fee market dynamics.
The main critique to uncapped max blocksizes which I've heard stems from
our incapacity to quantify the advantages that large miners have over
smaller ones. As I will show in an upcoming paper, these advantages do not
stem from the act of propagating large blocks but rather from the block
subsidies which allow miners to mine unnecessary large blocks irregardless
of the fees contained therein. One typical example is Peter Todd's
suggested attack whereby a miner creates a massive block filled with spam
transactions that pay himself solely to slow down the rest of the network
and gain an advantage. Putting aside the increased orphan risk arising from
the propagation of such a large block, this attack would never be viable if
it weren't for the existence of current block subsidies.
As such, exponential increases to the max blocksize make perfect sense
since the block reward decreases exponentially also. All arguments invoking
rates of technological advances (see Gavin's original posts) don't mean
anything. Rational miners will NOT be incentivized to mine gargantuan spam
filled blocks in the presence of a vanishing block reward.
I truly hope this matter gets the consideration it deserves. Particularly
with the upcoming scaling workshops.
"I ran some simulations, and if blocks take 20 seconds to propagate, a
network with a miner that has 30% of the hashing power will get 30.3% of
the blocks."
Peter_R's analysis of fee markets in the absence of blocksize limits [1]
shows that the hashrate advantage of a large miner is a side-effect of
coinbase subsidization. As the block rewards get smaller, so will large
miner advantages. An easy way to think about this is as follows:
Currently, the main critique of larger blocksizes is that we'll connected
miners can cut out smaller miners by gratuitously filling up blocks with
self-paying transactions. This only works because block subsidies exist.
The moment block rewards become comparable to block TX fees, this exploit
ceases to be functional.
Basically, large miners will still be forced to move full blocks, but it
will go against their interest to fill them with spam since their main
source of income is the fees themselves. As a result, large miners (unlike
smaller ones) will lose the incentive to mine an un full block this evening
the playing field.
In this context, large blocksizes as proposed by BIP100-101 hope to
stimulate the increase of TX fees by augmenting the network's capacity. The
sooner block rewards become comparable to block fees, the sooner we will
get rid of mine centralization.

@_date: 2015-08-29 18:43:23
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped Block 
I'd like to submit this paper to the dev-list which analyzes how miner
advantages scale with network and mempool properties in a scenario of
uncapped block sizes. The work proceeds, in a sense, from where Peter R's
work left off correcting a mistake and addressing the critiques made by the
community to his work.
The main result of the work is a detailed analysis of mining advantages
(defined as the added profit per unit of hash) as a function of miner
hashrate. In it, I show how large block subsidies (or better, low mempool
fees-to-subsidy ratios) incentivize the pooling of large hashrates due to
the steady increasing of marginal profits as hashrates grow.
The paper also shows that part of the large advantage the large miners have
today is due to there being a barrier to entry into a high-efficiency
mining class which has access to expected profits an order of magnitude
larger than everyone else. As block subsidies decrease, this
high-efficiency class is expected to vanish leading to a marginal profit
structure which decreases as a function of hashrate.
This work has vacuumed my entire life for the past two weeks leading me to
lag behind on a lot of work. I apologize for typos which I may not have
seen. I stand by for any comments the community may have and look forward
to reigniting consideration of a block size scaling proposal (BIP101)
which, due to the XT fork drama, I believe has been placed hastily and
undeservedly on the chopping block.

@_date: 2015-08-30 13:49:05
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Unlimited Max Blocksize (reprise) 
I think that the argument for Peter's optimal block construction algorithm
(leading to the definition of the Fee Demand Curve) is that in the limit of
a mempool with a very large number of transactions, you should be able to
assume that for any given transaction [image: i] of size [image: s_i] and
fee density [image: \phi_i], many transactions [image: j] will exist such

@_date: 2015-08-30 22:01:00
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] ERRATA CORRIGE + Short Theorem 
Since my longer post seems to be caught in moderator purgatory I will
rehash its results into this much smaller message. I apologize for the
I present a theorem whose thesis is obvious to many.
*THESIS: All hashrates* *h' > h generate a revenue per unit of hash v' >
v. *
Let us absurdly[1] assume that an optimal hashrate *h* exists where the
average revenue for each hash in service is maximized. This will result
from perpetually mining blocks of size *q,* is *v. *All larger hashrates *h'
conclusion of my paper) due to the higher orphan risk carried from having
to mine blocks of size *q' > q*. Leading from Peter's model and my
analysis, the origin of this balance lies in the fact that larger miners
must somehow be forced to mine larger blocks which in turn carry a larger
orphan risk.
What happens if a large miner *h'* chooses not to mine his optimal block
size *q' *in favor of a seemingly "sub-optimal" block size* q*?
Since he mines a block of identical size as the smaller miner, they will
both carry identical orphan risks[2], and win identical
amounts*R+M(q)* whenever
they successfully mine a block. Since the larger miner can statistically
expect to win *h'/h* more blocks than the smaller miner, they will each
earn an identical revenue per unit of hash *R+M(q)/h*.
This however directly contradicts the assumption that an optimal hashrate
exists beyond which the revenue per unit of hash *v' < v*if  *h' > h. *
*Q.E.D *
This theorem in turn implies the following corollary:
*COROLLARY: **The marginal profit curve is a monotonically increasing of
miner hashrate.*
This simple theorem, suggested implicitly by Gmaxwell disproves any and all
conclusions of my work. Most importantly, centralization pressures will
always be present.
[1] [2] Orphan risks will actually favor the larger hashrate miner leading to
greater revenues per unit of hash.
I thank the dev-list for its valuable time and exchange on the subject
matter. I stand by for any further comments and questions.

@_date: 2015-08-30 23:02:48
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
"However, that is outside the scope of the result that an individual
miner's profit per block is always maximized at a finite block size Q* if
Shannon Entropy about each transaction is communicated during the block
solution announcement.  This result is important because it explains how a
minimum fee density exists and it shows how miners cannot create enormous
spam blocks for "no cost," for example. "
Dear Peter,
This might very well not be the case. Since the expected revenue ** in
our formulas is but a lower bound to the true expected revenue, and the fee
supply curve [image: M_s(Q)\propto 1/\langle V\rangle], if the true
expected revenue doesn't decay faster than the mempool's average
transaction fee (or, more simply, if it doesn't decay to zero) then the
maximum miner surplus will be unbounded and unhealthy fee markets will
Daniele Pinna, Ph.D

@_date: 2015-12-09 13:28:52
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
If SegWit were implemented as a hardfork, could the entire blockchain be
reorganized starting from the Genesis block to free up historical space?

@_date: 2015-10-02 10:02:43
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
The following paper proposing an asymmetric memory-hard PoW had been
recently published:
My intent is not to promote the paper as I have not finished studying it
myself. I am however interested in the dev-list's stance on potentially
altering the bitcoin PoW protocol should an algorithm that guarantees
protection from ASIC/FPGA optimization be found.
I assume that, given the large amount of money invested by some miners into
their industrial farms this would represent a VERY contentious hard fork.
It is, however, also true that a novel optimization-resistant algorithm
could greatly ameliorate decentralization in the bitcoin network due to a
resurgence of desktop/cellphone mining.
Where do the core devs stand on this matter, hypothetical as it may be?

@_date: 2015-10-02 10:30:40
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
The recently published paper I referenced cite's the Cuckoo cycle
algorithm, discusses its limitations and explains how their proposed
algorithm greatly improves on it. Again.... you're probably in a WAYYY
better position to judge this than I am. My question was purely
hypothetical as I wanted to know where the core devs stand on flipping the
mining ecosystem upside down.
Thanks for your link though, I'll read it right now (before finishing the
research article i posted :) ).
Daniele Pinna, Ph.D

@_date: 2015-10-02 10:31:34
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
Interesting! I didn't notice BIP 99's anti-miner hardfork proposal....
thanks for pointing it out to me.
Daniele Pinna, Ph.D

@_date: 2015-09-01 10:52:46
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] ERRATA CORRIGE + Short Theorem 
My paper did show that the advantage decreased with the block reward.
However, in that limit, it also seemed to imply that a network state would
appear where the revenue per unit hash decreased with increasing hashrate
which should be impossible as just discussed.
In a followup email, I showed how the origin of this effect stems from the
orphaning factor used which doesn't preserve the full network revenue per
unit block. This led me to correct my assertions by pointing out that our
miner profit equations seemed to be just lower bounds to the miner's true
expected profit. As such, just because the *lower bound* on the revenue per
unit hash advantage decreases with the block reward, this doesn't
necessarily imply that the *real* revenue per unit hash advantage does also.
I suspect that the orphaning factor used, independently of the specific
form of the block relay time, is incorrect or incomplete as stated.
Daniele Pinna, Ph.D

@_date: 2016-12-10 13:23:49
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Managing block size the same way we do difficulty 
We have models for estimating the probability that a block is orphaned
given average network bandwidth and block size.
The question is, do we have objective measures of these two quantities?
Couldn't we target an orphan_rate < max_rate?
On Dec 10, 2016 1:01 PM, Send bitcoin-dev mailing list submissions to
        bitcoin-dev at lists.linuxfoundation.org
To subscribe or unsubscribe via the World Wide Web, visit
        or, via email, send a message with subject or body 'help' to
        bitcoin-dev-request at lists.linuxfoundation.org
You can reach the person managing the list at
        bitcoin-dev-owner at lists.linuxfoundation.org
When replying, please edit your Subject line so it is more specific
than "Re: Contents of bitcoin-dev digest..."
Today's Topics:
   1. Managing block size the same way we do difficulty (aka
      Block75) (t. khan)
   2. Re: Managing block size the same way we do difficulty (aka
      Block75) (s7r)

@_date: 2016-12-11 04:17:45
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Managing block size the same way we do difficulty 
How is the adverse scenario you describe different from a plain old 51%
attack? Each proposed protocol change  where 51% or more  of the network
can potentially game the rules and break the system should be considered
just as acceptable/unacceptable as another.
There comes a point where some form of basic honesty must be assumed on
behalf of participants benefiting from the system working properly and
Afterall, what magic line of code prohibits all miners from simultaneously
turning all their equipment off...  just because?
Maybe this 'one':
"As long as a majority of CPU power is controlled by nodes that are not
cooperating to attack the network, they'll generate the longest chain and
outpace attackers. The network itself requires minimal structure."
Is there such a thing as an unrecognizable 51% attack?  One where the
remaining 49% get dragged in against their will?

@_date: 2016-03-09 02:27:22
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] bitcoin-dev Digest, Vol 10, Issue 13 
This seems unnecessarily complicated ("don't use cannon to kill mosquito"
kind of thing). If the community were interested in a realtime hashrate
rebalancing proposal one could simply adjust difficulty at each new block
using the current method.
If faster relaxation in case of adversity is required, it suspect that it
would suffice to perform a weighted average of the previous 2016 blocks
instead of the standard averaging that is currently done. It should be
possible to find an optimal weighting based on historical interblock timing
data. I look into it over the next couple of days.

@_date: 2017-04-07 03:34:17
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
Can you please not forget to supply us more details on the claims made
regarding the reverse engineering of the Asic chip?
It is absolutely crucial that we get these independently verified ASAP.
Message: 2

@_date: 2017-08-23 00:17:19
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] UTXO growth scaling solution proposal 
Also.... how is this not a tax on coin holders? By forcing people to move
coins around you would be chipping away at their wealth in the form of
extorted TX fees.

@_date: 2017-01-27 13:12:57
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Three hardfork-related BIPs 
Your BIP implementation should stress the capacity to softfork the rate of
blocksize increase if necessary. You briefly mention that:
*If over time, this growth factor is beyond what the actual technology
offers, the intention should be to soft fork a tighter limit.*
However this can work both ways so that the rate can potentially be
increased also. I think just mentioning this will soothe a lot of future
*Message: 5Date: Fri, 27 Jan 2017 01:06:59 +0000From: Luke Dashjr
<201701270107.01092.luke at dashjr.org>>Content-Type: Text/Plain;
charset="utf-8"I've put together three hardfork-related BIPs. This is
parallel to the ongoingresearch into the MMHF/SHF WIP BIP, which might
still be best long-term.1) The first is a block size limit protocol change.
It also addresses threecriticisms of segwit: 1) segwit increases the block
size limit which isalready considered by many to be too large; 2) segwit
treats pre-segwittransactions ?unfairly? by giving the witness discount
only to segwittransactions; and 3) that spam blocks can be larger than
blocks mininglegitimate transactions. This proposal may (depending on
activation date)initially reduce the block size limit to a more sustainable
size in the short-term, and gradually increase it up over the long-term to
31 MB; it will alsoextend the witness discount to non-segwit transactions.
Should the initialblock size limit reduction prove to be too controversial,
miners can simplywait to activate it until closer to the point where it
becomes acceptableand/or increases the limit. However, since the BIP
includes a hardfork, theeventual block size increase needs community
consensus before it can bedeployed. Proponents of block size increases
should note that this BIP doesnot interfere with another more aggressive
block size increase hardfork in themeantime. I believe I can immediately
recommend this for adoption; however,peer and community review are welcome
to suggest
changes.Text: code changes only)2) The second is a *preparatory* change, that should
allow triviallytransforming certain classes of hardforks into softforks in
the future. Itessentially says that full nodes should relax their rule
enforcement, aftersufficient time that would virtually guarantee they have
ceased to beenforcing the full set of rules anyway. This allows these
relaxed rules to bemodified or removed in a softfork, provided the proposal
to do so is acceptedand implemented with enough advance notice. Attempting
to implement this hasproven more complicated than I originally expected,
and it may make more sensefor full nodes to simply stop functioning (with a
user override) after thecut-off date). In light of this, I do not yet
recommend its adoption, but amposting it for review and comments
only.Text: Third is an anti-replay softfork which can be used to prevent replayattacks
whether induced by a hardfork-related chain split, or even in
ordinaryoperation. It does this by using a new opcode
(OP_CHECKBLOCKATHEIGHT) for theBitcoin scripting system that allows
construction of transactions which arevalid only on specific
Daniele Pinna, Ph.D

@_date: 2017-03-29 21:33:58
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
What about periodically committing the entire UTXO set to a special
checkpoint block which becomes the new de facto Genesis block?
Message: 5
I believe that as we continue to add users to the system by scaling
capacity that we will see more new nodes appear, but I'm at a bit of a loss
as to how to empirically prove it.
I do see your point on increasing load on archival nodes, but the majority
of that load is going to come from new nodes coming online, they're the
only ones going after very old blocks.   I could see that as a potential
attack vector, overwhelm the archival nodes by spinning up new nodes
constantly, therefore making it difficult for a "real" new node to get up
to speed in a reasonable amount of time.
Perhaps the answer there would be a way to pay an archival node a small
amount of bitcoin in order to retrieve blocks older than a certain cutoff?
Include an IP address for the node asking for the data as metadata in the
transaction...  Archival nodes could set and publish their own policy, let
the market decide what those older blocks are worth.  Would also help to
incentivize running archival node, which we do need.  Of course, this isn't
very user friendly.
We can take this to bitcoin-discuss, if we're getting too far off topic.
On Wed, Mar 29, 2017 at 11:25 AM David Vorick Andrew Johnson

@_date: 2017-05-22 14:29:12
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Barry Silbert segwit agreement 
I couldn't agree more. It would require however for the Devs to throw their
weight behind this with a lot of momentum. Spoonnet has been under
development for quite some time now. Counter offering SegWit plus Spoonnet
12-24 months later would be a very progressive stance that I think would
catch the interest of large swaths of the community. I'd be curious to hear
Johnson's opinion on this. How much more testing would his proposal require?

@_date: 2017-09-29 12:43:22
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
Maybe I'm getting this wrong but wouldn't this scheme imply that a miner is
incentivized to limit the amount of transactions in a block to capture the
maximum fee of the ones included?
As an example, mined blocks currently carry ~0.8 btc in fees right now. If
I were to submit a transaction paying 1 btc in maximal money fees, then the
miner would be incentivized to include my transaction alone to avoid that
lower fee paying transactions reduce the amount of fees he can earn from my
transaction alone. This would mean that I could literally clog the network
by paying 1btc every ten minutes.
Am I missing something?

@_date: 2018-02-19 14:41:21
@_author: Daniele Pinna 
@_subject: [bitcoin-dev] Some thoughts on removing timestamps in PoW 
Granted that removing the 21M coin cap is basically a non-starter in de
bitcoin community I'd like to respond to a couple points in your proposal.
The Y% difficulty adjustment should still be calculated through some
averaging of a certain number N of past blocks. Otherwise two lucky high
difficulty blocks in a row could potentially grind the network to a halt.
Just imagine what would happen to the bitcoin network if the difficulty
target was magically increased by 40% all of a sudden. This is certainly
not likely but must be considered imo.
The second, and most interesting (to me at least) is how the reward should
scale with difficulty. By making the reward scale concavely
(logarithmically for example) with difficulty as well as depend on past
average difficulty AND total # of mined coins, it might be possible to
retain something close to the 21M coin cap while also disincentivizing
mining blocks with excessively large difficulties.
Let D and D_0 be the difficulty of the mind block and some reference
initial difficulty respectively. S and S_0 the total floating coin supply
and the reference initial supply. Then the reward function could look
something like this:
R(D, S; D_0,S_0) =R_0(S/S_0)*Log[1+D/D_0]/Log[2]
Where R_0(S/S_0) can be some decaying exponential function of the ratio
S/S_0 such that initially (when S=S_0) R_0=12.5.
But... As I said, this is solely for sake of argument.
---------- Forwarded message ----------
# Blockchain Timestamps Unnecessary In Proof-of-Work?
*Author: Greg Slepak ([ at mastodon.social](
The Bitcoin blockchain has a 10-minute target blocktime that is achieved by
a difficulty adjustment algorithm.
I assert, or rather, pose the hypothesis, that the use of timestamps in
Bitcoin's blockchain may be unnecessary, and that Bitcoin can operate with
the same security guarantees without it (except as noted in [Risks and
Mitigations]( and therefore does not need miners
to maintain global clock synchronization.
The alternative difficulty adjustment algorithm would work according to the
following principles:
- The incentive for miners is and always has been to maximize profit.
- The block reward algorithm is now modified to issue coins into perpetuity
(no maximum). Any given block can issue _up to_ `X` number of coins per
- The number of coins issued per block is now tied directly to the
difficulty of the block, and the concept of "epocs" or "block reward
halving" is removed.
- The chain selection rule remains "chain with most proof of work"
- The difficulty can be modified by miners in an arbitrary direction (up or
down), but is limited in magnitude by some maximum percentage (e.g. no more
than 20% deviation from the previous block), we call this `Y%`.
 Observations
- Miners are free to mine blocks of whatever difficulty they choose, up to
a maximum deviation
- The blockchain may at times produce blocks very quickly, and at other
times produce blocks more slowly
- Powerful miners are incentivized to raise the difficulty to remove
competitors (as is true today)
- Whether miners choose to produce blocks quickly or slowly is entirely up
to them. If they produce blocks quickly, each block has a lower reward, but
there are more of them. If they produce blocks slowly, each block has a
higher reward, but there are fewer of them. So an equilibrium will be
naturally reached to produce blocks at a rate that should minimize orphans.
A timestamp may still be included in blocks, but it no longer needs to be
used for anything, or represent anything significant other than metadata
about when the miner claims to have produced the block.
 Risks and Mitigations
Such a system may introduce risks that require further modification of the
protocol to mitigate.
The most straightforward risk comes from the potential increase in total
transaction throughput that such a change would introduce (these are the
same concerns that exist with respect to raising the blocksize). The
removal of timestamps would allow a cartel of miners to produce
high-difficulty blocks at a fast rate, potentially resulting in additional
centralization pressures not only on miners but also on full nodes who then
would have greater difficulty keeping up with the additional bandwidth and
storage demands.
Two equally straightforward mitigations exist to address this if we are
given the liberty of modifying the protocol as we wish:
1. Introducing state checkpoints into the chain itself could make it
possible for full nodes to skip verification of large sections of
historical data when booting up.
2. A sharded protocol, where each shard uses a "sufficiently different" PoW
algorithm, would create an exit for users should the primary blockchain
become captured by a cartel providing poor quality-of-service.
Please do not email me anything that you are not comfortable also sharing with
the NSA.
