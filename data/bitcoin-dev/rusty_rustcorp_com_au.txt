
@_date: 2014-06-03 22:15:23
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Lets discuss what to do if SHA256d is 
I normally just lurk, but I looked at this issue last year, so thought
I'd chime in.  I never finished my paper though...
In the event of an *anticipated* weakening of SHA256, a gradual
transition is possible which avoids massive financial disruption.
My scheme used a similar solve-SHA256-then-solve-SHA3 (requiring an
extra nonce for the SHA3), with the difficulty of SHA256 ramping down
and SHA3 ramping up over the transition (eg for a 1 year transition,
start with 25/26 SHA2 and 1/26 SHA3).
The hard part is to estimate what the SHA3 difficulty should be over
time.  My solution was to adjust only the SHA3 target on every *second*
difficulty change (otherwise assume that SHA2 and SHA3 have equally
changed rate and adjust targets on both).
This works reasonably well even if the initial SHA3 difficulty is way
off, and also if SHA2 breaks completely halfway through the transition.
I can provide more details if anyone is interested.

@_date: 2014-06-05 15:39:15
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Lets discuss what to do if SHA256d is 
OK, ignore the FIXMEs, but I rehashed my stupid sim code, added some
graphs to the (clearly unfinished) paper and uploaded it to github:
PDF is in there too, for easier reading.

@_date: 2014-10-31 09:48:34
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Increasing regularity of block times? 
Hi all,
        I've been toying with an algorithm to place a ceiling on
confirmation latency by allowing weaker blocks after a certain time.
Hope this isn't noise, but thought someone must have considered this
before, or know of flaws in the scheme?
Gory details:

@_date: 2014-10-31 14:01:12
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Increasing regularity of block times? 
Your point is well made.
If everyone published their easy blocks at the 20 minute mark,
divergence would be a problem (though with 6/7 blocks being normal, the
network would probably recover).  I was proposing to relay them as
normal, they're just not accepted until 20 minutes.
(Though with the suggested variant of accepting the most-compatible
rather than first-seen block, this isn't so critical).
I hadn't thought about it that way, but I assumed GHOST mitigate this
down to some lower limit.  Or?
Agreed, that would be foolish.  Note that in my proposal, block
timestamps wouldn't reflect the delay (removing incentive to push
timestamps forward, but making judging historical blockchain's validity
Nice idea.  Publishing WIP blocks like this could provide evidence, but
you'd have to incentivize miners to publish them.  Can you think of a
way to do that (which beats simply reducing the block time)?
Thanks for your time,

@_date: 2015-08-27 12:38:42
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
OK, having implemented lightning test code against the initial proposal,
I can give the following anecdata:
- I screwed up inversion in my initial implementation.  Please kill it.
- 256 second granularity would be be fine in deployment, but a bit
  painful for testing (I currently use 60 seconds, and "sleep 61").  64
  would work better for me, and works roughly as minutes.
- 1 year should be sufficient as a max; my current handwave is <= 1 day
  per lightning hop, max 12 hops, though we have no deployment data.
- We should immediately deploy an IsStandard() rule which insists that
  nSequence is 0xFFFFFFFF or 0, so nobody screws themselves when we
  soft fork and they had random junk in there.
Aside: I'd also like to have nLockTime apply even if nSequence !=
0xFFFFFFFF (another mistake I made).  So I'd like an IsStandard() rule
to say it nLockTime be 0 if an nSequence != 0xFFFFFFFF.  Would that
screw anyone currently?

@_date: 2015-08-31 07:03:57
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Ah thanks!  I missed the version bump in BIP68.
Yes, but Mark pointed out that it has uses, so I withdraw the

@_date: 2015-12-04 09:37:56
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks 
Gavin Andresen via bitcoin-dev It includes mempool contents and tx receipt logs for 1 week across 4
nodes.  I vaguely plan to update it every year.
A more ambitious version would add some topology information, but we
need to figure out some anonymization strategy for the data.

@_date: 2015-12-05 09:13:16
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Blockchain verification flag (BIP draft) 
Gavin Andresen via bitcoin-dev Yes, BIP9 need to be adjusted (setting bit 30 means BIP9 counts it as a
vote against all softforks).  BIP101 uses bits 0,1,2 AFAICT, so perhaps
start from the other end and use bit 29?  We can bikeshed that later
A delicate balance.  If we penalize these blocks too much, it's
disincentive to set the bit.  Fortunately it's easy for SPV clients to
decide for themselves, I think?

@_date: 2015-12-14 07:04:48
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
It just falls out naturally.  If the peer doesn't want the witnesses,
they don't get serialized into the IBLT.

@_date: 2015-12-20 14:44:25
@_author: Rusty Russell 
@_subject: [bitcoin-dev] On the security of softforks 
Jonathan Toomim via bitcoin-dev Pretty sure Bob's wallet will be looking for "OP_DUP OP_HASH160
 OP_EQUALVERIFY OP_CHECKSIG" scriptSig.  The SegWit-usable
outputs will (have to) look different, won't they?

@_date: 2015-01-21 15:15:26
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
Cut and paste bug in the last check:
    if (lenS > 1 && (sig[lenR + 6] == 0x00) && !(sig[lenR + 7] & 0x80))
    return false;
You mean "null bytes at the start of S".

@_date: 2015-01-22 11:02:35
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [softfork proposal] Strict DER signatures 
OK, I worked up a clearer (but more verbose) version with fewer
magic numbers.  More importantly, feel free to steal the test cases.
One weirdness is the restriction on maximum total length, rather than a
32 byte (33 with 0-prepad) limit on signatures themselves.
Apologies for my babytalk C++.  Am sure there's a neater way.
    CLARIFY
bool ConsumeByte(const std::vector &sig, size_t &off,
                 unsigned int &val)
    if (off >= sig.size()) return false;
    val = sig[off++];
    return true;
bool ConsumeTypeByte(const std::vector &sig, size_t &off,
                     unsigned int t)
    unsigned int type;
    if (!ConsumeByte(sig, off, type)) return false;
    return (type == t);
bool ConsumeNonZeroLength(const std::vector &sig, size_t &off,
                          unsigned int &len)
    if (!ConsumeByte(sig, off, len)) return false;
    // Zero-length integers are not allowed.
    return (len != 0);
bool ConsumeNumber(const std::vector &sig, size_t &off,
                   unsigned int len)
    // Length of number should be within signature.
    if (off + len > sig.size()) return false;
    // Negative numbers are not allowed.
    if (sig[off] & 0x80) return false;
    // Zero bytes at the start are not allowed, unless it would
    // otherwise be interpreted as a negative number.
    if (len > 1 && (sig[off] == 0x00) && !(sig[off+1] & 0x80)) return false;
    // Consume number itself.
    off += len;
    return true;
bool ConsumeDERInteger(const std::vector &sig, size_t &off) {
    unsigned int len;
    // Type byte must be "integer"
    if (!ConsumeTypeByte(sig, off, 0x02)) return false;
    if (!ConsumeNonZeroLength(sig, off, len)) return false;
    // Now the BE encoded value itself.
    if (!ConsumeNumber(sig, off, len)) return false;
    return true;
bool IsValidSignatureEncoding(const std::vector &sig) {
    // Format: 0x30 [total-length] 0x02 [R-length] [R] 0x02 [S-length] [S] [sighash]
    // * total-length: 1-byte length descriptor of everything that follows,
    //     excluding the sighash byte.
    // * R-length: 1-byte length descriptor of the R value that follows.
    // * R: arbitrary-length big-endian encoded R value. It cannot start with any
    //     null bytes, unless the first byte that follows is 0x80 or higher, in which
    //     case a single null byte is required.
    // * S-length: 1-byte length descriptor of the S value that follows.
    // * S: arbitrary-length big-endian encoded S value. The same rules apply.
    // * sighash: 1-byte value indicating what data is hashed.
    // Accept empty signature as correctly encoded (but invalid) signature,
    // even though it is not strictly DER.
    if (sig.size() == 0) return true;
    // Maximum size constraint.
    if (sig.size() > 73) return false;
    size_t off = 0;
    // A signature is of type "compound".
    if (!ConsumeTypeByte(sig, off, 0x30)) return false;
    unsigned int len;
    if (!ConsumeNonZeroLength(sig, off, len)) return false;
    // Make sure the length covers the rest (except sighash).
    if (len + 1 != sig.size() - off) return false;
    // Check R value.
    if (!ConsumeDERInteger(sig, off)) return false;
    // Check S value.
    if (!ConsumeDERInteger(sig, off)) return false;
    // There should exactly one byte left (the sighash).
    return off + 1 == sig.size() ? true : false;
bool IsValidSignatureEncoding(const std::vector &sig) {
    // Format: 0x30 [total-length] 0x02 [R-length] [R] 0x02 [S-length] [S] [sighash]
    // * total-length: 1-byte length descriptor of everything that follows,
    //     excluding the sighash byte.
    // * R-length: 1-byte length descriptor of the R value that follows.
    // * R: arbitrary-length big-endian encoded R value. It must use the shortest
    //     possible encoding for a positive integers (which means no null bytes at
    //     the start, except a single one when the next byte has its highest bit set).
    // * S-length: 1-byte length descriptor of the S value that follows.
    // * S: arbitrary-length big-endian encoded S value. The same rules apply.
    // * sighash: 1-byte value indicating what data is hashed (not part of the DER
    //     signature)
    // Accept empty signature as correctly encoded (but invalid) signature,
    // even though it is not strictly DER. This avoids needing full DER signatures
    // in places where any invalid signature would do. Given that the empty string is
    // always invalid as signature, this is safe.
    if (sig.size() == 0) return true;
    // Minimum and maximum size constraints.
    if (sig.size() < 9) return false;
    if (sig.size() > 73) return false;
    // A signature is of type 0x30 (compound).
    if (sig[0] != 0x30) return false;
    // Make sure the length covers the entire signature.
    if (sig[1] != sig.size() - 3) return false;
    // Extract the length of the R element.
    unsigned int lenR = sig[3];
    // Make sure the length of the S element is still inside the signature.
    if (5 + lenR >= sig.size()) return false;
    // Extract the length of the S element.
    unsigned int lenS = sig[5 + lenR];
    // Verify that the length of the signature matches the sum of the length
    // of the elements.
    if ((size_t)(lenR + lenS + 7) != sig.size()) return false;
    // Check whether the R element is an integer.
    if (sig[2] != 0x02) return false;
    // Zero-length integers are not allowed for R.
    if (lenR == 0) return false;
    // Negative numbers are not allowed for R.
    if (sig[4] & 0x80) return false;
    // Null bytes at the start of R are not allowed, unless R would
    // otherwise be interpreted as a negative number.
    if (lenR > 1 && (sig[4] == 0x00) && !(sig[5] & 0x80)) return false;
    // Check whether the S element is an integer.
    if (sig[lenR + 4] != 0x02) return false;
    // Zero-length integers are not allowed for S.
    if (lenS == 0) return false;
    // Negative numbers are not allowed for S.
    if (sig[lenR + 6] & 0x80) return false;
    // Null bytes at the start of S are not allowed, unless S would otherwise be
    // interpreted as a negative number.
    if (lenS > 1 && (sig[lenR + 6] == 0x00) && !(sig[lenR + 7] & 0x80)) return false;
    return true;
 COMPOUND 0x30
 NOT_COMPOUND 0x31
 LEN_OK 0
 LEN_TOO_BIG 1
 LEN_TOO_SMALL 0xff
 INT 0x02
 NOT_INT 0x03
 MINIMAL_SIGLEN 1
 MINIMAL_SIGVAL 0x0
 NORMAL_SIGLEN 32
 NORMAL_SIGVAL(S) S, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, \
        0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f,              \
        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,              \
        0x18, 0x19, 0x1a, 0x1b, 0x1c, 0x1d, 0x1e, 0x1f
 MAXIMAL_SIGLEN 33
 MAXIMAL_SIGVAL(S) NORMAL_SIGVAL(S), 0x20
 OVERSIZE_SIGLEN 34
 OVERSIZE_SIGVAL(S) MAXIMAL_SIGVAL(S), 0x21
 ZEROPAD_SIGLEN (1 + NORMAL_SIGLEN)
 ZEROPAD_SIGVAL(S) 00, NORMAL_SIGVAL(S)
 SIGHASH 0xf0
static bool check(const std::vector &sig)
    std::vector fixed = sig;
    // Fixup length
    if (fixed.size() > 1)
        fixed[1] += fixed.size() - 3;
    return IsValidSignatureEncoding(fixed);
 good(arr) assert(check(std::vector(arr, arr+sizeof(arr))))
 bad(arr) assert(!check(std::vector(arr, arr+sizeof(arr))))
static unsigned char zerolen[] = { };
static unsigned char normal[] = { COMPOUND, LEN_OK,
                                  INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                  INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                  SIGHASH };
static unsigned char min_r[] = { COMPOUND, LEN_OK,
                                 INT, MINIMAL_SIGLEN, MINIMAL_SIGVAL,
                                 INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                 SIGHASH };
static unsigned char min_s[] = { COMPOUND, LEN_OK,
                                 INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                 INT, MINIMAL_SIGLEN, MINIMAL_SIGVAL,
                                 SIGHASH };
static unsigned char max_r[] = { COMPOUND, LEN_OK,
                                 INT, MAXIMAL_SIGLEN, MAXIMAL_SIGVAL(0x1),
                                 INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                 SIGHASH };
static unsigned char max_s[] = { COMPOUND, LEN_OK,
                                 INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                 INT, MAXIMAL_SIGLEN, MAXIMAL_SIGVAL(0x2),
                                 SIGHASH };
static unsigned char wierd_s_len[] = { COMPOUND, LEN_OK,
                                       INT, OVERSIZE_SIGLEN, OVERSIZE_SIGVAL(0x1),
                                       INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                       SIGHASH };
static unsigned char wierd_r_len[] = { COMPOUND, LEN_OK,
                                       INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                       INT, OVERSIZE_SIGLEN, OVERSIZE_SIGVAL(0x2),
                                       SIGHASH };
static unsigned char zeropad_s[] = { COMPOUND, LEN_OK,
                                     INT, ZEROPAD_SIGLEN, ZEROPAD_SIGVAL(0x81),
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                     SIGHASH };
static unsigned char zeropad_r[] = { COMPOUND, LEN_OK,
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                     INT, ZEROPAD_SIGLEN, ZEROPAD_SIGVAL(0x82),
                                     SIGHASH };
static unsigned char not_compound[] = { NOT_COMPOUND, LEN_OK,
                                        INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                        INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                        SIGHASH };
static unsigned char short_len[] = { COMPOUND, LEN_TOO_SMALL,
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                     SIGHASH };
static unsigned char long_len[] = { COMPOUND, LEN_TOO_BIG,
                                    INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                    INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                    SIGHASH };
static unsigned char r_notint[] = { COMPOUND, LEN_OK,
                                    NOT_INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                    INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                    SIGHASH };
static unsigned char s_notint[] = { COMPOUND, LEN_OK,
                                    INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                    NOT_INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                    SIGHASH };
static unsigned char s_oversig[] = { COMPOUND, LEN_OK,
                                     INT, OVERSIZE_SIGLEN, OVERSIZE_SIGVAL(0x1),
                                     INT, MAXIMAL_SIGLEN, MAXIMAL_SIGVAL(0x2),
                                     SIGHASH };
static unsigned char r_oversig[] = { COMPOUND, LEN_OK,
                                     INT, MAXIMAL_SIGLEN, MAXIMAL_SIGVAL(0x1),
                                     INT, OVERSIZE_SIGLEN, OVERSIZE_SIGVAL(0x2),
                                     SIGHASH };
static unsigned char s_negative[] = { COMPOUND, LEN_OK,
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x81),
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                      SIGHASH };
static unsigned char r_negative[] = { COMPOUND, LEN_OK,
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x82),
                                      SIGHASH };
static unsigned char zeropad_bad_s[] = { COMPOUND, LEN_OK,
                                         INT, ZEROPAD_SIGLEN, ZEROPAD_SIGVAL(0x1),
                                         INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                         SIGHASH };
static unsigned char zeropad_bad_r[] = { COMPOUND, LEN_OK,
                                         INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                         INT, ZEROPAD_SIGLEN, ZEROPAD_SIGVAL(0x2),
                                         SIGHASH };
static unsigned char missing_sighash[] = { COMPOUND, LEN_OK,
                                           INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                           INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2) };
static unsigned char extra_byte[] = { COMPOUND, LEN_OK,
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                      INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                      SIGHASH, 0 };
static unsigned char zerolen_r[] = { COMPOUND, LEN_OK,
                                     INT, 0,
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                     SIGHASH };
static unsigned char zerolen_s[] = { COMPOUND, LEN_OK,
                                     INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                     INT, 0,
                                     SIGHASH };
static unsigned char overlen_r_by_1[] = { COMPOUND, LEN_OK,
                                          INT, NORMAL_SIGLEN + 1 + 1 + NORMAL_SIGLEN + 1 + 1, NORMAL_SIGVAL(0x1),
                                          INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                          SIGHASH };
static unsigned char overlen_s_by_1[] = { COMPOUND, LEN_OK,
                                          INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                          INT, NORMAL_SIGLEN+1+1, NORMAL_SIGVAL(0x2),
                                          SIGHASH };
static unsigned char underlen_r_by_1[] = { COMPOUND, LEN_OK,
                                           INT, NORMAL_SIGLEN-1, NORMAL_SIGVAL(0x1),
                                           INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x2),
                                           SIGHASH };
static unsigned char underlen_s_by_1[] = { COMPOUND, LEN_OK,
                                           INT, NORMAL_SIGLEN, NORMAL_SIGVAL(0x1),
                                           INT, NORMAL_SIGLEN-1, NORMAL_SIGVAL(0x2),
                                           SIGHASH };
int main()
    good(zerolen);
    good(normal);
    good(min_r);
    good(min_s);
    good(max_r);
    good(max_s);
    good(wierd_s_len);
    good(wierd_r_len);
    good(zeropad_s);
    good(zeropad_r);
    // Try different amounts of truncation.
    for (size_t i = 1; i < sizeof(normal)-1; i++)
        assert(!check(std::vector(normal, normal+i)));
    bad(not_compound);
    bad(short_len);
    bad(long_len);
    bad(r_notint);
    bad(s_notint);
    bad(s_oversig);
    bad(r_oversig);
    bad(s_negative);
    bad(r_negative);
    bad(s_negative);
    bad(r_negative);
    bad(zeropad_bad_s);
    bad(zeropad_bad_r);
    bad(zerolen_r);
    bad(zerolen_s);
    bad(overlen_r_by_1);
    bad(overlen_s_by_1);
    bad(underlen_r_by_1);
    bad(underlen_s_by_1);
    bad(missing_sighash);
    bad(extra_byte);
    return 0;

@_date: 2015-07-02 12:08:58
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 68 Questions 
Hi Mark,
        It looks like the code in BIP 68 compares the input's nSequence
against the transaction's nLockTime:
        if ((int64_t)tx.nLockTime < LOCKTIME_THRESHOLD)
            nMinHeight = std::max(nMinHeight, (int)tx.nLockTime);
        else
            nMinTime = std::max(nMinTime, (int64_t)tx.nLockTime);
        if (nMinHeight >= nBlockHeight)
            return nMinHeight;
        if (nMinTime >= nBlockTime)
            return nMinTime;
So if transaction B spends the output of transaction A:
1.  If A is in the blockchain already, you don't need a relative
    locktime since you know A's time.
2.  If it isn't, you can't create B since you don't know what
    value to set nLockTime to.
How was this supposed to work?

@_date: 2015-07-03 10:49:16
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 68 Questions 
No, forget this.  I misread the code.  Mark ELI5'd to me offlist, thanks!
FWIW, the code works :)

@_date: 2015-06-06 14:12:10
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering in 
Title: Canonical Input and Output Ordering
Author: Rusty Russell Discussions-To: "Bitcoin Dev" Status: Draft
Type: Standards Track
Created: 2015-06-06
This BIP provides a canonical ordering of inputs and outputs when
creating transactions.
Most bitcoin wallet implementations randomize the outputs of
transactions they create to avoid trivial linkage analysis (especially
change outputs), however implementations have made mistakes in this area
in the past.
Using a canonical ordering has the same effect, but is simpler, more
obvious if incorrect, and can eventually be enforced by IsStandard() and
even a soft-fork to enforce it.
Inputs should be ordered like so:
        index (lower value first)
        txid (little endian order, lower byte first)
Outputs should be ordered like so:
        amount (lower value first)
        script (starting from first byte, lower byte first, shorter wins)
Any single wallet is already free to implement this, but if other
wallets do not it would reduce privacy by making those transactions
stand out.  Thus a BIP is appropriate, especially if this were to
become an IsStandard() rule once widely adopted.
Because integers are fast to compare, they're sorted first, before the
lexographical ordering.
The other input fields do not influence the sort order, as any valid
transactions cannot have two inputs with the same index and txid.
Reference Implementation

@_date: 2015-06-06 16:14:07
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Ah, I forgot about that particular wart. Yech.  Implies that you can
order inputs or outputs, not both.
Something like "outputs must be in order, inputs which do not
CHECK(MULTI)SIG_(VERIFY) a SIGHASH_SINGLE sig must be in order with
respect to each other".  But that's much less trivial since it implies
script evaluation.
In other news, I noticed Kristov Atlas's concurrent proposal just after
I posted this (via reddit).  He used far more words, but didn't note
this issue either AFAICT.

@_date: 2015-06-10 12:10:38
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Consensus-enforced transaction 
Lightning channels want them exactly like this to revoke old
transactions, which could be ancient on long-lived channels.

@_date: 2015-06-15 11:59:11
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Yes, I hit this when I implemented an IsStandard change; upon input
evaluation the scriptsigs which used _SINGLE get disregarded from
ordering.  I think that one's pretty easy to fix (and they should fix it anyway, to
avoid leaking information due to ordering): they can receive an
unordered tx and sign it as if it were ordered canonically.
The softfork argument I find the most compelling, though it's tempting
to argue that every ordering use (including SIGHASH_SINGLE) is likely
a mistake.
I was prompted to propose something by this:
If that's the only one though, it's less compelling.
Indeed.  I was implementing deterministic permutations for lightning
(signature exchange requires both sides agree on ordering).

@_date: 2015-06-16 06:31:04
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
Yes, we can suit any one use case, but not all of them.
For example, outputs shall be sorted by:
        1.  First byte (or 0 if script is zero length) minus 107.
        2.  The remainder of the script in lexographical order.
This would put OP_RETURN outputs last.
Though Peter Todd's more general best-effort language might make more
sense.  It's not like you can hide an OP_RETURN transaction to make it
look like something else, so that transaction not going to be
distinguished by non-canonical ordering.

@_date: 2015-06-16 07:12:14
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Lexicographical Indexing of Transaction 
OK, I've modified my implementation to match your proposal:
        It compiles, and is fairly trivial, but will need some testing.

@_date: 2015-06-16 17:36:38
@_author: Rusty Russell 
@_subject: [Bitcoin-development] [RFC] Canonical input and output ordering 
I have no idea what they are? :)
Yes, my plan B would be an informational bip with simple code,
suggesting a way to permute a transaction based on some secret.  No
point everyone reinventing the wheel, badly.

@_date: 2015-06-23 15:23:27
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [RFC] IBLT block testing implementation 
Hi all,
        I've come up with a model for using IBLT to communicate blocks
between peers.  The gory details can be found on github: it's a
standalone C++ app for testing not integrated with bitcoin.
        Assumptions and details:
1. The base idea comes from Gavin's Block Propagation gist:
        2. It relies on similarity in mempools, with some selection hints.  This
   is designed to be as flexible as possible to make fewest assumptions
   on tx selection policy.
3. The selection hints are: minimum fee-per-byte (fixed point) and
   bitmaps of included-despite-that and rejected-despite-that.  The
   former covers things like child-pays-for-parent and the priority
   area.  The latter covers other cases like Eligius censoring "spam",
   bitcoin version differences, etc.
4. Cost to represent these exceptional added or excluded transactions is
   (on average) log2(transactions in mempool) bits.
5. At Peiter Wuille's suggestion, it is designed to be reencoded between
   nodes.  It seems fast enough for that, and neighboring nodes should
   have most similar mempools.
6. It performs reasonably well on my 100 block sample in bitcoin-corpus
   (chosen to include a mempool spike):
   Average block range (bytes):                         7988-999829
   Block size mean (bytes):                             401926
   Minimal decodable BLT size range (bytes):            314-386361
   Minimal decodable BLT size mean (bytes):             13265
7. Actual results will have to be worse than these minima, as peers will
   have to oversize the IBLT to have reasonable chance of success.
8. There is more work to do, and more investigation to be done, but I
   don't expect more than a 25% reduction in this "ideal minimum"
   result.
Special thanks to Kalle Rosenbaum for helping investigate IBLTs with me
last year.
PS. I work for Blockstream.  And I'm supposed to be working on
    Lightning, not this.

@_date: 2015-06-27 14:16:35
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [RFC] IBLT block testing implementation 
Yep, I stole the hashFunctionCount = 3 straight from there, same
with 64-byte bucket contents.
Yep!  I keep records of all the 1 and -1 buckets; separate lists
depending on how far they are off the base.  Lists for 0, 1, 2, ... 7,
then powers of 2.  See todo in iblt.cpp.
Absolutely, will do that offline.
No; they add or remove all matching.  If they add too many, that's the
easy case, of course.  They can't remove too many (since they know that
bit prefix was unique on the other end).
It's pretty easy to cut the bitmaps to zero and test this (comment them
out in wire_encode.cpp, for example).
But the overhead of IBLT is some factor greater than txsize (need to
measure that factor, too!); whereas these are a log( bits.
Yep, and estimation ability.  The theory is that adjacent nodes will
have closer mempools, allowing for smaller IBLTs.  The size of mempool
differences for each block can be fed back, so you have an idea of the
likely delta to peers (ie. add that to the actual difference between
your mempool and the new block, to estimate the amount of IBLT
reconstruction required).
Let's pick a 5% as our failure target (given most nodes will get blocks
from more than 1 peer, and our other estimates of mempool differences
will be conservative).  Seems like 16/16 transactions takes ~400 cells
for recovery, 64/64 takes ~1400, 128/128 takes ~2480, 256/256 says
Using that 128/128 => 198k number, and your txs were about 300B, that
implies an overhead of 2.6, right?
We're probably better estimating in terms of "cells recovered" (ie. sum
of cells for txs which we were missing, plus number of txs we added

@_date: 2015-05-07 11:05:47
@_author: Rusty Russell 
@_subject: [Bitcoin-development] CLTV opcode allocation; long-term plans? 
Mildly prefer to put that the other way around.
ie. the OP_NOP1 becomes OP_EXTENSION_PREFIX, the next op defines which
extended opcode it is (must be a push).

@_date: 2015-05-16 09:52:14
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
I like this too.
Some tweaks:
1) Nomenclature: call tx_size "tx_cost" and real_size "tx_bytes"?
2) If we have a reasonable hard *byte* limit, I don't think that we need
   the MAX().  In fact, it's probably OK to go negative.
3) ... or maybe not, if any consumed UTXO was generated before the soft
   fork (reducing Tier's perverse incentive).
4) How do we measure UTXO size?  There are some constant-ish things in
   there (eg. txid as key, height, outnum, amount).  Maybe just add 32
   to scriptlen?
5) Add a CHECKSIG cost.  Naively, since we allow 20,000 CHECKSIGs and
   1MB blocks, that implies a cost of 50 bytes per CHECKSIG (but counted
   correctly, unlike now).   This last one implies that the initial cost limit would be 2M, but in
practice probably somewhere in the middle.
  tx_cost = 50*num-CHECKSIG
                + tx_bytes
                + 4*utxo_created_size
                - 3*utxo_consumed_size
Now cost == 352.

@_date: 2015-05-18 11:12:11
@_author: Rusty Russell 
@_subject: [Bitcoin-development] Proposed alternatives to the 20MB step 
OK.  Be nice if these were cleaned up, but I guess it's a sunk cost.
He said "utxo_created_size" not "utxo_created" so I assumed scriptlen?
But you made that number up?  The soft cap and hard byte limit are
different beasts, so there's no need for soft cost cap < hard byte
Brilliant!  I completely missed that possibility...
I don't think so.  Again, you're mixing units.
Agreed!  Unfortunately there'll always be 2, because we really do want a
hard byte limit: it's total tx bytes which brings most concerns about
centralization.  But ideally it'll be so rarely hit that it can be ~
ignored (and certainly not optimized for).

@_date: 2015-11-16 12:22:28
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
We have strayed far from both the Subject line and from making progress
on bitcoin development.  Please redirect to bitcoin-discuss.
I have set the moderation bits on the three contributors from here down
(CC'd): your next post will go to moderation.

@_date: 2015-11-27 14:32:45
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Alternative name for CHECKSEQUENCEVERIFY (BIP112) 
As someone who actually developed scripts using CSV, I agree with Mark
(and Matt).  The relative locktime stuff isn't in this opcode, it's in
the nSequence calculation.
So, I vote to keep CSV called as it is.

@_date: 2015-10-01 09:36:03
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
John Winslow via bitcoin-dev Please take this off the -dev list.

@_date: 2015-10-01 09:34:19
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
That too, but let's not break existing software.  This proposal allows
that, and is trivial.

@_date: 2015-10-01 09:56:51
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
Introducing this later would trigger warnings on older clients, who
would consider the bit to represent a new soft fork :(
So if we want this middle ground, we should sew it in now, though it
adds a other state.  Simplest is to have miners keep setting the bit for
another 2016 blocks.  If we want to later, we can make this a consensus
"Bitcoin is hard, let's go shopping!"  "With Bitcoin!"  "..."

@_date: 2015-10-02 10:52:14
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
Rusty Russell via bitcoin-dev Actually, this isn't a decisive argument, since we can use the current
mechanism to upgrade versionbits, or as Eric says, tack it on to
an existing soft fork.
So, I think I'm back where I started.  We leave this for now.
There was no nak on the "keep setting bit until activation" proposal, so
I'm opening a PullReq for that now:

@_date: 2015-10-06 12:28:49
@_author: Rusty Russell 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Peter Todd via bitcoin-dev It could be implemented in other ways, but nSequence is the neatest and
most straightforward I've seen.
- I'm not aware of any other (even vague) proposal for its use?  Enlighten?
- BIP68 reserves much of it for future use already.
If we apply infinite caution we could never use nSequence, as there
might be a better use tommorrow.

@_date: 2015-10-09 12:08:06
@_author: Rusty Russell 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
Excellent, thanks!  It's good to have such ideas as a compass.  PoS
voting seems like it won't be a problem in 5 bits.
The "prevbits" idea would want more bits; naively 64 would be good, but
I think there are some tricks we can use to make 32 work OK.  We would
have to then split between nLocktime (if available) and multiple
nSequence fields, and it would weaken it for some txs.
There is one easy solution: change the BIP wording from:
-For transactions with an nVersion of 2 or greater,
+For transactions with an nVersion of 2, And on every tx bump, we decide whether to keep this scheme (mempool
would enforce it always).

@_date: 2015-10-16 09:48:18
@_author: Rusty Russell 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
I like it from a technical perspective.
with bitcoind's perception of time, so that's a very long sleep to
blackbox test (which is what my lightning test script does).
So consider this YA feature request :)

@_date: 2015-10-16 11:56:14
@_author: Rusty Russell 
@_subject: [bitcoin-dev] CHECKSEQUENCEVERIFY - We need more usecases to 
... Gavin just told me about setmocktime.  That's fast service!

@_date: 2015-10-22 11:59:11
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Mailing List Moderation Now Active. 
Hi all,
We aim to make the list more contentful and productive; to get devs to
resubscribe we need to maximize high-value interactions.
  - Currently 5 moderators.  BtcDrak, me, G1lius, Kanzure and Johnathan.
    As far as I know we're entirely unconnected, and we cover Asia/Europe/US.
  - Moderation will last 3 months.  Then we'll have an unmoderated on-list
    discussion as to whether it should continue, or change.
  - Appeals for/against moderation decisions should be directed to Jeff, who
    will have final say.  General moderation feedback just send to me.
  - All rejected posts will be forwarded to a list for public viewing:
      - Everyone starts moderated, and the mod bit gets cleared as they post.
    It gets set again if someone notices or reports a violation.
  - Moderation rules:
    - No offensive posts, no personal attacks.
    - Posts must concern the near-term development of the bitcoin core
      code or bitcoin protocol.
    - Posts must contribute to bitcoin development.
    - Generally encouraged: patches, notification of pull requests, BIP
      proposals, academic paper announcements.  And discussions that follow.
    - Generally discouraged: shower thoughts, wild speculation, jokes, +1s,
      non-technical bitcoin issues, rehashing settled topics without new
      data, moderation concerns.
    - Detailed patch discussion generally better on the GitHub PR.
    - Meta-discussion is better on bitcoin-discuss:
        - We will make mistakes, we are human, so please be patient.
Your friendly moderators.

@_date: 2015-10-23 17:11:49
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Memory leaks? 
Agreed.  I couldn't see an issue, so I've opened one.  Let's track this
there, please.

@_date: 2015-10-31 14:13:13
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Compatibility requirements for hard or soft forks 
============================== START ==============================
Not just lockTime; potentially any tx locked away in a safe.
Consider low-S enforcement: high chance a non-expert user will be unable
to spend an old transaction.  They need to compromise their privacy
and/or spend time and money.  A milder "confiscation" but a more likely
By that benchmark, we should aim for "reasonable certainty".  A
transaction which would never have been generated by any known software
is the minimum bar.  Adding "...which would have to be deliberately
stupid with many redundant OP_CHECKSIG etc" surpasses it.  The only extra
safeguard I can think of is clear, widespread notification of the

@_date: 2015-09-14 04:26:01
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and delay. 
Hi all,
Those who've seen the original versionbits bip, this adds:
   1) Percentage checking only on retarget period boundaries.
   2) 1 retarget period between 95% and activation.
   3) A stronger suggestion for timeout value selection.
And pasted below, de-formatted a little.
  BIP: ??
  Title: Version bits with timeout and delay
  Author: Pieter Wuille , Peter Todd , Greg Maxwell , Rusty Russell   Status: Draft
  Type: Informational Track
  Created: 2015-10-04
This document specifies a proposed change to the semantics of the 'version' field in Bitcoin blocks, allowing multiple backward-compatible changes (further called called "soft forks") being deployed in parallel. It relies on interpreting the version field as a bit vector, where each bit can be used to track an independent change. These are tallied each retarget period. Once the consensus change succeeds or times out, there is a "fallow" pause after which the bit can be reused for later changes.
BIP 34 introduced a mechanism for doing soft-forking changes without predefined flag timestamp (or flag block height), instead relying on measuring miner support indicated by a higher version number in block headers. As it relies on comparing version numbers as integers however, it only supports one single change being rolled out at once, requiring coordination between proposals, and does not allow for permanent rejection: as long as one soft fork is not fully rolled out, no future one can be scheduled.
In addition, BIP 34 made the integer comparison (nVersion >= 2) a consensus rule after its 95% threshold was reached, removing 2^31 +2 values from the set of valid version numbers (all negative numbers, as nVersion is interpreted as a signed integer, as well as 0 and 1). This indicates another downside this approach: every upgrade permanently restricts the set of allowed nVersion field values. This approach was later reused in BIP 66, which further removed nVersion = 2 as valid option. As will be shown further, this is unnecessary. '''Bit flags'''
We are permitting several independent soft forks to be deployed in parallel. For each, a bit B is chosen from the set {0,1,2,...,28}, which is not currently in use for any other ongoing soft fork. Miners signal intent to enforce the new rules associated with the proposed soft fork by setting bit 1B in nVersion to 1 in their blocks.
'''High bits'''
The highest 3 bits are set to 001, so the range of actually possible nVersion values is [0x20000000...0x3FFFFFFF], inclusive. This leaves two future upgrades for different mechanisms (top bits 010 and 011), while complying to the constraints set by BIP34 and BIP66. Having more than 29 available bits for parallel soft forks does not add anything anyway, as the (nVersion >= 3) requirement already makes that impossible.
With every softfork proposal we associate a state BState, which begins
at ''defined'', and can be ''locked-in'', ''activated'',
or ''failed''.  Transitions are considered after each
retarget period.
'''Soft Fork Support'''
Software which supports the change should begin by setting B in all blocks
mined until it is resolved.
 if (BState == defined) {
     SetBInBlock();
 }
'''Success: Lock-in Threshold'''
If bit B is set in 1916 (1512 on testnet) or more of the 2016 blocks
within a retarget period, it is considered ''locked-in''.  Miners should
stop setting bit B.
 if (NextBlockHeight % 2016 == 0) {
    if (BState == defined && Previous2016BlocksCountB() >= 1916) {
        BState = locked-in;
        BActiveHeight = NextBlockHeight + 2016;
    }
 }
'''Success: Activation Delay'''
The consensus rules related to ''locked-in'' soft fork will be enforced in
the second retarget period; ie. there is a one retarget period in
which the remaining 5% can upgrade.  At the that activation block and
after, the bit B may be reused for a different soft fork.
 if (BState == locked-in && NextBlockHeight == BActiveHeight) {
    BState = activated;
    ApplyRulesForBFromNextBlock();
    /* B can be reused, immediately */
 }
'''Failure: Timeout'''
A soft fork proposal should include a ''timeout''.  This is measured
as the beginning of a calendar year as per this table (suggested
three years from drafting the soft fork proposal):
Timeout Year    >= Seconds              Timeout Year    >= Seconds
2018            1514764800              2026            1767225600
2019            1546300800              2027            1798761600
2020            1577836800              2028            1830297600
2021            1609459200              2029            1861920000
2022            1640995200              2030            1893456000
2023            1672531200              2031            1924992000
2024            1704067200              2032            1956528000
2025            1735689600              2033            1988150400
If the soft fork still not ''locked-in'' and the
GetMedianTimePast() of a block following a retarget period is at or
past this timeout, miners should cease setting this bit.
 if (NextBlockHeight % 2016 == 0) {
    if (BState == defined && GetMedianTimePast(nextblock) >= BFinalYear) {
         BState = failed;
    }
 }
After another retarget period (to allow detection of buggy miners),
the bit may be reused.
'''Warning system'''
To support upgrade warnings, an extra "unknown upgrade" is tracked, using the "implicit bit" mask = (block.nVersion & ~expectedVersion) != 0. Mask will be non-zero whenever an unexpected bit is set in nVersion.  Whenever lock-in for the unknown upgrade is detected, the software should warn loudly about the upcoming soft fork.  It should warn even more loudly after the next retarget period.
It should be noted that the states are maintained along block chain
branches, but may need recomputation when a reorganization happens.
===Support for future changes===
The mechanism described above is very generic, and variations are possible for future soft forks. Here are some ideas that can be taken into account.
'''Modified thresholds'''
The 95% threshold (based on in BIP 34) does not have to be maintained for eternity, but changes should take the effect on the warning system into account. In particular, having a lock-in threshold that is incompatible with the one used for the warning system may have long-term effects, as the warning system cannot rely on a permanently detectable condition anymore.
'''Conflicting soft forks'''
At some point, two mutually exclusive soft forks may be proposed. The naive way to deal with this is to never create software that implements both, but that is a making a bet that at least one side is guaranteed to lose. Better would be to encode "soft fork X cannot be locked-in" as consensus rule for the conflicting soft fork - allowing software that supports both, but can never trigger conflicting changes.
'''Multi-stage soft forks'''
Soft forks right now are typically treated as booleans: they go from an inactive to an active state in blocks. Perhaps at some point there is demand for a change that has a larger number of stages, with additional validation rules that get enabled one by one. The above mechanism can be adapted to support this, by interpreting a combination of bits as an integer, rather than as isolated bits. The warning system is compatible with this, as (nVersion & ~nExpectedVersion) will always be non-zero for increasing integers.
== Rationale ==
The failure timeout allows eventual reuse of bits even if a soft fork was
never activated, so it's clear that the new use of the bit refers to a
new BIP.  It's deliberately very course grained, to take into account
reasonable development and deployment delays.  There are unlikely to be
enough failed proposals to cause a bit shortage.
The fallow period at the conclusion of a soft fork attempt allows some
detection of buggy clients, and allows time for warnings and software
upgrades for successful soft forks.

@_date: 2015-09-17 05:49:22
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Tier Nolan via bitcoin-dev I couldn't see a use for it, since partial enforcement of a soft fork is
pretty useless.
Your point about checking that miners are actually doing it is true,
though all stuff being forked in in future will be nonstandard AFAICT.
I bias towards simplicity for this.
Miners already have that date in their calendar, so prefer to anchor to
OK, *that* variant makes perfect sense, and is no more complex, AFAICT.
So, there's two weeks to detect bad implementations, then you everyone
stops setting the bit, for later reuse by another BIP.
Easier for code, but harder for BIP authors.
You need a timeout: an ancient (non-mining, thus undetectable) node
should never fork itself off the network because someone reused a failed
BIP bit.
SPV clients don't experience a security change when a soft fork occurs?
They're already trusting miners.
Greg pointed out that soft forks tend to get accompanied by block forks
across activation, but SPV clients should *definitely* be taking those
into account whenever they happen, right?

@_date: 2015-09-18 07:30:27
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Three years from drafting is reasonable.  How many blocks is that?  Hmm,
better make it 6 years of blocks just in case we have a hash race.
Deployment speed is measured in months, not blocks.  It's hard enough to
guess without adding another variable.

@_date: 2015-09-18 07:27:51
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Tier Nolan via bitcoin-dev Indeed.  There are three believable failure possibilties:
1) You don't implement the rule at all, and don't set the bit.
2) You implement it and set bit, but think some valid block is invalid.
3) You implement it and set bit, but think some invalid block is valid.
 is by far the most common, and the proposal is designed so they
*always* get ~2 weeks warning before those drop to SPV security.
Assuming the mining majority isn't buggy (otherwise, it's arguably not a
bug but a feature!)  is the worst case: some miners fork off and don't
So there is a slight advantage in doing this early: those buggy miners
no longer contribute to the 95% threshold.  But that's outweighed IMHO
1) We would need another delay at 75% so  nodes can upgrade.
2) The new feature won't be exercised much before impliciation, since
   it's useless before then, so it might not find bugs anyway.
In conclusion, I'm not convinced by the extra complexity.

@_date: 2015-09-18 10:51:19
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP-draft] CHECKSEQUENCEVERIFY - An opcode for 
Mark Friedenbach via bitcoin-dev Indeed.  Simplicity wins.  We have half the number space left for the
future, too.  If people are paranoid, reserve the top *two* bits.

@_date: 2015-09-18 10:49:05
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
This could be added if we approach one failed soft fork every 5 weeks,
I guess (or it could be just for specific soft forks).

@_date: 2015-09-20 13:26:43
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Thanks, I'll have to dig through bitcoin-dev and find it.
BIP text is pretty clear that it's median block time.
This is only for timeout, not for soft fork rule change (which *is* 2016
blocks after 95% is reached).

@_date: 2015-09-21 20:04:06
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Thanks, read and digested.
The good news is that timeout via GetMedianTimePast() doesn't have any
effect on "should I accept this to mempool", and seems pretty
uncontroversial.   Activation is by block number once vote hits 95%, so
that too is fairly simple to implement.

@_date: 2015-09-24 10:41:03
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Weak block thoughts... 
Gavin Andresen via bitcoin-dev That won't help SPV nodes, unfortunately.
We already see non-validating mining, but they do empty blocks.  This
just makes it more attractive in the future, since you can collect fees
But I think it's clear we'll eventually need some UTXO commitment so
full nodes can tell SPV nodes about bad blocks.

@_date: 2015-09-24 11:02:34
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Weak block thoughts... 
Gavin Andresen via bitcoin-dev The bandwidth/latency argument is solid.  And if a block encodes to <
~3k, then we can just spray it to (some?) peers rather than using INV.
But validation is only trivially cachable if the delta to the previous
weak block is zero.  The "partially validated" cases need to be coded
with care (eg. total opcode constraints, tx order).
I was thinking as a first cut we do the opposite: don't validate weak
blocks at all (other than PoW), and just use them as a bandwidth
Ambition is good though!
PS.  Original idea came to me from Greg Maxwell; Peter Todd called it
     "near blocks" and extolled their virtues 2 years ago...

@_date: 2015-09-30 12:00:23
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Versionbits BIP (009) minor revision proposal. 
Hi all,
        Pieter and Eric pointed out that the current BIP has miners
turning off the bit as soon as it's locked in (75% testnet / 95%
mainnet).  It's better for them to keep setting the bit until activation
(2016 blocks later), so network adoption is visible.
I'm not proposing another suggestion, though I note it for future:
miners keep setting the bit for another 2016 blocks after activation,
and have a consensus rule that rejects blocks without the bit.  That
would "force" upgrades on those last miners.  I feel we should see how
this works first.
diff --git a/bip-0009.mediawiki b/bip-0009.mediawiki
index c17ca15..b160810 100644
--- a/bip-0009.mediawiki
+++ b/bip-0009.mediawiki
 -37,14 +37,15  retarget period.
 Software which supports the change should begin by setting B in all blocks
 mined until it is resolved.
-    if (BState == defined) {
+    if (BState != activated && BState != failed) {
         SetBInBlock();
     }
 '''Success: Lock-in Threshold'''
 If bit B is set in 1916 (1512 on testnet) or
 more of the 2016 blocks within a retarget period, it is considered
-''locked-in''.  Miners should stop setting bit B.
+''locked-in''.  Miners should continue setting bit B, so uptake is
     if (NextBlockHeight % 2016 == 0) {
         if (BState == defined && Previous2016BlocksCountB() >= 1916) {
 -57,7 +58,7  more of the 2016 blocks within a retarget period, it is considered
 The consensus rules related to ''locked-in'' soft fork will be enforced in
 the second retarget period; ie. there is a one retarget period in
 which the remaining 5% can upgrade.  At the that activation block and
-after, the bit B may be reused for a different soft fork.
+after, miners should stop setting bit B, which may be reused for a different soft fork.
     if (BState == locked-in && NextBlockHeight == BActiveHeight) {
         BState = activated;

@_date: 2015-09-30 11:35:47
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Proposal] Version bits with timeout and 
Tom Harding via bitcoin-dev Yeah, but Gavin's right.  If you can't account for all the corner cases,
all you can do is keep it simple and well defined.

@_date: 2015-09-30 13:35:42
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Let's deploy BIP65 CHECKLOCKTIMEVERIFY! 
"Wladimir J. van der Laan via bitcoin-dev"
Agreed.  Unfortunately, a simple "block version >= 4" check is
insufficient, due to XT which sets version bits 001....111.
Given that, I suggest using the simple test:
        if (pstart->nVersion & 0x8)
            ++nFound;
Which means:
1) XT won't trigger it.
2) It won't trigger XT.
3) You can simply set block nVersion to 8 for now.
4) We can still use versionbits in parallel later.

@_date: 2016-02-29 10:55:53
@_author: Rusty Russell 
@_subject: [bitcoin-dev] SIGHASH_NOINPUT in Segregated Witness 
============================== START ==============================
AFAICT we need more than this.  Or are you using something other than
the deployable lightning commit tx style?
If each HTLC output is a p2sh[1], you need the timeout and rhash for
each one to build the script to redeem it.  In practice, there's not
much difference between sending a watcher a tx for every commit tx and
sending it information for every new HTLC (roughly a factor of 2).
So we also need to put more in the scriptPubKey for this to work; either
the entire redeemscript, or possibly some kind of multiple-choice P2SH
where any one of the hashes will redeem the payment.
[1] eg. from         OP_HASH160 OP_DUP # Replace top element with two copies of its hash
         OP_EQUAL # Test if they supplied the HTLC R value
        OP_SWAP  OP_EQUAL OP_ADD
                          # Or the commitment revocation hash
        OP_IF # If any hash matched.
                 # Pay to B.
        OP_ELSE # Must be A, after HTLC has timed out.
                 OP_CHECKLOCKTIMEVERIFY Ensure (absolute) time has passed.
                 OP_CHECKSEQUENCEVERIFY # Delay gives B enough time to use revocation if it has it.
                OP_2DROP # Drop the delay and htlc-timeout from the stack.
                 # Pay to A.
        OP_ENDIF
        OP_CHECKSIG # Verify A or B's signature is correct.

@_date: 2016-01-04 10:01:26
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [BIP Draft] Decentralized Improvement Proposals 
I thought about it, but it's subject to change.  Frankly, the number of
deployed forks is low enough that they can sort it out themselves.  If
we need something more robust, I'm happy to fill that role.

@_date: 2016-01-08 14:00:11
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks 
Pieter Wuille via bitcoin-dev FWIW, this attack would effect the current lightning-network "deployable
lightning" design at channel establishment; we reveal our pubkey in the
opening packet (which is used to redeem a P2SH using normal 2of2).
At least you need to grind before replying (which will presumably time
out), rather than being able to do it once the channel is open.
We could pre-commit by exchanging hashes of pubkeys first, but contracts
on bitcoin are hard enough to get right that I'm reluctant to add more

@_date: 2016-01-08 22:32:01
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision 
I don't think this is true?  Even if you can generate a collision in
RIPEMD160, that doesn't help you since you need to create a specific
SHA256 hash for the RIPEMD160 preimage.
Even a preimage attack only helps if it leads to more than one preimage
fairly cheaply; that would make grinding out the SHA256 preimage easier.
AFAICT even MD4 isn't this broken.
But just with Moore's law (doubling every 18 months), we'll worry about
economically viable attacks in 20 years.[1]
That's far enough away that I would choose simplicity, and have all SW
scriptPubKeys simply be "<0> RIPEMD(SHA256(WP))" for now, but it's
not a no-brainer.
[1] Assume bitcoin-network-level compute (collision in 19 days) costs
    $1B to build today.  Assume there will be 100 million dollars a day
    in vulnerable txs, and you're on one end of all of them (or can MITM
    if you find a collision), *and* can delay them all by 10 seconds,
    and none are in parallel so you can attack all of them.  IOW, just
    like a single $100M opportunity for 3650 seconds each year.
    Our machine has a 0.11% chance of finding a collision in 1 hour, so
    it's worth about $110,000.  We can build it for that in about 20
    years.

@_date: 2016-01-11 14:27:15
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Time to worry about 80-bit collision attacks 
vanitygen can generate keypairs pretty fast (on my CPU it's comparable
with hashing time), and there are ways to make it faster.  Since you can
generate multiple script variations, too, I think hashing is the
Antminer S7 can do 4.73 Terahash per second for $1.2k.  (Double SHA, but
let's assume RIPEMD160(SHA256()) is the same speed).
766,760,562,123 seconds to do 3*2^80, so you'd need over 200 million
S7s to do it in an hour.[1] If you want to do that for $1M, wait 27
years and hope Moore's Law holds?
Also, a colleague points out you could use this attack against a site
like bitrated.com which publishes one side's pubkey, giving you a much
longer attack window.
[1] Weirdly, the bitcoin network is doing this much work every 57
    days, for about $92M.  If that's all the attack costs, it's under
    1M in 10 years.

@_date: 2016-01-21 11:20:46
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Three Month bitcoin-dev Moderation Review 
Hi all!
        As planned, this is the three month review[1]: discussion of how
moderation should change is encouraged in this thread.
        First, thanks to everyone for the restraint shown in sending
(and responding to!) inflammatory or sand-in-the-gears mails, and being
tolerant with our mistakes and variances in moderation.
The only changes we made to the plan so far:
1) We've stopped clearing the "needs mod" bit after first posts, and
2) Trivially answerable emails or proposals have been answered in the
   reject message itself.
You can see almost all (there was some lossage) rejects at:
        So, what should moderation look like from now on?
- Stop moderating altogether?
- Moderate  more/less harshly?
- Use a different method/criteria for moderation?
- Add/remove moderators?
- Other improvements?
[1]

@_date: 2016-01-21 15:14:47
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Three Month bitcoin-dev Moderation Review 
+1s here means simpling say "+1" or "me too" that carries no additional
information.  ie. if you like an idea, that's great, but it's not worth
interruping the entire list for.
If you say "I prefer proposal X over Y because " that's
different.  As is "I dislike X because " or "I need X because
Hope that clarifies!

@_date: 2016-01-21 15:30:20
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Three Month bitcoin-dev Moderation Review 
Yes, it's because we simply forward them to the bitcoin-dev-moderation
mailing list, and it strips them out as attachments.
I'd really love a filter which I could run them through (on ozlabs.org)
to fix this.  Volunteers welcome :)

@_date: 2016-07-01 12:55:17
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 151 use of HMAC_SHA512 
Not quite, there's an important subtlety that SHA256 appends the
bitlength, so you can only create:
y' = SHA256(key|cipher-type|mesg|padding|bitlength|any values I want).
But we're not using this for a MAC in BIP151, we're using this to
generate the encryption keys.
Arthur Chen  said:
Bitcoin already relies on SHA256's robustness, but again, we don't need
a MAC here.
I'm happy to buy "we just copied ssh" if that's the answer, and I can't
see anything wrong with using HMAC here, it just seems odd...

@_date: 2016-06-28 12:01:34
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 151 use of HMAC_SHA512 
To quote:
This seems a weak reason to introduce SHA512 to the mix.  Can we just
K_1 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="header encryption key")
K_2 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="body encryption key")

@_date: 2016-06-29 10:30:29
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 151 use of HMAC_SHA512 
Good point; I would argue that mistake has already been made.  But I was
looking at appropriating your work for lightning inter-node comms, and
adding another hash algo seemed unnecessarily painful.
It's also not clear to me why the HMAC, vs just
SHA256(key|cipher-type|mesg).  But that's probably just my crypto

@_date: 2016-05-01 13:46:20
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Simple Bitcoin Payment Channel Protocol v0.1 
Rune Kj?r Svendsen via bitcoin-dev
        CHECKLOCKTIMEVERIFY [...] allows payment channel
        setup to be risk free [...] something that was
        not the case before, when the refund Bitcoin transaction
        depended on another, unconfirmed Bitcoin transaction. Building
        on unconfirmed transactions is currently not safe in Bitcoin
With Segregated Witness, this is now safe.  With that expected soon, I'd
encourage you to take advantage of it.

@_date: 2016-05-10 14:58:19
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Indeed!  Sorry for the delayed feedback.
I did this for IBLT testing.
I used variable-length bit encodings, and used the shortest encoding
which is unique to you (including mempool).  It's a little more work,
but for an average node transmitting a block with 1300 txs and another
~3000 in the mempool, you expect about 12 bits per transaction.  IOW,
about 1/5 of your current size.  Critically, we might be able to fit in
two or three TCP packets.
The wire encoding of all those bit arrays was:
  [varint-min-numbits] - Shortest bit array length
  [varint-array-size]  - Number of bit arrays.
          [varint-num].... - Number of entries in array N (x varint-array-size)
  [packed-bit-arrays...]
  Last byte was padded with zeros.
  See: I would also avoid the nonce to save recalculating for each node, and
instead define an id as:
        [<64-bit-short-id>][txid]
Since you only ever send as many bits as needed to distinguish, this only
makes a difference if there actually are collisions.
As Peter R points out, we could later enhance receiver to brute force
collisions (you could speed that by sending a XOR of all the txids, but
really if there are more than a few collisions, give up).
And a prototype could just always send 64-bit ids to start.

@_date: 2016-05-11 06:53:55
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
That's a good question; I was assuming a best-case in which we have
mempool set reconciliation (handwave) thus know they are close.  But
there's also an alterior motive: any later more sophisticated approach
will want variable-length IDs, and I'd like Matt to do the work :)
In particular, you can significantly narrow the possibilities for a
block by sending the min-fee-per-kb and a list of "txs in my mempool
which didn't get in" and "txs which did despite not making the
fee-per-kb".  Those turn out to be tiny, and often make set
reconciliation trivial.  That's best done with variable-length IDs.
I'm not convinced on UDP; it always looks impressive, but then ends up
reimplementing TCP in practice.  We should be well within a TCP window
for these, so it's hard to see where we'd win.
"Greatly increase"?  I don't see that.
Let's assume an attacker grinds out 10,000 txs with 128 bits of the same
TXID, and gets them all in a block.  They then win the lottery and get a
collision.  Now we have to transmit ~48 bytes more than expected.
Not quite true, since if their mempools differ they'll use different
encoding lengths, but yes, you'll get less of this.
I'm not worried about the sender: The recipient needs to encode all the
Indeed, I would be adding extra bits in the sender and not implementing
brute force in the receiver.  But I welcome someone else to do so.

@_date: 2016-10-01 13:31:04
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
Prefer a three-arg version (gbits-to-compare, blocknum, hash):
- If  is 0 or > 256, invalid.
- If the hash length is not ( + 7) / 8, invalid.
- If the hash unused bits are not 0, invalid.
- Otherwise  of hash is compared to lower  of blockhash.
This version also lets you play gambling games on-chain!
Or maybe I've just put another nail in CBAH's coffin?

@_date: 2016-09-05 11:02:19
@_author: Rusty Russell 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
Johnson Lau via bitcoin-dev My current scripts use OP_IF and OP_NOTIF only after OP_EQUAL, except
for one place where they use OP_EQUAL ... OP_EQUAL... OP_ADD OP_IF
(where the two OP_EQUALs are comparing against different hashes, so only
0 or 1 of the two OP_EQUAL can return 1).
So there's no effect either way on the c-lightning implementation, at

@_date: 2017-06-01 10:58:20
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [RFC] Lightning invoice/payment format draft 
Hi all,
        There's a pull request for a lightning payment format which I'd
love wider review on.  It's bech32 encoded, with optional parts tagged:
        There's an implementation with a less formal description here, too:
                Send 2500 microbitcoin using payment hash 00010203040506...0102 to me
                within 1 minute from now (Tue 30 May 12:17:36 UTC 2017):
        lnbc2500u1qpvj6chqqqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqypqdq5xysxxatsyp3k7enxv4jsxqz8slk6hqew9z5kzxyk33ast248j3ykmu3wncvgtgk0ewf5c6qnhen45vr43fmtzsh02j6ns096tcpfga0yfykc79e5uw3gh5ltr96q00zqppy6lfy

@_date: 2017-05-15 10:44:13
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP: Block signal enforcement via tx fees 
Russell O'Connor via bitcoin-dev I have a similar proposal to Russell; use tx nVersion.  However, my
subset is simpler, and uses fewer precious nVersion bits:
1. Top version 26 bits must be 1 (say)
2. Next bit indicates positive (must have bit set) or negative (must NOT
   have bit set).
3. Bottom 5 bits refer to which BIP8/9 bit we're talking about.
This only allows specifying a single bit, and only support BIP8/9-style
I believe we can skip the timeout: miners don't signal 100% either way
anyway.  If a BIP is in LOCKIN, wallets shouldn't set positive on that
bit (this gives them two weeks).  Similarly, if a BIP is close to
FAILED, don't set positive on your tx.  Wallets shouldn't signal until
any bit until see some minimal chance it's accepted (eg. 1 in 20 blocks).
This is gentler on miners than a UASF flag day, and does offer some
harder-to-game signalling from bitcoin users.
False signalling miners still have the 2 week LOCKIN period to upgrade,
otherwise they can already lose money.  You could argue they're *more*
likely to upgrade with a signal that significant parts of the economy
have done so.

@_date: 2017-05-23 14:17:48
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
I do prefer the (2) approach, BTW, as it reuses existing primitives, but
I know "simpler" means a different thing to mathier brains :)
Since it wasn't explicit in the proposal, I think the txout information
placed in the hash here is worth discussing.
I prefer a simple txid||outnumber[1], because it allows simple validation
without knowing the UTXO set itself; even a lightweight node can assert
that UTXOhash for block N+1 is valid if the UTXOhash for block N is
valid (and vice versa!) given block N+1.  And miners can't really use
that even if they were to try not validating against UTXO (!) because
they need to know input amounts for fees (which are becoming
If I want to hand you the complete validatable UTXO set, I need to hand
you all the txs with any unspent output, and some bitfield to indicate
which ones are unspent.
OTOH, if you serialize more (eg. ...||amount||scriptPubKey ?), then the UTXO
set size needed to validate the utxohash is a little smaller: you need
to send the txid, but not the tx nVersion, nLocktime or inputs.  But in a
SegWit world, that's actually *bigger* AFAICT.
[1] I think you could actually use txid^outnumber, and if that's not a
    curve point SHA256() again, etc.  But I don't think that saves any
    real time, and may cause other issues.

@_date: 2017-05-24 13:56:56
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
Gregory Maxwell via bitcoin-dev Agreed, I would suggest 16th December, 2017 (otherwise, it should be
16th January 2018; during EOY holidays seems a bad idea).
This means this whole debacle has delayed segwit exactly 1 (2) month(s)
beyond what we'd have if it used BIP8 in the first place.

@_date: 2017-05-27 10:49:33
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
I was assuming it would be included in the next point release.

@_date: 2017-11-10 12:01:14
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Covenants through merkelized txids. 
Hi all,
        This is an alternative to jl2012's BIPZZZ (OP_PUSHTXDATA[1]).
It riffs on the (ab)use of OP_CHECKSIGFROMSTACK that Russell[2]
used to implement covenants[3].  I've been talking about it to random
people for a while, but haven't taken time to write it up.
The idea is to provide a OP_TXMERKLEVERIFY that compares the top stack
element against a merkle tree of the current tx constructed like so[4]:
        TXMERKLE = merkle(nVersion | nLockTime | fee, inputs & outputs)
        inputs & outputs = merkle(inputmerkle, outputmerkle)
        input = merkle(prevoutpoint, nSequence | inputAmount)
        output = merkle(nValue, scriptPubkey)
Many variants are possible, but if we have OP_CAT, this makes it fairly
easy to test a particular tx property.  A dedicated OP_MERKLE would make
it even easier, of course.
If we one day HF and add merklized TXIDs[5], we could also use this method
to inspect the tx *being spent* (which was what I was originally trying to
Thanks for reading!
[1] [2] Aka Dr. "Not Rusty" O'Connor.  Of course both of us in the same thread will
    probably break the internet.
[3] [4] You could put every element in a leaf, but that's less compact to
    use: cheaper to supply the missing parts with OP_CAT than add another level.
[5] Eg. use the high nVersion bit to say "make my txid a merkle".

@_date: 2018-04-03 09:16:45
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Signature bundles 
Hi all!
        Since there's activity on new signature types, I think it's
worth considering a more flexible alternative to
SIGHASH_SINGLE|SIGHASH_ANYONECANPAY.  See Usefulness for why.
Proposal: Two bits: SIGHASH_BUNDLESTART/SIGHASH_INBUNDLE
A signature needs to indicate that signs only part of a transaction's
inputs and outputs (a.k.a. a "bundle").  Bundles can be combined
together into larger transactions, and non-bundled signature inputs /
outputs appended.
Two per-tx counters are kept: bundle_inputs_used and
bundle_outputs_used, both starting at 0.
SIGHASH_BUNDLESTART indicates two var_int sit between the sighash flags
and the signature itself: the first is the number of inputs in this
bundle starting at bundle_inputs_used, the second is the number of
outputs starting at bundle_outputs_used.  bundle_inputs_used and
bundle_outputs_used have these values added, for next time.
SIGHASH_INBUNDLE indicates that this signature applies to the current
bundle.  The txCopy is reduced to cover only the inputs and
outputs in the current bundle, and the signature commits to the two
var_ints from SIGHASH_BUNDLESTART along with the sighash flags.
(A proper BIP would detail how any weird stuff makes the tx invalid:
take that as read).
You can use this to sign a transaction just like now, with only two
extra bytes and these SIGHASH flags.  But this transaction can now be
aggregated by pasting on another bundle, or attaching other normal
inputs and/or outputs.  You can aggregate as many transactions as you
want this way.
One of the issues we've struck with lightning is trying to guess future
fees for commitment transactions: we can't rely on getting another
signature from our counterparty to increase fees.  Nor can we use
parent-pays-for-child since the outputs we can spend are timelocked.
This "holding a valid tx but I want to add fees later without
re-signing" seems like a general problem.  The only current method would
be to engineer transactions as a single-input-single-output tx and use
SIGHASH_SINGLE|SIGHASH_ANYONECANPAY; this is very limiting.
The other obvious application would be to run public aggregators, which
would provide throughput promises ("if you send me a tx with feerate X,
I will make sure it goes onchain within a week").  This service would
sometimes profit, if it can do so cheaper than it quoted, and sometimes
have to add additional fees.  The existence of such services should
smooth the current fee cliff by allowing users and services to offer
"slow mode" payment options without requiring interaction.
Feedback welcome!

@_date: 2018-04-03 15:01:24
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Signature bundles 
This is a potential problem, yes :( And I'm not sure how to solve it,
unless you do some crazy thing like commit to a set of keys which are
allowed to bundle, which kind of defeats the generality of outsourcing.
Not without SIGHASH_NOINPUT, no.
Yeah, let's not do that.
What would it spend though?  Can't use an existing output, so this
really needs to be stashed in an *output script*, say a zero-amount
output which is literally a push of txids, and is itself unspendable.
        ... That's pretty large though, and it's non-witness data (though
discardable).  How about 'OP_NOP4  '?
Then the miner just bundles those tx all together?

@_date: 2018-12-04 14:03:53
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I like this idea.
Firstly, it's incentive-compatible[1]: assuming blocks are full, miners
should always take a higher feerate tx if that tx would be in the
current block and the replaced txs would not.[2]
Secondly, it reduces the problem that the current lightning proposal
adds to the UTXO set with two anyone-can-spend txs for 1000 satoshis,
which might be too small to cleanup later.  This rule would allow a
simple single P2WSH(OP_TRUE) output, or, with IsStandard changed,
a literal OP_TRUE.
Could be done client-side, right?  Do a quick check if this is above 250
satoshi per kweight but below minrelayfee, put it in a side-cache with a
60 second timeout sweep.  If something comes in which depends on it
which is above minrelayfee, then process them as a pair[3].
[1] Miners have generally been happy with Defaults Which Are Good For The
    Network, but I feel a long term development aim should to be reduce
    such cases to smaller and smaller corners.
[2] The actual condition is subtler, but this is a clear subset AFAICT.
[3] For Lightning, we don't care about child-pays-for-grandparent etc.

@_date: 2018-12-12 20:12:10
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I'm asking on-list because I'm sure I'm not the only confused one.
Having the SIGHASH_SCRIPTMASK flag is redundant AFAICT: why not always
perform mask-removal for signing?
If you're signing arbitrary scripts, you're surely in trouble already?
And I am struggling to understand the role of scriptmask in a taproot
world, where the alternate script is both hidden and general?
I look forward to learning what I missed!

@_date: 2018-12-13 10:19:02
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
OK, removing OP_MASKs unconditionally would introduce a hole without
some explicit flag to say they've been removed (the "real script" could
be something different with OP_MASKs).  We could have the signature
commit to the outputscript, but that's a bit meh.
This is *not* true of Eltoo; the script itself need not change for the
rebinding (Christian, did something change?).
So, can we find an example where OP_MASK is useful?
If I'm using SIGHASH_NOINPUT, I'm already required to take care with key
Without a concrete taproot proposal it's hard to make assertions, but
if the signature flags that it's using the taproot script, it's
no less safe, and more general AFAICT.

@_date: 2018-12-13 11:07:28
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
This is wrong, sorry.  I re-checked the paper, and the constant for the
timelock comparison changes on each new update.
(The alternative was a new opcode like OP_TIMELOCKGREATERVERIFY which
required remembering the nLocktime for the UTXO).
So now my opinion is closer to yours: what's the use for NOINPUT &&
And is it worthwhile doing the mask complexity, rather than just
removing the commitment to script with NOINPUT?  It *feels* safer to
restrict what scripts we can sign, but is it?
Note that NOINPUT is only useful when you can't just re-sign the tx, and
you need to be able to create a new tx even if this input is spent once
(an attacker can do this with SIGHASH_MASK or not!).  ie. any other
inputs need to be signed NOINPUT or this one
You already need both key-reuse and amount-reuse to be exploited.
SIGHASH_MASK only prevents you from reusing this input for a "normal"
output; if you used this key for multiple scripts of the same form,
you're vulnerable[1].  Which, given the lightning software will be using
the One True Script, is more likely that your normal wallet using the
same keys.
So I don't think it's worth it.  SIGHASH_NOINPUT is simply dangerous
with key-reuse, and Don't Do That.
[1] Attacker can basically clone channel state to another channel.

@_date: 2018-12-16 17:25:48
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
There are many complexities we could add, each of which would prevent
loss of funds in some theoretical case.
other things is not how people will lose funds[1].
It *is* however non-trivially more complicated for wallets; they
currently have a set of script templates which they will sign (ie. no
OP_CODESEPARATOR) and I implemented BIP 143 with only the simplest of
naive code[2].  In particular, there is no code to parse scripts.
Bitcoind developers are not in a good position to assess complexity
here.  They have to implement *everything*, so each increment seems
minor.  In addition, none of these new script versions will ever make
bitcoind simpler, since they have to support all prior ones.  Wallets,
however, do not have to.
I also think that minimal complexity for (future) wallets is an
underappreciated feature: the state of wallets in bitcoin is poor[3]
so simplicity should be a key goal.
[1] Reusing your revocation base point across two channels will lose
    funds in a much more trivial way, as will reusing payment hashes
    across invoices.
[2] In fact, I added SIGHASH_ANYONECANPAY and SIGHASH_SINGLE recently
    for Segwit and it worked first time!  Kudos to BIP143's authors for
    such a clear guide.
[3] Bitcoind's wallet can't restore from seed; this neatly demonstrates
    how hard the wallet problem is, but there are many others.
code, as modern wallets currently don't have to parse the scripts they
I'm telling you that this is not how people are losing funds.

@_date: 2018-12-17 13:40:42
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
My anti-complexity argument leads me to ask why we'd support
OP_CODESEPARATOR at all?  Though my argument is weaker here: no wallet
need support it.
But I don't see how OP_CODESEPARATOR changes anything here, wrt NOINPUT?
Remember, anyone can create an output which can be spent by any NOINPUT,
whether we go for OP_MASK or simply not commiting to the input script.

@_date: 2018-12-19 11:09:26
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
I think we're getting confused here.  I'm contributing my thoughts from
the lightning implementer's point of view; there are other important
considerations, but this is my contribution.
In *lightning* there are more ways to lose funds via secret reuse.
Meanwhile, both SIGHASH_NOINPUT and OP_MASK have the reuse-is-dangerous
property; with OP_MASK the danger is limited to reuse-on-the-same-script
(ie. if you use the same key for a non-lightning output and a lightning
output, you're safe with OP_MASK.  However, this is far less likely in
I state again: OP_MASK doesn't seem to gain *lightning* any significant
security benefit.
Our current transaction signing code is quite generic (and, if I may say
so, readable and elegant).  We could, of course, special case
GetMaskedScript() for the case we need (the Eltoo examples I've seen
have a single OP_MASK at the front, which makes it trivial).
The mailing list seems a bit backed up or something; I replied to that
in the hope you can clear my confusion on that one.
A would use the same words to encourage you to create the simplest
possible implementation?
I don't think we disagree on philosophy, just trade-offs.  And that's

@_date: 2018-12-21 09:47:15
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT 
Thanks, I hadn't seen this before!  That's also the first time I've seen
SIGHASH_NONE used.
Yes, I read this as proposed, it is clever.  Not sure we'd be
introducing it if OP_CODESEPARATOR didn't already exist, but at least
it's a simplfication.
My question is more fundamental.  If NOINPUT doesn't commit to the input
at all, no script, no code separator, nothing.  I'm struggling to
understand your original comment was "without signing the script or
masked script, OP_CODESEPARATOR becomes unusable or insecure with
I mean, non-useful, sure.  Its purpose is to alter signature behavior,
and from the script POV there's no signature with this form of NOINPUT.
But other than the already-established "I reused keys for multiple
outputs" oops, I don't see any new dangers?

@_date: 2018-01-09 21:52:18
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 117 Feedback 
I've just re-read BIP 117, and I'm concerned about its flexibility.  It
seems to be doing too much.
The use of altstack is awkward, and makes me query this entire approach.
I understand that CLEANSTACK painted us into a corner here :(
The simplest implementation of tail recursion would be a single blob: if
a single element is left on the altstack, pop and execute it.  That
seems trivial to specify.  The treatment of concatenation seems like
trying to run before we can walk.
Note that if we restrict this for a specific tx version, we can gain
experience first and get fancier later.
BIP 117 also drops SIGOP and opcode limits.  This requires more
justification, in particular, measurements and bounds on execution
times.  If this analysis has been done, I'm not aware of it.
We could restore statically analyzability by rules like so:
1.  Only applied for tx version 3 segwit txs.
2.  For version 3, top element of stack is counted for limits (perhaps
    with discount).
3.  The blob popped off for tail recursion must be identical to that top
    element of the stack (ie. the one counted above).
Again, future tx versions could drop such restrictions.

@_date: 2018-01-16 11:36:14
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP 117 Feedback 
BIP-141: "The script must not fail, and result in exactly a single TRUE
on the stack."  And it has long been non-standard for P2SH scripts to
not do the same (don't know exactly when).
The rule AFAICT is "standard transactions must still work".  This was
violated with low-S, but the transformation was arguably trivial.  OTOH, use of altstack is completely standard, though in practice it's
unused and so only a theoretical concern.
My concern remains unanswered: I want hard numbers on the worst-case
time taken by sigops with the limit removed.  It's about 120 usec per
sigop (from [1]), so how bad could it be?  I think Russell had an
estimate like 1 in 3 ops, so 160 seconds to validate a block?
[1]

@_date: 2018-07-03 14:26:53
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP sighash_noinput 
I agree with the DO_NOT_WANT-style naming.  REUSE_VULNERABLE seems to
capture it: the word VULNERABLE should scare people away (or at least
cause them to google further).

@_date: 2018-07-13 09:34:14
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev]  BIP sighash_noinput 
A wallet should *never* create a SIGHASH_NOINPUT to spend its own UTXOs.
SIGHASH_NOINPUT is useful for smart contracts which have unique
conditions, such as a pair of peers rotating keys according to an agreed
schedule (eg. lightning).

@_date: 2018-06-22 10:02:01
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] eltoo: A Simplified update 
Yes, RBF Rule  again :( It makes RBF unusable in adversarial
conditions, and it's not miner incentive-compatible.
The only mitigations I have been able to come up with are:
1. Reduce the RBF grouping depth to 2, not 10.  This doesn't help
   here though, since you can still have ~infinite fan-out of txs
   (create 1000 outputs, spend each with a 400ksipa tx).
2. Revert  to a simple "greater feerate" rule, but delay propagation
   proportional to tx weight, say 60 seconds (fuzzed) for a 400 ksipa
   tx.  That reduces your ability to spam the network (you can always
   connect directly to nodes and waste their time and bandwidth, but you
   can do that pretty much today).
Frankly, I'd also like a similar mechanism to not reject low-fee txs
(above 250 satoshi per ksipa) but simply not propagate them.  Drop them
after 60 seconds if there's no CPFP to increase their effective feerate.
That would allow us to use CPFP on lightning commitment txs today,
without having to guess what fees will be sometime in the future.

@_date: 2018-05-09 09:27:11
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Hi all,
        The largest problem we are having today with the lightning
protocol is trying to predict future fees.  Eltoo solves this elegantly,
but meanwhile we would like to include a 546 satoshi OP_TRUE output in
commitment transactions so that we use minimal fees and then use CPFP
(which can't be done at the moment due to CSV delays on outputs).
Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is
non-standard.  Are there any reasons not to suggest such a policy

@_date: 2018-05-10 11:38:43
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
I expect we will, but thougth I'd ask :)
I get annoyed when people say "We found this issue, but we worked around
it and so never bothered you with it!" for my projects :)

@_date: 2018-05-10 08:34:58
@_author: Rusty Russell 
@_subject: [bitcoin-dev] BIP sighash_noinput 
If I can convince you to sign with SIGHASH_NONE, it's already a problem
That was also suggested by Mark Friedenbach, but I think we'll end up
with more "magic key" a-la Schnorr/taproot/graftroot and less script in
That means we'd actually want a different Segwit version for
"NOINPUT-can-be-used", which seems super ugly.
In a world where SIGHASH_NONE didn't exist, this might be an argument :)

@_date: 2018-05-10 11:36:41
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
That won't propagate :(
No, that would change the TXID, which we rely on for HTLC transactions.
But in the long term we'll have Eltoo and SIGHASH_NOINPUT which both
allow different solutions.

@_date: 2018-05-17 12:14:53
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Your zero-val-OP_TRUE-can't-be-spent-after-same-block SF is interesting,
but if we want a SF just give us SIGHASH_NOINPUT and we'll not need this
at all (though others still might).  It's nicer than the previous
discussions on after-the-fact feebumping[1] though.
Meanwhile, our best mitigation against UTXO bloat is:
1. Make the fees as low as possible[2]
2. Put a CSV delay on the to-remote output (currently there's asymmetry)
3. Attach more value to the OP_TRUE output, say 1000 satoshi.
But turns out we probably don't want an OP_TRUE output nor P2SH, because
then the spending tx would be malleable.  So P2WSH is is.
This brings us another theoretical problem: someone could spend our
OP_TRUE with a low-fee non-RBF tx, and we'd not be able to use it to
CPFP the tx.  It'd be hard to do, but possible.  I think the network
benefits from using OP_TRUE (anyone can clean, and size, vs some
only-known-to-me pubkey) outweighs the risk, but it'd be nice if OP_TRUE
P2WSH spends were always considered RBF.
[1] [2] Because bitcoin core use legacy measurements, this is actually 253
satoshi per kilosipa for us, see

@_date: 2018-05-21 13:14:06
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
Marco points out that if the parent is RBF, this child inherits it, so
we're actually good here.
However, Matt Corallo points out that you can block RBF will a
large-but-lowball tx, as BIP 125 points out:
   will be replaced by a new transaction...:
   3. The replacement transaction pays an absolute fee of at least the sum
      paid by the original transactions.
I understand implementing a single mempool requires these kind of
up-front decisions on which tx is "better", but I wonder about the
consequences of dropping this heuristic?  Peter?

@_date: 2018-05-30 12:17:20
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
AFAICT the optimal DoS is where:
1.  Attacker sends a 100,000 vbyte tx 2.  Replaces it with a 108 vbyte tx  which spends one of
    those inputs.
3.  Replaces that spent input in the 100k tx and does it again.
It takes 3.5 seconds to propagate to 50% of network[1] (probably much worse
given 100k txs), so they can only do this about 86 times per block.
That means they send 86 * (100000 + 108) = 8609288 vbytes for a cost of
86 * 2 * 108 + 100000 / 2 = 68576 satoshi (assuming 50% chance 100k tx
gets mined).
That's a 125x cost over just sending 1sat/vbyte txs under optimal
conditions[2], but it doesn't really reach most low-bandwidth nodes
Given that this rule is against miner incentives (assuming mempool is
full), and makes things more complex than they need to be, I think
there's a strong argument for its removal.
[1] [2] Bandwidth overhead for just sending a 108-vbyte tx is about 160
    bytes, so our actual bandwidth per satoshi is closer to 60x
    even under optimal conditions.

@_date: 2018-05-31 12:17:58
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Making OP_TRUE standard? 
============================== START ==============================
This 50% chance assumption is wrong; it's almost 0% for a low enough
fee.  Thus the cost is only 18576, making the cost for the transactions
463x lower than just sending 1sat/vbyte txs under optimal conditions.
That's a bit ouch.[1]
I think a better solution is to address the DoS potential directly:
if a replacement doesn't meet  or  but *does* increase the feerate
by at least minrelayfee, processing should be delayed by 30-60 seconds.
That means that eventually you will RBF a larger tx, but it'll take
much longer.  Should be easy to implement, too, since similar timers
will be needed for dandelion.
[1] Christian grabbed some more detailed propagation stats for me: larger
    txs do propagate slower, but only by a factor of 2.5 or so.

@_date: 2019-02-13 14:52:39
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
[ Digging through old mail. ]
Doesn't really matter.  Lightning close algorithm would be:
1.  Give bitcoind unileratal close.
2.  Ask bitcoind what current expidited fee is (or survey your mempool).
3.  Give bitcoind child "push" tx at that total feerate.
4.  If next block doesn't contain unilateral close tx, goto 2.
In this case, if you allow a simpified RBF where 'you can replace if
1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3. old tx isnt',
it works.
It allows someone 100k of free tx spam, sure.  But it's simple.
We could further restrict it by marking the unilateral close somehow to
say "gonna be pushed" and further limiting the child tx weight (say,
5kSipa?) in that case.

@_date: 2019-01-08 16:20:20
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction 
I don't think it's different, in practice.
I was defining "top of mempool" as "in the first 4 MSipa", ie. next
block, and assumed you'd only allow RBF if the old package wasn't in the
top and the replacement would be.  That seems incentive compatible; more
than the current scheme?
The attack against this is to make a 100k package which would just get
into this "top", then push it out with a separate tx at slightly higher
fee, then repeat.  Of course, timing makes that hard to get right, and
you're paying real fees for it too.
Sure, an attacker can make you pay next-block high fees, but it's still
better than our current "*always* overpay and hope!", and you can always
decide at the time based on whether the expiring HTLC(s) are worth it.
But I think whatever's simplest to implement should win, and I'm not in
a position to judge that accurately.

@_date: 2019-06-02 14:11:39
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
Hi all,
        I want to propose a modification to rules 3, 4 and 5 of BIP 125:
To remind you of BIP 125:
 3. The replacement transaction pays an absolute fee of at least the sum
    paid by the original transactions.
 4. The replacement transaction must also pay for its own bandwidth at
    or above the rate set by the node's minimum relay fee setting.
 5. The number of original transactions to be replaced and their
    descendant transactions which will be evicted from the mempool must not
    exceed a total of 100 transactions.
The new "emergency RBF" rule:
 6. If the original transaction was not in the first 4,000,000 weight
    units of the fee-ordered mempool and the replacement transaction is,
    rules 3, 4 and 5 do not apply.
This means:
1. RBF can be used in adversarial conditions, such as lightning
   unilateral closes where the adversary has another valid transaction
   and can use it to block yours.  This is a problem when we allow
   differential fees between the two current lightning transactions
   (aka "Bring Your Own Fees").
2. RBF can be used without knowing about miner's mempools, or that the
   above problem is occurring.  One simply gets close to the required
   maximum height for lightning timeout, and bids to get into the next
   block.
3. This proposal does not open any significant new ability to RBF spam,
   since it can (usually) only be used once.  IIUC bitcoind won't
   accept more that 100 descendents of an unconfirmed tx anyway.
4. This proposal makes RBF miner-incentive compatible.  Currently the
   protocol tells miners they shouldn't accept the highest bidding tx
   for the good of the network.  This conflict is particularly sharp
   in the case where the replacement tx would be immediately minable,
   which this proposal addresses.
Unfortunately I haven't found time to code this up in bitcoin, but if
there's positive response I can try.
Thanks for reading!

@_date: 2019-06-06 12:38:50
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
Good point.  This will cost Alice approximately one tx every block, but
that may still be annoying.  My intuition says it's hard to play these
games across swathes of non-direct peers, since mempools are in constant
flux and propagation is a bit random.
What mitigations were you thinking?

@_date: 2019-06-06 14:46:54
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
This is a good question, which is why I really wanted to look into the
implementation details.  There are some approximations possible wrt. pre-
and post- tx bundle feerate, but they have to be examined closely.
I *think* you can currently create a tx at 1 sat/byte, have it
propagate, then RBF it to 2 sat/byte, 3... and do that a few thousand
times before your transaction gets mined.
If that's true, I don't think this proposal makes it worse.
I thought we still meet rule 5 in practice since bitcoind will never
even accept a tree of unconfirmed txs which is > 100 txs?  That would
still stand, it's just that we'd still consider a replacement.
The bitcoin network offers no propagation guarantees; it's all best
effort anyway.  This makes it no worse, and we can tunnel txs through
the lightning network if we have to.
If miners have a conflicting tx in the top 4MSipa, you don't have a
problem.  So an attacker needs to limit propagation in a way which
isolates the miners from either the new tx or the conflicting one, which
is much harder.
Define client-side here?
I'd say from the lightning side it's as simple as a normal RBF policy
until you get within a few blocks of a deadline, then you increase the
fees until it's well within reach of the next block.  You can even
approximate this by looking at fees on the previous block, with some
care for outliers.
I like that it's conceptually simple and inventive-robust, and doesn't
really rely on bitcoind's internal policy mechanics of RBF.
I think in the longer term we're going to need other mechanisms for
restricting abusive propagation anyway, but that's a bit out-of-scope.

@_date: 2019-06-14 15:20:17
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [PROPOSAL] Emergency RBF (BIP 125) 
[ Snip ]
Indeed :(
To be fair, if I have a transaction of median size (250 bytes) and I use
the current estimatefee 2 of '0.00068325' I get to replace is 68 times;
that's $0 for an additional 1GB across all nodes.
So, I don't think the current rules are sufficient.  But I understand
the desire not to make things worse.  I'll roll in some changes and
As the deadline approaches, a lightning wallet would RBF with increasing
desparation until it gets into a block.  It doesn't really matter *why*
the tx isn't going through, there's nothing else it can do.
I think you mean any proposal which relies on a deadline?  If so, that
bus has already left.
When you see a block you can guess the fees required for the next block.
You need some smoothing to avoid wild spikes, but in practice you can
start this "desperation mode" 10 blocks before your deadline.
Without RBF changes, it needs to assume that it needs to replace a
400kSipa tx @ feerate-for-next-block.  With some RBF change, it need
only replace I don't understand this at all, sorry.

@_date: 2019-03-20 10:52:05
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
AJ, this was a meandering random walk which shed very little light.
I don't find the differentiation between malicious and non-malicious
double-spends convincing.  Even if you trust A, you already have to
worry about person-who-sent-the-coins-to-A.  This expands that set to be
"miner who mined coins sent-to-A", but it's very hard to see what
difference that makes to how you'd handle coins from A.
... followed by two paragraphs describing how it's not a "fundamental
way to cause problems" that you (or I) can see.
As we've never seen with SIGHASH_NONE?
I don't find this remotely credible.
In theory, sure.  But not feel-good and complex "safety measures" which
don't actually help in practical failure scenarios.
If this is considered necessary, can it be a standardness rule rather
than consensus?

@_date: 2019-03-20 14:03:55
@_author: Rusty Russell 
@_subject: [bitcoin-dev] [Lightning-dev] More thoughts on NOINPUT safety 
Sorry AJ, my prior email was not constructive :(
I consider the "my software reused my keys" the most reasonable attack
scenario, though still small compared to other lightning attack surfaces.
But I understand the general wariness of third-parties reusing
SIGHASH_NOINPUT signatures.
Since "must have a non-SIGHASH_NOINPUT" rule addresses the first reuse
scenario (as well as the second), I'd be content with that proposal.
Future segwit versions may choose to relax it.[1]
[1] Must be consensus, not standardness; my prev suggestion was bogus.

@_date: 2019-05-22 12:17:31
@_author: Rusty Russell 
@_subject: [bitcoin-dev] SIGHASH_ANYPREVOUT proposal 
I really like this proposal, and am impressed with how cleanly it
separated from taproot/tapscript.
I believe the chaparone signature requirement should be eliminated: I am
aware of four suggested benefits, which I don't believe are addressed
adaquately enough by chaparones to justify enshrining this complexity
into the protocol.
1. "These features could be used dangerously, and chaparone signatures make
   them harder to use, thus less likely to be adopted by random wallet
   authors."
   This change is already hard to implement, even once you're on v1
   segwit; you can't just use it with existing outputs.  I prefer to
   change the bip introduction to expliclty shout "THESE SIGNATURE
   HASHES ARE UNSAFE FOR NORMAL WALLET USAGE.", and maybe rename it
   SIGHASH_UNSAFE_ANYPREVOUT.
2. "Accidental key reuse can make this unsafe."
   This is true, but chaparones don't seem to help.  The main purpose of
   ANYPREV is where you can't re-sign; in particular, in lightning you
   are co-signing with an untrusted party up-front.  So you have to
   share the chaparone privkeys with one untrusted party.
   The BIP says "SHOULD limit the distribution of those private keys".
   That seems ridiculously optimistic: don't tell the secret to more
   than *one* untrusted party?
   In fact, lightning will also need to hand chaparone keys to
   watchtowers, so we'll probably end up using some fixed known secret.
3. "Miners can reorg and invalidate downstream txs".
   There's a principle (ISTR reading it years ago from Greg Maxwell?)
   that if no spender is malicious, a transaction should generally not
   become invalid.  With ANYPREV, a miner could reattach a transaction
   during a reorg, changing its txid and invalidating normal spends from
   it.
   In practice, I believe this principle will remain just as generally
   true with ANYPREV:
   1. For lightning the locktime will be fairly high before these txs are
      generally spendable.
   2. Doing this would require special software, since I don't see bitcoin
      core indexing outputs to enable this kind of rewriting.
   3. We already added such a common possibility with RBF, but before I
      brought it up I don't believe anyone realized.  We certainly
      haven't seen any problems in practice, for similar practical
      reasons.
4. "Rebinding is a new power in bitcoin, and it makes me uncomfortable".
   I have a degree of sympathy for this view, but objections must be
   backed in facts.  If we don't understand it well enough, we should
   not do it.  If we do understand it, we should be able to point out
   real problems.
Finally, it seems to me that chaparones can be opt-in, and don't need to
burden the protocol.

@_date: 2019-05-27 10:56:01
@_author: Rusty Russell 
@_subject: [bitcoin-dev] SIGHASH_ANYPREVOUT proposal 
The DO_NOT_WANT naming is to prevent people who *don't* want to use it
from using it because it's the "new hotness".
It cannot both be powerful enough to do what we need (rebinding) and
safe enough for general use (no rebinding).
I disagree?  Paying to share with an untrusted party is *insecure*
without further, complex arrangements.  Those arrangements (already a
requirement for lightning) worry me far more than the bitcoin-level
rebinding, TBH.
Lightning relies on  not   I don't know of any use for  in fact;
in practice if you have control of keys you can generally sign a new tx,
not requiring ANYPREVOUT.  If you're trying to blindly spend a tx which
may be RBF'd, ANYPREVOUT won't generally help you (amount changes).
Yes, I think our primary concern is risk to non-ANYPREVOUT using txs.
That would make ANYPREVOUT a bad idea, but seems we're concluding that's
not the case.
Secondary, is the accidentally-using-ANYPREVOUT scenario, which I
consider unlikely (like accidentally-using-SIGHASHNONE), especially
since you need to actually mark your keys now, so you can't do it
post-hoc to existing outputs.
Final concern is the using-correctly-but-nasty-gotchas.  This seems to
be inherent in rebinding, and is fully addressed by Don't Reuse
Addresses.  That is already a requirement for lightning (reusing
revocation keys is fatal).  Others reusing your addresses is already a
thing we have to deal with in bitcoin (Enjoy/Sochi).

@_date: 2019-09-27 11:38:27
@_author: Rusty Russell 
@_subject: [bitcoin-dev] New BIP for p2p messages/state enabling 
Hi Gleb,
        Minor feedback on reading the draft:
At risk of quoting myself[1]: data doesn't have requirements.  Actors do.
In this case, I assume you mean "writers must set this to 1".  What do
readers do if it's not?
You describe how to calculate q (as a floating point value), but not how
to encode it?
"*a* set..." or is it "two sets" (if you include the snapshot?).
And " *for* every peer" (maybe "which supports tx reconciliation?")
Remove "To the best of our knowledge, ": that makes it sound like it's
up for debate.  I've implemented and experimented with IBLT, and it's
[1]

@_date: 2020-04-28 06:56:19
@_author: Rusty Russell 
@_subject: [bitcoin-dev] RBF Pinning with Counterparties and Competing 
To be fair, if the feerate you want is 100x the minimum permitted, you
can always use 100x as much bandwidth as necessary without extra cost.
If everyone (or some major tx producers) were to do that, it would suck.
To fix this properly, you really need to agressively delay processing
(thus propagation) of transactions which aren't likely to be in the next
(few?) blocks.  This is a more miner incentive compatible scheme.
However, I realize this is a complete rewrite of bitcoind's logic, and
I'm not volunteering to do it!

@_date: 2020-10-08 10:51:10
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
Hi all,
        I propose an alternative to length restrictions suggested by
Russell in  use the
 variant,
unless the first byte is 0.
Here's a summary of each proposal:
Length restrictions (future segwits must be 10, 13, 16, 20, 23, 26, 29,
32, 36, or 40 bytes)
  1. Backwards compatible for v1 etc; old code it still works.
  2. Restricts future segwit versions, may require new encoding if we
     want a diff length (or waste chainspace if we need to have a padded
     version for compat).
Checksum change based on first byte:
  1. Backwards incompatible for v1 etc; only succeeds 1 in a billion.
  2. Weakens guarantees against typos in first two data-part letters to
     1 in a billion.[1]
I prefer the second because it forces upgrades, since it breaks so
clearly.  And unfortunately we do need to upgrade, because the length
extension bug means it's unwise to accept non-v0 addresses.
(Note non-v0 segwit didn't relay before v0.19.0 anyway, so many places
may already be restricting to v0 segwit).
The sooner a decision is reached on this, the sooner we can begin
upgrading software for a taproot world.
PS. Lightning uses bech32 over longer lengths, but the checksum is less critical; we'd prefer to follow whatever
bitcoin chooses.
[1] Technically less for non-v0: you have a 1 in 8 chance of a typo in the second letter changing the checksum
     algorithm, so it's 1 in 8 billion.

@_date: 2020-10-15 12:10:30
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
If we go for option 2, v1 (generated from bitcoin core) will simply fail
the first time you try test it.  So it will force an upgrade.  There
are fewer places generating addresses than accepting them, so this
seems the most likely scenario.
OTOH, with option 1, anyone accepting v1 addresses today is going to
become a liability once v1 addresses start being generated.
Yes, I too wish we weren't here.  :(
Deferring a hard decision is not useful unless we expect things to be
easier in future, and I only see it getting harder as time passes and
userbases grow.
The good news it that the change is fairly simple and the reference
implementations are widely used so change is not actually that hard
once the decision is made.
If we are prepared to commit to restrictions on future addresses.
We don't know enough to do that, however, so I'm reluctant; I worry that
a future scheme where we could save (e.g.) 2 bytes will impractical due
to our encoding restrictions, resulting in unnecessary onchain bloat.

@_date: 2020-10-19 11:19:17
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
If software supports v1 today and doesn't get upgraded, and we persue
option 1 then a trailing typo can make trouble.  Not directly lose money
(since the tx won't get propagated), but for most systems (e.g. hosted
wallets) someone has to go in and figure out the error and fix it up.
Option 2 means they're likely to fix their systems the first time
someone tries a v1 send, not the first time someone makes a trailing
typo (which might be years).
Anecdata: c-lightning doesn't allow withdraw to segwit > 0.  It seems
that the contributor John Barboza (CC'd) assumed that future versions
should be invalid:
Hmm, I'd rather cleanly break zombie infra, since they're exactly the
kind that won't/cant fix the case where someone trailing-typos?
Hmm, good point.  These can all be done with version bumps.
The only use for extra bytes I can see is per-UTXO flags, but even then
I can't see why you'd need to know them until their spent (in which case
you stash the flag in the input, not the output).
And fewer bytes seems bad for fungibility, since multisig would be
But the future keeps surprising me, so I'm still hesitant.
This *seems* to leave the option of later removing size restrictions,
but I think this is an illusion.  Upgrading will only get harder over
time: we would instead opt for some kind of backward compatiblity hack
(i.e. 33 byte address, but you can optionally add 3 zero pad bytes)
which *will* have consensus effect.
OK, time to waste some money!
Can you provide a mainnet v1 address, and I'll try to spam it from as
many things as I can find.  If we're really lucky, you can collect it
post-fork and donate it to charity.  Or a miner can steal it pre-fork :)

@_date: 2020-10-20 11:12:06
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
Here are my initial results:
c-lightning: "Could not parse destination address, destination should be a valid address"
Phoenix: "Invalid data.  Please try again."
Green: ef1662fd2eb736612afb1b60e3efabfdf700b1c4822733d9dbe1bfee607a5b9b
blockchain.info: 64b0fcb22d57b3c920fee1a97b9facec5b128d9c895a49c7d321292fb4156c21
Will keep exploring (and others are welcome to try too!)

@_date: 2020-10-20 14:01:45
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
PEBKAC.  Pasted wrong address.  Here are correct results:
c-lightning: "Could not parse destination address, destination should be a valid address"
Phoenix: "Invalid data.  Please try again."
blockchain.info: "Your Bitcoin transaction failed to send. Please try again."
Green: 9e4ab6617a2983439181a304f0b4647b63f51af08fdd84b0676221beb71a8f21

@_date: 2020-10-21 15:09:32
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
Ah, sorry!
Yes, I mentioned it here because I've found this to be a common
misconception; the *idea* was that application's segwit code would not
have to be reworked for future upgrades, but that information propagated
(Just as well, because of overly strict standardness rules, the overflow
bug, and now the proposed validation changes, turns out this lack of
forward compat is a feature!)

@_date: 2020-10-21 15:21:34
@_author: Rusty Russell 
@_subject: [bitcoin-dev] Progress on bech32 for future Segwit Versions 
Mike Schmidt via bitcoin-dev Thanks!  I am a little disappointed that I won't get to ask Bitcoin
Twitter to send tips to Pieter[1] though...
I would like to hear from the services who currently support v1+ (who
thus *would* have to change their software) whether they have a
technical preference for option 1 or 2.
[1] Or just maybe, tips to some random miner...
