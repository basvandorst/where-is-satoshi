
@_date: 2015-08-03 23:40:17
@_author: Peter R 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block Size 
Dear Bitcoin-Dev Mailing list,
I?d like to share a research paper I?ve recently completed titled ?A Transaction Fee Market Exists Without a Block Size Limit.?  In addition to presenting some useful charts such as the cost to produce large spam blocks, I think the paper convincingly demonstrates that, due to the orphaning cost, a block size limit is not necessary to ensure a functioning fee market.  The paper does not argue that a block size limit is unnecessary in general, and in fact brings up questions related to mining cartels and the size of the UTXO set.   It can be downloaded in PDF format here:
Or viewed with a web-browser here:
Abstract.  This paper shows how a rational Bitcoin miner should select transactions from his node?s mempool, when creating a new block, in order to maximize his profit in the absence of a block size limit. To show this, the paper introduces the block space supply curve and the mempool demand curve.  The former describes the cost for a miner to supply block space by accounting for orphaning risk.  The latter represents the fees offered by the transactions in mempool, and is expressed versus the minimum block size required to claim a given portion of the fees.  The paper explains how the supply and demand curves from classical economics are related to the derivatives of these two curves, and proves that producing the quantity of block space indicated by their intersection point maximizes the miner?s profit.  The paper then shows that an unhealthy fee market?where miners are incentivized to produce arbitrarily large blocks?cannot exist since it requires communicating information at an arbitrarily fast rate.  The paper concludes by considering the conditions under which a rational miner would produce big, small or empty blocks, and by estimating the cost of a spam attack.  Best regards,

@_date: 2015-08-05 03:26:19
@_author: Peter R 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
Thank you for the feedback, Benjamin.
I define what I mean by healthy, unhealthy, and non-existent markets in Section 7, and I show a figure to illustrate the supply and demand curves in each of these three cases.  A healthy market is defined as one where a rational miner would be incentivized to produce a finite block.  An unhealthy market is one where a miner would be incentivized to produce an arbitrarily large block.  A non-existant market is one where a miner is better off publishing an empty block.  I show that so long as block space in a normal economic commodity that obeys the Law of Demand, and that the Shannon-Hartley theorem applies to the communication of the block solutions between miners, that an unhealthy market is not possible.  Take a look at my definitions for the mempool demand curve (Sec 4) and the block space supply curve (Sec 5).  I show that the miner's profit is a maximum at the point where the derivatives of these two curves intersect.  I think of this as when "demand and supply are matched."
Do you not find the definitions presented in the paper for these curves useful?  The mempool demand curve represents the empirical demand measureable from a miner?s mempool, while the block space supply curve represents the additional cost to create a block of size Q by accounting for orphaning risk.  Supply and demand do react.  For example, if the cost to produce block space decreases (e.g., due to improvements in network interconnectivity) then a miner will be able to profitably include a greater number of transactions in his block.  Furthermore, not only is there a minimum fee density below which no rational miner should include any transactions as Gavin observed, but the required fee density for inclusion also naturally increases if demand for space within a block is elevated.  A rational miner will not necessarily include all fee-paying transactions, as urgent higher-paying transactions bump lower-fee transactions out, thereby bidding up the minimum fee density exponentially with demand.
Agreed.  Best regards,

@_date: 2015-08-05 15:15:43
@_author: Peter R 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
Hi Dave,
Thank you for the feedback regarding my paper.  With the benefit of hindsight, I think the paper would be stronger if it also analyzed how the model changes (or doesn't) if we assume zero propagation impedance for intra-miner communication, as you suggested (the "you don't orphan your own blocks" idea).  Note that the paper did briefly discuss miner-dependent propagation times in the second paragraph of page 9 and in note 13.  Agreed.  In this case there's no information to communicate (since the miner has no peers) and so the Shannon-Hartley limit doesn't apply.  My model makes no attempt to explain this case.  I'd like to explore this in more detail.  Although a miner may not orphan his own block, by building on his own block he may now orphan two blocks in a row.  At some point, his solution or solutions must be communicated to his peers.  And if there's information about the transactions in his blocks to communicate, I think there's a cost associated with that.  It's an interesting problem and I'd like to continue working on it.  It will be interesting to see.  I suspect that the main result that "a healthy fee market exists" will still hold (assuming of course that a single miner with >50% of the hash power isn't acting maliciously).  Whether miners with larger value of h/H have a profit advantage, I'm not sure (but that was outside the scope of the paper anyways).  Best regards,

@_date: 2015-08-07 11:36:32
@_author: Peter R 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Yes, I see this as correct as well.  If demand for space within a particular block is elevated (e.g., when the network has not found a block for 30 minutes), the minimum fee density for inclusion will be greater than the minimum fee density when demand for space is low (e.g., when the network has found several blocks in quick succession, as Gavin pointed out).  Lower-fee paying transaction will just wait to be included at one of the network lulls where a bunch of blocks were found quickly in a row.  The feemarket.pdf paper (  ) shows that this will always be the case so long as the block space supply curve (i.e., the cost in BTC/byte to supply additional space within a block [rho]) is a monotonically-increasing function of the block size (refer to Fig. 6 and Table 1).  The curve will satisfy this condition provided the propagation time for block solutions grows faster than log Q where Q is the size of the block.  Assuming that block solutions are propagated across physical channels, and that the quantity of pure information communicated per solution is proportional to the amount of information contained within the block, then the communication time will always grow asymptotically like O(Q) as per the Shannon-Hartely theorem, and the fee market will be healthy.  Best regards,

@_date: 2015-08-29 16:17:12
@_author: Peter R 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
Hello Matt and Daniele,
The effect of block compression protocols is included.  This is what I call the "coding gain" and use the Greek letter "gamma" to represent. As long as the block solution announcements contain information (i.e., Shannon Entropy) about the transactions included in a block, then the fee market will be "healthy" according to the definitions given in the linked paper (see below).  This is the case right now, this is the case with your relay network, and this would be the case using any implementation of IBLTs that I can imagine, so long as miners can still construct blocks according to their own volition.  The "healthy fee market" result follows from the Shannon-Hartley theorem; the SH-theorem describes the maximum rate at which information (Shannon Entropy) can be transmitted over a physical communication channel.    I've exchanged emails with Greg Maxwell about (what IMO is) an academic scenario where the block solutions announcements contain no information at all about the transactions included in the blocks.  Although the fee market would not be healthy in such a scenario, it is my feeling that this also requires miners to relinquish their ability to construct blocks according to their own volition (i.e., the system would already be centralized).  I look forward to a white paper demonstrating otherwise!
Best regards,

@_date: 2015-08-29 18:59:58
@_author: Peter R 
@_subject: [bitcoin-dev] Fwd: Your Gmaxwell exchange 
Dear Greg,
I am moving our conversation into public as I've recently learned that you've been forwarding our private email conversation verbatim without my permission [I received permission from dpinna to share the email that proves this fact: The proof is not "problematic."  Right now you're providing an example of what Mike Hearn refers to as "black-and-white" thinking.  Just because the proof makes simplifying assumptions, doesn't mean it's not useful in helping us to understand the dynamics of the transaction fee market.  Proofs about physical systems need to make simplifying assumptions because the physical world is messy (unlike the world of pure math).  My proof assumes very reasonably that block solutions contain information (i.e., Shannon Entropy) about the transactions included in a block.  As long as this is true, and as long as miners act rationally to maximize their profit, then the fee market will remain "healthy" according to the definitions given in my paper.  This is the case right now, this is the case with the Relay Network, and this would be the case using any implementation of IBLTs that I can imagine, so long as miners retain the ability to construct blocks according to their own volition.  The "healthy fee market" result follows from the Shannon-Hartley theorem; the SH-theorem describes the maximum rate at which information (Shannon Entropy) can be transmitted over a physical communication channel.   You are imagining an academic scenario (to use your own words: "perhaps of little practical relevance") where all of the block solutions announcements contain no information at all about the transactions included in the blocks.  Although I agree that the fee market would not be healthy in such a scenario, it is my feeling that this also requires miners to relinquish their ability to construct blocks according to their own volition (i.e., the system would already be centralized).  I look forward to reading a white paper where you show:
(a) Under what assumptions/requirements such a communication scheme is physically possible.
(b) That such a configuration is not equivalent to a single entity[1] controlling >50% of the hash power.
(c) That the network moving into such a configuration is plausible.
Lastly, I'd like to conclude by saying that we are all here trying to learn about this new amazing thing called Bitcoin.  Please go ahead and write a paper that shows under what network configuration my results don't hold.  I'd love to read it!  This is how we make progress in science!!
Sincerely, [1] For example, if--in order to achieve such a configuration with infinite coding gain--miners can no longer choose how to structure their blocks according to their own volition, then I would classify those miners as slaves rather than as peers, and the network as already centralized.
Link to forwarded email pastebin:

@_date: 2015-08-29 19:49:23
@_author: Peter R 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
Hello Matt,
The results of my paper hold unless the information about the included transactions is identically zero.  If this information is extremely small, then I agree that something other than the orphan cost would drive the fee market. I don't think this will ever by the case, however.
In this paragraph, you are talking about intra-miner communication (you refer to other nodes "of theirs").   I agree that this can be very fast because there's only one entity involved. I look forward to hearing your talk about the Relay Network.  Let's image there was no block size limit.  Using the Relay Network as it currently operates now, how big would the block solution announcement be:
(1) if a miner published a 10 MB block where 100% of the transactions are known by the other nodes?  (2) if a miner published a 10 MB block where 90% of the transactions are known by the other nodes?  (3) if a miner published a 100 MB spam block where 0% of the transactions are known by the other nodes?
Best regards,

@_date: 2015-08-29 20:08:32
@_author: Peter R 
@_subject: [bitcoin-dev] On the Nature of Miner Advantages in Uncapped 
If all the mining nodes are in one data center, and if all the nodes are programmed to build blocks in essentially the same way, then I would agree that the orphan cost would be negligible!  I will add this as an example of a network configuration where the results of my paper would be less relevant.  Peter

@_date: 2015-08-29 21:13:43
@_author: Peter R 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
Hi Greg,
I agree that miners may change their level of centralization.  This neither affects the model nor the results presented in the paper.
No I don't.  I assume the inflation rate is R/T, where R is a variable.  The last paragraph of the conclusion speaks to the paradox of what happens when R -> 0 as an area for future research. I assume, very reasonably, that the block solutions contain information about the transactions included in the block.  This is the case today, this is the case using the relay network, and this would be the case using any compression scheme I can personally imagine actually occurring in the future.  (I've always agreed that if block solutions can be communicated to all of the hash power in such a way that they don't include any information about the transactions they contain, then the conditions for a healthy fee market would not be met [this would be gamma --> infinity in my model]). As would I.  I appreciated the discussion we were having and I thought we had come to some kind of an understanding.  I acknowledged that when I made the other corrections to my paper that I would further clarify the assumptions (I agreed that the presentation could be improved).      What was not courteous was that you forwarded the entire private email chain to other people without my permission.
I stand by all of these four points.  My paper wasn't perfectly presented.  Making these clarifications will strengthen the manuscript by showing how strong the claim "a healthy transaction fee market exists without a block size limit" is.  My public comments have been factual.  I've even gone out of my way in several public threads to point out your objection that the coding gain could be zero (even though I think it is flawed "black-and-white thinking" about an academic scenario that will never unfold and might actually be physically impossible without Bitcoin already being centralized).
I'll end by saying that I am the one describing things as the presently are.  You are talking about a hypothetical future that may or may not exist (and may not even be possible).  The results of my paper logically follow from the assumptions made. You think the assumption that "block solutions contain information about the transactions included in the block" will not hold in the future.  Can you show:
(a) Under what assumptions/requirements your communication scheme is physically possible.
(b) That such a configuration is not equivalent to a single entity[1] controlling >50% of the hash power.
(c) That the network moving into such a configuration is plausible.
Best regards,

@_date: 2015-08-30 00:41:16
@_author: Peter R 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
Hi Greg,
The paper makes no claims about "miners changing their level of centralization."  Whether it is or is not significant, it is outside the scope of the paper and irrelevant to the result that a fee market exists without a block size limit (given the assumption that some information about the transactions included in a block is communicated between miners during block solution propagation).      Again, what you are calling "errors" are assumptions--in fact very reasonable assumptions that we know are valid for the network as a whole right now.  You are proposing that maybe they won't be valid in the future if the miners move to some hypothetical configuration that you worry may be possible.  I say prove it: show rigorously what the network would look like in a case where information about the transactions contained in a block is never communicated with the block solution.  How do you get the miners to agree (assuming such a configuration is even physically possible)?  Do you need all the miners to agree or only a portion of them?  How reasonable is it that the systems moves into such a configuration?  I know you spoke to this in your last response, but I mean really prove it...with a mathematical model, with diagrams and charts, with equations, with your assumptions clearly stated so that they can be put to the test--not by waving your hands on the bitcoin-dev mailing list.    This is an exciting field of research and we should encourage people to continue to explore these questions.   OK, I actually sort of agree with your very last sentence above.  My paper indeed suggests that the most "profitable" configuration is one where there is only one "super pool." Is that likely to happen?  I personally don't think so because of the game theory involved.  But regardless of my opinion or your opinion on that matter, whether it is or isn't likely to happen is outside the scope of my paper.  It is irrelevant to the result that a fee market exists without a block size limit given the assumption that there is more than a single miner.
I do call them out.  Furthermore, I've already agreed with you to further clarify these assumptions when I make the other corrections to the paper.  Doing so will make the paper stronger.  I'm not going to address the remainder of your email because I believe that we are now talking past each other.  I do appreciate the interest and attention that you've paid to my work on the Transaction Fee Market, and I also recognize the important contributions you've made such as coinjoin and HD wallets.  Best regards,

@_date: 2015-08-30 10:00:37
@_author: Peter R 
@_subject: [bitcoin-dev]  Unlimited Max Blocksize (reprise) 
Hello Tom, Daniele --
Thank you Tom for pointing out the knapsack problem to all of us.  I will include a note about it when I make the other corrections to the Fee Market paper.
I agree with what Daniele said previously.  The other "non-greedy" solutions to the knapsack problem are most relevant when one is choosing from a smaller number of items where certain items have a size similar to the size of the knapsack itself.  For example, these other solutions would be useful if transactions were often 50 kB, 100 kB, 400 kB in size, and we were trying to find the optimal TX set to fit into a 1 MB block.  However, since the average transaction size is ~500 bytes, even with a block size limit of 1 MB, we are looking at up to 2000 transactions.  The quantization effects become small.  Rather than 22 triangles as shown in Fig. 3 ( there are hundreds or a few thousands, and we can reasonably approximate the discrete set of points as a continuous curve.  Like Daniele pointed out, the greedy algorithm assumed in the paper is asymptotically optimal in such a case.
Best regards,

@_date: 2015-08-30 13:08:58
@_author: Peter R 
@_subject: [bitcoin-dev] "A Transaction Fee Market Exists Without a Block 
Hi Daniele,
I don't think there is any contention over the idea that miners that control a larger percentage of the hash rate, h / H, have a profitability advantage if you hold all the other variables of the miner's profit equation constant.  I think this is important: it is a centralizing factor similar to other economies of scale.  However, that is outside the scope of the result that an individual miner's profit per block is always maximized at a finite block size Q* if Shannon Entropy about each transaction is communicated during the block solution announcement.  This result is important because it explains how a minimum fee density exists and it shows how miners cannot create enormous spam blocks for "no cost," for example.  Best regards,

@_date: 2015-08-31 16:32:55
@_author: Peter R 
@_subject: [bitcoin-dev] Your Gmaxwell exchange 
I agree.  What about decentralization in development?  Gavin recently said that he wants to "get to the point where there will be multiple robust implementations of the core protocol."
When I look at this image ( illustrating centralization in nodes, mining and development, the biggest source of concern for me is the 85% node share around Bitcoin Core.  With this level of centralization, it may be possible in the future for a group of coders to prevent important changes from being made in a timely fashion (e.g., should their interests no longer align with those of the larger Bitcoin community).  It is my opinion, then, that we should support multiple implementations of the Bitcoin protocol, working to reduce the network's dependency on Core.  Best regards,
Peter R

@_date: 2015-11-14 13:45:25
@_author: Peter R 
@_subject: [bitcoin-dev] How to evaluate block size increase suggestions. 
I agree.  In fact, I?ll go meta on your meta and suggest that we should first discuss how Bitcoin should be governed in the first place.  Should Bitcoin evolve from the ?bottom up,? or from the ?top down??
If one?s answer is from the ?top-down,? then the meta-level criteria can be endlessly debated, for they all involve some sort of tradeoff, they all require some sort of compromise.  The ?top down? perspective holds that people might make poor choices if given the freedom to easily do so--it holds that the trade-offs must be balanced instead by experts.  However, if one's answer is from the ?bottom up,? then the meta-level criteria is very easy: we do what the people wants. We allow the people to weigh the tradeoffs and then we watch as consensus emerges through a decentralized process, objectively represented by the longest proof-of-work chain.  Regarding the block size limit debate, at the end of the day it comes down to two things:
1.  How big of a block will my node accept today?
2.  What do I want my node to do if the longest chain includes a block larger than the limit I set?
If one concedes that Bitcoin should be governed from the ?bottom up,? then it is already possible to empower each node operator to more easily express his free choice regarding the size of blocks he is willing to accept, while simultaneously ensuring that his node tracks consensus.
Best regards,

@_date: 2015-11-14 17:02:33
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Hi Greg,
Like you said, the issue with using more than one database technology is not that one node would prove that Block X is valid while the another node proves that Block X is NOT valid.  Instead, the problem is that one node might say ?valid? while the other node says ?I don?t know.?
It is often said that what caused the Level DB fork was that the old version determined that the triggering block was invalid while the new version determined the opposite.  This interpretation of the fork event has lead to the ?bug-for-bug?-compatibility fear regarding multiple implementations of the protocol (or multiple database technologies).  In reality, this fear is largely unfounded.  The old version ran out of LDB locks and couldn?t execute properly.  If the software was written with the philosophy that tracking consensus was more important than bug-for-bug compatibility, it would have returned an exception rather than ?invalid.?  At that point, the software would have checked what decision the rest of the network came to about the block in question.   My node would have forked to the longest chain, automatically assuming that the questionable block was valid; your node may have made a different decision (it?s a question of ideology at that point).      A group of us have been exploring this ?meta-cognition? idea with Bitcoin Unlimited.  For example, Bitcoin Unlimited can be (optionally) made to automatically fork to the longest chain if it ?gets stuck? and can neither prove that a block is valid nor that the block is invalid.  Similarly, Bitcoin Unlimited can be (optionally) made to automatically fork to a chain that contains a block marginally bigger than its block size limit?once that block is buried sufficiently in what has emerged as the longest persistent chain. Thinking from this perspective might go along way towards decentralizing development, the emergence of multiple competing implementations of the protocol, and true ?bottom up? governance.  Best regards,

@_date: 2015-11-14 17:45:03
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Agreed.  There are two cases to consider:
Type 1.  One implementation says ?yes? or ?no,? while the other says ?I don?t know?, and
Type 2.  One implementation says ?yes? and the other says ?no,? because of a bug.  My previous email described how Type 1 consensus failures can be safely dealt with.  These include many kinds of database exceptions (e.g., the LevelDB fork at block  or consensus mismatches regarding the max size of a block.  Type 2 consensus failures are more severe but also less likely (I?m not aware of a Type 2 consensus failure besides the 92 million bitcoin bug from August 2010).  If Core was to accept a rogue TX that created another 92 million bitcoins, I think it would be a good thing if the other implementations forked away from it (we don?t want bug-for-bug compatibility here).   This once again reveals the benefits of multiple competing implementations.  Peter

@_date: 2015-11-14 18:58:50
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
You are looking at the problem from a ?top down? governance perspective assuming you know what code is actually being run and what rules the market wants.  The reality is that you can only make estimates about these two things.  As Bitcoin evolves away from the current development monoculture, rules such as the max block size may no longer be perfectly clear.  However, we can prove that the following results will continue to hold:
1. A node with a block size limit greater than the hash-power weighted median will always follow the longest chain.
2. An excessive (e.g., greater than 1 MB) block will be accepted into the longest chain if it is smaller than the hash-power weighted median block size limit.
Already today, different nodes have different block size limits.  There are even some miners today that will attempt to build upon blocks larger than 1 MB if anyone dares to create one.  At some point, the majority of the hash power will support something larger than 1 MB.  When that first bigger block comes and gets buried in the longest chain, hold-out nodes (e.g., MP says he will never increase his limit) will have to make a choice: fight consensus or track consensus!  I know that I would want my node to give up on its block size limit and track consensus.  You may decide to make a different choice.  You said that "a block that violates this [block size limit] threshold is invalid.?  I agree.  If the nodes and miners rejected the block as invalid then it would not persist as the longest chain. If the nodes and miners accepted the block and continued to build on top of it, then that chain would be Bitcoin (whether you personally agree of not).  Bitcoin is ultimately a creature of the market, governed by the code people freely choose to run. Consensus is then an emergent property, objectively represented by the longest persistent chain.  Proof-of-work both enforces and defines the rules of the network.  I think you?re being intentionally obtuse here: accepting a block composed entirely of valid transactions that is 1.1 MB is entirely different than accepting a TX that creates a ten thousand bitcoins out of thin air.  The market would love the former but abhor the later.  I believe you can recognize the difference.  Thank you for conceding on that point. Please don?t take my comments and observations as criticisms.  I think the Core Dev team has done excellent work!  What I am saying instead is that as we move forward?as development becomes decentralized and multiple protocol implementations emerge?development philosophies will change. Tracking consensus and the will of the market will be most important.  Personally, I hope to see design philosophies that support ?bottom up? governance instead of the current ?top down? model.  Best regards,

@_date: 2015-11-14 19:17:08
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Great point, Luke! Indeed, whether the program can or cannot continue after a Type 1 consensus mismatch depends on the specifics of the situation and exactly how the code was written.  But I agree: there are cases where the program *can?t* continue.  In those cases it would halt.  This would require manual intervention to fix but avoids the problem of potential double-spends during the fork event.  This would be preferable to knowingly causing a fork.

@_date: 2015-11-14 20:10:30
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Hi Greg,
I apologize for that, Greg.  I have not read enough on the issues you brought up to comment intelligibly.  I should have conceded that you could very well be correct that those were Type 2 consensus failures.  The difference between a 1.1 MB block full of valid transactions and an invalid TX that creates 10,000 BTC out of thin air is *not* a matter of ?politics.?  If people could freely award themselves coins, then Bitcoin would not be money.  It?s like saying that ?technically? there?s no difference between picking up a penny from the sidewalk and holding up a bank teller at gunpoint.  Ask the average person: there is more than a ?political? difference between creating coins out of thin air and increasing the block size limit. What rules does Bitcoin obey?  What is Bitcoin?s nature?  This brings us to the age-old debate between Rationalism versus Empiricism.
Rationalism holds that some propositions are known to be true by intuition alone and that others are knowable by being deduced from intuited propositions. The Rationalist may hold the view that Bitcoin has a 21-million coin limit or a 1 MB block size limit, based on deductive reasoning from the rules enforced by the Bitcoin Core source code. Such a Rationalists might believe that the code represents some immutable truth and then his understanding of Bitcoin follows from axiomatic deductions from that premise.
The Empiricist rejects the Rationalist?s intuition and deduction, believing instead that knowledge is necessarily a posteriori, dependent upon observation and sense experience. The Empiricist questions the notion that Bitcoin has a 21-million coin limit, instead observing that its money supply grew by 50 BTC per block for the first 210,000 and then 25 BTC per block ever since. The Empiricist rejects the idea that Bitcoin has any sort of block size limit, having observed previous empirical limits collapse in the face of increased demand.
I am not convinced that Bitcoin even *has* a block size limit, let alone that it can enforce one against the invisible hand of the market.  You were the one who just brought up politics, Greg.  Not I. Best regards,

@_date: 2015-11-15 09:06:58
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Hello Jorge:
I applaud your work on the consensus library.  I think it an important step to encouraging multiple competing implementations of the protocol.
What technical difference is there between a cat and a dog? They both have four legs and a furry coat. I think you?re using the term ?technical difference? to mean something very specific.  Perhaps you could clarify exactly how you are defining that term because to me it is crystal clear that creating coins out of thin air is very different than accepting a block 1.1 MB in size and full of valid TXs.  There are many technical differences between the two. For example, technically the first allows coins to be created randomly while the second doesn?t.  Correct.  Bitcoin is an experiment and could still fail (e.g., the network could allow people to move coins without valid signatures).  For Bitcoin to be viable, the network of miners and node operators being net-econo-rational actually is probably a core axiom that must be accepted.  It is fact that two competing forks can persist for at least a short amount of time?we saw this a few years ago with the LevelDB bug and again this summer with the SPV mining incident.  In both cases, there was tremendous pressure to converge back to a single chain.
Could two chains persist indefinitely?  I don?t know.  No one knows.  My gut feeling is that since users would have coins on both sides of the fork, there would be a fork arbitrage event (a ?forkbitrage?) where speculators would sell the coins on the side they predict to lose in exchange for additional coins on the side they expect to win.  This could actually be facilitated by exchanges once the fork event is credible and before the fork actually occurs, or even in a futures market somehow.  I suspect that the forkbitrage would create an unstable equilibrium where coins on one side quickly devalue.  Miners would then abandon that side in favour of the other, killing the fork because difficulty would be too high to find new blocks.  Anyways, I think even *this* would be highly unlikely.  I suspect nodes and miners would get inline with consensus as soon as the fork event was credible.  Cryptocurrency is a new area of interdisciplinary research.  We are all learning together and no one knows how things will unfold.  Best regards,

@_date: 2015-10-02 09:38:26
@_author: Peter R 
@_subject: [bitcoin-dev] Dev-list's stance on potentially altering the PoW 
I encourage Alex and Dmitry to consider submitting their paper to Ledger, where it will be reviewed objectively and with an open mind.  The authors have motivated their work, framed it in its scholarly context, and made explicit the contributions their paper makes.  Their manuscript, "Asymmetric proof-of-work based on the Generalized Birthday problem," clearly represents a great deal of work by the authors and I commend them for their efforts.  In the link Adam Back provided, Greg Maxwell mentioned that ?it is far from clear that 'memory hardness' is actually a useful goal.?  I agree with this statement; however, regardless of whether memory hardness turns out to be a useful goal in regards to cryptocurrency or not, a paper analyzing memory-hard proof-of-work schemes is certainly useful in helping us to figure that out. Best regards,

@_date: 2015-10-05 10:33:55
@_author: Peter R 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Dear Bitcoin Development Community:
I would like to share my opinion that Mike is correct regarding the soft fork versus hard fork debate. I agree that CLTV should be done with a hard fork for the reasons that Mike has discussed several times in the past (mainly that a hard forks requires active consensus while a soft fork requires only indifference).  I believe this is a controversial change and?if Core Dev believes that controversial changes to the consensus rules must not happen?then my interpretation is that CLTV should not happen in its current form.  I also agree with Mike that Core's requirement for unanimous consensus results in development grid lock and should be revisited.  In my opinion, the idea that unanimity is required should be replaced with the idea that the longest chain composed of valid transactions is the correct chain.  It shouldn?t matter really how the chain becomes the longest?only that it does.  I believe that a good way to return power to the bitcoin community is to foster mutiple forkwise-compatible implementations of the protocol.  Each implementation could have its own governance model and design objectives and use techniques like BIP101?s 750/1000 signalling mechanism to activate changes that may be desirable to the community.  If a super majority does not support the change, then it won?t be activated.  I created an animated GIF that visualizes one possibility for how multiple protocol implementations might emerge over time:
 Decentralizing development and supporting multiple forkwise-compatible implementations of the protocol is a worthwhile goal that will simultaneously make Bitcoin more robust and more responsive to the will of the market.
Nodes would express their acceptance of a block by mining on top of it.  Consensus would be determined by the code we choose to run. Best regards,
Peter

@_date: 2015-10-05 14:27:30
@_author: Peter R 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Furthermore, Bitcoin is significantly more than a "software project": it sits at a unique intersection of computer science, economics, physics, law and more.  While I agree that minor bug-fixes and code-maintenance-type issues should be dealt with quietly by developers, decisions regarding Bitcoin?s governance and its evolution should be shaped by an interdisciplinary group of stakeholders from across the community.  The hard- vs soft-fork debate is not just a code maintenance issue.  Once again, let?s use the current gridlock in Core to rally the growth of new forkwise-compatible implementations of the protocol.  Gavin and Mike?s initiative with BIP101 and Bitcoin XT should be encouraged as one possible model for coming to consensus on hard-forking changes.  Best regards,

@_date: 2015-10-05 14:37:03
@_author: Peter R 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
I disagree.  There is gridlock in the Core Dev development process.

@_date: 2015-10-05 20:20:33
@_author: Peter R 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Great point, Tom! In fact, you?ve just explained the dynamics that create ?centralizing pressure? in regards to development:  If the weight of a person?s opinion is proportional to how many commits that person has made, and if the probability of getting a commit pulled is proportional to the weight of that person?s opinion, well?I?m pretty sure this results in a differential equation that has a solution that results in ever-increasing centralized control of the code base.  I believe we should work to deprecate the idea that Core is somehow the ?core of Bitcoin," in favour of multiple competing implementations. XT and btcd are two working examples of this idea.  Let?s make it easier for the community to determine the evolution of Bitcoin by making it easier for the community to express their vote based on the code we choose to run.  Best regards,

@_date: 2015-10-05 22:33:43
@_author: Peter R 
@_subject: [bitcoin-dev] This thread is not about the soft/hard fork 
Haha great stuff, NotMike!
Indeed, it?s not enough to keep the block size limit small so that every man can run his own node, we also must also implement your proposed ?one man, one commit? policy!  Think of the decentralization if everyone Bitcoin user is also contributing code!    /s

@_date: 2015-10-29 21:04:22
@_author: Peter R 
@_subject: [bitcoin-dev] [patch] Switching Bitcoin Core to sqlite db 
Can you give a specific example of how nodes that used different database technologies might determine different answers to whether a given transaction is valid or invalid?  I?m not a database expert, but to me it would seem that if all the unspent outputs can be found in the database, and if the relevant information about each output can be retrieved without corruption, then that?s all that really matters as far as the database is concerned.
Let?s use an unspent pay-to-pubkey-hash output as an example: Alice spends this to Bob (she signs it properly), the TX propagates across the network and?then what?  Do some nodes disagree on whether or not the TX is valid?  What exactly would they disagree on?  Are you suggesting that a database bug would cause some nodes to think the output was actually already spent, while others can correctly see that it?s unspent?  Or maybe some nodes think the output doesn?t exist while others do?  Or are you suggesting that the details about this output might be retrieved with errors from certain databases but correctly from others?  I?d like a concrete example to help me understand why more than one implementation of something like the UTXO database would be unreasonable.

@_date: 2015-08-31 19:16:22
@_author: Peter R 
@_subject: [bitcoin-dev] Let's kill Bitcoin Core and allow the green shoots of 
I agree, s7r, that Bitcoin Core represents the most stable code base.  To create multiple implementations, other groups would fork Bitcoin Core similar to what Bitcoin XT did.  We could have:
- Bitcoin-A (XT)
- Bitcoin-B (Blockstream)
- Bitcoin-C (promoting BIP100)
- Bitcoin-D
- etc.
Innovation from any development group would be freely integrated by any other development group, if desired.  Of course, each group would have a very strong incentive to remain fork-wise compatible with the other implementations.  In fact, this just gave me a great idea!  Since Wladimir has stated that he will not integrate a forking change into Core without Core Dev consensus, I suggest we work together to never reach consensus with Bitcoin Core.  This will provide impetus for new implementations to fork from Core (like XT did) and implement whatever scaling solution they deem best.  The users will then select the winning solution simply based on the code they choose to run.  The other implementations will then rush to make compatible changes in order to keep their dwindling user bases.  This is the decentralized spirit of Bitcoin in action.  Creative destruction.  Consensus formed simply by the code that gets run.  Let's kill Bitcoin Core and allow the green shoots of a garden of new implementations to grow from its fertile ashes.  Peter R

@_date: 2015-09-01 01:06:30
@_author: Peter R 
@_subject: [bitcoin-dev] ERRATA CORRIGE + Short Theorem 
I don't believe anyone is arguing otherwise.  Miners with a larger fraction of the network hash rate, h/H, have a theoretical advantage, all other variables in the miner's profitability equation held constant.  Dpinna originally claimed (unless I'm mistaken) that his paper showed that this advantage decreased as the block reward diminished or as the total fees increased.  This didn't seem unreasonable to me, although I never checked the math.  Best regards,

@_date: 2015-09-23 09:28:20
@_author: Peter R 
@_subject: [bitcoin-dev] Weak block thoughts... 
Hi Gavin,
One thing that's not clear to me is whether it is even necessary--from the perspective of the block size limit--to consider block propagation.  Bitcoin has been successfully operating unconstrained by the block size limit over its entire history (except for in the past few months)--block propagation never entered into the equation.  Imagine that the limit were raised to significantly above the free market equilibrium block size Q*.  Mining pools and other miners would then have an incentive to work out schemes like "weak blocks," relay networks, IBLTs, etc., in order to reduce the risk of orphaning larger blocks (blocks with more fees that they'd like to produce if it were profitable).  Shouldn't mining pools and miners be paying you guys for coding solutions that improve their profitability?   Best regards,

@_date: 2015-09-23 10:49:04
@_author: Peter R 
@_subject: [bitcoin-dev] Weak block thoughts... 
Thanks for the reply, Gavin.  I agree on all points.

@_date: 2016-05-09 11:34:47
@_author: Peter R 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Hi Pieter,
[9 May 16 @ 11am PDT]  We worked on this with respect to ?Xthin" for Bitcoin Unlimited, and came to a similar conclusion.  But we (I think it was theZerg) also noticed another trick: if the node receiving the thin blocks has a small number of collisions with transactions in its mempool (e.g., 1 or 2), then it can test each possible block against the Merkle root in the block header to determine the correct one.  Using this technique, it should be possible to further reduce the number of bytes used for the txids.  That being said, even thin blocks built from 64-bit short IDs represent a tremendous savings compared to standard block propagation.  So we (Bitcoin Unlimited) decided not to pursue this optimization any further at that time.
It?s also interesting to ask what the information-theoretic minimum amount of information necessary for a node to re-construct a block is. The way I?m thinking about this currently[1] is that the node needs all of the transactions in the block that were not initially part of its mempool, plus enough information to select and ordered subset from that mempool that represents the block.  If m is the number of transactions in mempool and n is the number of transactions in the block, then the number of possible subsets (C') is given by the binomial coefficient:
  C' =  m! / [n! (m - n)!]
Since there are n! possible orderings for each subset, the total number of possible blocks (C) of size n from a mempool of size m is
  C = n! C? = m! / (m-n)!
Assuming that all possible blocks are equally likely, the Shannon entropy (the information that must be communicated) is the base-2 logarithm of the number of possible blocks.  After making some approximations, this works out very close to
   minimum information ~= n * log2(m),
which for your case of 20,000 transactions in mempool (m = 20,000) and a 2500-transaction block (n = 2500), yields
   minimum information = 2500 * log2(20,000) ~ 2500 * 15 bits.
In other words, a lower bound on the information required is about 2 bytes per transactions for every transaction in the block that the node is already aware of, as well as all the missing transactions in full. Of course, this assumes an unlimited number of round trips, and it is probably complicated by other factors that I haven?t considered (queue the ?spherical cow? jokes :), but I thought it was interesting that a technique like Xthin or compact blocks is already pretty close to this limit.  Peter [1] There are still some things that I can?t wrap my mind around that I?d love to discuss with another math geek :)

@_date: 2016-05-09 16:37:00
@_author: Peter R 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
[9 May 16 @ 4:30 PDT]
I?m trying to understand the collision attack that you're explaining to Tom Zander.  Mathematica is telling me that if I generated 2^32 random transactions, that the chances that the initial 64-bits on one of the pairs of transactions is about 40%.  So I am following you up to this point.  Indeed, there is a good chance that a pair of transactions from a set of 2^32 will have a collision in the first 64 bits.  But how do you actually find that pair from within your large set?  The only way I can think of is to check if the first 64-bits is equal for every possible pair until I find it.  How many possible pairs are there?  It is a standard result that there are     m! / [n! (m-n)!] ways of picking n numbers from a set of m numbers, so there are
    (2^32)! / [2! (2^32 - 2)!] ~ 2^63
possible pairs in a set of 2^32 transactions.  So wouldn?t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?
Perhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected?
Best regards,

@_date: 2016-05-09 18:42:45
@_author: Peter R 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
[9 May 16 @ 6:40 PDT]
For those interested in the hash collision attack discussion, it turns out there is a faster way to scan your set to find the collision:  you?d keep a sorted list of the hashes for each TX you generate and then use binary search to check that list for a collision for each new TX you randomly generate. Performing these operations can probably be reduced to N lg N complexity, which is doable for N ~2^32.   In other words, I now agree that the attack is feasible.  hat tip to egs

@_date: 2016-05-30 08:41:15
@_author: Peter R 
@_subject: [bitcoin-dev] Towards Massive On-Chain Scaling: Presenting Our 
============================== START ==============================
Dear all,
For the past two months, Andrew Clifford, Andrew Stone,  Peter Tschipper and I have been collecting empirical data regarding block propagation with Xthin???both across the normal P2P network and over the Great Firewall of China. We have six Bitcoin Unlimited (BU) nodes running, including one located in Shenzhen and another in Shanghai, and we have collected data on the transmission and reception for over nine thousand blocks.
Here is a link to Part 1(Methodology) of our 5 part article series on the testing we performed for this exciting new block relay technology:
 We thank Jihan Wu from AntPool for the block source within Mainland China and  for the funding.  Best regards,

@_date: 2016-11-22 08:31:50
@_author: Peter R 
@_subject: [bitcoin-dev] The Excessive-Block Gate: How a Bitcoin Unlimited 
Dear all,
Bitcoin Unlimited?s market-based solution to the block-size limit is slowly winning support from node operators and miners.  With this increased attention, many people are asking for a better explanation of how Bitcoin Unlimited actually works.  The article linked below describes how Bitcoin Unlimited?s excessive-block logic works from the perspective of a single node. (I?m hoping to do a follow-up article that describe how this ?node-scale? behavior facilitates the emergence of a fluid and organic block size limit at the network scale.)
 Best regards,
Peter R

@_date: 2016-11-26 15:35:49
@_author: Peter R 
@_subject: [bitcoin-dev] The Excessive-Block Gate: How a Bitcoin Unlimited 
Great discussion, Sergio and Tom!
Right, miners who set their block size limits (BSL) above OR below the "effective BSL" are disadvantaged.  Imagine that we plot the distribution (by hash power) for all miners' BSLs.  We might get a chart that looks like this:
 In this chart, the "effective BSL" is defined as the largest block size that no less than half the hash power will accept.  If a block is mined with a size Q that is less than the "effective BSL," then all the hash power with BSLs between BSL_min and Q will be forked from the longest chain (until they update their software if they're running Core or until their acceptance depth is hit if they're running BU).  This wastes these miners' hash power.  However, if a block is mined with a size Q that is greater than the effective BSL, then all the hash power with BSLs between Q and BSL_max will temporarily be mining on a "destined to be orphaned" chain.  This also wastes these miners' hash power.  Therefore, it is in the best interest of miners to all set the same block size limit (and reliably signal in their coinbase TX what that limit is, as done by Bitcoin Unlimited miners).  We have empirical evidence the miners in fact behave this way: (1) No major miner has ever set his block size limit to less than 1 MB (not even those such as Luke-Jr who think 1 MB is too big) because doing so would just waste money.  (2) After switching to Bitcoin Unlimited, both ViaBTC and the Bitcoin.com pool temporarily set their BSLs to 2 MB and 16 MB, respectively (of course keeping their _generation limit_ at 1MB).  However, both miners quickly reduced these limits back to 1 MB when they realized how it was possible to lose money in an attack scenario.  (This actually surprised me because the only way they could lose money is if some _other_ miner wasted even more money by purposely mining a destined-to-be-orphaned block.)   The follow-up article I'm working on is about the topics we're discussing now, particularly about how Bitcoin Unlimited's ?node-scale? behavior facilitates the emergence of a fluid and organic block size limit at the network scale.  Happy to keep continue with this current discussion, however.
Best regards

@_date: 2017-03-25 13:28:44
@_author: Peter R 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
One of the purported benefits of a soft-forking change (a tightening of the consensus rule set) is the reduced risk of a blockchain split compared to a loosening of the consensus rule set.  The way this works is that miners who fail to upgrade to the new tighter ruleset will have their non-compliant blocks orphaned by the hash power majority.  This is a strong incentive to upgrade and has historically worked well.  If a minority subset of the network didn?t want to abide by the new restricted rule set, a reasonable solution would be for them to change the proof-of-work and start a spin-off from the existing Bitcoin ledger (
In the case of the coming network upgrade to larger blocks, a primary concern of both business such as Coinbase and Bitpay, and most miners, is the possibility of a blockchain split and the associated confusion, replay risk, etc.  By applying techniques that are known to be successful for soft-forking changes, we can likewise benefit in a way that makes a split less likely as we move towards larger blocks.  Two proposed techniques to reduce the chances of a split are:
1. That miners begin to orphan the blocks of non-upgraded miners once a super-majority of the network hash power has upgraded. This would serve as an expensive-to-ignore reminder to upgrade.
2. That, in the case where a minority branch emerges (unlikely IMO), majority miners would continually re-org that minority branch with empty blocks to prevent transactions from confirming, thereby eliminating replay risk.
Just like after a soft forking change, a minority that does not want to abide by the current ruleset enforced by the majority could change the proof-of-work and start a spin-off from the existing Bitcoin ledger, as suggested by Emin.  Best regards,
Peter R

@_date: 2017-03-26 12:05:38
@_author: Peter R 
@_subject: [bitcoin-dev] Defending against empty or near empty blocks from 
Hello Alex,
Thank you for the thoughtful reply.  Yes, it is different.  It?s different because the future network upgrade to larger blocks includes a loosening of the consensus ruleset whereas previous upgrades have included a tightening of the rule set.  (BTW?this is not my proposal, I am describing what I have recently learned through my work with Bitcoin Unlimited and discussions with miners and businesses).  With a tightening of the rule set, a hash power minority that has not upgraded will not produce a minority branch; instead they will simply have any invalid blocks they produce orphaned, serving as a wake-up call to upgrade.  With a loosening of the consensus rule set, the situation is different: a hash power minority that has not upgraded will produce a minority branch, that will also drag along non-upgraded node operators, leading to potential confusion.  The idea behind orphaning the blocks of non-upgraded miners was to serve as a wake-up call to upgrade, to reduce the chances of a minority chain emerging in the first place, similar to what happens automatically with a soft-forking change.  If one's worry is a chain split, then this seems like a reasonable way to reduce the chances of that worry materializing.  The Level 3 anti-split protection takes this idea one step further to ensure that if a minority branch does emerge, that transactions cannot be confirmed on that branch.
I?m very confident that most people do NOT want a split, especially the miners.  The upgrade to larger blocks will not happen until miners are confident that no minority chain will survive.  I agree that the soft-fork mechanism usually works well.  I believe this mechanism (or perhaps a modified version of it) to increase the block size limit will likewise work well.  All transactions types that are currently valid will be valid after the upgrade, and no new types of transactions are being created.  The ?block-size-limit gene" of network nodes is simply evolving to allow the network to continue to grow in the way it has always grown. (If you?re interested, here is my talk at Coinbase where I discuss this:  )
My read is completely different.  I still have never talked with a person in real life who doesn?t want the block size limit to increase.  Indeed, I have met people who worry that Bitcoin Unlimited is ?trying to take over??and thus they are worried for other reasons?but this couldn?t be further from the truth.  For example, what most people within BU would love to see is a simple patch to Bitcoin Core 0.14 that allows node operators to adjust the size of blocks their nodes will accept, so that these node operators can follow consensus through the upgrade if they choose to.  This is not a fight about ?Core vs. BU?; Bitcoin?s future is one of ?genetic diversity? with multiple implementations, so that a bug in one doesn?t threaten the network as a whole.  To me it seems this is largely a fight about whether node operators should be easily able to adjust the size of blocks their nodes accept.  BU makes it easy for node operators to accept larger blocks; Core doesn?t believe users should have this power (outside of recompiling from source, which few users can do).  Once again, this is not my proposal.  I am writing about what I have come to learn over the past several weeks.  When I first heard about these ideas, I was initially against them too.  They seemed harsh and merciless.  It wasn?t until I got out their and started talking to more people in the community that the rationale started to make sense to me: the biggest concern people had was a chain split!
So I guess the ?ethics? here depend on the lens through which one is looking. People who believe that an important outcome of the upgrade to larger blocks is to avoid a blockchain split may be more favourable to these ideas than people who want the upgrade to result in a split (or are OK with a split), as it sounds like you do (is this true that you?d rather split than accept blocks with more than 1,000,000 bytes of transaction information in them? Sorry if I misunderstood).  But if one's intention is to split and not follow the majority hash power when blocks become larger, then why not change the proof-of-work?  This would certainly result in a peaceful splitting, as you said you desire.  Best regards,
Peter R

@_date: 2017-03-29 13:28:29
@_author: Peter R 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
I believe nearly everyone at Bitcoin Unlimited would be supportive of a UTXO check-pointing scheme.  I?d love to see this happen, as it would greatly reduce the time needed to get a new node up-and-running, for node operators who are comfortable trusting these commitments.  I?m confident that we could work with the miners who we have good relationships with to start including the root hash of the (lagging) UTXO set in their coinbase transactions, in order to begin transforming this idea into reality.  We could also issue regular transactions from ?semi-trusted? addresses controlled by known people that include the same root hash in an OP_RETURN output, which would allow cross-checking against the miners? UTXO commitments, as part of this initial ?prototype? system.
This would "get the ball rolling" on UTXO commitments in a permissionless way (no one can stop us from doing this). If the results from this prototype commitment scheme were positive, then perhaps there would be support from the community and miners to enforce a new rule which requires the (lagging) root hashes be included in new blocks.  At that point, the UTXO commitment scheme is no longer a prototype but a trusted feature of the Bitcoin network.    On that topic, are there any existing proposals detailing a canonical ordering of the UTXO set and a scheme to calculate the root hash?
Best regards,

@_date: 2017-05-15 13:53:45
@_author: Peter R 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
Hi Pieter,
I wanted to say that I thought this write-up was excellent!  And efficiently hashing the UTXO set in this rolling fashion is a very exciting idea!! Peter R
