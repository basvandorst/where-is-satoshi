
@_date: 2014-08-10 14:07:19
@_author: Bob McElrath 
@_subject: [Bitcoin-development] Synchronization: 19.5 % orphaned blocks 
I had the same problem (repeatedly) which came down a hardware problem.  Bitcoin
more than other applications is very sensitive to single bit flips in memory or
during computation.  (In the end I under-clocked my CPU and RAM to fix the
Attached is a small python script which will run sha256 on random data
repeatedly and will print a message if a mismatch is found.  For me it took many
hours of running before a sha256 mismatch, but one is enough to fork the
Cheers, Bob McElrath
"The individual has always had to struggle to keep from being overwhelmed by
the tribe.  If you try it, you will be lonely often, and sometimes frightened.
But no price is too high to pay for the privilege of owning yourself."     -- Friedrich Nietzsche

@_date: 2015-12-19 19:30:45
@_author: Bob McElrath 
@_subject: [bitcoin-dev] We need to fix the block withholding attack 
The only possible other bad actors are other miners.  So who are the "bad actor"
miners?  It's a short list of candidates.
I've been trying to understand this source of "vulnerabilities and technical
issues" with p2pool and have received a lot of contradictory information.  Can
someone in the know summarize what the problems with p2pool are?
The economic situation where miners can be deprived of profit due to the lack of
synchronicity in block updates is a physics problem due to the size of the Earth
and will never be removed.  This is a design flaw in Bitcoin.  Therefore a
different, more comprehensive solution is called for.
My solution to this is somewhat longer term and needs more simulation but
fundamentally removes the source of contention and fixes the design flaw, while
remaining as close "in spirit" to bitcoin as possible:
    Not only does block withholding simply not work to deny other miners income due
to the absence of orphans, but I explicitly added a dis-incentive against
withholding blocks in terms of the "cohort difficulty".  Other graph-theoretic
quantities are in general possible in the reward function to better align the
incentives of miners with the correct operation of the system.  Also by lowering
the target difficulty and increasing the block (bead) rate, one lowers the
variance of miner income.
Part of the reason I ask is that there has been some interest in testing my
ideas in p2pool itself (or a new similar share pool), but I'm failing to
understand the source of all the complaints about p2pool.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-12-30 19:00:43
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Increasing the blocksize as a (generalized) 
This is a very complex way to enter zombie mode.
A simpler way is to track valid PoW chains by examining only the header, that
are rejected for other reasons.
Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should
enter "zombie mode" and refuse to mine or relay, and alert the operator, because
we don't know what we're doing and we're out of date.  This way doesn't require
any modifications to block structure at all.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-12-31 00:04:42
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Increasing the blocksize as a (generalized) 
That's an over-generalization.  There are two kinds of soft-forks WRT mining,
those which:
1. involve new validation rules by data-hiding from non-upgraded modes
    (e.g. extension blocks, generalized softfork)
2. involve NO new validation logic (e.g. P2SH)
Miners which are not validating transactions *should* be deprived of revenue,
because their role is transaction validation, not simply brute forcing sha256d.
So I'm very strongly against this "generalized softfork" idea -- I also don't
see how upgraded nodes and non-upgraded nodes can possibly end up with the same
UTXO set.
Which is why it should be put into core long before forks.  ;-)
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-06-08 18:06:00
@_author: Bob McElrath 
@_subject: [Bitcoin-development] New attack identified and potential 
There was this wonderful technology invented a few years ago to deal with spam. It's called Hashcash. All these hacky heuristics like block size are just dancing around the problem, and the natural solution is already present in bitcoin: smaller blocks, (down to the point of individual transactions) each mined. Don't relay things that haven't been mined. As spam or transaction levels go up, mining targets for submission go up too.
Of course this is a pretty serious redesign of bitcoin, and I'm not offering a concrete proposal at this time (but have one in the works, and I'd like to see others).
I call the parameters of these hacky heuristics "Consensus Threatening Quantities" (CTQs) because changing them induces a hard fork. Bitcoin is full of them (block time, block size, target difficulty, retarget time, etc) and bitcoin would do well to face difficult redesign questions head on, and remove them entirely. (Proposal to appear...)

@_date: 2015-05-10 13:35:25
@_author: Bob McElrath 
@_subject: [Bitcoin-development] A suggestion for reducing the size of 
This is my biggest headache with practical bitcoin usage. I'd love to hear it if
anyone has any clever solutions to the wallet/utxo locked problem. Spending
unconfirmed outputs really requires a different security model on the part of
the receiver than  but isn't inherently bad if the receiver has a
better security model and knows how to compute the probability that an
unconfirmed-spend will get confirmed. Of course the bigger problem is wallet
software that refuses to spend unconfirmed outputs.
I've thought a bit about a fork/merge design: if the change were computed by the
network instead of the submitter, two transactions having the same change
address and a common input could be straightforwardly merged or split (in a
reorg), where with bitcoin currently it would be considered a double-spend.  Of
course that has big privacy implications since it directly exposes the change
address, and is a hard fork, but is much closer to what people expect of a
debit-based "account" in traditional banking.
The fact of the matter is that having numerous sequential debits on an account
is an extremely common use case, and bitcoin is obtuse in this respect.

@_date: 2015-05-10 14:42:54
@_author: Bob McElrath 
@_subject: [Bitcoin-development] A suggestion for reducing the size of the 
That's a lot of work, a lot of extra utxo's, and a lot of blockchain spam, just
so I can do a convoluted form of arithmetic on my balance.
If a tx contained an explicit miner fee and a change address, but did not
compute the change, letting the network compute it (and therefore merge
transactions spending the same utxo), could one add some form of ring signature
a la Dash to alleviate the worsened privacy implications?
Cheers, Bob McElrath
"The individual has always had to struggle to keep from being overwhelmed by
the tribe.  If you try it, you will be lonely often, and sometimes frightened.
But no price is too high to pay for the privilege of owning yourself."     -- Friedrich Nietzsche

@_date: 2015-11-09 21:04:49
@_author: Bob McElrath 
@_subject: [bitcoin-dev] request BIP number for: "Support for Datastream 
I would expect that since a block contains mostly hashes and crypto signatures,
it would be almost totally incompressible.  I just calculated compression ratios:
zlib    -15%    (file is LARGER)
gzip     28%
bzip2    25%
So zlib compression is right out.  How much is ~25% bandwidth savings worth to
people?  This seems not worth it to me.  :-/
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-10-14 20:52:35
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Bitcoin-NG whitepaper. 
So it seems to me that all I need to do is figure out who the current leader is,
and DDoS him off the network to shut Bitcoin-NG down.
This is a significant advantage to bitcoin's ex-post-facto blocks: no one knows
where the next one will come from.  The only way to shut the network down is to
shut all nodes down.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-10-30 16:36:04
@_author: Bob McElrath 
@_subject: [bitcoin-dev] UTXO set commitment hash 
The state of bitcoin transactions can be committed to in blocks by keeping two
running hashes, one of unspent transaction outputs and one of spent transaction
outputs.  A "running hash" $R$ I define as being computed by taking the previous
value of the hash $r$, concatenating it with the new data $x$, and hashing it:
    R = hash(r|x).
In the case of the UTXO set, the data $x$ can be taken to be the concatenation
(txid|vout|amount) for all outputs, let's call this running hash hTXO.  Because
data cannot be "removed" from this set commitment, a second hash can be computed
consisting of the spent outputs, let's call this hSTXO.  Thus the pair of hashes
(hTXO, hSTXO) is equivalent to a hash of all unspent outputs.  These hashes can
be placed into a block's Merkle tree by miners with a soft fork.  It can be
reduced to a single hash hUXTO = hash(hTXO|hSXTO) if desired.
By defining *how* to compute (hTXO, hSXTO) we can define an implementation
independent definition of consensus that is extremely cheap to compute.  The
order in which outputs are hashed is clearly important, but bitcoin has a well
defined ordering already in terms of the order in which transactions appear in
blocks, and the sequential order of outputs.
In the recent discussion surrounding leveldb and jgarzik's new sqlite branch, it
has been brought up repeatedly by gmaxwell that this db is "consensus critical".
As a data structure storing the state of transactions, of course it's consensus
critical.  However there's only one right answer to what the set of UTXOs is.
Any other result reported by the db is simply wrong.  By creating and publishing
(hTXO, hSXTO), miners can publish their view of the transaction state, and any
implementation can be validated against it.
As I understand it, leveldb is in the bitcoin core source tree because it could
have bugs and give the wrong answer for a given UTXO (see BIP50).  This is worse
than a consensus failure, it's just wrong, and the argument that we have to keep
leveldb around and maintain it because it could be wrong is pretty ugly, and I
don't think anyone actually wants to do this.  Let's not be wrong in the first
place, and let's choose databases based on performance and other considerations.
"Not being wrong" should go without saying, regardless of implementation
It should be noted that (hTXO, hSXTO) can be computed twice, once without the
database (while processing a new block) and once by requesting the same data
from the database.  So bad database behavior can be detected and prevented from
causing consensus failures.  And then we can remove leveldb from the core.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2015-09-11 14:21:00
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Three Challenges for Scaling Bitcoin 
I will be unable to attend the Scaling Bitcoin conference this weekend, but I
wrote down a few thoughts, to hopefully move us past this block size debate and
onto something more constructive:
    Comments/criticism welcome.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2016-03-08 22:05:07
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
I have no comment on whether this will be *needed* but there's a simple
algorithm that I haven't seen any coin adopt, that I think needs to be: the
critically damped harmonic oscillator:
    In dynamical systems one does a derivative expansion.  Here we want to find the
first and second derivatives (in time) of the hashrate.  These can be determined
by a method of finite differences, or fancier algorithms which use a quadratic
or quartic polynomial approximation.  Two derivatives are generally all that is
needed, and the resulting dynamical system is a damped harmonic oscillator.  A damped harmonic oscillator is basically how your car's shock absorbers work.
The relevant differential equation has two parameters: the oscillation frequency
and damping factor.  The maximum oscillation frequency is the block rate.  Any
oscillation faster than the block rate cannot be measured by block times.  The
damping rate is an exponential decay and for critical damping is twice the
oscillation frequency.
So, this is a zero parameter, optimal damping solution for a varying hashrate.
This is inherently a numeric approximation solution to a differential equation,
so questions of approximations for the hashrate enter, but that's all.  Weak
block proposals will be able to get better approximations to the hashrate.
If solving this problem is deemed desirable, I can put some time into this, or
direct others as to how to go about it.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2016-03-09 06:17:50
@_author: Bob McElrath 
@_subject: [bitcoin-dev] bitcoin-dev Digest, Vol 10, Issue 13 
That proposal is equivalent to an under-damped oscillator, and would still see
significant oscillation between blocks if miners were switching on and off
The optimal solution is the one I quote, and is well known, just not in the
bitcoin community.
"faster relaxation time" refers to the time constant of the exponential damping.
if you make it too fast, you create an over-damped oscillator.  The system
cannot measure oscillation faster than the block time, so damping on shorter
timescales is useless.  The optimal damping is given by the critically damped
Tonight at BitDevsNYC, Mike Wozniak pointed out that SPV wallets must also
calculate retargeting, but this is a terribly simple calculation, and while more
complex from a coding perspective, would not be noticeable in run-time of SPV
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2016-03-09 20:21:36
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
measurement, and error propagation can be performed to derive an error on the
The statistical theory of Bitcoin's block timing is known as a Poisson Point
Process:  or temporal point
process.  If you google those plus "estimation" you'll find a metric shit-ton of
literature on how to handle this.
You don't want to assume it's constant in order to get a better measurement.
The assumption is clearly false.  But, errors can be calculated, and retargeting
can take errors into account, because no matter what we'll always be dealing
with a finite sample.
Personally I don't think difficulty target variations are such a big deal, if
the algorithm targets that over any long time interval, the average block time
is 10 min.  Bitcoin's current algorithm fails here, with increasing hashrate (as
we have), it issues coins faster than its assumed schedule.
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2016-05-11 20:06:48
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Committed bloom filters for improved wallet 
I like this idea, but let's run some numbers...
Bloom filters completely obfuscate the required size of the filter for a desired
false-positive rate.  But, an optimal filter is linear in the number of elements
it contains for fixed false-positive rate, and logarithmic in the false-positive
rate.  (This comment applies to a RLL encoded Bloom filter Greg mentioned, but
that's not the only way)  That is for N elements and false positive rate
    filter size = - N \log_2 \epsilon
Given that the data that would be put into this particular filter is *already*
hashed, it makes more sense and is faster to use a Cuckoo[1] filter, choosing a
fixed false-positive rate, given expected wallet sizes.  For Bloom filters,
multiply the above formula by 1.44.
To prevent light clients from downloading more blocks than necessary, the
false-positive rate should be roughly less than 1/(block height).  If we take
the false positive rate to be 1e-6 for today's block height ~ 410000, this is
about 20 bits per element.  So for todays block's, this is a 30kb filter, for a
3% increase in block size, if blocks commit to the filter.  Thus the required
size of the filter commitment is roughly:
    filter size = N \log_2 H
where H is the block height.  If bitcoin had these filters from the beginning, a
light client today would have to download about 12MB of data in filters.  My
personal SPV wallet is using 31MB currently.  It's not clear this is a bandwidth
win, though it's definitely a win for computing load on full nodes.
[1] Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken

@_date: 2016-05-11 20:29:33
@_author: Bob McElrath 
@_subject: [bitcoin-dev] Committed bloom filters for improved wallet 
Eerrrr....let me revise that last paragraph.  That's 12 *GB* of filters at
today's block height (at fixed false-positive rate 1e-6.  Compared to block
headers only which are about 33 MB today.  So this proposal is not really
compatible with such a wallet being "light"...
Damn units...
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken
