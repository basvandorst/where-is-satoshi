
@_date: 2017-12-02 03:55:22
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Idea: Marginal Pricing 
I also am for the idea of removing blocksize limits if it is workable, however, would propose an alternative method for selecting transactions to be included in a block.
Some of the issues discussed in other replies to this thread are valid.
Alternative proposal:
Provide each transaction with a transaction weight, being a function of the fee paid (on a curve), and the time waiting in the pool (also on a curve) out to n days (n=30 ?), the transaction weight serving as the likelihood of a transaction being included in the current block, and then use an uncapped block size. The curve allows that the higher a fee allows a transaction to be much more likely to be included, the highest fee gets 100%, and, transactions at the n days limit get near 100%. Would need protocol enforcement since, as I understand, no miner would mine more transactions than are necessary to meet min blocksize. Other than that it should function fine. Non-urgent transactions pay a lower fee, people choose fees from fee recommendation based on how many days before a tx begins confirmation, all transactions are eventually included in the blockchain.
Damian Williamson
Sent: Thursday, 30 November 2017 11:47:43 AM
Long term, tx fees must support hash power by themselves. The following is an economic approach to maximize total fee collection, and therefore hashpower.
Maximize total transaction fees
Reduce pending transaction time
Reduce individual transaction fees
Validators must agree on the maximum block size, else miners can cheat and include extra transactions.
Allowing too many transactions per block will increase the cost of the mining without collecting much income for the network.
In the transaction market, users are the demand curve, because they will transact less when fees are higher, and prefer altcoins. The block size is the supply curve, because it represents miners' willingness to accept transactions.
Currently, the supply curve is inelastic:
?Increasing the block size will not affect the inelasticity for any fixed block size. The downsides of a fixed block size limit are well-known:
- Unpredictable transaction settlement time
- Variable transaction fees depending on network congestion
- Frequent overpay
1. Miners implicitly choose the market sat/byte rate with the cheapest-fee transaction included in their block. Excess transaction fees are refunded to the inputs.
2. Remove the block size limit, which is no longer necessary.
- Dynamic block size limit regulated by profit motive
- Transaction fees maximized for every block
- No overpay; all fees are fair
?Miners individually will make decisions to maximize their block-reward profit.
Miners are incentivized to ignore low-fee transactions because they would shave the profits of their other transactions and increase their hash time.
Users and services are free to bid higher transaction fees in order to reach the next block, since their excess bid will be refunded.
The block size limit was added as a spam-prevention measure, but in order for an attacker to spam the network with low-fee transactions, they would have to offset the marginal cost of reducing the price with their own transaction fees. Anti-spam is thus built into the marginal system without the need for an explicit limit.
Rarely, sections of the backlog would become large enough to be profitable. This means every so many blocks, lower-fee transactions would be included en masse after having been ignored long enough. Low-fee transactions thus gain a liveness property not previously enjoyed: low-fee transactions will eventually confirm. Miners targeting these transactions would be at a noteworthy disadvantage because they would be hashing a larger block. I predict that this scheme would result in two markets: a backlog market and a real-time market. Users targeting the backlog market would match the price of the largest backlog section in order to be included in the next backlog block.
Scenario 1
Sat/byte        Bytes   Reward
400     500000  200000000
300     700000  210000000
200     1000000 200000000
100     1500000 150000000
50      5000000 250000000
20      10000000        200000000
A miner would create a 5MB block and receive 0.25 BTC
Scenario 2
Sat/byte        Bytes   Reward
400     600000  240000000
300     700000  210000000
200     1000000 200000000
100     1800000 180000000
50      4000000 200000000
20      10000000        200000000
A miner would create a 600KB block and receive 0.24 BTC
William Morriss

@_date: 2017-12-03 04:07:16
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight For 
# BIP Proposal: UTWFOTIB - Use Transaction Weight For Ordering Transactions In Blocks
I admit, with my limited experience in the operation of the protocol, the section entitled 'Solution operation' may not be entirely correct but you will get the idea. If I have it wrong, please correct it back to the list.
 The problem:
Everybody wants value. Miners want to maximize revenue from fees. Consumers want transaction reliability and, (we presume) low fees.
Current transaction bandwidth limit is a limiting factor for both.
 Solution summary:
Provide each transaction with a transaction weight, being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=30 ?); the transaction weight serving as the likelihood of a transaction being included in the current block, and then use a target block size.
Protocol enforcement to prevent high or low blocksize cheating to be handled by having the protocol determine the target size for the current block using; current transaction pool size x ( 1 / (144 x n days ) ) = transactions to be included in the current block.
The curves used for the weight of transactions would have to be appropriate.
 Pros:
* Maximizes transaction reliability.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, confidence and uptake are greater; therefore, more transactions and more transactions total at priority fees.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy and greater security since there is less probability of predicting the next block.
 Cons:
* ?
* Must be first be programmed.
* Anything else?
 Solution operation:
As I have said, my simplistic view of the operation. If I have this wrong, please correct it back to the list.
1. The protocol determines the target block size.
2. Assign each transaction in the pool a transaction weight based on fee and time waiting in the transaction pool.
3. Begin selecting transactions to include in the current block using transaction weight as the likelihood of inclusion until target block size is met.
4. Solve block.
Damian Williamson

@_date: 2017-12-06 09:27:30
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Good afternoon ZmnSCPxj,
I have posted some discussion on the need for this proposal and, some refinements to the proposal explanation (not changes to the intended operation) to the bitcoin-discuss list. I didn't exactly mean to double post but thought it could use the discussion and, not to post it again, I will link to it when (if) it turns up, or will post it back here as an update on request. Currently, that post is awaiting moderator approval. I have also rewritten the solution operation section a bit in that post, not the idea that is being conveyed. I have added an additional step, reject blocks that do not meet the target block size for the current block.
I suggest it still should be added to the solution operation, to broadcast the next target block size with the current block when it is solved. Using that method may answer a part of your concern.
As I understand it, each node would be aware independently of x transactions waiting for confirmation, the transaction pool. Each node would no doubt have its own idea about how many waiting transactions there are and which particular transactions exist. I do not see why each node could not just work with the information at hand, it is my understanding that is what happens now, even with solved blocks vs the longest chain. I have not followed why you foresee from my proposal the need for fullnodes to back confirm the previous blocks in that manner.
If next blocksize is broadcast with the completed block it would be a simple matter to back confirm that. With transaction weight (transaction priority) I am suggesting that value gives the likelihood of a transaction being included, presuming an element of randomness as to whether any particular transaction is then included or not. Back confirmations on a transaction basis would be impossible anyway, all that could be confirmed is that a particular block has transactions that conform to a probability curve, if the variables are known, fee amount and time waiting in the pool, then the transaction priority can be recreated to establish that the probability of a particular block conforms. I certainly do not foresee including the full transaction pool in each block.
I am also presuming blocksize as a number of transactions rather than KB.
My suggestion, if adopted, is to directly make the operation of transaction priority a part of the operational standard - even without verification that it is being followed. The result of full transactional reliability is actually in the interests of miners as much as anyone.
The benefit of the proposal, not directly stated, is variable sized blocks (uncapped block size).
Yes, I have learned not to recycle terminology. My apologies, I had not been made aware that terminology already had use. Perhaps it would be simpler to call the proposal that I am putting forward here Transaction Priority.
Damian Williamson
Sent: Wednesday, 6 December 2017 4:46:45 PM
Good morning Damian,
The primary problem in your proposal, as I understand it, is that the "transaction pool" is never committed to and is not part of consensus currently.  It is unlikely that the transaction pool will ever be part of consensus, as putting the transaction pool into consensus is effectively lifting the block size to include the entire transaction pool into each block.  The issue is that the transaction pool is transient and future fullnodes cannot find the contents of the transaction pool at the time blocks were made and cannot verify the correctness of historical blocks.  Also, fullnodes using -blocksonly mode have no transaction pool and cannot verify incoming blocks for these new rules.
Applying a patch that follows this policy into Bitcoin Core without enforcing it in all fullnodes will simply lead to miners removing the patch in software they run, making it an exercise in futility to write, review, and test this code in the first place.
In addition, you reuse the term "weight" for something different than its current use.  Current use, is that the "weight" of a transaction, is the computed weight from the SegWit weight equation, measured in virtual units called "sipa", using the equation (4sipa / non-witness byte + 1sipa/witness byte).
Sent with ProtonMail Secure Email.
-------- Original Message --------
UTC Time: December 5, 2017 7:38 PM
# BIP Proposal: UTWFOTIB - Use Transaction Weight For Ordering Transactions In Blocks
I admit, with my limited experience in the operation of the protocol, the section entitled 'Solution operation' may not be entirely correct but you will get the idea. If I have it wrong, please correct it back to the list.
 The problem:
Everybody wants value. Miners want to maximize revenue from fees. Consumers want transaction reliability and, (we presume) low fees.
Current transaction bandwidth limit is a limiting factor for both.
 Solution summary:
Provide each transaction with a transaction weight, being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=30 ?); the transaction weight serving as the likelihood of a transaction being included in the current block, and then use a target block size.
Protocol enforcement to prevent high or low blocksize cheating to be handled by having the protocol determine the target size for the current block using; current transaction pool size x ( 1 / (144 x n days ) ) = transactions to be included in the current block.
The curves used for the weight of transactions would have to be appropriate.
 Pros:
* Maximizes transaction reliability.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, confidence and uptake are greater; therefore, more transactions and more transactions total at priority fees.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy and greater security since there is less probability of predicting the next block.
 Cons:
* ?
* Must be first be programmed.
* Anything else?
 Solution operation:
As I have said, my simplistic view of the operation. If I have this wrong, please correct it back to the list.
1. The protocol determines the target block size.
2. Assign each transaction in the pool a transaction weight based on fee and time waiting in the transaction pool.
3. Begin selecting transactions to include in the current block using transaction weight as the likelihood of inclusion until target block size is met.
4. Solve block.
Damian Williamson

@_date: 2017-12-07 06:34:39
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Hello Jim,
The variable block sizes would not, as I understand it, be easily implemented by a solo miner.
You are right, there is presently nothing stopping a miner from ordering the transactions included by a priority that is not entirely based on the fee.
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conform to a probability distribution curve, if that is necessary and,  *if* the individual transaction priority can be recreated. I am not that deep into the mathematics, however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Needs a clever mathematician.
It is certainly possible to verify that blocks conform to the expected size.
Honour is why people follow policy without enforcement. I may be in the wrong group. (sic)
Damian Williamson
Sent: Wednesday, 6 December 2017 4:18:11 PM
As i understand it, the transactions to be included in a block are entirely up to the miner of that block.
What prevents a miner from implementing the proposal on their own?
If this is adopted as some kind of "policy", what forces a miner to follow it?
Jim Renkel
# BIP Proposal: UTWFOTIB - Use Transaction Weight For Ordering Transactions In Blocks
I admit, with my limited experience in the operation of the protocol, the section entitled 'Solution operation' may not be entirely correct but you will get the idea. If I have it wrong, please correct it back to the list.
 The problem:
Everybody wants value. Miners want to maximize revenue from fees. Consumers want transaction reliability and, (we presume) low fees.
Current transaction bandwidth limit is a limiting factor for both.
 Solution summary:
Provide each transaction with a transaction weight, being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=30 ?); the transaction weight serving as the likelihood of a transaction being included in the current block, and then use a target block size.
Protocol enforcement to prevent high or low blocksize cheating to be handled by having the protocol determine the target size for the current block using; current transaction pool size x ( 1 / (144 x n days ) ) = transactions to be included in the current block.
The curves used for the weight of transactions would have to be appropriate.
 Pros:
* Maximizes transaction reliability.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, confidence and uptake are greater; therefore, more transactions and more transactions total at priority fees.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy and greater security since there is less probability of predicting the next block.
 Cons:
* ?
* Must be first be programmed.
* Anything else?
 Solution operation:
As I have said, my simplistic view of the operation. If I have this wrong, please correct it back to the list.
1. The protocol determines the target block size.
2. Assign each transaction in the pool a transaction weight based on fee and time waiting in the transaction pool.
3. Begin selecting transactions to include in the current block using transaction weight as the likelihood of inclusion until target block size is met.
4. Solve block.
Damian Williamson
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-07 08:13:14
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Good morning ZmnSCPxj, it must be where you are,
I suppose that we are each missing each other's point some.
I understand that nodes would not be expected to agree on the transaction pool and do not propose validating that the correct transactions are included in a block. I speak of probability and likelihood of a transaction being included in a block, implying a random element. I do not propose rejecting blocks on the basis that the next block size is stated too large or too small for the transaction pool, only that the block received conforms to the next block size given on the previous block. Yes, it could be cheated. Also, various nodes may have at times wildly different amounts of transactions waiting in the transaction pool compared to each other and there could be a great disparity between them. It would not be possible in any case I can think of to validate the next block size is correct for the current transaction pool. Even as it is now, nodes may include transactions in a block that no other nodes have even heard of, nodes have no way to validate that either. If the block is built on sufficiently, it is the blockchain.
I will post back the revised proposal to the list. I have fleshed parts of it out more, given more explanation and, tried this time not to recycle terminology.
Damian Williamson
Sent: Thursday, 7 December 2017 5:46:08 PM
Good morning Damian,
Each long-running node would have a view that is roughly the same as the view of every other long-running node.
However, suppose a node, Sleeping Beauty, was temporarily stopped for a day (for various reasons) then is started again.  That node cannot verify what the "consensus" transaction pool was during the time it was stopped -- it has no view of that.  It can only trust that the longest chain is valid -- but that means it is SPV for this particular rule.
It would not. Suppose Sleeping Beauty slept at block height 500,000.  On awakening, some node provides some purported block at height 500,001.  This block indicates some "next blocksize" for the block at height 500,002.  How does Sleeping Beauty know that the transaction pool at block 500,001 was of the correct size to provide the given "next blocksize"?  The only way, would be to look if there is some other chain with greater height that includes or does not include that block: that is, SPV confirmation.
How does Sleeping Beauty know it has caught up, and that its transaction pool is similar to that of its neighbors (who might be lying to it, for that matter), and that it should therefore stop using SPV confirmation and switch to strict fullnode rejection of blocks that indicate a "next blocksize" that is too large or too small according to your equation?  OR will it simply follow the longest chain always, in which case, it trusts miners to be honest about how loaded (or unloaded) the transaction pool is?
As a general rule, consensus rules should restrict themselves to:
1.  The characteristics of the block.
2.  The characteristics of the transactions within the block.
The transaction pool is specifically those transaction that are NOT in any block, and thus, are not safe to depend on for any consensus rules.

@_date: 2017-12-07 20:49:41
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight 
Good morning ZmnSCPxj,
Actually, there is no incentive to cheat target block size by providing a next block size that is higher or lower than the proposal would give. Under the proposal the transaction pool can grow quite large. A low next block size just defers collecting transaction fees, while a high next block size shrinks the transaction pool and thereby lowers fees. It seems like a standoff. This is especially true if the curve for time waiting in the transaction pool is extended beyond n days, since it is a curve, after waiting longer than 60 days (if n = 60 days) a transaction would have a priority greater than one-hundred and would therfore be the first transaction included with no possibility of failing the likelihood, so, even low fee paying transactions would be included first if the pool size is growing through incorrectly providing the next block size.
As it is now, I presume, a miner could include exactly one transaction in a block and pad?
Damian Williamson
Sent: Thursday, 7 December 2017 7:13:14 PM
Good morning ZmnSCPxj, it must be where you are,
I suppose that we are each missing each other's point some.
I understand that nodes would not be expected to agree on the transaction pool and do not propose validating that the correct transactions are included in a block. I speak of probability and likelihood of a transaction being included in a block, implying a random element. I do not propose rejecting blocks on the basis that the next block size is stated too large or too small for the transaction pool, only that the block received conforms to the next block size given on the previous block. Yes, it could be cheated. Also, various nodes may have at times wildly different amounts of transactions waiting in the transaction pool compared to each other and there could be a great disparity between them. It would not be possible in any case I can think of to validate the next block size is correct for the current transaction pool. Even as it is now, nodes may include transactions in a block that no other nodes have even heard of, nodes have no way to validate that either. If the block is built on sufficiently, it is the blockchain.
I will post back the revised proposal to the list. I have fleshed parts of it out more, given more explanation and, tried this time not to recycle terminology.
Damian Williamson
Sent: Thursday, 7 December 2017 5:46:08 PM
Good morning Damian,
Each long-running node would have a view that is roughly the same as the view of every other long-running node.
However, suppose a node, Sleeping Beauty, was temporarily stopped for a day (for various reasons) then is started again.  That node cannot verify what the "consensus" transaction pool was during the time it was stopped -- it has no view of that.  It can only trust that the longest chain is valid -- but that means it is SPV for this particular rule.
It would not. Suppose Sleeping Beauty slept at block height 500,000.  On awakening, some node provides some purported block at height 500,001.  This block indicates some "next blocksize" for the block at height 500,002.  How does Sleeping Beauty know that the transaction pool at block 500,001 was of the correct size to provide the given "next blocksize"?  The only way, would be to look if there is some other chain with greater height that includes or does not include that block: that is, SPV confirmation.
How does Sleeping Beauty know it has caught up, and that its transaction pool is similar to that of its neighbors (who might be lying to it, for that matter), and that it should therefore stop using SPV confirmation and switch to strict fullnode rejection of blocks that indicate a "next blocksize" that is too large or too small according to your equation?  OR will it simply follow the longest chain always, in which case, it trusts miners to be honest about how loaded (or unloaded) the transaction pool is?
As a general rule, consensus rules should restrict themselves to:
1.  The characteristics of the block.
2.  The characteristics of the transactions within the block.
The transaction pool is specifically those transaction that are NOT in any block, and thus, are not safe to depend on for any consensus rules.

@_date: 2017-12-07 21:01:43
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson

@_date: 2017-12-15 09:42:42
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson

@_date: 2017-12-15 20:59:51
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson

@_date: 2017-12-17 04:14:39
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.
It may be helpful to have the discussion from the previous thread linked here.
Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.
Damian Williamson
Sent: Saturday, 16 December 2017 7:59 AM
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson

@_date: 2017-12-19 07:48:37
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.
It is difficult to define then, what is a valid transaction? Clearly, my definition was insufficient.
Damian Williamson
Sent: Monday, 18 December 2017 11:09 PM
Regarding "problem"  where you say "How do we ensure that all valid transactions are eventually included in the blockchain?":  I do not believe that all people would (a) agree this is a problem or (b) that we do want to *ENSURE* that *ALL* valid transactions are eventually included in the blockchain.  There are many *valid* transactions that oftentimes miners do not (and should not) wish to require be confirmed and included in the blockchain.  Spam transactions for example can be valid, but used to attack bitcoin by using no or low fee.  Any valid transaction MAY be included by a miner, but requiring it in some fashion at this point would open the network to other attack vectors.  Perhaps you meant it a different way.

@_date: 2017-12-19 07:51:39
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.
Do you have any critical suggestion as to how transaction bandwidth limit could be addressed, it will eventually become an issue if nothing is changed regardless of how high fees go?
Damian Williamson
Sent: Tuesday, 19 December 2017 3:08 AM
Damian, you seem to be misunderstanding that either
(1) the strong form of your proposal requires validating the commitment to the mempool properties, in which case the mempool becomes consensus critical (an impossible requirement); or
(2) in the weak form where the current block is dependent on the commitment in the last block only it is becomes a miner-selected field they can freely parameterize with no repercussions for setting values totally independent of the actual mempool.
If you want to make the block size dependent on the properties of the mempool in a consensus critical way, flex cap achieves this. If you want to make the contents or properties of the mempool known to well-connected nodes, weak blocks achieves that. But you can?t stick the mempool in consensus because it fundamentally is not something the nodes have consensus over. That?s a chicken-and-the-egg assumption.
Finally in terms of the broad goal, having block size based on the number of transactions is NOT something desirable in the first place, even if it did work. That?s effectively the same as an infinite block size since anyone anywhere can create transactions in the mempool at no cost.
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.
It may be helpful to have the discussion from the previous thread linked here.
Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.
Damian Williamson
Sent: Saturday, 16 December 2017 7:59 AM
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-19 09:05:34
@_author: Damian Williamson 
@_subject: [bitcoin-dev] A DNS-like decentralized mapping for wallet addresses? 
There is no reason it should not be easily possible to develop a Bitcoin wallet that has an integrated name to address mapping feature. It might be a good idea for a software product, it could even be based on Bitcoin Core. There is no specific reason that people wanting that sort of feature could not use it. In fact, you could map names, strings, email addresses, it could be very flexible.
Relying on an additional service like DNS which is flexible enough to handle the job, does introduce an additional availability risk. There is no additional privacy risk provided each mapped name or address is only used once to send/receive one payment unless you directly use something personally identifiable like an email address which could be used to map bitcoin addresses to an individual. Personally, I am not concerned about privacy so much but can understand that some highly value their privacy.
If you get it right it will be a service better than namecoin transacting in Bitcoin. If you think that is valuable, go for it.
Damian Williamson
Sent: Monday, 18 December 2017 10:26 PM
Have you thought about combining this with BIP-47? You could associate payment codes with email via DNS.
It would be nice if there was a way to get rid of the announcement transaction in BIP-47 and establish a shared secret out of bound. That would simplify things, at the cost of an additional burden of storing more than an HD seed to recover a wallet that received funds this way.
Perhaps the sender can email to the recipient the information they need to retrieve the funds. The (first) transaction could have a time locked refund in it, in case the payment code is stale.

@_date: 2017-12-21 11:19:52
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Sign / Verify message against SegWit P2SH addresses. 
In all seriousness, being able to sign a message is an important feature whether it is with Bitcoin Core or, with some other method. It is a good feature and it would be worthwhile IMHO to update it for SegWit addresses. I don't know about renewing it altogether, I like the current simplicity.
Damian Williamson
(Of course, signed messages will verify better usually with plain text and not HTML interpreted email - need a switch for outlook.com to send plaintext.)
Sent: Wednesday, 20 December 2017 8:58 AM
For what it?s worth, I think it would be quite easy to do better than the implied solution of rejiggering the message signing system to support non-P2PKH scripts. Instead, have the signature be an actual bitcoin transaction with inputs that have the script being signed. Use the salted hash of the message being signed as the FORKID as if this were a spin-off with replay protection. This accomplishes three things:
(1) This enables signing by any infrastructure out there ? including hardware wallets and 2FA signing services ? that have enabled support for FORKID signing, which is a wide swath of the ecosystem because of Bitcoin Cash and Bitcoin Gold.
(2) It generalizes the message signing to allow multi-party signing setups as complicated (via sighash, etc.) as those bitcoin transactions allow, using existing and future tools based on Partially Signed Bitcoin Transactions; and
(3) It unifies a single approach for message signing, proof of reserve (where the inputs are actual UTXOs), and off-chain colored coins.
There?s the issue of size efficiency, but for the single-party message signing application that can be handled by a BIP that specifies a template for constructing the pseudo-transaction and its inputs from a raw script.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-22 06:22:40
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
If the cash value of Bitcoin was high enough and zero fee transactions were never accepted and not counted when calculating the transaction pool size then I do not think it would be such an issue. Why is it even possible to create zero fee transactions?
Damian Williamson
Sent: Tuesday, 19 December 2017 6:51 PM
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.
Do you have any critical suggestion as to how transaction bandwidth limit could be addressed, it will eventually become an issue if nothing is changed regardless of how high fees go?
Damian Williamson
Sent: Tuesday, 19 December 2017 3:08 AM
Damian, you seem to be misunderstanding that either
(1) the strong form of your proposal requires validating the commitment to the mempool properties, in which case the mempool becomes consensus critical (an impossible requirement); or
(2) in the weak form where the current block is dependent on the commitment in the last block only it is becomes a miner-selected field they can freely parameterize with no repercussions for setting values totally independent of the actual mempool.
If you want to make the block size dependent on the properties of the mempool in a consensus critical way, flex cap achieves this. If you want to make the contents or properties of the mempool known to well-connected nodes, weak blocks achieves that. But you can?t stick the mempool in consensus because it fundamentally is not something the nodes have consensus over. That?s a chicken-and-the-egg assumption.
Finally in terms of the broad goal, having block size based on the number of transactions is NOT something desirable in the first place, even if it did work. That?s effectively the same as an infinite block size since anyone anywhere can create transactions in the mempool at no cost.
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.
It may be helpful to have the discussion from the previous thread linked here.
Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.
Damian Williamson
Sent: Saturday, 16 December 2017 7:59 AM
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-23 01:24:28
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
I suppose what I intended is (2) the weak form but, what is essentially needed is (1) the strong form. The answer may be somewhere in-between.
I do not see that an entire consensus for the mempool is needed, each node just needs a loose understanding of the average number of non-zero fee transactions in the mempool.
As a pre-rollout, it would be possible to give each node a serial ID and, calculate the average number of non-zero fee transactions from the information it has and, say every ten minutes, distribute information it has about the number of transactions in the mempool. Each node would be able to form its own picture of the average number of non-zero fee transactions in the mempool.
At rollout, this information would be the basis a node would use when a block is solved to provide the next expected block size. This would still not stop cheating by providing especially a number lower than the proposal would allow for, to game the system and hike fees. If miners will not act in the long-term interest of the stability and operation of the system then they should be ignored. If most miners will adhere to the proposal then the average effect would be stability in the operation of the proposal, having a few or even several nodes posting low numbers for the number of transactions expected in the next expected block size would not destroy the operation. If some node posted an insanely high number for next expected block size that resulted in the mempool being emptied then the proposal would be offended but I do not actually care. If no number is posted, just create a block the appropriate size ensure conformity. Nodes that have not adopted the proposal could just continue to create 1MB blocks.
Actually, the operation could be simplified using the distributed information directly to just create blocks of the appropriate size with no need to provide next block size. Flexible block size.
The proposal should also specify a minimum number of transactions to include for the next block to give at a minimum a 1MB block.
I currently have no information on flex cap, do you have a link?
Damian Williamson
Sent: Tuesday, 19 December 2017 3:08 AM
Damian, you seem to be misunderstanding that either
(1) the strong form of your proposal requires validating the commitment to the mempool properties, in which case the mempool becomes consensus critical (an impossible requirement); or
(2) in the weak form where the current block is dependent on the commitment in the last block only it is becomes a miner-selected field they can freely parameterize with no repercussions for setting values totally independent of the actual mempool.
If you want to make the block size dependent on the properties of the mempool in a consensus critical way, flex cap achieves this. If you want to make the contents or properties of the mempool known to well-connected nodes, weak blocks achieves that. But you can?t stick the mempool in consensus because it fundamentally is not something the nodes have consensus over. That?s a chicken-and-the-egg assumption.
Finally in terms of the broad goal, having block size based on the number of transactions is NOT something desirable in the first place, even if it did work. That?s effectively the same as an infinite block size since anyone anywhere can create transactions in the mempool at no cost.
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.
It may be helpful to have the discussion from the previous thread linked here.
Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.
Damian Williamson
Sent: Saturday, 16 December 2017 7:59 AM
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-24 02:57:38
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP 177: UTPFOTIB - Use Transaction Priority For 
BIP 177: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
This BIP proposes to address the issue of transactional reliability in Bitcoin, where valid transactions may be stuck in the mempool for extended periods.
There are two key issues to be resolved:
  1.  The current transaction bandwidth limit.
  2.  The current ad-hoc methods of including transactions in blocks resulting in variable and confusing confirmation times for valid transactions, including transactions with a valid fee that may never confirm.
It is important with any change to protect the value of fees as these will eventually be the only payment that miners receive. Rather than an auction model for limited bandwidth, the proposal results in a stable fee for priority service auction model.
I will post the full proposal up on to my blog in the coming days and, re-review incorporating feedback that I have received on and off thread. It would not be true to suggest that all feedback received has been entirely positive although, most of it has been constructive.
The previous threads for this BIP are available here:
Damian Williamson

@_date: 2017-12-24 01:13:27
@_author: Damian Williamson 
@_subject: [bitcoin-dev]  what do you think about having a maximum fee rate? 
If all transactions pay the proposed max then fee there are still going to be an awful lot of never confirming transactions once the transaction bandwidth limit is surpassed, as I suppose that it roughly is now:
This is what I have been working on as an alternative:
There is a previous thread, linked later on in the linked thread.
Damian Williamson
Sent: Friday, 22 December 2017 7:26:12 PM
I'm not a bitcoin developer, but I'd like to receive feedback on what
I think is a serious problem. Hope I'm not wasting your time.
I'm also sure this was already discussed, but google doesn't give me
any good result.
Let me explain: I think that the current incentive system doesn't
really align with the way miners are distributed (not very
decentralized, due to pools and huge asic producers).
I think big miners are incentivized to spam the network with low(ish)
fee transactions, thereby forcing regular users into paying extremely
high fees to be able to get their transactions confirmed.
Obviously this is the result of insufficient mining decentralization,
but as I will try to show, such an attack could be profitable even if
you are controlling just 5-10% of the hashing power, which could
always be easy for a big player and with some collusion.
Let's look at some numbers: These are 10 blocks mined yesterday, and they all have rewards hugely
exceeding the normal 12.5 mining output. Even taking the lowest value
of 20, it's a nice 60% extra profit for the miner. Let's say you
control 10% of the hashing power, and you spam enough transactions to
fill 144 blocks (1 day's worth) at 50 satoshi/byte, losing just 72 BTC
in fees.
(blocksize-in-bytes * fee-per-byte * Nblocks)/satoshis-in-btc => (1e6
* 50 * 144)/1e8 => 72
At the same time you will discover about 144*0.1=14.4 blocks per day.
Assuming the situation we see in the previous screenshot is what
happens when you have a mempool bigger than one day's worth of blocks,
you would get 20-12.5=7.5 extra BTC per block, which is 14.4*7.5=108
BTC, given your investment of 72 to spam the mempool. 32 btc extra
The big assumption here is that spamming 1 day of backlog in the
50satoshi/b range will get people to compete enough to push 7.5 btc of
fees in each block, but:
*  this seems to confirm that
Johoe's Mempool Size Statistics - jochen-hoenicke.de/queue
This page displays the number and size of the unconfirmed bitcoin transactions, also known as the transactions in the mempool. It gives a real-time view and shows how ...
about half the mempool is in the 50satoshi/b range or less.
*  there are miners that control more than 10%
Bitcoin Hashrate Distribution - Blockchain.info
A pie chart showing the hashrate distribution between the major bitcoin mining pools - Blockchain
* if you get enough new real transactions, it's not necessary to spam
a full 144 blocks worth each day, probably just ~50 would be enough,
cutting the spam cost substantially
* other miners could be playing the same game, helping you spam and
further reduce the costs of the attack
* you actually get 10% of the fees back by avoiding mining your spam
transactions in your own blocks
* most of the spam transactions won't actually end up in blocks if
there is enough pressure coming from real usage
This seems to indicate that you would actually get much higher profit
margins than my estimates. **PLEASE** correct me if my calculations or
my assumptions are wrong.
You might also say that doing this would force users out of the
system, decreasing the value of btc and disincentivizing miners from
continuing. On the other hand, a backlogged mempool could create the
impression of high(er) usage and increase scarcity by slowing down
movements, which could actually push the price upwards.
Of course, it's impossible to prove that this is happening. But the
fact that it is profitable makes me believe that it is happening.
I see some solutions to this, all with their own downsides:
- increasing block size every time there is sustained pressure
this attack wouldn't work, but the downsides have already been
discussed to death.
- change POW
Not clear it would fix this, aside from stimulating terrible
infighting. Controlling 5 to 10% of the hashing power seems too easy,
and I don't think it would be practical to change pow every time that
happens, as it would prevent the development of a solid POW support.
- protocol level MAX transaction fee
I personally think this would totally invalidate the attack by making
the spam more expensive than the fees you would recover.
There already is a minimum fee accepted by the nodes, at 1 satoshi per
byte. The maximum fee could be N times the minimum, maybe 100-200.
Meaning a maximum of 1-2btc in total fee rewards when the block size
is 1mb. Of course the actual values need more analysis, but 2btc -
together with the deflationary structure - seems enough to continue
motivating miners, without giving unfair advantage.
Yes, this would make it impossible to spend your way out of a
congested mempool. But if the mempool stays congested after this
change, you could have a bigger confidence that it's coming from real
usage or from someone willfully burning money, making a block size
increase much more justified.
Hope to hear your opinion,
have a nice day.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev Info Page - Linux Foundation
Bitcoin development and protocol discussion. This list is lightly moderated. - No offensive posts, no personal attacks. - Posts must concern development of bitcoin ...

@_date: 2017-12-24 22:20:50
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP 177: UTPFOTIB - Use Transaction Priority For 
My mistake, apologies all.
 - I honestly thought everyone just took the next available number and published up their BIP's.
And, I see you have something of a master list.
As a suggestion, would it be worth considering linking to some of that information in the list welcome email? Web search is not always your friend for locating everything relevant.
Damian Williamson
Sent: Sunday, 24 December 2017 6:21:24 PM
BIP 177 is NOT assigned. Do not self-assign BIP numbers!
Please read BIP 2:
    bips/bip-0002.mediawiki at master ? bitcoin/bips ? GitHub
Abstract. A Bitcoin Improvement Proposal (BIP) is a design document providing information to the Bitcoin community, or describing a new feature for Bitcoin or its ...

@_date: 2017-12-24 03:44:26
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Alright, but, we seem to agree, the current system also has flaws. The transaction bandwidth limit is a serious issue for transactional reliability.
What you have proposed is interesting but seems to do nothing for the issue of transaction bandwidth, which seems to be approaching its threshold:
Damian Williamson
Sent: Saturday, 23 December 2017 5:07:49 AM
Hi Damian,
Thought I'd chip in.  This is a hard fork scenario. This system has flaws, they all do.
If you had a fixed fee per block, so that every txn in that block paid the same fee, that might make it easier to include all txns eventually, as you envisage.
The fee could be calculated as the average of the amount txns are prepared to pay in the last 1000 blocks.
A txn would say ' I'll pay up to X bitcoins ' and as long as that is more than the value required for the block your txn can be added. This is to ensure you don't pay more than you are willing.  It also ensures that putting an enormous fee will not ensure your txn is processed quickly..
Calculating what the outputs are given a variable fee needs a new mechanism all of it's own, but I'm sure it's possible.
The simple fact is that there is currently no known system that works as well as the current system..
But there are other systems.
If the cash value of Bitcoin was high enough and zero fee transactions were never accepted and not counted when calculating the transaction pool size then I do not think it would be such an issue. Why is it even possible to create zero fee transactions?
Damian Williamson
Sent: Tuesday, 19 December 2017 6:51 PM
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.
Do you have any critical suggestion as to how transaction bandwidth limit could be addressed, it will eventually become an issue if nothing is changed regardless of how high fees go?
Damian Williamson
Sent: Tuesday, 19 December 2017 3:08 AM
Damian, you seem to be misunderstanding that either
(1) the strong form of your proposal requires validating the commitment to the mempool properties, in which case the mempool becomes consensus critical (an impossible requirement); or
(2) in the weak form where the current block is dependent on the commitment in the last block only it is becomes a miner-selected field they can freely parameterize with no repercussions for setting values totally independent of the actual mempool.
If you want to make the block size dependent on the properties of the mempool in a consensus critical way, flex cap achieves this. If you want to make the contents or properties of the mempool known to well-connected nodes, weak blocks achieves that. But you can?t stick the mempool in consensus because it fundamentally is not something the nodes have consensus over. That?s a chicken-and-the-egg assumption.
Finally in terms of the broad goal, having block size based on the number of transactions is NOT something desirable in the first place, even if it did work. That?s effectively the same as an infinite block size since anyone anywhere can create transactions in the mempool at no cost.
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.
It may be helpful to have the discussion from the previous thread linked here.
Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.
Damian Williamson
Sent: Saturday, 16 December 2017 7:59 AM
There are really two separate problems to solve.
  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?
Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.
I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.
Not a necessary function, just an effect of using a probability-based distribution.
I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.
It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.
Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.
I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.
If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.
Damian Williamson
Sent: Saturday, 16 December 2017 3:38 AM
Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)
But just some quick notes:
* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.
* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.
* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.
If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.
-------- Original Message --------
UTC Time: December 15, 2017 9:42 AM
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?
I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.
It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.
If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?
Damian Williamson
Sent: Friday, 8 December 2017 8:01 AM
Good afternoon,
The need for this proposal:
We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.
I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.
Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.
Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.
I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.
I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.
I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.
# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
 The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.
Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.
Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.
 Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.
Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.
The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?
**Explanation of the operation of priority:**
I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.
 Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.
 Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.
 Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.
1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.
 Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.
I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.
Damian Williamson
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2017-12-26 05:14:14
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
I have needed to re-tac my intentions somewhat, there is still much
work to be done.
This is a request for assistance and further discussion of the re-
revised proposal. I am sure there are still issues to be resolved.
 Proposal: UTPFOTIB - Use Transaction Priority For Ordering
Transactions In Blocks
Document: BIP Proposal??
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Author: Damian Williamson <willtech at live.com.au> ?
Licence: Creative Commons Attribution-ShareAlike 4.0 International
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.??The current transaction bandwidth limit.
2.??The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here: ?
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm when 2x
transactions is reached. Presently, not even using fee recommendations
can ensure a sufficiently high fee is paid to ensure transaction
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.??Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
information survives node restart. If there is a more reliable way to
determine when a transaction was first seen on the network then it
should be utilised.
Use a dynamic target block size to make the current block. If the block
size is consistently too small then I expect ageing transactions will
be overrepresented as a portion of the block contents, to the point
where blocks will only contain the oldest transactions as they age past
n days. If block size is too large on average then this will shrink the
transaction pool. Determine the target block size using; pre-
rollout(current average valid transaction pool size) x ( 1 / (144 x n
days ) ) = number of transactions to be included in the current block.
The block created should be a minimum 1MB in size regardless if the
target block size is lower.
Nodes that have not yet adopted the proposal will just continue to
create 1MB unordered blocks.
The default value for mempoolexpiry may in future need to be adjusted
to match n days or, perhaps using less than n = 14 days may be a more
sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part??(yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*???Maximizes transaction reliability.
*???Overcomes transaction bandwidth limit.
*???Fully scalable.
*???Maximizes possibility for consumer and business uptake.
*???Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*???Market determines fee paid for transaction priority.
*???Fee recommendations work all the way out to 30 days or greater.
*???Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*???Could initially lower total transaction fees per block.
*???Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
**The service must:**
*???Have an individual temporary (runtime permanent only) Serial Node
*???Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*???Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*???Expire any value not updated for k minutes (k = 30 minutes?).
*???Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*???Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*???Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*???All known mempool information must survive node restart.
*???If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*???Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.??Determine the target block size for the current block.
2.??Assign a transaction priority to each valid transaction in the
3.??Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.??Solve block.
5.??Broadcast the current block when it is solved.
6.??Block is received.
7.??Block verification process.
8.??Accept/reject block based on verification result.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Damian Williamson
[![Creative Commons License](
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at [
Permissions beyond the scope of this license may be available at [https

@_date: 2017-12-27 12:29:41
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Good evening ZmnSCPxj,
That you for your considered discussion.
Am I wrong to think that any fullnode can validate blocks conform to a probability distribution? In my understanding after adoption of the proposal, any full node could validate all properties that a block has that they now validate, apart from block size, and additionally that the block conforms to a probability distribution. It seems a yes-no result. Let us assume that such a probability distribution exists since the input is a probability.
Before or after the proposal, miners could falsify transactions if there is a feasible way for them to do this. The introduction of the proposal does not change that fact. At the moment the incentive to falsify transactions is to fill blocks so that real transactions must pay the highest possible fees in the auction for limited transaction bandwidth resulting in a net gain for miners. Simply making bigger blocks serves no economic purpose in itself, since the miners we presume must pay the fees for their falsified transactions, there is no net gain, the fee will be distributed through the pool. Unless, by miners, I may presume we mostly mean mining pools and collusion. Still, where is the gain? It is only the blocks that will be larger with no economic advantage.
In a fee for priority service auction, there is always limited space in each new block since it represents only a small fraction of the size of the mempool. Presenting fraudulent transactions at the bottom end of the scale has limited effect on the cost of being near the front of the queue, at priority. As the fraudulent transactions age they would be included in blocks presuming the fee is above dust level, but the block size would grow to accommodate them since the valid mempool is larger. The auction for priority still continues uninterrupted at the top of the priority curve. There is nothing stopping a motivated individual now from writing a script to create a million pointless dust transactions per day, flooding the mempool. Even if the fee is above dust level the proposal does not change this but, ensures transactional reliability for valid transactions.
In an idealist world, all nodes could agree on the state of the mempool. I agree, there is no feasible way currently to hold the mempool to consensus without a network of dedicated mempool servers. As it is, it has been suggested that all long-running nodes will have approximately a similar view of the mempool. Sweeping the entire mempool contents per block would achieve what is required if there was a mempool consensus but since it will just be one node's view of the mempool that will not be the result.
My speculation is that as a result of the proposal, through increased adoption of Bitcoin over time there would, in fact, be more transactions and greater net fees paid per day. An increased value of BTC that we suppose would follow from increased usage would augment this fee value increase. It surely follows that a more stable and reliable service will have greater consumer and business acceptance, and there it follows that this is in miners financial interest.
I have not considered a maxblocksize since I consider that the mempool can eventually grow infinitely in size just in valid transactions, without even any fraudulent transactions. I suppose that in time it will become necessary to start all new nodes in pruned mode by default due to the onerous storage requirements of the full blockchain. I do not think that the proposed changes alter this.
I am sure that there is much more to write.
Damian Williamson
Sent: Wednesday, 27 December 2017 2:55 PM
Good morning Damian,
I see you have modified your proposal to be purely driven by miners, with fullnodes not actually being able to create a strict "yes-or-no" answer as to block validity under your rules.  This implies that your rules cannot be enforced and that rational miners will ignore your proposal unless it brings in more money for them.  The fact that your proposal provides some mechanism to increase block size means that miners will be incentivized to falsify data (by making up their own transactions just above your fixed "dust size" threshhold whatever that threshhold may be -- and remember, miners get at least 12.5 BTC per block, so they can make a lot of little falsified transactions to justify every block size increase) until the block size increase per block is the maximum possible block size increase.
Let me then explain proof-of-work and the arrow of time in Physics.  It may seem a digression, but please, bear with me.
Proof-of-work proves that work was performed, and (crucially) that this work was done in the past.
This is important because of the arrow of time.
In principle, every physical interaction is reversible.  Visualize a video of two indivisible particles.  The two particles move towards each other, collide, and because of the collision, fly apart. If you ran this video in reverse, or in forward, it would not be distinguishable to you, as an outside observer, whether the video was running in reverse or not.  It seems at some level, time does not exist.
And yet time exists.
Consider another video, that of a vase being dropped on a hard surface.  The vase hits the surface and shatters.  Played in reverse, we can judge it as nonsensical: scattered pieces of ceramic spontaneously forming a vase and then flying upwards.  This orients our arrow of time: the arrow of time points from states of the universe where lesser entropy exists (the vase is whole) to where greater entropy exists (the vase is in many pieces).
Indeed, all measures of time are, directly or indirectly, measures of increases in entropy.  Consider a simple hourglass: you place it into a state of low entropy and high energy with most of the sand is in the upper part of the hourglass.  As sand falls, and more of that energy is lost into entropy, you judge that time passes.
Consider a proof-of-work algorithm: you place electrons into a state of low entropy and high energy.  As electrons go through the mining hardware, producing hashes that pass the difficulty requirement, the energy in those electrons is lost into entropy (heat), and from the hashes produced (which proves not only that work was done, but in particular, that entropy increased due to work being done), you judge that time passes.
Thus, the blockchain itself is already a service that provides a measure of time.  When a block commits to a transaction, then that transaction is known to have existed at that block height, at the latest.
Thus one idea, is to have each block commit to some view of the mempool.  If a transaction exists in this mempool-view, then you know that the transaction is at least that old, and can judge the age from this and use this to compute the "transaction priority".
Unfortunately, transferring the data to prove that the mempool-view is valid, is equivalent to always sweeping the entire mempool contents per block.  In that case you might as well not have a block size limit.
In addition, miners may still commit to a falsely-empty mempool and deny that your transaction is old and therefore priority and therefore will simply fill their blocks with transactions that have high feerates rather than high priority.  Thus feerate will still be the ultimate measure.
Rather than attempt this, perhaps developers should be encouraged to make use of existing mechanisms, RBF and CPFP, to allow transactions to be sped up by directly manipulating feerates, as priority (by your measure) is not practically computable.

@_date: 2018-04-01 14:37:13
@_author: Damian Williamson 
@_subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation 
I note that Electrum v3.0.6 has an option to use multiple change addresses. It is off by default.
Damian Williamson
Sent: Monday, 19 March 2018 5:59:28 AM
Without commenting on the merits of this proposal, I?d just like to correct this common misperception. There is no necessary additional cost to the network from the count of unspent outputs. This perception arises from an implementation detail of particular node software. There is no requirement for redundant indexing of unspent outputs.

@_date: 2018-02-06 07:07:26
@_author: Damian Williamson 
@_subject: [bitcoin-dev] NIST 8202 Blockchain Technology Overview 
Then you have my apology, I will not claim to be any kind of advocate or user of Bitcoin Cash but *had* understood that segwith had been enabled. Clearly my mistake.
Damian Williamson
Sent: Tuesday, 6 February 2018 1:08:24 PM
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
You do realize that segwit includes many improvements of which are unrelated to scaling? These same improvements of which
simply increasing the blocksize alone would not fix or enable. Segwit is not just a blocksize increase.
Bitcoin Cash, while increasing the blocksize directly, from my understanding has yet to implement the
improvements and capabilities that segwit enables.
One example being, with transactions hashes being able to be calculated in advanced prior to signing
(due to the signature being in different section than that used
to calculate the transaction ID) it is possible to create transaction trees, enhanced smart contracts, trustless mixing protocols,
micropayment networks, etc...
Segwit also increases the security of signatures.
There are lots of other things segregated witness enables as well.
By saying "..segwit changes alone.... decided to also..." Bitcoin Cash has not implemented segwit. Bitcoin Cash only
increased the blocksize.
that wording above at least from the way I read it, seems to imply that Bitcoin Cash has segwit.

@_date: 2018-02-14 10:09:25
@_author: Damian Williamson 
@_subject: [bitcoin-dev]  Possible change to the MIT license 
I do not know that Bitcoin's position is any weaker because of the terms that the software is licenced under.
Cory Fields said:
I disagree completely with any licence change. As well as allowing for the use of a software, a licence is also a disclaimer for those responsible for the release. Changing a single word can have drastic implications should there ever be any action considered against any involved party or group. The current MIT licence is IMHO the right fit.
Damian Williamson

@_date: 2018-02-19 09:04:02
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Some thoughts on removing timestamps in PoW 
What you are suggesting, unless I am mistaken, is that new full nodes should have no way of knowing if an output is spent or even if it exists. Since large sections of the blockchain will potentially be skipped, the full node will not have complete knowledge of utxo's just for starters.
Damian Williamson
Sent: Monday, 19 February 2018 12:29:50 PM
Copied from: # Blockchain Timestamps Unnecessary In Proof-of-Work?
*Author: Greg Slepak ([ at mastodon.social](
The Bitcoin blockchain has a 10-minute target blocktime that is achieved by a difficulty adjustment algorithm.
I assert, or rather, pose the hypothesis, that the use of timestamps in Bitcoin's blockchain may be unnecessary, and that Bitcoin can operate with the same security guarantees without it (except as noted in [Risks and Mitigations]( and therefore does not need miners to maintain global clock synchronization.
The alternative difficulty adjustment algorithm would work according to the following principles:
- The incentive for miners is and always has been to maximize profit.
- The block reward algorithm is now modified to issue coins into perpetuity (no maximum). Any given block can issue _up to_ `X` number of coins per block.
- The number of coins issued per block is now tied directly to the difficulty of the block, and the concept of "epocs" or "block reward halving" is removed.
- The chain selection rule remains "chain with most proof of work"
- The difficulty can be modified by miners in an arbitrary direction (up or down), but is limited in magnitude by some maximum percentage (e.g. no more than 20% deviation from the previous block), we call this `Y%`.
 Observations
- Miners are free to mine blocks of whatever difficulty they choose, up to a maximum deviation
- The blockchain may at times produce blocks very quickly, and at other times produce blocks more slowly
- Powerful miners are incentivized to raise the difficulty to remove competitors (as is true today)
- Whether miners choose to produce blocks quickly or slowly is entirely up to them. If they produce blocks quickly, each block has a lower reward, but there are more of them. If they produce blocks slowly, each block has a higher reward, but there are fewer of them. So an equilibrium will be naturally reached to produce blocks at a rate that should minimize orphans.
A timestamp may still be included in blocks, but it no longer needs to be used for anything, or represent anything significant other than metadata about when the miner claims to have produced the block.
 Risks and Mitigations
Such a system may introduce risks that require further modification of the protocol to mitigate.
The most straightforward risk comes from the potential increase in total transaction throughput that such a change would introduce (these are the same concerns that exist with respect to raising the blocksize). The removal of timestamps would allow a cartel of miners to produce high-difficulty blocks at a fast rate, potentially resulting in additional centralization pressures not only on miners but also on full nodes who then would have greater difficulty keeping up with the additional bandwidth and storage demands.
Two equally straightforward mitigations exist to address this if we are given the liberty of modifying the protocol as we wish:
1. Introducing state checkpoints into the chain itself could make it possible for full nodes to skip verification of large sections of historical data when booting up.
2. A sharded protocol, where each shard uses a "sufficiently different" PoW algorithm, would create an exit for users should the primary blockchain become captured by a cartel providing poor quality-of-service.
Please do not email me anything that you are not comfortable also sharing with the NSA.

@_date: 2018-02-24 06:00:48
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Activation Reference 
Would it be possible or desirable to add a `nBlockHeight Activated` column to the [README.mediawiki]( to show a specific reference to when a BIP was activated? - And/or include such information in the BIP Header format?
Damian Williamson

@_date: 2018-01-01 11:04:57
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Happy New Year all.
This proposal has been further amended with several minor changes and a
few additions.
I believe that all known issues raised so far have been sufficiently
addressed. Either that or, I still have more work to do.
 BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks
Schema: ?
 ?
Document: BIP Proposal ?
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Blocks ?
Published: 26-12-2017 ?
Revised: 01-01-2018 ?
Author: Damian Williamson ??
Licence: Creative Commons Attribution-ShareAlike 4.0 International
License. ?
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.??The current transaction bandwidth limit.
2.??The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here: ?
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
While the miners have discovered a gold mine, it is the service they
provide that is valuable. If the service is unreliable they are not
worth the gold that they mine. This is reflected in the value of
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm by the time
2x transactions is reached. Presently, not even using fee
recommendations can ensure a sufficiently high fee is paid to ensure
transaction confirmation.
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.??Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
Bitcoin should be capable of reliably and inexpensively processing
casual transactions, and also priority processing of fee paying at
auction for priority transactions in the shortest possible timeframe.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
full mempool and information survives node restart. If there is a more
reliable way to determine when a transaction was first seen on the
network then it should be utilised.
currently seem to save and load the mempool on restart, despite the
notes in the command line options panel that the default for
persistmempool is 1. In the debug panel, some 90,000 transactions
before restart, some 200 odd shortly after. Manually setting
persistmempool=1 in the conf file does not seem to make any difference.
Perhaps it is operating as expected and I am not sure what to observe,
but does not seem to be observably saving and loading the mempool on
restart. This will need to be resolved.
Use a dynamic target block size to make the current block. This marks a
shift from using block size or weight to a count of transactions.
Determine the target block size using; pre-rollout(current average
valid transaction pool size) x ( 1 / (144 x n days ) ) = number of
transactions to be included in the current block. The block created
should be a minimum 1MB in size regardless if the target block size is
If the created block size consistently contains too few transactions
and the number of new transactions created is continuously greater than
the block size will accommodate then I expect eventually ageing
transactions will be over-represented as a portion of the block
contents. Once another new node conforming to the proposal makes a
block, the block size will be proportionately larger as the transaction
pool has grown.??If block size is too large on average then this will
shrink the transaction pool.
Miners will likely want to conform to the proposal, since making blocks
larger than necessary makes more room in each block potentially
lowering the highest fees paid for priority service. Always making
blocks smaller than the proposal requires will in time lower the
utility value of Bitcoin, a different situation but akin to the
current. Transactions will still always confirm but with longer and
longer wait periods. The auction at the front of the queue for priority
will be destroyed as there will be eventually no room in blocks besides
ageing transations and, there will be little value paying higher than
the minimum fee. Obviously, neither of these scenarios are in a miner's
Without a consensus as to what size dynamic block to create,
enforcement of dynamic block size is not currently possible. It may be
possible for a consensus to be formed in the future but here I cannot
speculate. I can only suggest that it is in the interest of Bitcoin as
a whole and, in the interest of each node to conform to the proposal.
Some nodes failing to conform to the proposed requirements of dynamic
size or transaction priority in this proposal will not be destructive
to the operation of the proposal.
If necessary, nodes that have not yet adopted the proposal will just
continue to create standard fixed size unordered blocks, although, if
the current mechanisms of block validation include the fixed block size
then it is unlikely that these nodes will be able to validate the
blockchain going forward. In this case a hard fork and a full transfer
to the new method should be required. If dynamic blocks with ordered
transactions will be valid to existing nodes then only a soft fork is
required. There is no proposed change to the internal construction of
blocks, only to the block size and using an ordered method of
transaction selection.
need to be adjusted to match something more than n days or, perhaps
using less than n = 14 days may be a more sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.?
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part??(yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*???Maximizes transaction reliability.
*???Overcomes transaction bandwidth limit.
*???Fully scalable.
*???Maximizes possibility for consumer and business uptake.
*???Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*???Market determines fee paid for transaction priority.
*???Fee recommendations work all the way out to 30 days or greater.
*???Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*???Could initially lower total transaction fees per block.
*???Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
For all operations we count only valid transactions.
**The service must:**
*???Have an individual temporary (runtime permanent only) Serial Node
*???Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*???Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*???Expire any value not updated for k minutes (k = 30 minutes?).
*???Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*???Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*???Alternatively, if loading nodes own full mempool from disk on node
restart (o = 30 minutes ?)
*???Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*???All known mempool information must survive node restart.
*???If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*???Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.??Determine the target block size for the current block.
2.??Assign a transaction priority to each valid transaction in the
3.??Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.??Solve block.
5.??Broadcast the current block when it is solved.
6.??Block is received.
7.??Block verification process.
8.??Accept/reject block based on verification result.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
It is trivial to this proposal to offer that a node provides the next
block size with a block when it is solved. I am not sure that this
creates any actual benefit since the provided next block size is only
one node's view, as it is the node may seemingly just as well use its
own view and create the block. Providing a next block size only adds
additional complexity to the required operation, however, perhaps
providing the next block size is not trivial in what is accomplished
and the feature can be included in the operation.
Instead of the pre-rollout network service providing data as to valid
transactions in mempool, it could directly provide data as to the
suggested next block size if that is preferred, using a similar
operation as is suggested now and averaging all received suggested next
block sizes.
It may be foreseeable in the future for Bitcoin to operate with a
network of dedicated full blockchain & mempool servers. This would not
be without challenges to overcome but would offer several benefits,
including to the operation of this proposal, and especially as the RAM
and storage requirements of a full node grows. It is easy to foresee
that in just another seven years of operation a Bitcoin Full Node will
require at least 300GB of storage and, if the mempool only doubles in
size, over 1GB of RAM.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
Notwithstanding this proposal, all blocks including those with dynamic
size each have limited transaction space per block. This proposal
results in a fee for priority service auction, where the probability of
a transaction to be included in limited space in the next available
block is auctioned to the highest bidders and all other transactions
must wait until they reach priority by ageing to gain significant
probability. Under this proposal the mempool can grow quite large while
the confirmation service continues in a stable and reliable manner.
Several incentives for attackers are removed, where there is no longer
multiple potential incentives for unnecessarily filling blocks or
flooding the mempool with transactions, whether such transactions are
fraudulent, valid or, otherwise. Adoption of this proposal and
adherence results in a reliable, stable fee paying transaction
confirmation service and a beneficial auction.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Regards, ?
Damian Williamson
[![Creative Commons License](
88x31.png)]( ?
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at Permissions beyond the scope of this license may be available at [https

@_date: 2018-01-04 09:01:10
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
This proposal has a new update, mostly minor edits. Additionally, I had a logic flaw in the hard fork / soft fork declaration statement. The specific terms of the CC-BY-SA-4.0 licence the document is published under have now been updated to include additional permissions available under the MIT licence.
Recently, on Twitter:
I am looking for a capable analyst/programmer to work on a BIP proposal as co-author. Will need to format several Full BIP's per these BIP process requirements: (  ) from a BIP Proposal, being two initially for non-consensus full-interoperable pre-rollout on peer service layer & API/RPC layer and, a reference implementation for Bitcoin Core per: (  ). Interested parties please reply via this list thread: (  )  Damian Williamson
Sent: Monday, 1 January 2018 10:04 PM
Happy New Year all.
This proposal has been further amended with several minor changes and a
few additions.
I believe that all known issues raised so far have been sufficiently
addressed. Either that or, I still have more work to do.
 BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks
Document: BIP Proposal
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Published: 26-12-2017
Revised: 01-01-2018
Author: Damian Williamson Licence: Creative Commons Attribution-ShareAlike 4.0 International
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.  The current transaction bandwidth limit.
2.  The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here:
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
While the miners have discovered a gold mine, it is the service they
provide that is valuable. If the service is unreliable they are not
worth the gold that they mine. This is reflected in the value of
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm by the time
2x transactions is reached. Presently, not even using fee
recommendations can ensure a sufficiently high fee is paid to ensure
transaction confirmation.
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.  Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
Bitcoin should be capable of reliably and inexpensively processing
casual transactions, and also priority processing of fee paying at
auction for priority transactions in the shortest possible timeframe.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
full mempool and information survives node restart. If there is a more
reliable way to determine when a transaction was first seen on the
network then it should be utilised.
currently seem to save and load the mempool on restart, despite the
notes in the command line options panel that the default for
persistmempool is 1. In the debug panel, some 90,000 transactions
before restart, some 200 odd shortly after. Manually setting
persistmempool=1 in the conf file does not seem to make any difference.
Perhaps it is operating as expected and I am not sure what to observe,
but does not seem to be observably saving and loading the mempool on
restart. This will need to be resolved.
Use a dynamic target block size to make the current block. This marks a
shift from using block size or weight to a count of transactions.
Determine the target block size using; pre-rollout(current average
valid transaction pool size) x ( 1 / (144 x n days ) ) = number of
transactions to be included in the current block. The block created
should be a minimum 1MB in size regardless if the target block size is
If the created block size consistently contains too few transactions
and the number of new transactions created is continuously greater than
the block size will accommodate then I expect eventually ageing
transactions will be over-represented as a portion of the block
contents. Once another new node conforming to the proposal makes a
block, the block size will be proportionately larger as the transaction
pool has grown.  If block size is too large on average then this will
shrink the transaction pool.
Miners will likely want to conform to the proposal, since making blocks
larger than necessary makes more room in each block potentially
lowering the highest fees paid for priority service. Always making
blocks smaller than the proposal requires will in time lower the
utility value of Bitcoin, a different situation but akin to the
current. Transactions will still always confirm but with longer and
longer wait periods. The auction at the front of the queue for priority
will be destroyed as there will be eventually no room in blocks besides
ageing transations and, there will be little value paying higher than
the minimum fee. Obviously, neither of these scenarios are in a miner's
Without a consensus as to what size dynamic block to create,
enforcement of dynamic block size is not currently possible. It may be
possible for a consensus to be formed in the future but here I cannot
speculate. I can only suggest that it is in the interest of Bitcoin as
a whole and, in the interest of each node to conform to the proposal.
Some nodes failing to conform to the proposed requirements of dynamic
size or transaction priority in this proposal will not be destructive
to the operation of the proposal.
If necessary, nodes that have not yet adopted the proposal will just
continue to create standard fixed size unordered blocks, although, if
the current mechanisms of block validation include the fixed block size
then it is unlikely that these nodes will be able to validate the
blockchain going forward. In this case a hard fork and a full transfer
to the new method should be required. If dynamic blocks with ordered
transactions will be valid to existing nodes then only a soft fork is
required. There is no proposed change to the internal construction of
blocks, only to the block size and using an ordered method of
transaction selection.
need to be adjusted to match something more than n days or, perhaps
using less than n = 14 days may be a more sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part  (yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*   Maximizes transaction reliability.
*   Overcomes transaction bandwidth limit.
*   Fully scalable.
*   Maximizes possibility for consumer and business uptake.
*   Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*   Market determines fee paid for transaction priority.
*   Fee recommendations work all the way out to 30 days or greater.
*   Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*   Could initially lower total transaction fees per block.
*   Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
For all operations we count only valid transactions.
**The service must:**
*   Have an individual temporary (runtime permanent only) Serial Node
*   Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*   Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*   Expire any value not updated for k minutes (k = 30 minutes?).
*   Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*   Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*   Alternatively, if loading nodes own full mempool from disk on node
restart (o = 30 minutes ?)
*   Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*   All known mempool information must survive node restart.
*   If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*   Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.  Determine the target block size for the current block.
2.  Assign a transaction priority to each valid transaction in the
3.  Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.  Solve block.
5.  Broadcast the current block when it is solved.
6.  Block is received.
7.  Block verification process.
8.  Accept/reject block based on verification result.
9.  Repeat.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
It is trivial to this proposal to offer that a node provides the next
block size with a block when it is solved. I am not sure that this
creates any actual benefit since the provided next block size is only
one node's view, as it is the node may seemingly just as well use its
own view and create the block. Providing a next block size only adds
additional complexity to the required operation, however, perhaps
providing the next block size is not trivial in what is accomplished
and the feature can be included in the operation.
Instead of the pre-rollout network service providing data as to valid
transactions in mempool, it could directly provide data as to the
suggested next block size if that is preferred, using a similar
operation as is suggested now and averaging all received suggested next
block sizes.
It may be foreseeable in the future for Bitcoin to operate with a
network of dedicated full blockchain & mempool servers. This would not
be without challenges to overcome but would offer several benefits,
including to the operation of this proposal, and especially as the RAM
and storage requirements of a full node grows. It is easy to foresee
that in just another seven years of operation a Bitcoin Full Node will
require at least 300GB of storage and, if the mempool only doubles in
size, over 1GB of RAM.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
Notwithstanding this proposal, all blocks including those with dynamic
size each have limited transaction space per block. This proposal
results in a fee for priority service auction, where the probability of
a transaction to be included in limited space in the next available
block is auctioned to the highest bidders and all other transactions
must wait until they reach priority by ageing to gain significant
probability. Under this proposal the mempool can grow quite large while
the confirmation service continues in a stable and reliable manner.
Several incentives for attackers are removed, where there is no longer
multiple potential incentives for unnecessarily filling blocks or
flooding the mempool with transactions, whether such transactions are
fraudulent, valid or, otherwise. Adoption of this proposal and
adherence results in a reliable, stable fee paying transaction
confirmation service and a beneficial auction.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Damian Williamson
[![Creative Commons License](
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at Permissions beyond the scope of this license may be available at [https
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-01-13 02:11:08
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Plausible Deniability (Re: Satoshilabs secret 
The same problems exist for users of whole disk encrypted operating systems. Once the device (or, the initial password authentication) is found, the adversary knows that there is something to see. The objective of plausible deniability is to present some acceptable (plausible) alternative while keeping the actual hidden (denied).
If the adversary does not believe you, you do indeed risk everything.
Damian Williamson
Sent: Friday, 12 January 2018 10:06:33 PM
I despise the term ?plausible deniability?; and that?s really the wrong
term to use in this discussion.
?Plausible deniability? is a transparent excuse for explaining away an
indisputable fact which arouses suspicion?when you got some serious
?splain? to do.  This is usually used in the context of some pseudolegal
argument about introducing ?reasonable doubt?, or even making ?probable
cause? a wee bit less probable.
?Why yes, officer:  I was seen carrying an axe down the street near the
site of an axe murder, at approximately the time of said axe murder.
But I do have a fireplace; so it is plausible that I was simply out
gathering wood.?
I rather suspect the concept of ?plausible deniability? of having been
invented by a detective or agent provocateur.  There are few concepts
more useful for helping suspects shoot themselves in the foot, or
frankly, for entrapping people.
One of the worst examples I have seen is in discussions of Monero,
whereby I?ve seen proponents claim that even under the worst known
active attacks, their mix scheme reduces transaction linking to a
maximum of 20?40% probability.  ?That?s not good enough to convince a
jury!?  No, but it is certainly adequate for investigators to identify
you as a person of interest.  Then, your (mis)deeds can be subjected to
powerful confirmation attacks based on other data; blockchains do not
exist in isolation.  I usually stay out of such discussions; for I have
no interest in helping the sorts of people whose greatest concern in
life is what story to foist on a jury.
In the context of devices such as Trezor, what is needed is not
?plausible deniability?, but rather the ability to obviate any need to
deny anything at all.  I must repeat, information does not exist in
If you are publicly known to be deepy involved in Bitcoin, then nobody
will believe that your one-and-only wallet contains only 0.01 BTC.
That?s not even ?plausible?.  But if you have overall privacy practices
which leave nobody knowing or suspecting that you have any Bitcoin at
all, then there is nothing to ?deny?; and should a Trezor with
(supposedly) 0.01 BTC be found in your possession, that?s much better
than ?plausible?.  It?s completely unremarkable.
Whereas if you are known or believed to own large amounts of BTC, a
realistic bad guy?s response to your ?decoy? wallet could be, ?I don?t
believe you; and it costs me nothing to keep beating you with rubber
hose until you tell me the *real* password.?
It could be worse, too.  In a kidnapping scenario, the bad guys could
say, ?I don?t believe you.  Hey, I also read Trezor?s website about
?plausible deniability?.  Now, I will maim your kid for life just to
test whether you told me the *real* password.  And if you still don?t
tell me the real password after you see that little Johnny can no longer
walk, then I will kill him.?
The worst part is that you have no means of proving that you really
*did* give the real password.  Indeed, it can be proved if you?re lying
by finding a password which reveals a hidden wallet?but *you* have no
means of affirmatively proving that you are telling the truth!  If the
bad guys overestimated your riches (or if they?re in a bad mood), then
little Johnny is dead either way.
In a legalistic scenario, if ?authorities? believe you have 1000 BTC and
you only reveal a password for 0.01 BTC, the likely response will not be
to let you go.  Rather, ?You will now sit in jail until you tell the
*real* password.?  And again:  You have no means of proving that you did
give the real password!
?Plausible deniability? schemes can backfire quite badly.
What about data obtained via the network?  I don?t *only* refer to
dragnet surveillance.  See for but one e.g., Goldfelder, et al., ?When
the cookie meets the blockchain:  Privacy risks of web payments via
cryptocurrencies?   Your identity can be
tied to your wallet all sorts of ways, any of which could be used to
prove that you have more Bitcoin than you?re revealing.  Do you know
what databases of cross-correlated analysis data customs agents have
immediate access to nowadays?or will, tomorrow?  I don?t.
In the scenario under discussion, that may not immediately prove ?beyond
a reasonable doubt? that you lied specifically about your Trezor.  But
it could give plenty of cause to keep you locked up in a small room
while your hard drive is examined for evidence that Trezor apps handled
*addresses already known to be linked to you*.  Why even bother with
bruteforce?  Low-hanging fruit abound.
nullius at nym.zone | PGP ECC: 0xC2E91CD74A4C57A105F6C21B5A00591B2F307E0C
Bitcoin: bc1qcash96s5jqppzsp8hy8swkggf7f6agex98an7h | (Segwit nested:
3NULL3ZCUXr7RDLxXeLPDMZDZYxuaYkCnG)  (PGP RSA: 0x36EBB4AB699A10EE)
??If you?re not doing anything wrong, you have nothing to hide.?
No!  Because I do nothing wrong, I have nothing to show.? ? nullius

@_date: 2018-01-18 08:24:40
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Proposal to reduce mining power bill 
It probably could be noted, although it is well known, pools, in some views, act as one large individual miner, not just when separately considering the actions of pools.
Given the operation of pools, would a pool be required to mine the new-miner-blocks, or would you propose operation in a pool be restricted individually? How would this operate?
Damian Williamson
Sent: Thursday, 18 January 2018 9:34:11 AM
Thanks "nullius" for your remarks. Notice my first post was not an RFC but just a blur idea to inspect if something similar had already been discussed in the group. In fact your post has helped me a lot to improve my first mail.
I'm not sure what you are referring to in this paragraph. Imagine for example that there are a total of, let's say, 2^10 available next-coinbase/miners and the algorithm just allow 50% or 2^9 of them to mine, ?how could it be possible that one among them could have 51% of power by chance? (Please, read comments bellow before replying)
I do not think it would be easy even for a large miner but that made me think for an alternative algorithm. Let's introduce the concept of "spent" next-coinbase versus "un-spent" one, something like similarly to UTXO. A next-coinbase would only be valid if it has not been previously used to mine a block. Simplifying, with the spent vs unspent a large miner is restricted to a single next-coinbase as anyone else. Being more precise it's allowed a single next-coinbase for each mined new-miner-block mined creating a "thread" of mining blocks for each new new-miner-block. Schematically a thread would look like:
new-miner-block:next-coinbase_1 -> mined-block:next-coinbase_2 ->  ... -> (thread expired - see comment below about expiration)
In this case a large miner A with T times more power than another one B could potentially spent mining power to create T parallel threads for each thread created by miner B. A solution that could fix this issue is to allow a maximum life time for each thread expressed in number of blocks. After a given number of blocks have being mined the miner is forced to create new new-miner-block to continue participating. The algorithm to choose the life-time must be such that if a miner tries to create many parallel threads (many new-miner-block), by the time it start mining transaction blocks the first new-miner-block will start to expire, so he will be punished.
If the famous phrase "a degree of indirection solve all programming problems" I think this is an example applied to blockchain. First the consensus chooses who can participate in the next round, then selected miners participate in the round.
Now, questions:
How is N determined?  By a wave of the hands?
Great question. I left it unspecified in the first mail. An algorithm comes to my mind (You are welcome to propose others). Let's imagine the list of registered non-expired next-coinbase addresses  is 2^10. The consensus checks that for N=1 there is *about* 1/2^N == 1/2 of unspent next-coinbase addresses that match (it must be close to 1/2 of the total 2^10 addresses with maybe an small +/- 1% statistical deviation). Then N=1 will be accepted. Check now for N=2. If there are 1/(2^N) = 1/4 next-coinbase addresses matching then N=2 is accepted. The algorithm continues until some "++N" fails. Initially N=0 and so all miners are welcome to the game. They all will start producing next-coinbase addresses and when there are enough different ones N will become 1, then 2, ... This system will will keep an equilibrium naturally. If new miners stop producing new new-miner-blocks, eventually the threads will expire and N will be automatically be decreased. Miners will act rationally to keep enough threads open in their own interest since that will decrease the electricity bill.
Thinking about it, the hash must run over "many" different blocks and it must include the next next-coinbase (to force calculating the hash after choosing a next-coinbase). Since it's not expected that many blocks are mined by the same miner in a row (maybe no more than 2 o 3) the "many" number must be for example twice as much as the expected maximum numbers of blocks that a large miner can mine in average.
I think reorgs are not affected by this algorithm. The next-coinbase objective is just to randomly choose which miner will be allowed for the next round.
As I see it, if the network wants to keep the same pace of new blocks each N seconds, and N=1 (only half of the miners are allowed to mine)  then difficulty/power-bill is lowered by two, for N=2 by 4, ...
This new-miner-block will have NO transactions inside.
I think the previous algorithm I mention to replace the "wave of hands" (test N=1, then N=2,... ) plus the "expiring threads" would suffice to fix it.
Again, that is fixed by the previous algorithm
Fixed by the expiring thread model?
How could miner anonymity be preserved under a scheme whereby each ?next-coinbase? address can be linked to a previous ?next-coinbase? address?  The only way to start fresh would be with a prohibitively expensive no-payout block.  Mining can be totally anonymous at present, and must so remain.  Miners are only identified by certain information they choose to put in a block header, which they could choose to change or omit?or by IP address, which is trivially changed and is never a reliable identifier.
The anonymity decreases in the sense that if you know a next-coinbase address owner you know all its related next-coinbase for the expiring (physical-time-limited) thread. The anonymity increases in the sense that miner will consume fewer energy. Electricity bill is the easiest way today to trace miners.
 > How does this even save electricity, when there is much mining equipment (especially on large mining farms) which cannot be easily shut down and restarted?  (Allegedly, this is one reason why some big miners occasionally mine empty blocks.)  Though I suppose that difficulty would drop by unspecified means.
As explained above, the difficulty is reduced by 1/2^N for each round. And of course, miners that want to save more energy will have to adapt to put their systems on stand-by while they  are not chosen for the next round. I think based on my limited experience with ASIC mining that just by not sending new orders to the miner the power comsumption will decrease dramatically even if the equipment is still on.
Further observations:
This scheme drastically increases the upfront investment required for a new miner to start mining.  To mine even one new block all by oneself, without a pool, already requires a huge investment.
Once introduced the concept of "expiring" thread I think he will be pretty much in the same condition. To obtain bitcoins he will first need to mine a new-miner-block to enter the game and then an standard block before the thread expires. Notice the expire time/block-length start just after the new-miner-block has been mined so the probabilities to mine before the expiration time will be proportional to its mining power, as for everyone else.
I think it could be clearly compensated by the save in energy for standards blocks.
I don't think so after the new fixes. What do you think? My opinion is that, based on the "theory of games", miners acting in their own interest will try to maximize "N".
With the addition of thread expiration, nobody will be really motivated to shell/buy "next-coinbase" addresses since their utility is limited.
Just a remark: Notice this algorithm reduces the electricity bill, but the hardware needed stays the same, since for each round the miner participates in, it will try to mine as fast as possible and so use as much hardware as possible. No reduction costs are expected in hardware.
Best Regards,
Enrique Ariz?n Benito
I have not even tried to imagine what oddball attacks might be possible for any miner with sufficient hashpower to deliberately cause a small reorg.
nullius at nym.zone | PGP ECC: 0xC2E91CD74A4C57A105F6C21B5A00591B2F307E0C
Bitcoin: bc1qcash96s5jqppzsp8hy8swkggf7f6agex98an7h | (Segwit nested:
3NULL3ZCUXr7RDLxXeLPDMZDZYxuaYkCnG)  (PGP RSA: 0x36EBB4AB699A10EE)
??If you?re not doing anything wrong, you have nothing to hide.?
No!  Because I do nothing wrong, I have nothing to show.? ? nullius

@_date: 2018-01-19 23:25:43
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
An example curve:
The curve curently described here is ineffective at acheiving the requirements. It seems to be not nearly steep enough resulting in too many inclusions (as it happens, this may not metter - needs further evaluation) and, the lower end values seem problematically small but, results in a number between 100 for the highest fee BTC/KB and a small fraction of 1 for the lowest. This math needs to be improved.
pf(tx) = sin2((fx-(fl-0.00000001))/(fh-(fl-0.00000001))*1.570796326795)*100
pf is the calculated priority number for the fee for tx the specifc valid transaction.
fx is the fee in BTC/KB for the specific transaction.
fl is the lowest valid fee in BTC/KB currently in the nodes mempool.
fh is the highest valid fee in BTC/KB currently in the nodes mempool.
Sent: Thursday, 4 January 2018 8:01:10 PM
This proposal has a new update, mostly minor edits. Additionally, I had a logic flaw in the hard fork / soft fork declaration statement. The specific terms of the CC-BY-SA-4.0 licence the document is published under have now been updated to include additional permissions available under the MIT licence.
Recently, on Twitter:
I am looking for a capable analyst/programmer to work on a BIP proposal as co-author. Will need to format several Full BIP's per these BIP process requirements: (  ) from a BIP Proposal, being two initially for non-consensus full-interoperable pre-rollout on peer service layer & API/RPC layer and, a reference implementation for Bitcoin Core per: (  ). Interested parties please reply via this list thread: (  )  Damian Williamson
Sent: Monday, 1 January 2018 10:04 PM
Happy New Year all.
This proposal has been further amended with several minor changes and a
few additions.
I believe that all known issues raised so far have been sufficiently
addressed. Either that or, I still have more work to do.
 BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks
Document: BIP Proposal
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Published: 26-12-2017
Revised: 01-01-2018
Author: Damian Williamson Licence: Creative Commons Attribution-ShareAlike 4.0 International
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.  The current transaction bandwidth limit.
2.  The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here:
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
While the miners have discovered a gold mine, it is the service they
provide that is valuable. If the service is unreliable they are not
worth the gold that they mine. This is reflected in the value of
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm by the time
2x transactions is reached. Presently, not even using fee
recommendations can ensure a sufficiently high fee is paid to ensure
transaction confirmation.
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.  Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
Bitcoin should be capable of reliably and inexpensively processing
casual transactions, and also priority processing of fee paying at
auction for priority transactions in the shortest possible timeframe.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
full mempool and information survives node restart. If there is a more
reliable way to determine when a transaction was first seen on the
network then it should be utilised.
currently seem to save and load the mempool on restart, despite the
notes in the command line options panel that the default for
persistmempool is 1. In the debug panel, some 90,000 transactions
before restart, some 200 odd shortly after. Manually setting
persistmempool=1 in the conf file does not seem to make any difference.
Perhaps it is operating as expected and I am not sure what to observe,
but does not seem to be observably saving and loading the mempool on
restart. This will need to be resolved.
Use a dynamic target block size to make the current block. This marks a
shift from using block size or weight to a count of transactions.
Determine the target block size using; pre-rollout(current average
valid transaction pool size) x ( 1 / (144 x n days ) ) = number of
transactions to be included in the current block. The block created
should be a minimum 1MB in size regardless if the target block size is
If the created block size consistently contains too few transactions
and the number of new transactions created is continuously greater than
the block size will accommodate then I expect eventually ageing
transactions will be over-represented as a portion of the block
contents. Once another new node conforming to the proposal makes a
block, the block size will be proportionately larger as the transaction
pool has grown.  If block size is too large on average then this will
shrink the transaction pool.
Miners will likely want to conform to the proposal, since making blocks
larger than necessary makes more room in each block potentially
lowering the highest fees paid for priority service. Always making
blocks smaller than the proposal requires will in time lower the
utility value of Bitcoin, a different situation but akin to the
current. Transactions will still always confirm but with longer and
longer wait periods. The auction at the front of the queue for priority
will be destroyed as there will be eventually no room in blocks besides
ageing transations and, there will be little value paying higher than
the minimum fee. Obviously, neither of these scenarios are in a miner's
Without a consensus as to what size dynamic block to create,
enforcement of dynamic block size is not currently possible. It may be
possible for a consensus to be formed in the future but here I cannot
speculate. I can only suggest that it is in the interest of Bitcoin as
a whole and, in the interest of each node to conform to the proposal.
Some nodes failing to conform to the proposed requirements of dynamic
size or transaction priority in this proposal will not be destructive
to the operation of the proposal.
If necessary, nodes that have not yet adopted the proposal will just
continue to create standard fixed size unordered blocks, although, if
the current mechanisms of block validation include the fixed block size
then it is unlikely that these nodes will be able to validate the
blockchain going forward. In this case a hard fork and a full transfer
to the new method should be required. If dynamic blocks with ordered
transactions will be valid to existing nodes then only a soft fork is
required. There is no proposed change to the internal construction of
blocks, only to the block size and using an ordered method of
transaction selection.
need to be adjusted to match something more than n days or, perhaps
using less than n = 14 days may be a more sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part  (yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*   Maximizes transaction reliability.
*   Overcomes transaction bandwidth limit.
*   Fully scalable.
*   Maximizes possibility for consumer and business uptake.
*   Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*   Market determines fee paid for transaction priority.
*   Fee recommendations work all the way out to 30 days or greater.
*   Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*   Could initially lower total transaction fees per block.
*   Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
For all operations we count only valid transactions.
**The service must:**
*   Have an individual temporary (runtime permanent only) Serial Node
*   Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*   Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*   Expire any value not updated for k minutes (k = 30 minutes?).
*   Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*   Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*   Alternatively, if loading nodes own full mempool from disk on node
restart (o = 30 minutes ?)
*   Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*   All known mempool information must survive node restart.
*   If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*   Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.  Determine the target block size for the current block.
2.  Assign a transaction priority to each valid transaction in the
3.  Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.  Solve block.
5.  Broadcast the current block when it is solved.
6.  Block is received.
7.  Block verification process.
8.  Accept/reject block based on verification result.
9.  Repeat.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
It is trivial to this proposal to offer that a node provides the next
block size with a block when it is solved. I am not sure that this
creates any actual benefit since the provided next block size is only
one node's view, as it is the node may seemingly just as well use its
own view and create the block. Providing a next block size only adds
additional complexity to the required operation, however, perhaps
providing the next block size is not trivial in what is accomplished
and the feature can be included in the operation.
Instead of the pre-rollout network service providing data as to valid
transactions in mempool, it could directly provide data as to the
suggested next block size if that is preferred, using a similar
operation as is suggested now and averaging all received suggested next
block sizes.
It may be foreseeable in the future for Bitcoin to operate with a
network of dedicated full blockchain & mempool servers. This would not
be without challenges to overcome but would offer several benefits,
including to the operation of this proposal, and especially as the RAM
and storage requirements of a full node grows. It is easy to foresee
that in just another seven years of operation a Bitcoin Full Node will
require at least 300GB of storage and, if the mempool only doubles in
size, over 1GB of RAM.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
Notwithstanding this proposal, all blocks including those with dynamic
size each have limited transaction space per block. This proposal
results in a fee for priority service auction, where the probability of
a transaction to be included in limited space in the next available
block is auctioned to the highest bidders and all other transactions
must wait until they reach priority by ageing to gain significant
probability. Under this proposal the mempool can grow quite large while
the confirmation service continues in a stable and reliable manner.
Several incentives for attackers are removed, where there is no longer
multiple potential incentives for unnecessarily filling blocks or
flooding the mempool with transactions, whether such transactions are
fraudulent, valid or, otherwise. Adoption of this proposal and
adherence results in a reliable, stable fee paying transaction
confirmation service and a beneficial auction.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Damian Williamson
[![Creative Commons License](
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at Permissions beyond the scope of this license may be available at [https
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-01-20 12:04:20
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Tried a different approach for the curves, would appreciate it if someone has the energy to work on this and help me to resolve it a bit more scientifically:
p(tx) = (((((fx - (fl - 0.00000001)) / (fh - (fl - 0.00000001))) * 100) + 1) ^ y) + (((((wx - 0.9) / ((86400 * n) - 0.9)) * 100) + 1) ^ y)
p is the calculated priority number for tx the specific valid transaction.
fx is the fee in BTC/KB for the specific transaction.
fl is the lowest valid fee in BTC/KB currently in the nodes mempool.
fh is the highest valid fee in BTC/KB currently in the nodes mempool.
wx is the current wait in seconds for tx the specific valid transaction.
n is the number of days maximum wait consensus value.
y can be 10 or, y can be a further developed to be a formula based on the number of required inclusions to vary the steepness of the curve as the mempool size varies.
In the next step, the random value must be:
if random(101^y) < p then transaction is included;
Damian Williamson
Sent: Saturday, 20 January 2018 10:25:43 AM
An example curve:
The curve curently described here is ineffective at acheiving the requirements. It seems to be not nearly steep enough resulting in too many inclusions (as it happens, this may not metter - needs further evaluation) and, the lower end values seem problematically small but, results in a number between 100 for the highest fee BTC/KB and a small fraction of 1 for the lowest. This math needs to be improved.
pf(tx) = sin2((fx-(fl-0.00000001))/(fh-(fl-0.00000001))*1.570796326795)*100
pf is the calculated priority number for the fee for tx the specifc valid transaction.
fx is the fee in BTC/KB for the specific transaction.
fl is the lowest valid fee in BTC/KB currently in the nodes mempool.
fh is the highest valid fee in BTC/KB currently in the nodes mempool.
Sent: Thursday, 4 January 2018 8:01:10 PM
This proposal has a new update, mostly minor edits. Additionally, I had a logic flaw in the hard fork / soft fork declaration statement. The specific terms of the CC-BY-SA-4.0 licence the document is published under have now been updated to include additional permissions available under the MIT licence.
Recently, on Twitter:
I am looking for a capable analyst/programmer to work on a BIP proposal as co-author. Will need to format several Full BIP's per these BIP process requirements: (  ) from a BIP Proposal, being two initially for non-consensus full-interoperable pre-rollout on peer service layer & API/RPC layer and, a reference implementation for Bitcoin Core per: (  ). Interested parties please reply via this list thread: (  )  Damian Williamson
Sent: Monday, 1 January 2018 10:04 PM
Happy New Year all.
This proposal has been further amended with several minor changes and a
few additions.
I believe that all known issues raised so far have been sufficiently
addressed. Either that or, I still have more work to do.
 BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks
Document: BIP Proposal
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Published: 26-12-2017
Revised: 01-01-2018
Author: Damian Williamson Licence: Creative Commons Attribution-ShareAlike 4.0 International
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.  The current transaction bandwidth limit.
2.  The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here:
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
While the miners have discovered a gold mine, it is the service they
provide that is valuable. If the service is unreliable they are not
worth the gold that they mine. This is reflected in the value of
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm by the time
2x transactions is reached. Presently, not even using fee
recommendations can ensure a sufficiently high fee is paid to ensure
transaction confirmation.
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.  Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
Bitcoin should be capable of reliably and inexpensively processing
casual transactions, and also priority processing of fee paying at
auction for priority transactions in the shortest possible timeframe.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
full mempool and information survives node restart. If there is a more
reliable way to determine when a transaction was first seen on the
network then it should be utilised.
currently seem to save and load the mempool on restart, despite the
notes in the command line options panel that the default for
persistmempool is 1. In the debug panel, some 90,000 transactions
before restart, some 200 odd shortly after. Manually setting
persistmempool=1 in the conf file does not seem to make any difference.
Perhaps it is operating as expected and I am not sure what to observe,
but does not seem to be observably saving and loading the mempool on
restart. This will need to be resolved.
Use a dynamic target block size to make the current block. This marks a
shift from using block size or weight to a count of transactions.
Determine the target block size using; pre-rollout(current average
valid transaction pool size) x ( 1 / (144 x n days ) ) = number of
transactions to be included in the current block. The block created
should be a minimum 1MB in size regardless if the target block size is
If the created block size consistently contains too few transactions
and the number of new transactions created is continuously greater than
the block size will accommodate then I expect eventually ageing
transactions will be over-represented as a portion of the block
contents. Once another new node conforming to the proposal makes a
block, the block size will be proportionately larger as the transaction
pool has grown.  If block size is too large on average then this will
shrink the transaction pool.
Miners will likely want to conform to the proposal, since making blocks
larger than necessary makes more room in each block potentially
lowering the highest fees paid for priority service. Always making
blocks smaller than the proposal requires will in time lower the
utility value of Bitcoin, a different situation but akin to the
current. Transactions will still always confirm but with longer and
longer wait periods. The auction at the front of the queue for priority
will be destroyed as there will be eventually no room in blocks besides
ageing transations and, there will be little value paying higher than
the minimum fee. Obviously, neither of these scenarios are in a miner's
Without a consensus as to what size dynamic block to create,
enforcement of dynamic block size is not currently possible. It may be
possible for a consensus to be formed in the future but here I cannot
speculate. I can only suggest that it is in the interest of Bitcoin as
a whole and, in the interest of each node to conform to the proposal.
Some nodes failing to conform to the proposed requirements of dynamic
size or transaction priority in this proposal will not be destructive
to the operation of the proposal.
If necessary, nodes that have not yet adopted the proposal will just
continue to create standard fixed size unordered blocks, although, if
the current mechanisms of block validation include the fixed block size
then it is unlikely that these nodes will be able to validate the
blockchain going forward. In this case a hard fork and a full transfer
to the new method should be required. If dynamic blocks with ordered
transactions will be valid to existing nodes then only a soft fork is
required. There is no proposed change to the internal construction of
blocks, only to the block size and using an ordered method of
transaction selection.
need to be adjusted to match something more than n days or, perhaps
using less than n = 14 days may be a more sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part  (yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*   Maximizes transaction reliability.
*   Overcomes transaction bandwidth limit.
*   Fully scalable.
*   Maximizes possibility for consumer and business uptake.
*   Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*   Market determines fee paid for transaction priority.
*   Fee recommendations work all the way out to 30 days or greater.
*   Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*   Could initially lower total transaction fees per block.
*   Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
For all operations we count only valid transactions.
**The service must:**
*   Have an individual temporary (runtime permanent only) Serial Node
*   Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*   Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*   Expire any value not updated for k minutes (k = 30 minutes?).
*   Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*   Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*   Alternatively, if loading nodes own full mempool from disk on node
restart (o = 30 minutes ?)
*   Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*   All known mempool information must survive node restart.
*   If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*   Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.  Determine the target block size for the current block.
2.  Assign a transaction priority to each valid transaction in the
3.  Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.  Solve block.
5.  Broadcast the current block when it is solved.
6.  Block is received.
7.  Block verification process.
8.  Accept/reject block based on verification result.
9.  Repeat.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
It is trivial to this proposal to offer that a node provides the next
block size with a block when it is solved. I am not sure that this
creates any actual benefit since the provided next block size is only
one node's view, as it is the node may seemingly just as well use its
own view and create the block. Providing a next block size only adds
additional complexity to the required operation, however, perhaps
providing the next block size is not trivial in what is accomplished
and the feature can be included in the operation.
Instead of the pre-rollout network service providing data as to valid
transactions in mempool, it could directly provide data as to the
suggested next block size if that is preferred, using a similar
operation as is suggested now and averaging all received suggested next
block sizes.
It may be foreseeable in the future for Bitcoin to operate with a
network of dedicated full blockchain & mempool servers. This would not
be without challenges to overcome but would offer several benefits,
including to the operation of this proposal, and especially as the RAM
and storage requirements of a full node grows. It is easy to foresee
that in just another seven years of operation a Bitcoin Full Node will
require at least 300GB of storage and, if the mempool only doubles in
size, over 1GB of RAM.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
Notwithstanding this proposal, all blocks including those with dynamic
size each have limited transaction space per block. This proposal
results in a fee for priority service auction, where the probability of
a transaction to be included in limited space in the next available
block is auctioned to the highest bidders and all other transactions
must wait until they reach priority by ageing to gain significant
probability. Under this proposal the mempool can grow quite large while
the confirmation service continues in a stable and reliable manner.
Several incentives for attackers are removed, where there is no longer
multiple potential incentives for unnecessarily filling blocks or
flooding the mempool with transactions, whether such transactions are
fraudulent, valid or, otherwise. Adoption of this proposal and
adherence results in a reliable, stable fee paying transaction
confirmation service and a beneficial auction.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Damian Williamson
[![Creative Commons License](
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at Permissions beyond the scope of this license may be available at [https
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-01-21 05:49:25
@_author: Damian Williamson 
@_subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction 
Good afternoon Alan,
It is stated in the proposal that it is intended for blocks to be validated as the output of the priority method, to ensure that they conform. Unfortunately, the math necessary for this sort of statistical function is outside the scope of my formal education and I will need to rely on someone to develop what is necessary. If it does turn out that this is not ultimately possible then I suppose at that stage the proposal would need to be abandoned since I agree - validation must be necessary. Blocks created with cheating should be too unlikely.
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
Damian Williamson
Sent: Sunday, 21 January 2018 1:46:41 AM
I don't see any modifications to the proposal that addresses the issue that miners will always be free to choose their own priority that a few people brought up before.
I understand you think it's in the miners best long-term interest to follow these rules, but even if a miner agrees with you, if that miner thinks the other miners are following the fee curve, they will know it makes no overall difference if they cheat (you can't prove how long a miner has had a transaction in their mempool).
The opportunity to cheat, the anonymity of mining, the low negative effect of a single cheating instance, all combined with a financial incentive to cheat means that cheating will be rife.
Tried a different approach for the curves, would appreciate it if someone has the energy to work on this and help me to resolve it a bit more scientifically:
p(tx) = (((((fx - (fl - 0.00000001)) / (fh - (fl - 0.00000001))) * 100) + 1) ^ y) + (((((wx - 0.9) / ((86400 * n) - 0.9)) * 100) + 1) ^ y)
p is the calculated priority number for tx the specific valid transaction.
fx is the fee in BTC/KB for the specific transaction.
fl is the lowest valid fee in BTC/KB currently in the nodes mempool.
fh is the highest valid fee in BTC/KB currently in the nodes mempool.
wx is the current wait in seconds for tx the specific valid transaction.
n is the number of days maximum wait consensus value.
y can be 10 or, y can be a further developed to be a formula based on the number of required inclusions to vary the steepness of the curve as the mempool size varies.
In the next step, the random value must be:
if random(101^y) < p then transaction is included;
Damian Williamson
Sent: Saturday, 20 January 2018 10:25:43 AM
An example curve:
The curve curently described here is ineffective at acheiving the requirements. It seems to be not nearly steep enough resulting in too many inclusions (as it happens, this may not metter - needs further evaluation) and, the lower end values seem problematically small but, results in a number between 100 for the highest fee BTC/KB and a small fraction of 1 for the lowest. This math needs to be improved.
pf(tx) = sin2((fx-(fl-0.00000001))/(fh-(fl-0.00000001))*1.570796326795)*100
pf is the calculated priority number for the fee for tx the specifc valid transaction.
fx is the fee in BTC/KB for the specific transaction.
fl is the lowest valid fee in BTC/KB currently in the nodes mempool.
fh is the highest valid fee in BTC/KB currently in the nodes mempool.
Sent: Thursday, 4 January 2018 8:01:10 PM
This proposal has a new update, mostly minor edits. Additionally, I had a logic flaw in the hard fork / soft fork declaration statement. The specific terms of the CC-BY-SA-4.0 licence the document is published under have now been updated to include additional permissions available under the MIT licence.
Recently, on Twitter:
I am looking for a capable analyst/programmer to work on a BIP proposal as co-author. Will need to format several Full BIP's per these BIP process requirements: (  ) from a BIP Proposal, being two initially for non-consensus full-interoperable pre-rollout on peer service layer & API/RPC layer and, a reference implementation for Bitcoin Core per: (  ). Interested parties please reply via this list thread: (  )  Damian Williamson
Sent: Monday, 1 January 2018 10:04 PM
Happy New Year all.
This proposal has been further amended with several minor changes and a
few additions.
I believe that all known issues raised so far have been sufficiently
addressed. Either that or, I still have more work to do.
 BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks
Document: BIP Proposal
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Published: 26-12-2017
Revised: 01-01-2018
Author: Damian Williamson >
Licence: Creative Commons Attribution-ShareAlike 4.0 International
URL:  1. Abstract
This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.
There are two key issues to be resolved to achieve this:
1.  The current transaction bandwidth limit.
2.  The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.
It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.
The previous threads for this proposal are available here:
In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.
In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.
It is generally assumed that miners currently prefer to include
transactions with higher fees.
 2. The need for this proposal
We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.
I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.
I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.
While the miners have discovered a gold mine, it is the service they
provide that is valuable. If the service is unreliable they are not
worth the gold that they mine. This is reflected in the value of
Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm by the time
2x transactions is reached. Presently, not even using fee
recommendations can ensure a sufficiently high fee is paid to ensure
transaction confirmation.
I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.
This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.  Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.
I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.
 3. The problem
Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.
The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.
Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.
Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.
The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.
Bitcoin should be capable of reliably and inexpensively processing
casual transactions, and also priority processing of fee paying at
auction for priority transactions in the shortest possible timeframe.
 4. Solution summary
 Main solution
Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.
Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
full mempool and information survives node restart. If there is a more
reliable way to determine when a transaction was first seen on the
network then it should be utilised.
currently seem to save and load the mempool on restart, despite the
notes in the command line options panel that the default for
persistmempool is 1. In the debug panel, some 90,000 transactions
before restart, some 200 odd shortly after. Manually setting
persistmempool=1 in the conf file does not seem to make any difference.
Perhaps it is operating as expected and I am not sure what to observe,
but does not seem to be observably saving and loading the mempool on
restart. This will need to be resolved.
Use a dynamic target block size to make the current block. This marks a
shift from using block size or weight to a count of transactions.
Determine the target block size using; pre-rollout(current average
valid transaction pool size) x ( 1 / (144 x n days ) ) = number of
transactions to be included in the current block. The block created
should be a minimum 1MB in size regardless if the target block size is
If the created block size consistently contains too few transactions
and the number of new transactions created is continuously greater than
the block size will accommodate then I expect eventually ageing
transactions will be over-represented as a portion of the block
contents. Once another new node conforming to the proposal makes a
block, the block size will be proportionately larger as the transaction
pool has grown.  If block size is too large on average then this will
shrink the transaction pool.
Miners will likely want to conform to the proposal, since making blocks
larger than necessary makes more room in each block potentially
lowering the highest fees paid for priority service. Always making
blocks smaller than the proposal requires will in time lower the
utility value of Bitcoin, a different situation but akin to the
current. Transactions will still always confirm but with longer and
longer wait periods. The auction at the front of the queue for priority
will be destroyed as there will be eventually no room in blocks besides
ageing transations and, there will be little value paying higher than
the minimum fee. Obviously, neither of these scenarios are in a miner's
Without a consensus as to what size dynamic block to create,
enforcement of dynamic block size is not currently possible. It may be
possible for a consensus to be formed in the future but here I cannot
speculate. I can only suggest that it is in the interest of Bitcoin as
a whole and, in the interest of each node to conform to the proposal.
Some nodes failing to conform to the proposed requirements of dynamic
size or transaction priority in this proposal will not be destructive
to the operation of the proposal.
If necessary, nodes that have not yet adopted the proposal will just
continue to create standard fixed size unordered blocks, although, if
the current mechanisms of block validation include the fixed block size
then it is unlikely that these nodes will be able to validate the
blockchain going forward. In this case a hard fork and a full transfer
to the new method should be required. If dynamic blocks with ordered
transactions will be valid to existing nodes then only a soft fork is
required. There is no proposed change to the internal construction of
blocks, only to the block size and using an ordered method of
transaction selection.
need to be adjusted to match something more than n days or, perhaps
using less than n = 14 days may be a more sensible approach?
All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.
The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?
**Explanation of the operation of priority:**
and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.
time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part  (yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.
In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.
The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.
 Pros
*   Maximizes transaction reliability.
*   Overcomes transaction bandwidth limit.
*   Fully scalable.
*   Maximizes possibility for consumer and business uptake.
*   Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*   Market determines fee paid for transaction priority.
*   Fee recommendations work all the way out to 30 days or greater.
*   Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._
 Cons
*   Could initially lower total transaction fees per block.
*   Must be first be programmed.
 Pre-rollout
Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.
A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.
For all operations we count only valid transactions.
**The service must:**
*   Have an individual temporary (runtime permanent only) Serial Node
*   Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*   Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*   Expire any value not updated for k minutes (k = 30 minutes?).
*   Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*   Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*   Alternatively, if loading nodes own full mempool from disk on node
restart (o = 30 minutes ?)
*   Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*   All known mempool information must survive node restart.
*   If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*   Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.
Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.
The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.
Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.
 5. Solution operation
This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.
1.  Determine the target block size for the current block.
2.  Assign a transaction priority to each valid transaction in the
3.  Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.  Solve block.
5.  Broadcast the current block when it is solved.
6.  Block is received.
7.  Block verification process.
8.  Accept/reject block based on verification result.
9.  Repeat.
 6. Closing comments
It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.
It is trivial to this proposal to offer that a node provides the next
block size with a block when it is solved. I am not sure that this
creates any actual benefit since the provided next block size is only
one node's view, as it is the node may seemingly just as well use its
own view and create the block. Providing a next block size only adds
additional complexity to the required operation, however, perhaps
providing the next block size is not trivial in what is accomplished
and the feature can be included in the operation.
Instead of the pre-rollout network service providing data as to valid
transactions in mempool, it could directly provide data as to the
suggested next block size if that is preferred, using a similar
operation as is suggested now and averaging all received suggested next
block sizes.
It may be foreseeable in the future for Bitcoin to operate with a
network of dedicated full blockchain & mempool servers. This would not
be without challenges to overcome but would offer several benefits,
including to the operation of this proposal, and especially as the RAM
and storage requirements of a full node grows. It is easy to foresee
that in just another seven years of operation a Bitcoin Full Node will
require at least 300GB of storage and, if the mempool only doubles in
size, over 1GB of RAM.
There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.
Notwithstanding this proposal, all blocks including those with dynamic
size each have limited transaction space per block. This proposal
results in a fee for priority service auction, where the probability of
a transaction to be included in limited space in the next available
block is auctioned to the highest bidders and all other transactions
must wait until they reach priority by ageing to gain significant
probability. Under this proposal the mempool can grow quite large while
the confirmation service continues in a stable and reliable manner.
Several incentives for attackers are removed, where there is no longer
multiple potential incentives for unnecessarily filling blocks or
flooding the mempool with transactions, whether such transactions are
fraudulent, valid or, otherwise. Adoption of this proposal and
adherence results in a reliable, stable fee paying transaction
confirmation service and a beneficial auction.
This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.
I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.
Damian Williamson
[![Creative Commons License](
BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks by [Damian Williamson
<willtech at live.com.au>](
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](
Based on a work at Permissions beyond the scope of this license may be available at [https
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-01-28 11:05:59
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Does Lightning require millisatoshi unit? 
It seems in a document that I was referenced with this very question that the unit for creating invoices on Lightning is millisatoshi. Do we really need to invoice for 1000 millisatoshi for a 1 sat transaction?
lightning/README.md at master ? ElementsProject/lightning ...
c-lightning is a standard compliant implementation of the Lightning Network protocol. The Lightning Network is a scalability solution for Bitcoin, enabling secure and ...

@_date: 2018-03-13 13:26:17
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Sign / Verify message against SegWit P2SH and Bech32 
Current implementation of sign/verify is broken for SegWit and Bech32 addresses.
Please add the following reference to the use cases:
# Does blockchain.info show balances for addresses that are in cold storage?
Since they use Bitcoin Core, yes, there is a way to verify that they hold the addresses that they claim. Have them sign a message with each address that they claim to have the holdings on, using Bitcoin Core you can verify that they indeed have those addresses and check them on blockchain.info to find the current balance.
Only works in Bitcoin Core currently for addresses starting with a '1' (not Segwit addresses starting with a '3' and not Bech32 addresses starting with 'bc1' - the developers are aware of this and I will remind them shortly.)
In Bitcoin Core, your transaction opposite goes to File -> Sign Message and signs any message with one of the holding addresses. Copy the message, address and signature and send to you via probably plain text format email is the easiest. Repeat for each additional address holding the balance of BTC that they are offering to sell.
In Bitcoin Core, you go to File -> Verify Message and key the details provided EXACTLY - spaces, new lines and all characters must be an EXACT match. Click on verify and voil?.
I prefer the form of signed message as follows (don't key the top and bottom bar rows for the message, just the contents and you can check this yourself, the bottom row is the signature). I like to key the address used for verifying as a part of the message but that is not strictly necessary:
    ------------------------------
    Something that I want to sign.
    bitcoin:1PMUf9aaQ41M4bgVbCAPVwAeuKvj8CwxJg
    ------------------------------
    Signture:
    IGaXlQNRHHM6ferJ+Ocr3cN9dRJhIWxo+n9PGwgg1uPdOLVYIeCuaccEzDygVgYPJMXqmQeSaLaZVoG6FMHPJkg=
This contains all of the compact information necessary to verify the message.
Example of verified message:
![verified message][1]
[1]: Solution seems to be straight-forward, as noted in Issue# [10542](
This is an important feature, there are few other ways to verify that an address is held. Note that the linked issue is not currently labeld GUI and probably could be - unless a new issue should also be opened?
Damian Williamson

@_date: 2018-03-15 10:15:17
@_author: Damian Williamson 
@_subject: [bitcoin-dev] {sign|verify}message replacement 
That is very helpful Luke. I would not have been concerned if it was necessary to sign multiple times for multiple utxo's on different addresses but, since it is a feature it may as well be best usable. Signing for multiple inputs verifying that you have the priv key for each in your wallet is certainly usable for this popular misuse.
but also "proof of funds" (as a separate feature) since this is a popular
misuse of the current message signing (which doesn't actually prove funds at
all). To do this, it needs to be capable of signing for multiple inputs.
Sent: Wednesday, 14 March 2018 11:36:47 PM
I don't see a need for a new RPC interface, just a new signature format.
Ideally, it should support not only just "proof I receive at this address",
but also "proof of funds" (as a separate feature) since this is a popular
misuse of the current message signing (which doesn't actually prove funds at
all). To do this, it needs to be capable of signing for multiple inputs.
Preferably, it should also avoid disclosing the public key for existing or
future UTXOs. But I don't think it's possible to avoid this without something
MAST-like first. Perhaps it can be a MAST upgrade later on, but the new
signature scheme should probably be designed with it in mind.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-03-18 01:34:20
@_author: Damian Williamson 
@_subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation 
Application: Bitcoin Core
Feature: Enhanced privacy by change obfuscation
Operation: Provide a user selectable 'Enhanced privacy' option for transaction creation, when true the transaction randomly distributes change across up to twenty output addresses (minimum five?), provided each output is not dust.
Suggestions: Perhaps limit the total random number of addresses to distribute to by change amount. Optionally: If necessary, additional inputs can be selected if available to increase change although consider if this may eventually result in a decrease in obfuscation in some cases when the outputs are spent.
Issues: Transaction cost will be higher for the initial spend with the change due to increased outputs and, possibly for later spending the change depending on the future spend amount(s) and the number of inputs required.
Argument: If transaction linkage is possible, it is still possible with the obfuscated change but, it is far more difficult to guess what was retained by the owner of the originating utxo's unless the new change outputs are spent together in the same transaction.
Damian Williamson

@_date: 2018-03-18 07:07:58
@_author: Damian Williamson 
@_subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation 
Alright, but even if two (or more) of the change outputs were linked in a future transaction, no-one can tell if they are still linked to your wallet or not unless there is also an additional re-used address on the new transaction input side that has also been previously linked to one of the inputs on the transaction creating the change.
Yes, I understand the additional cost but still thought it worthy of consideration.
Damian Williamson
Sent: Sunday, 18 March 2018 4:50:34 PM
Damian Williamson via bitcoin-dev
This would be really expensive for the network due to the bloat in UTXO
size, a cost everyone has to pay for. Not to mention the fact that it
doesn't really seem that private, as the wallet is likely going to have
to rejoin those inputs in future transactions (and the user will have to
pay a high transaction fee as a result).
Evan Klitzke

@_date: 2018-05-20 05:12:23
@_author: Damian Williamson 
@_subject: [bitcoin-dev] [bitcoin-discuss] Checkpoints in the Blockchain. 
I do understand your point, however, 'something like stuxnet' cannot be used to create valid data without re-doing all the PoW. Provided some valid copies of the blockchain continue to exist, the network can re-synchronise.
Unrelated, it would seem useful to have some kind of deep blockchain corruption recovery mechanism if it does not exist; where blocks are altered at a depth exceeding the re-scan on init, efficient recovery is possible on detection. Presumably, it would be possible for some stuxnet like thing to modify blocks by modifying the table data making blocks invalid but without causing a table corruption. I would also suppose that if the node is delving deep into the blockchain for transaction data, that is would also validate the block at least that it has a valid hash (apart from Merkle tree validation for the individual transaction?) and that the hash of its immediate ancestor is also valid.
Damian Williamson
Sent: Sunday, 20 May 2018 11:58 AM
I wouldn't limit my rendering to words, but that is a decent starting point.  The richer the rendering, the harder it will be to forget, but it needn't all be developed at once. My goal here is to inspire the creation of art that is, at the same time, highly useful and based on randomness.
Anyway, I asked what "premise that this is needed" you meant and I still don't know the answer.
"The archive is a shared memory" - yes, a shared computer memory, and growing larger (ie more cumbersome) every day. If something like stuxnet is used to change a lot of the copies of it at some point, it seems likely that people will notice a change, but which history is correct won't be so obvious because for the humans whose memories are not so easily overwritten, computer data is remarkably non-memorable in it's normal form (0-9,a-f, whatever).  If we ever want to abandon the historical transaction data, having a shared memory of the state of a recent UTXO Set will help to obviate the need for it.  Yes, of course the blockchain is the perfect solution, as long as there is exactly one and everyone can see that it's the same one that everyone else sees.  Any other number of archives presents a great difficulty.
In that scenario, there's no other way to prove that the starting point is valid.  Bitcoin has included a hardcoded-checkpoint in the code which served the same purpose, but this ignores the possibility that two versions of the code could be created, one with a fake checkpoint that is useful to a powerful attacker.  If the checkpoint were rendered into something memorable at the first opportunity, there would be little question about which one is correct when the difference is discovered.
I just don't see the point of needing to know it any different from the hex value. Or maybe I should say I can't imagine it being useful because I can't imagine what you're after is possible. There might be a theoretical proof that what you're after is impossible. Hard to forget is almost the opposite of many options and what we're trying to do is decide between many options. I'll assume English because  it's the only starting point I have that's even in the ballpark of being possible. You might need to constrain the word selection and the structure in which you put it. I can only imagine that you are talking about putting the words into some sort of story. The only kind of story that would be hard to forget  is one that  fits into an overall structure that we are familiar with but those types of structures are few  compared to the possibilities that we're trying to encode. "Hard to deny" is a type of "hard to forget". Besides trying to connect it to external reality or history that we can all agree on there is also an internal consistency that could be used like a checksum such as the structure I mentioned. The only thing that seems to fit the bill is the blockchain itself. It's on everyone's computer so it's impossible to forget and it has internal consistency. Is the only shared memory we have that can't be subject to a Sybil attack or other hijacking of our memory of the true history. Our translation of the hash into words and a story could potentially be subject to hijacking if it's not done perfectly correct. It just seems best to me to use the hash itself. They archived existence of the prior parts of the blockchain are what make that particular hash hard to forget. Supposedly it can't be forged to reference  a fake history. The archive is a shared memory that fits the encoding rules.
Did you mean the premise that we have "the need to retain the full blockchain in order to validate the UTXO Set"?
I hadn't thought of just making it easier to remember, as your suggestion does (12-13 words), and that's a great idea.  I have passphrases of that kind, but I noticed a kind of mandela effect just with myself.  For example, I one of the words I chose was like "olfactory" but after a few months, what I remembered was like "ontology". The solution I came up with is to couple the data with far more items than we normally do.  Every ten minutes, we get a new set of 256 bits that can be used to create something potential very difficult to forget, and that's what I'm after.
An algorithm could be used to do this with the Bip39 word list.  If we categorized the words according to parts of speech, the bits could be used to determine which word goes next in a kind of ad-lib, but this creates a phrase that is only memorable in the language for which the algorithm is developed.  As a thought experiment, I'll try adjective noun verb(as past tense) noun preposition adjective adjective noun: Stuff would come out like "Able abstract abused accident across actual adult affair."  not memorable at all, but sense can be made of it and sometimes the sense will be remarkable.  As it is, I will have forgotten it in an hour or two.
I disagree with your premise that this is needed, I like the question. Humans are experts at language and I don't think we have another repository at hand that we can categorize for memory that is better than words. Using words is a common way of doing what you're thinking about. If the checkpoint could be a hash that meets a difficulty target the possibilities are 2^182 at the current hashrate instead of 2^256. So we need only 12 or 13 common words with their various possible endings (30,000).  I would find it easier to insert  a letter and 3 numbers after every 2 words so there would be only 8 words used. There are probably other tricks people have figured out but there can't be any kind of advanced encoding because it wouldn't benefit more than one word. There might be a way to convert the words into something that almost sounds like English sentences but it would probably come out a cost of at least doubling the number of words.
I got the idea that a SHA256 hash could be rendered into something difficult to forget.  The rendering would involve using each of the 256 bits to specify something important about the rendering - important in an instinctive human-memory way.
Let's assume such a rendering is possible, and that at any time, any person can execute the rendering against the SHA256 hash of a consistent representation of the UTXO Set.  Sometimes, someone will execute the rendering and discover that it is remarkable in some way (making it even more memorable), and therefore will publish it.
The published, memorable rendering now becomes a kind of protection against any possible re-writing of the blockchain from any point prior to that UTXO Set.  When everyone involved in Bitcoin recognizes this protection, it relieves us of the need to retain the full blockchain in order to validate the UTXO Set at that point, because enough people will recognize it, and it can be validated without reference to any kind of prior computer record.
This does leave open the possibility that an attacker could create a more favorable UTXO Set that happens to have a rendering similar enough to fool people, or one that has exactly the same SHA256-hash, but that possibility is remote enough to ignore (just as we all ignore the possibility that whatever creates the master seed for our HD wallet will create a unique master seed).
I've been working on how such a rendering could happen.  It could describe music, characters, colors, plot points, memorable elements of characters, etc.
Dave Scotese
bitcoin-discuss mailing list
bitcoin-discuss at lists.linuxfoundation.org
I like to provide some work at no charge to prove my value. Do you need a techie?
I own Litmocracy and Meme Racing (in alpha).
I'm the webmaster for The Voluntaryist which now accepts Bitcoin.
I also code for The Dollar Vigilante.
"He ought to find it more profitable to play by the rules" - Satoshi Nakamoto
I like to provide some work at no charge to prove my value. Do you need a techie?
I own Litmocracy and Meme Racing (in alpha).
I'm the webmaster for The Voluntaryist which now accepts Bitcoin.
I also code for The Dollar Vigilante.
"He ought to find it more profitable to play by the rules" - Satoshi Nakamoto

@_date: 2018-09-06 08:48:34
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Guiding transaction fees towards a more 
Humour me please,
Where you say "create transactions which are only valid if they are mined on top of a specific block." - in practice, does that usually means at any height above a specific block?
Sent: Sunday, 2 September 2018 3:26:54 AM
When a user creates a transaction with a fee attached, they are
incentivizing miners to add this transaction to the blockchain. The
task is usually not very specific -- as long as it ends up in a valid
chain with the most Proof-of-Work, miners get paid. The payment is an
incentive for miners to act in the way that users desire.
To the user, there?s an individual benefit: their transaction gets
added. To the network, there?s a shared benefit: all fees add to the
security of other transactions in the chain. Miners can choose to
ignore the incentives, but they would be leaving money on the table
(and eventually get replaced by more competitive miners).
Transactions from Bitcoin Core are slightly more specific about what
they ask miners to do. Every transaction is only valid at a block
height that is one higher than the last block. This incentivizes
miners to build on top of the last block, instead of going back and
reorganizing the blockchain. This is especially important in a future
scenario where the fee reward is larger than the block reward.
BIP 115* by Luke-jr is even more specific. It enables users to create
transactions which are only valid if they are mined on top of a
specific block. While originally designed as a form of replay
protection, it actually serves as a deterrent for miners to reorganize
the blockchain. If they orphan a block, it will invalidate
transactions that demanded the inclusion of the orphaned block. This
increases the cost of intentionally reorganizing the blockchain.
Coinjoin**, the act of combining payments of multiple users into a
single transaction, can be seen as yet another method for users to be
more specific. The fate of their payments are now intertwined with
that of others. If miners wish to censor a single payment, they have
to reject the entire transaction, and the associated fee amount.
Techniques like mimblewimble simplify this process, by making coinjoin
This brings us to a theoretical scenario where:
- every block contains only a single coinjoin transaction
- the validity of this transaction depends on the inclusion of a
specific previous block
- the block reward is negligible compared to transaction fees
In this scenario, if miners wish to undo a specific transaction that
happened two blocks ago, they would have to create three empty blocks
(receiving negligible compensation) in order to overtake the longest
chain. And even then, users can still refuse to let their new
transactions be mined on top of the empty blocks, disincentivizing
such behavior completely.
While not easy to achieve in practice (e.g. resolving natural forks
becomes more complicated), it demonstrates that users can become more
empowered than they are today, benefitting censorship resistance***.
It is this line of thinking that I wish to convey. Perhaps it may
inspire further ideas in this direction.

@_date: 2018-09-15 05:29:20
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
Anticipating ongoing hashrate rise shows that you have not yet thought about moving outside of the current greed model, a model wherein mining will consume all available resources within the colony's objective just to spread as far as possible with each new miner bringing diminishing individual returns and shortening the life of Earth for no additional gain. Greed model := bacteria.
Sent: Friday, 14 September 2018 9:19:37 AM
I discussed this more at bitcointalk:
The attacks I'm interested in preventing are not only selfish mining
and collusion, but also more subtle attacks like block withholding,
and in general anything that aims to drive out the competition in
order to increase hashrate fraction. I also scrapped the idea of
changing the block subsidies, and I am only focuses on fees.
You can read more about the motivation and details in the bitcointalk
thread, but my proposal in short would be to add the concept of
"reserve fees". When a user makes a transaction, for each txout
script, they can add parameters that specify the fraction of the total
fee that is held in "reserve" and the time it is held in "reserve"
(can set a limit of 2016 blocks). This "reserve" part of the fee will
be paid to miners if the hashrate rises. So if hashrate is currently h
and peak hashrate (from past year) is p, then for each period (1 day),
a new hashrate is calculated h1, and if h1 > h, then the fraction
(h1-h)/p from the reserve fees created in the past 2016 blocks will be
released to miners for that period (spread out over the 144 blocks in
that period). And this will keep happening as long as hashrate keeps
rising, until the "contract" expires, and the leftover part can be
used by the owner of the unspent output, but it can only be used for
paying fees, not as inputs for future transactions (to save on block
This should incentivize miners to not drive out the competition, since
if they do, there will be less of these reserve fees given to miners.
Yes in the end the miners will get all the fees, but with rising
hashrate they get an unconditional subsidy that does not require
transactions, thus more space for transactions with fees.
I can make a formal BIP and pull request, but I need to know if there
is interest in this. Now fees don't play such a large part of the
block reward, but they will get more important, and this change
wouldn't force anything (would be voluntary by each user), just miners
have to agree to it with a soft fork (so they don't spend from the
anyone-can-spend outputs used for reserve fees). Resource requirements
for validation are quite small I believe.
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2018-09-15 22:45:55
@_author: Damian Williamson 
@_subject: [bitcoin-dev] Selfish Mining Prevention 
I see what you say, however, since the proposal as I have read it says "And this will keep happening as long as hashrate keeps rising," - what actually happens in the case hashrate stagnates or falls?
I would prefer to see (not only with your proposal) greater bias toward hashrate being exponentially more uneconomical the more it rises to stifle greed. The current level of mining already greatly exceeds that necessary for the stability of the network and far lower hashrates are completely acceptible provided the network is protected from large switch-ons, which I say is achievable.
I do have other thoughts to approach greed that I have not yet made formal on this list, much unrelated to your proposal, but, I see freedom of use of Bitcoin needing to be censorship resistant but not necessarily mining provided it is protected enough or free or flexible enough to allow for, say, 50k globally distributed standard mining hardware units to exist operating at any one time profitably. That said, I am PoW only and not PoS orientated.
Sent: Sunday, 16 September 2018 2:01:19 AM
 Agent: No problem. I did ask in the first post what the current
plans are for selfish miner prevention. So if anyone has any other
relevant ideas (not just for selfish mining but for making mining more
decentralized and competetive), then please post it, but I just prefer
to focus on my proposal more than others.
 I think you are concerned that this will incentivize more
global resource consumption and will be detrimental to our
environment? Personally, I believe centralization of energy does more
harm to the environment rather than total energy consumption. If
Bitcoin helps "power" to become more decentralized, then I wouldn't be
surprised if total (global) energy consumption actually decreases. The
debt based economy is forcing us to continuously grow and use up more
resources, and collectivism is turning individuals into
super-organisms capable of doing much more harm to the environment
than can be done by one or a just a few individuals working
independently. In my proposal, I actually allow for changing
environmental conditions by measuring only the peak hashrate of the
past year, and not the full history of blocks.
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647

@_date: 2018-09-25 21:55:49
@_author: Damian Williamson 
@_subject: [bitcoin-dev] [bitcoin-discuss] Proposal to replace full 
A fairly decent rework would be needed but it seems that the idea has merit initially.
As it is now, it is not only that a utxo exists but, that the transaction it references and the block it is within can also be fully validated.
So, if a utxo block set type existed then by consensus every so often a bunch of blocks containing just the validated utxo set to a given height, say 100,000 blocks below the current blockheight, and necessary header data could be appended onto the valid chain and nodes would be free to drop all preceding blocks. I suspect that many wouldn't and that even many new nodes would still desire to download the full blockchain but, for the use case you mention it would make sense.
If done [right/wrong] it may even make Satoshi's fortune spendable. Something to watch out for.
Sent: Wednesday, 26 September 2018 1:46:54 AM
The image at imgur and the pastebin both reference block 542324 but the correct block is 542322.  As the pastebin shows, the decimal and hex representations I gave for the block height did not match, and this is why.  If you use the Merkle root for block 542322 instead of 542324, you'll be able to see the correct Game of Life play out and make the apron image.
I thought I didn't have access to the dev list and so intended to post the following proposal to this discussion list, but used the wrong email address.  Anyway, my email did get into the dev list ( but I'll repeat it here:
I've been working on an idea that relieves full nodes of storing the entire blockchain. Open source software generally relies on the fact that "enough" people agree that it's secure. Bitcoin software works that way too. So if you understand enough to see that a UTXO set is valid at a certain block height, and there are enough other people who agree and that set is recognizable by humans, then we can use that UTXO set and ditch the blockchain that existed up to that point. It would save a lot of storage and make it a lot easier to run a full node.
Have you reviewed the source code from which your wallets were compiled? At some point, we all trust third parties, but generally (at least among people who understand Bitcoin) they are large composite groups so that no small group or individual can profit from cheating.
I look forward to answering any concerns and also to any offers of help.   I used block 542324 of the Bitcoin blockchain to make a memorable experience using the game of life. I wrote a script for the open-source Game-of-Life software Golly and shared it in the paste at  It produces the image at  If someone can tell me how to get a UTXO Set from the bitcoin client, I'll send them $50 of bitcoin. Then I could get the SHA256 hash of that set and try to make a recognizable checkpoint for the Bitcoin blockchain. If someone runs Golly and shares a video of the game playing out (into the apron-shaped image), I'll send them $50 of bitcoin too.
In a few decades when the blockchain has grown to a few terabytes and the UTXO Set is still just a few gigabytes, I'd like to see more people start running full nodes without the hassle of a long wait and loads of storage space. That's what stops me from running one.
I like to provide some work at no charge to prove my value. Do you need a techie?
I own Litmocracy and Meme Racing (in alpha).
I'm the webmaster for The Voluntaryist which now accepts Bitcoin.
I also code for The Dollar Vigilante.
"He ought to find it more profitable to play by the rules" - Satoshi Nakamoto

@_date: 2019-04-01 08:55:00
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] Softfork proposal for minimum price of 
It is April 1st. Nonetheless, I am agreed that the first part to track the exchange rate in USD (why not use Gold $/oz?) has merit if properly implemented. Voluntary data is notoriously difficult to enforce for accuracy.
Sent: Monday, 1 April 2019 2:07 PM
Hi Everyone,
     First portion of your First BIP is excellent- with this field the protocol can actually attain HFT- by  pinning of value at time of trade for later unblock scaling as a mark to market tool which is used in HFT.
The Second BIP of a minimum price would never allow Bitcoin to pass the SEC test of a viable asset. - I hope that portion was actually a Joke since it makes no sense economically.
If one has ever seen a stock drop 90% in one day then you would understand the volatile nature of future valuation.
Prices have to be able to go higher and lower based on market demand and not tethering to perceived norms. If the US dollar was crashed and it went down to Venezuelan currency levels [hypothetically then where would your 50k limit lead the currency} Never say never-
Best Regards
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2019-04-14 14:44:53
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] List of proposals for hard fork/soft fork 
Is anybody keeping a list of the solid proposals > BIP's to be included in any actual future consensus-driven fork? Perhaps pre-consensus voting of what to include in the fork packages?
Surely not every or each proposal ever scouted is on for consideration.
This may actually help to build momentum for useful and valuable implementations that may otherwise languish.
LORD HIS EXCELLENCY JAMES HRMH

@_date: 2019-08-03 00:51:12
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] Add a moving checkpoint to the Bitcoin protocol 
I have but one point to make in a brief catch-up read over.
With the current protocol the fix to a network split is simple, the longest chain win. But with the moving checkpoint I'm proposing we have a problem if both chains began to differ more than N blocks ago, the forks are permanent. So we need an additional rule to ignore the moving checkpoint, a limit of X blocks:
It is not to be considered the longest chain, it is to be considered the longest chain with the most proof of work.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Friday, 2 August 2019 11:08 PM
Hi all,
Very good points. I did some clarifications in a private conversation, the new rule is making the moving checkpoint valid only if the difference in blocks between the main chain and the new fork is smaller than X blocks, like for example 3 days of blocks, so after a long network split everyone can finally follow the longest chain:
With the current protocol the fix to a network split is simple, the longest chain win. But with the moving checkpoint I'm proposing we have a problem if both chains began to differ more than N blocks ago, the forks are permanent. So we need an additional rule to ignore the moving checkpoint, a limit of X blocks:
If a node sees a fork longer than his main chain, and the fork has at least X blocks more than the main chain, then the node ignore the moving checkpoint rule, and it follows the fork, the longest chain.
So as an example, the moving checkpoint could be 24 hours of blocks, and the limit of X blocks, the blocks of 3 days.
So we have 2 possible situations to consider:
- 51% attack:  the blocks older than 24 hours are protected against a history rewrite during at least 3 days, in that time developers could release an emergency release with another mining algorithm to stop the attack.
- Network split: if the network split is older than N blocks, we have 2 permanent forks (or chains), but in 3 days (or more) the blockchain heights will differ in more than X blocks (the blocks of 3 days) because there will be more miners in one chain than in the other so finally the loser chain will be abandoned and everyone will follow the longest chain.
It could be even more conservative, like 48 hours for the moving checkpoint and a block limit of 7 days of blocks.
Sent: Friday, August 2, 2019 14:19
Attack 1:
I partition (i.e. eclipse) a bunch of nodes from the network this partition contains no mining power . I then mine 145 blocks for this partition. I don't even need 51% of the mining power because I'm not competing with any other miners. Under this rule this partition will hardfork from the network permanently. Under current rules this partition will be able to rejoin the network as the least weight chain will be orphaned.
Attack 2:
I pre-mine 145 blocks. A node goes offline for 24 hours, when it rejoins I feed it 145 blocks which fork off from the consensus chain. I have 24+24 hours to mine these 145 blocks so I should be able to do this with 25% of the current hash rate at the time the node went offline. Under your rule each of these offline-->online nodes I attack this way will hardfork themselves from the rest of the network.
I believe a moving-checkpoint rule as describe above would make Bitcoin more vulnerable to 51% attacks.
A safer rule would be if a node detects a fork with both sides of the split having  length > 144 blocks, it halts and requests user intervention to determine which chain to follow.  I don't think 144 blocks is a great number to use here as 24 hours is very short. I suspect you could improve the security of the rule by making the number of blocks a fork most reach to halt the network proportional to the difference in time between the timestamp in the block prior to the fork and the current time. I am **NOT** proposing Bitcoin adopt such a rule.
NXT has a fundamentally different security model as it uses Proof-of-stake rather than Proof-of-Work.
P.S.: To be clearer, in this example I set an N value of 144 blocks, which is approximately 24 hours.
Sent: Wednesday, July 31, 2019 16:40
It would be detected by the community much before reaching the reorg limit of N blocks (it's 24 hours) so nodes could stop until the netsplit is fixed.
In the extreme case no one notice the network split during more than N blocks (24 hours) and there are 2 permanent forks longer than N, nodes from one branch could delete their local history so they would join the other branch.
Sent: Wednesday, July 31, 2019 15:59
How would a (potentially, state-sponsored) netsplit lasting longer than N be
Alistair Mann
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2019-03-10 14:25:47
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The 
Opinion: Lock in a blockheight to get rid of it 10 years in the future. Use it as press that Bitcoin is going to lose $1,000,000 if some mystery person does not put their transaction through by then, try for global presses. Use the opportunity to get rid of it while you are able. Once gazetted anything is public knowledge.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Saturday, 9 March 2019 7:14 AM
Aside from the complexity issues here, note that for a user to be adversely affect, they probably have to have pre-signed lock-timed transactions. Otherwise, in the crazy case that such a user exists, they should have no problem claiming the funds before activation of a soft-fork (and just switching to the swgwit equivalent, or some other equivalent scheme). Thus, adding additional restrictions like tx size limits will equally break txn.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2019-03-12 07:34:55
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
I have not seen all of the emails in reply come through on the mailing list, I am sure it is always that way. There are a couple to reply to, replies indented>:
What about putting it in a deprecated state for some time. Adjust the transaction weight so using the op code is more expensive (10x, 20x?) and get the word out that it will be removed in the future.
You could even have nodes send a reject code with the message ?OP_CODESEPARATOR is depcrecated.?
Sent: Monday, 11 March 2019 5:24 AM
And then make UTXOs containing OP_CODESEAPRATOR (etc.) and mined prior to the soft fork activation standard, with weight penalties as appropriate, so there would be no difficulty spending them before the cutoff?
Opinion: Lock in a blockheight to get rid of it 10 years in the future. Use it as press that Bitcoin is going to lose $1,000,000 if some mystery person does not put their transaction through by then, try for global presses. Use the opportunity to get rid of it while you are able. Once gazetted anything is public knowledge.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Saturday, 9 March 2019 7:14 AM
Aside from the complexity issues here, note that for a user to be adversely affect, they probably have to have pre-signed lock-timed transactions. Otherwise, in the crazy case that such a user exists, they should have no problem claiming the funds before activation of a soft-fork (and just switching to the swgwit equivalent, or some other equivalent scheme). Thus, adding additional restrictions like tx size limits will equally break txn.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2019-11-08 17:03:02
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] CVE-2017-18350 disclosure 
It goes without saying in that all privately known CVE should be handled so professionally but, that is, well done team.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Saturday, 9 November 2019 2:07 AM
CVE-2017-18350 is a buffer overflow vulnerability which allows a malicious
SOCKS proxy server to overwrite the program stack on systems with a signed
`char` type (including common 32-bit and 64-bit x86 PCs).
The vulnerability was introduced in 60a87bce873ce1f76a80b7b8546e83a0cd4e07a5
(SOCKS5 support) and first released in Bitcoin Core v0.7.0rc1 in 2012 Aug 27.
A fix was hidden in d90a00eabed0f3f1acea4834ad489484d0012372 ("Improve and
document SOCKS code") released in v0.15.1, 2017 Nov 6.
To be vulnerable, the node must be configured to use such a malicious proxy in
the first place. Note that using *any* proxy over an insecure network (such
as the Internet) is potentially a vulnerability since the connection could be
intercepted for such a purpose.
Upon a connection request from the node, the malicious proxy would respond
with an acknowledgement of a different target domain name than the one
requested. Normally this acknowledgement is entirely ignored, but if the
length uses the high bit (ie, a length 128-255 inclusive), it will be
interpreted by vulnerable versions as a negative number instead. When the
negative number is passed to the recv() system call to read the domain name,
it is converted back to an unsigned/positive number, but at a much wider size
(typically 32-bit), resulting in an effectively infinite read into and beyond
the 256-byte dummy stack buffer.
To fix this vulnerability, the dummy buffer was changed to an explicitly
unsigned data type, avoiding the conversion to/from a negative number.
Credit goes to practicalswift ( for
discovering and providing the initial fix for the vulnerability, and Wladimir
J. van der Laan for a disguised version of the fix as well as general cleanup
to the at-risk code.
- 2012-04-01: Vulnerability introduced in PR - 2012-05-08: Vulnerability merged to master git repository.
- 2012-08-27: Vulnerability published in v0.7.0rc1.
- 2012-09-17: Vulnerability released in v0.7.0.
- 2017-09-21: practicalswift discloses vulnerability to security team.
- 2017-09-23: Wladimir opens PR  to quietly fix vulernability.
- 2017-09-27: Fix merged to master git repository.
- 2017-10-18: Fix merged to 0.15 git repository.
- 2017-11-04: Fix published in v0.15.1rc1.
- 2017-11-09: Fix released in v0.15.1.
- 2019-06-22: Vulnerability existence disclosed to bitcoin-dev ML.
- 2019-11-08: Vulnerability details disclosure to bitcoin-dev ML.
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2019-11-17 20:04:04
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] v3 onion services 
For those perhaps not so well versed in the operation of Bitcoin (and Bitcoin Core) with Tor, connectivity through the outgoing connection to other nodes is all accomplished via the socks5 proxy which enables all current gossip and the distribution of the nodes own transactions to other nodes. This is a full connectivity feature.
For listening, Bitcoin Core instructs Tor to create an ephemeral hidden service which, depending on the various factors, may be currently v2 only or v3 (future implementation). This is not necessary for the functionality of node connectivity in any way and is only used to allow for hidden listening so that other nodes connecting out on the onion can connect privately without hopping on the public internet at all and without exposing the nodes public IP or ports as listening (no port forwarding required and no listing on nodes list with public IP). It is currently possible for many nodes to exist as onion only nodes.
For the time being, although I did wonder myself, the use of v3 ephemeral service is not a requirement of operation on Bitcoin and hardly adds anything to security especially if we enable transient onion listening (a feature proposal is currently waiting for consideration/approval on bitcoin-core-dev and GitHub), however, eventually it will be essential to make use of the v3 ephemeral service as the v2 service support will, as I understand, eventually be dropped from the Tor network. I do not know if there are any current distinct plans.
My opinion is, v3 support is a nice idea but hardly urgent yet. Good luck if it ends up with an ack as I suspect some of the changes required will be complex and this may be perhaps the best reason to begin on it while there is interest.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Monday, 18 November 2019 2:35 AM
Yes, that is correct. Currently at present moment only v2 onion services
are supported. Bitcoin Core is limited at 128 bit 'addresses' in the p2p
protocol, so it requires a rework of the p2p protocol. v3 onion services
are whole ed25519 public keys, base32 encoded with .onion at the end.
Same reason applies to I2P 'address types' as well. However, I am not an
expert in I2P and don't actually know how many bitcoin full nodes might
exist in I2P.
For the default `ADD_ONION` feature, the onion service key was
downgraded to explicitly RSA1024 (legacy, v2 onion services) to ensure
the feature still works out of the box:
If you want a Tor only full node, you are best to use v2 onion services
for now. Why do you need the bitcoin node to explicitly have a v3 onion
address? You can have a service that is accessible to the general public
as a v3 onion service, and in the back uses a bitcoin full node that
uses v2 onion service to talk to other nodes. The v2 onion service
bitcoin network is extended fairly.
You can use in the same torrc (Tor configuration file), implicitly same
Tor process/daemon simultaneously v2 and v3 onion services by setting
HiddenServiceVersion parameter after every HiddenServiceDir parameter.

@_date: 2019-11-18 12:34:49
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] v3 onion services 
The Tor team encourages active participating Tor nodes, preferably exit/ middle/guard nodes and not only client nodes, which is actually a significant part of the reason that the documentation I put together in Bitcoin.SE does not deal much with configuration tweaking Tor; as out of the box Tor participates actively in the Tor network.
As for applications other than web browsing, i is simply not true to suggest that Tor is implemented solely for web browsing and I suppose that this view has come about because of the Tor browser, an attempt to engage more active Tor nodes while providing an OOB privacy solution to simplify setup for the not-so-technical. As just one example of other uses, you will note the Tor configuration item `LongLivedPorts` and its implications. No, it is completely not necessary to tweak this option for Bitcoin although you may.
I encourage you to forward these comments to the Tor mailing list.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Monday, 18 November 2019 10:59 PM
As I briefly sketched here before I think that a better long term solution would be to link the bitcoin traffic with something like node-Tor (
Much more light (the whole code not minified is only ~1MB), not using tons of libraries prone to security/maintenance issues, easy to use/configure/maintain and you don't need the (heavy/complicate) onions RDV concepts and addresses, which in addition is useless for bitcoin
As simple as a duplex stream bitcoin.pipe(node-Tor) inside servers or browsers (difficult to imagine full nodes and the blocks inside browsers but why not one day, so for light clients probably implementing part of the bitcoin protocol like  for now it's a standalone offline webapp but of course it would be interesting to connect it in a secure way to bitcoin nodes to retrieve info from the utxo set and send txs for example since it's not obvious for users to create their txs in its current form)
This would be a separate network using the Tor protocol over TCP, WebSockets and WebRTC, making it possible also for browsers to relay the traffic, probably the nodes discovery (to get the keys) could be linked to the bitcoin peer discovery system (we just have to add the onion key to the peer profile, and maybe long term id key), anyway that's simple to setup, and probably for a p2p network 2 hops will be enough
I really don't think that the Tor network is designed and adapted to support bitcoin nodes, using it for something else than browsing is just a workaround and I would be surprised that the Tor project team contradicts this and/or encourage this use
Le 18/11/2019 ? 00:42, Matt Corallo via bitcoin-dev a ?crit :
There is effort ongoing to upgrade the Bitcoin P2P protocol to support other address types, including onion v3. There are various posts on this ML under the title ?addrv2?. Further review and contributions to that effort is, as always, welcome.
?Right now bitcoin client core supports use of tor hidden service. It
supports v2 hidden service. I am in progress of creating a new bitcoin
node which will use v3 hidden service instead of v2. I am looking at
bitcoin core and btcd to use. Do any of these or current node software
support the v3 onion addresses for the node address? What about I2P
addresses? If not what will it take to get it to support the longer
addresses that is used by i2p and tor v3?
lee.chiffre at secmail.pro
PGP 97F0C3AE985A191DA0556BCAA82529E2025BDE35
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org

@_date: 2020-03-22 11:58:33
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] Block solving slowdown question/poll 
There seems to be the real possibility that miners are simply trying to optimise mining profit by limiting the average hash rate during the retargeting, saving some electricity but poorly considering the overall situation where they give opportunity to other miners probably raising the hashrate for the next period. It is far more profitable for the ecosystem considering the whole to hold a lottery for minig as has been discussed elsewhere some time ago.
LORD HIS EXCELLENCY JAMES HRMH
Sent: Sunday, 22 March 2020 6:54 PM
There are only two practical solutions I'm aware of:
1. Do nothing
2. Hard fork a difficulty reduction
If bitcoins retain even a small fraction of their value compared to the
previous retarget period and if most mining equipment is still available
for operation, then doing nothing is probably the best choice---as block
space becomes scarcer, transaction feerates will increase and miners
will be incentivized to increase their block production rate.
If the bitcoin price has plummeted more than, say, 99% in two weeks
with no hope of short-term recovery or if a large fraction of mining
equipment has become unusable (again, say, 99% in two weeks with no
hope of short-term recovery), then it's probably worth Bitcoin users
discussing a hard fork to reduce difficulty to a currently sustainable

@_date: 2020-09-29 03:10:36
@_author: LORD HIS EXCELLENCY JAMES HRMH 
@_subject: [bitcoin-dev] Floating-Point Nakamoto Consensus 
Good Afternoon,
Re: [bitcoin-dev] Floating-Point Nakamoto Consensus
I note that the discussion thread for this proposal has previously received HARD_NAK
I note in the whitepaper the following basic introduction of need:
The said race condition, it is not noted, is 'self-resolving' with the network adopting the longest chain with the highest proof of work as any contentious tip is built on. This is the proper reason for waiting for two confirmations for a transaction as a minimum proof of its inclusion in the blockchain (without fraud), and for waiting for six confirmations before considering that a transaction is 'final'.
I take it that your work is intended to allow the network to decide immediately without waiting for a chain to be further extended, in that case, the solution should be as noted elsewhere to accept the higher proof of work with the greater precision proof. When comparing two competing blocks there is an indirect reference to a higher proof of work due to the greater precision in the block hash, although the answer could have been arrived with fewer attempts. As it is, the total proof of work is not directly calculated but is evaluated as the chain with more blocks being the chain with more proof-of-work, whereas in the cases two blocks are received as alternates extending the same chain tip there is currently no method of comparison to determine which of the blocks, and the correct tip is not chosen without further proof-of-work to extend a tip. Resolving this reduces the network expense of reorganisation in ordinary conditions but in the case of a network-split resolves nothing.
KING JAMES HRMH
Great British Empire
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
et al.
and other projects
m. 0487135719
f. 61261470192
Sent: Friday, 25 September 2020 5:40 AM
  Hey Everyone,
 A lot of work has gone into this paper, and the current revision has been well received and there is a lot of excitement on this side to be sharing it with you today. There are so few people that truly understand this topic, but we are all pulling in the same direction to make Bitcoin better and it shows.  It is wildly underrated that future proofing was never really a consideration in the initial design - but here we are a decade later with amazing solutions like SegWit which gives us a real future-proofing framework.  The fact that future-proofing was added to Bitcoin with a softfork gives me goosebumps. I'd just like to take the time to thank the people who worked on SegWit and it is an appreciation that comes up in conversation of how difficult and necessary that process was, and this appreciation may not be vocalized to the great people who worked on it. The fact that Bitcoin keeps improving and is able to respond to new threats is nothing short of amazing - thank you everyone for a great project.
This current proposal really has nothing to do with SegWit - but it is an update that will make the network a little better for the future, and we hope you enjoy the paper.
Pull Request:
Floating-Point Nakamoto Consensus
Abstract ? It has been shown that Nakamoto Consensus is very useful in the formation of long-term global agreement ? and has issues with short-term disagreement which can lead to re-organization (?or-org?) of the blockchain.  A malicious miner with knowledge of a specific kind of denial-of-service (DoS) vulnerability can gain an unfair advantage in the current Bitcoin network, and can be used to undermine the security guarantees that developers rely upon.  Floating-Point Nakamoto consensu makes it more expensive to replace an already mined block vs. creation of a new block, and by resolving ambiguity of competition solutions it helps achieve global consumers more quickly.  A floating-point fitness test strongly incentivises the correct network behavior, and prevents disagreement from ever forming in the first place.
The Bitcoin protocol was created to provide a decentralized consensus on a fully distributed p2p network.  A problem arises when more than one proof-of-work is presented as the next solution block in the blockchain.  Two solutions of the same height are seen as authoritative equals which is the basis of a growing disagreement. A node will adopt the first solution seen, as both solutions propagate across the network a race condition of disagreement is formed. This race condition can be controlled by byzentiene fault injection commonly referred to as an ?eclipsing? attack.  When two segments of the network disagree it creates a moment of weakness in which less than 51% of the network?s computational resources are required to keep the network balanced against itself.
Nakamoto Consensus
Nakamoto Consensus is the process of proving computational resources in order to determine eligibility to participate in the decision making process.  If the outcome of an election were based on one node (or one-IP-address-one-vote), then representation could be subverted by anyone able to allocate many IPs. A consensus is only formed when the prevailing decision has the greatest proof-of-work effort invested in it. In order for a Nakamoto Consensus to operate, the network must ensure that incentives are aligned such that the resources needed to subvert a proof-of-work based consensus outweigh the resources gained through its exploitation. In this consensus model, the proof-of-work requirements for the creation of the next valid solution has the exact same cost as replacing the current solution. There is no penalty for dishonesty, and this has worked well in practice because the majority of the nodes on the network are honest and transparent, which is a substantial barrier for a single dishonest node to overcome.
A minimal network peer-to-peer structure is required to support Nakamoto Conesus, and for our purposes this is entirely decentralized. Messages are broadcast on a best-effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.  This design makes no guarantees that the peers connected do not misrepresent the network or so called ?dishonest nodes.? Without a central authority or central view - all peers depend on the data provided by neighboring peers - therefore a dishonest node can continue until a peer is able to make contact an honest node.
In this threat model let us assume a malicious miner possesses knowledge of an unpatched DoS vulnerability (?0-day?) which will strictly prevent honest nodes from communicating to new members of the network - a so-called ?total eclipse.?  The kind of DoS vulnerability needed to conduct an eclipse does not need to consume all CPU or computaitly ability of target nodes - but rather prevent target nodes from forming new connections that would undermine the eclipsing effect. These kinds of DoS vulnerabilities are somewhat less substional than actually knocking a powerful-mining node offline.  This class of attacks are valuable to an adversary because in order for an honest node to prove that a dishonest node is lying - they would need to form a connection to a segment of the network that isn?t entirely suppressed. Let us assume a defense-in-depth strategy and plan on this kind of failure.
Let us now consider that the C++ Bitcoind has a finite number of worker threads and a finite number of connections that can be serviced by these workers.  When a rude client occupies all connections - then a pidgin-hole principle comes into play. If a network's maximum capacity for connection handlers ?k?, is the sum of all available worker threads for all nodes in the network, establishing ?k+1? connections by the pidgin-hole principle will prevent any new connections from being formed by honest nodes - thereby creating a perfect eclipse for any new miners joining the network would only be able to form connections with dishonest nodes.
Now let?s assume a dishonest node is modified in two ways - it increases the maximum connection handles to hundreds of thousands instead of the current value which is about 10. Then this node is modified to ignore any solution blocks found by honest nodes - thus forcing the dishonest side of the network to keep searching for a competitive-solution to split the network in two sides that disagree about which tip of the chain to use.  Any new solution propagates through nodes one hop at a time. This propagation can be predicted and shaped by dishonest non-voting nodes that are being used to pass messages for honest nodes.
At this point an attacker can expedite the transmission of one solution, while slowing another. If ever a competing proof-of-work is broadcasted to the network, the adversary will use their network influence to split knowledge of the proof-of-work as close to ? as possible. If the network eclipse is perfect then an adversary can leverage an eigen-vector of computational effort to keep the disagreement in balance for as long as it is needed. No mechanism is stopping the attacker from adding additional computation resources or adjusting the eclipsing effect to make sure the system is in balance.   As long as two sides of the network are perfectly in disagreement and generating new blocks - the attacker has intentionally created a hard-fork against the will of the network architects and operators. The disagreement needs to be kept open until the adversary?s transactions have been validated on the honest chain - at which point the attacker will add more nodes to the dishonest chain to make sure it is the ultimate winner - thus replacing out the honest chain with the one generated by dishonest miners.
This attack is convenient from the adversary?s perspective,  Bitcoin being a broadcast network advertises the IP addresses of all active nodes - and Shodan and the internet scanning project can find all passive nodes responding on TCP 8333.  This should illuminate all honest nodes on the network, and even honest nodes that are trying to obscure themselves by not announcing their presence.  This means that the attacker doesn?t need to know exactly which node is used by a targeted exchange - if the attacker has subdued all nodes then the targeted exchange must be operating a node within this set of targeted honest nodes.
During a split in the blockchain, each side of the network will honor a separate merkel-tree formation and therefore a separate ledger of transactions. An adversary will then broadcast currency deposits to public exchanges, but only on the weaker side, leaving the stronger side with no transaction from the adversary. Any exchange that confirms one of these deposits is relying upon nodes that have been entirely eclipsed so that they cannot see the competing chain - at this point anyone looking to confirm a transaction is vulnerable to a double-spend. With this currency deposited on a chain that will become ephemeral, the attacker can wire out the account balance on a different blockchain - such as Tether which is an erc20 token on the Ethereum network which would be unaffected by this attack.  When the weaker chain collapses, the transaction that the exchange acted upon is no longer codified in Bitcoin blockchain's global ledger, and will be replaced with a version of the that did not contain these deposits.
Nakamoto Consensus holds no guarantees that it?s process is deterministic.  In the short term, we can observe that the Nakamoto Consensus is empirically non-deterministic which is evident by re-organizations (re-org) as a method of resolving disagreements within the network.   During a reorganization a blockchain network is at its weakest point, and a 51% attack to take the network becomes unnecessary. An adversary who can eclipse honest hosts on the network can use this as a means of byzantine fault-injection to disrupt the normal flow of messages on the network which creates disagreement between miners.
DeFi (Decentralized Finance) and smart-contract obligations depend on network stability and determinism.  Failure to pay contracts, such as what happened on ?black thursday? resulted in secured loans accidentally falling into redemption.  The transactions used by a smart contract are intended to be completed quickly and the outcome is irreversible.  However, if the blockchain network has split then a contract may fire and have it?s side-effects execute only to have the transaction on the ledger to be replaced.  Another example is that a hard-fork might cause the payer of a smart contract to default - as the transaction that they broadcasted ended up being on the weaker chain that lost. Some smart contracts, such as collateral backed loans have a redemption clause which would force the borrower on the loan to lose their deposit entirely.
With two sides of the network balanced against each other - an attacker has split the blockchain and this hard-fork can last for as long as the attacker is able to exert the computational power to ensure that proof-of-work blocks are regularly found on both sides of the network.  The amount of resources needed to balance the network against itself is far less than a 51% attack - thereby undermining the security guarantees needed for a decentralized untrusted payment network to function.  An adversary with a sufficiently large network of dishonest bots could use this to take a tally of which miners are participating in which side of the network split. This will create an attacker-controlled hard fork of the network with two mutually exclusive merkle trees. Whereby the duration of this split is arbitrary, and the decision in which chain to collapse is up to the individual with the most IP address, not the most computation.
In Satoshi Nakamoto?s original paper it was stated that the electorate should be represented by computational effort in the form of a proof-of-work, and only these nodes can participate in the consues process.  However, the electorate can be misled by non-voting nodes which can reshape the network to benefit an individual adversary.
Chain Fitness
Any solution to byzantine fault-injection or the intentional formation of disagreements must be fully decentralized. A blockchain is allowed to split because there is ambiguity in the Nakamoto proof-of-work, which creates the environment for a race-condition to form. To resolve this, Floating-Point Nakamoto Consensus makes it increasingly more expensive to replace the current winning block. This added cost comes from a method of disagreement resolution where not every solution block is the same value, and a more-fit solution is always chosen over a weaker solution. Any adversary attempting to have a weaker chain to win out would have to overcome a kind of relay-race, whereby the winning team?s strength is carried forward and the loser will have to work harder and harder to maintain the disagreement.  In most cases Floating-Point Nakamoto Consensus will prevent a re-org blockchain from ever going past a single block thereby expediting the formation of a global consensus.  Floating-Point Nakamoto Consensus cements the lead of the winner and to greatly incentivize the network to adopt the dominant chain no matter how many valid solutions are advertised, or what order they arrive.
The first step in Floating-Point Nakamoto Consensus is that all nodes in the network should continue to conduct traditional Nakamoto Consensus and the formation of new blocks is dictated by the same zero-prefix proof-of-work requirements.  If at any point there are two solution blocks advertised for the same height - then a floating-point fitness value is calculated and the solution with the higher fitness value is the winner which is then propagated to all neighbors. Any time two solutions are advertised then a re-org is inevitable and it is in the best interest of all miners to adopt the most-fit block, failing to do so risks wasting resources on a mining of a block that would be discarded.  To make sure that incentives are aligned, any zero-prefix proof of work could be the next solution, but now in order to replace the current winning solution an adversary would need a zero-prefix block that is also more fit that the current solution - which is much more computationally expensive to produce.
Any changes to the current tip of the blockchain must be avoided as much as possible. To avoid thrashing between two or more competitive solutions, each replacement can only be done if it is more fit, thereby proving that it has an increased expense.  If at any point two solutions of the same height are found it means that eventually some node will have to replace their tip - and it is better to have it done as quickly as possible so that consensus is maintained.
In order to have a purely decentralized solution, this kind of agreement must be empirically derived from the existing proof-of-work so that it is universally and identically verifiable by all nodes on the network.  Additionally, this fitness-test evaluation needs to ensure that no two competing solutions can be numerically equivalent.
Let us suppose that two or more valid solutions will be proposed for the same block.  To weigh the value of a given solution, let's consider a solution for block 639254, in which the following hash was proposed:
    00000000000000000008e33faa94d30cc73aa4fd819e58ce55970e7db82e10f8
There are 19 zeros, and the remaining hash in base 16 starts with 9e3 and ends with f8.  This can value can be represented in floating point as:
    19.847052573336114130069196154809453027792121882588614904
To simplify further lets give this block a single whole number to represent one complete solution, and use a rounded floating-point value to represent some fraction of additional work exerted by the miner.
   1.847
Now let us suppose that a few minutes later another solution is advertised to the network shown in base16 below:
    000000000000000000028285ed9bd2c774136af8e8b90ca1bbb0caa36544fbc2
The solution above also has 19 prefixed zeros, and is being broadcast for the same blockheight value of 639254 - and a fitness score of 1.282.  With Nakamoto Consensus both of these solutions would be equivalent and a given node would adopt the one that it received first.  In Floating-Post Nakamoto Consensus, we compare the fitness scores and keep the highest.  In this case no matter what happens - some nodes will have to change their tip and a fitness test makes sure this happens immediately.
With both solutions circulating in the network - any node who has received both proof-of-works should know 1.847 is the current highest value, and shouldn?t need to validate any lower-valued solution.  In fact this fitness value has a high degree of confidence that it won?t be unseated by a larger value - being able to produce a proof-of-work with 19 0?s and a decimal component greater than 0.847 is non-trivial.  As time passes any nodes that received a proof-of-work with a value 1.204 - their view of the network should erode as these nodes adopt the 1.847 version of the blockchain.
All nodes are incentivized to support the solution with the highest fitness value - irregardless of which order these proof-of-work were validated. Miners are incentivized to support the dominant chain which helps preserve the global consensus.
Let us assume that the underlying cryptographic hash-function used to generate a proof-of-work is an ideal primitive, and therefore a node cannot force the outcome of the non-zero component of their proof-of-work.  Additionally if we assume an ideal cipher then the fitness of all possible solutions is gaussian-random. With these assumptions then on average a new solution would split the keyspace of remaining solutions in half.  Given that the work needed to form a  new block remains a constant at 19 blocks for this period - it is cheaper to produce a N+1 block that has any floating point value as this is guaranteed to be adopted by all nodes if it is the first solution.  To leverage a chain replacement on nodes conducting Floating-Point Nakamoto Consensus a malicious miner would have to expend significantly more resources.
Each successive n+1 solution variant of the same block-height must therefore on average consume half of the remaining finite keyspace. Resulting in a the n+1 value not only needed to overcome the 19 zero prefix, but also the non-zero fitness test.   It is possible for an adversary to waste their time making a 19 where n+1 was not greater, at which point the entire network will have had a chance to move on with the next solution.  With inductive reasoning, we can see that a demissiniong keyspace increases the amount of work needed to find a solution that also meets this new criteria.
Now let us assume a heavily-fragmented network where some nodes have gotten one or both of the solutions.  In the case of nodes that received the proof-of-work solution with a fitness of 1.847, they will be happily mining on this version of the blockchain. The nodes that have gotten both 1.847 and .240 will still be mining for the 1.847 domainite version, ensuring a dominant chain.  However, we must assume some parts of the network never got the message about 1.847 proof of work, and instead continued to mine using a value of 1.240 as the previous block.   Now, let?s say this group of isolated miners manages to present a new conflicting proof-of-work solution for 639255:
     000000000000000000058d8ebeb076584bb5853c80111bc06b5ada35463091a6
The above base16 block has a fitness score of 1.532  The fitness value for the previous block 639254 is added together:
     2.772 = 1.240 + 1.532
In this specific case, no other solution has been broadcast for block height 639255 - putting the weaker branch in the lead.  If the weaker branch is sufficiently lucky, and finds a solution before the dominant branch then this solution will have a higher overall fitness score, and this solution will propagate as it has the higher value.  This is also important for transactions on the network as they benefit from using the most recently formed block - which will have the highest local fitness score at the time of its discovery.  At this junction, the weaker branch has an opportunity to prevail enterally thus ending the split.
Now let us return to the DoS threat model and explore the worst-case scenario created by byzantine fault injection. Let us assume that both the weaker group and the dominant group have produced competing proof-of-work solutions for blocks 639254 and 639255 respectively.  Let?s assume that the dominant group that went with the 1.847 fitness score - also produces a solution with a similar fitness value and advertises the following solution to the network:
3.262 = 1.847 + 1.415
A total of 3.262 is still dominant over the lesser 2.772 - in order to overcome this - the 2nd winning block needs to make up for all of the losses in the previous block.  In this scenario, in order for the weaker chain to supplant the dominant chain it must overcome a -0.49 point deficit. In traditional Nakamoto Consensus the nodes would see both forks as authoritative equals which creates a divide in mining capacity while two groups of miners search for the next block.  In Floating-Point Nakamoto Consensus any nodes receiving both forks, would prefer to mine on the chain with an overall fitness score of +3.262 - making it even harder for the weaker chain to find miners to compete in any future disagreement, thereby eroding support for the weaker chain. This kind of comparison requires an empirical method for determining fitness by miners following the same same system of rules will insure a self-fulfilled outcome.  After all nodes adopt the dominant chain normal Nakamoto Consuess can resume without having to take into consideration block fitness. This example shows how disagreement can be resolved more quickly if the network has a mechanism to resolve ambiguity and de-incentivise dissent.
Soft Fork
Blockchain networks that would like to improve the consensus generation method by adding a fitness test should be able to do so using a ?Soft Fork? otherwise known as a compatible software update.  By contrast a ?Hard-Fork? is a separate incompatible network that does not form the same consensus.  Floating-Point Nakamoto Consensus can be implemented as a soft-fork because both patched, and non-patched nodes can co-exist and non-patched nodes will benefit from a kind of herd immunity in overall network stability.  This is because once a small number of nodes start following the same rules then they will become the deciding factor in which chain is chosen.  Clients that are using only traditional Nakamoto Consensus will still agree with new clients over the total chain length. Miners that adopt the new strategy early, will be less likely to lose out on mining invalid solutions.
Floating-Point Nakamoto consensus allows the network to form a consensus more quickly by avoiding ambiguity allowing for determinism to take hold. Bitcoin has become an essential utility, and attacks against our networks must be avoided and adapting, patching and protecting the network is a constant effort. An organized attack against a cryptocurrency network will undermine the guarantees that blockchain developers are depending on.
Any blockchain using Nakamoto Consensus can be modified to use a fitness constraint such as the one used by a Floating-Point Nakamoto Consensus.  An example implementation has been written and submitted as a PR to the bitcoin core which is free to be adapted by other networks.
A complete implementation of Floating-Point Nakamoto consensus is in the following pull request:
