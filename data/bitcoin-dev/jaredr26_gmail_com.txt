
@_date: 2017-03-31 23:15:09
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
When I first began to enter the blocksize debate slime-trap that we
have all found ourselves in, I had the same line of reasoning that you
have now.  It is clearly untenable that blockchains are an incredibly
inefficient and poorly designed system for massive scales of
transactions, as I'm sure you would agree.  Therefore, I felt it was
an important point for people to accept this reality now and stop
trying to use Blockchains for things they weren't good for, as much
for their own good as anyone elses.  I backed this by calculating some
miner fee requirements as well as the very issue you raised.  A few
people argued with me rationally, and gradually I was forced to look
at a different question: Granted that we cannot fit all desired
transactions on a blockchain, how many CAN we effectively fit?
It took another month before I actually changed my mind.  What changed
it was when I tried to make estimations, assuming all the reasonable
trends I could find held, about future transaction fees and future
node costs.  Did they need to go up exponentially?  How fast, what
would we be dealing with in the future?  After seeing the huge
divergence in node operational costs without size increases($3 vs
$3000 after some number of years stands out in my memory), I tried to
adjust various things, until I started comparing the costs in BTC
terms.  I eventually realized that comparing node operational costs in
BTC per unit time versus transaction costs in dollars revealed that
node operational costs per unit time could decrease without causing
transaction fees to rise.  The transaction fees still had to hit $1 or
$2, sometimes $4, to remain a viable protection, but otherwise they
could become stable around those points and node operational costs per
unit time still decreased.
None of that may mean anything to you, so you may ignore it all if you
like, but my point in all of that is that I once used similar logic,
but any disagreements we may have does not mean I magically think as
you implied above.  Some people think blockchains should fit any
transaction of any size, and I'm sure you and I would both agree
that's ridiculous.  Blocks will nearly always be full in the future.
There is no need to attempt to handle unusual volume increases - The
fee markets will balance it and the use-cases that can barely afford
to fit on-chain will simply have to wait for awhile.  The question is
not "can we handle all traffic," it is "how many use-cases can we
enable without sacrificing our most essential features?"  (And for
that matter, what is each essential feature, and what is it worth?)
There are many distinct cut-off points that we could consider.  On the
extreme end, Raspberry Pi's and toasters are out.  Data-bound mobile
phones are out for at least the next few years if ever.  Currently the
concern is around home user bandwidth limits.  The next limit after
that may either be the CPU, memory, or bandwidth of a single top-end
PC.  The limit after that may be the highest dataspeeds that large,
remote Bitcoin mining facilities are able to afford, but after fees
rise and a few years, they may remove that limit for us.  Then the
next limit might be on the maximum amount of memory available within a
single datacenter server.
At each limit we consider, we have a choice of killing off a number of
on-chain usecases versus the cost of losing the nodes who can't reach
the next limit effectively.  I have my inclinations about where the
limits would be best set, but the reality is I don't know the numbers
on the vulnerability and security risks associated with various node
distributions.  I'd really like to, because if I did I could begin
evaluating the costs on each side.
That's a good question, and one I don't have a good handle on.  How
does Bitcoin's current memory usage scale?  It can't be based on the
UTXO, which is 1.7 GB while my node is only using ~450mb of ram.  How
does ram consumption increase with a large block versus small ones?
Are there trade-offs that can be made to write to disk if ram usage
grew too large?
If that proved to be a prohibitively large growth number, that becomes
a worthwhile number to consider for scaling.  Of note, you can
currently buy EC2 instances with 256gb of ram easily, and in 14 years
that will be even higher.
I believe this is exactly the kind of discussion we should be having
14 years before it might be needed.  Also, this wouldn't be unique -
Some software I have used in the past (graphite metric collection)
came pre-packaged with the ability to scale out to multiple machines
split loads and replicate the data, and so could future node software.
Bandwidth costs are, as intra-datacenter bandwidth is generally free.
The other ones warrant evaluation for the distant future.  I would
expect that CPU resources is the first thing we would have to change -
13 thousand transactions per second is an awful lot to process.  I'm
not intimately familiar with the processing - Isn't it largely
signature verification of the transaction itself, plus a minority of
time spent checking and updating utxo values, and finally a small
number of hashes to check block validity?  If signature verification
was controlling, a specialized asic chip(on a plug-in card) might be
able to verify signatures hundreds of times faster, and it could even
be on a cheap 130nm chipset like the first asic miners rushed to
market.  Point being, there are options and it may warrant looking
into after the risk to node reductions.
I don't think this is as big a deal as it first might seem.  The
software would already come written to be spanned onto multiple
machines - it just needs to be configured.  For the specific question
at hand, the exchange would already have IT staff and datacenter
capacity/operations for their other operations.  In the more general
case, the numbers involved don't work out to extreme concerns at that
level.  The highest cpu usage I've observed on my nodes is less than
5%, less than 1% for the time I just checked, handling ~3 tx/s.  So
being conservative, if it hits 100% on one core at 60-120 tx/s, that
works out to ~25-50 8-core machines.  But again, that's a 2-year old
laptop CPU and we're talking about 14 years into the future.  Even if
it was 25 machines, that's the kind of operation a one or two man IT
team just runs on the side with their extra duties.  It isn't enough
to hire a fulltime tech for.
I mean, that's literally what Amazon does for you with S3, which was
even cheaper than the EBS datastore pricing I was looking at.  So....
Even disregarding that, raid operation was a solved thing more than 10
years ago, and hard drives 14 years out would be roughly ~110 TB for a
$240 hard drive at a 14%/year growth rate.  In 2034 the blockchain
would fit on 10 of those.  Not exactly a "failing every day" kind of
problem.  By 2040, you'd need *gasp* 22 $240 hard drives.  I mean, it
is a lot, but not a lot like you're implying.
That depends heavily upon the tradeoffs the businesses can make.  I
don't think node operation at an exchange is a five-nines uptime
operation.  They could probably tolerate 3 nines.  The worst that
happens is occasionally people's withdrawals and deposit are delayed
slightly.  It won't shut down trading.
Visa stores the only copy.  They can't afford to lose the data.
Bitcoin isn't like that, as others pointed out.  And for most
businesses, if their node must be rebooted periodically, it isn't a
huge deal.
Ok, when is that point, and what is the tradeoff in terms of nodes?
Just because something is hard doesn't mean it isn't worth doing.
That's just a defeatist attitude.  How big can we get, for what
tradeoffs, and what do we need to do to get there?
This is really not that hard.  Have a central database, update/check
the utxo values in block-store increments.  If a utxo has already been
used this increment, the block is invalid.  If the database somehow
got too big(not going to happen at these scales, but if it did), it
can be sharded trivially on the transaction information.  These are
solved problems, the free database software that's available is pretty
NO, NOT CLEVER.  WE CAN'T DO THAT.
Sorry, I had to. :)
I know of and have experience working with systems that handled
several orders of magnitude more data than this.  None of the issues
brought up above are problems that someone hasn't solved.  Transaction
commitments to databases?  Data consistency across multiple workers?
Data storage measured in exabytes?  Data storage and updates
approaching hundreds of millions of datapoints per second?  These
things are done every single day at numerous companies.

@_date: 2017-03-31 23:18:29
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
If you can't describe an attack that is made possible when typical
personal computers can't run nodes, this kind of logic has no place in
this discussion.
On Fri, Mar 31, 2017 at 4:13 PM, Eric Voskuil via bitcoin-dev

@_date: 2017-03-31 23:55:42
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For 
Can you explain this statement a little better?  What do you mean by
that?  What does the total bitcoins transacted have to do with
hashpower required?
On Fri, Mar 31, 2017 at 2:22 PM, Matt Corallo via bitcoin-dev

@_date: 2017-04-01 11:42:50
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
That's a quoted general statement that is highly subjective, not a
description of an attack.  If you can't articulate a specific attack vector
that we're defending against, such a defense has no value.
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
"Governments are good at cutting off the heads of a centrally
controlled networks..."

@_date: 2017-04-06 10:26:52
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
I'd just like to point out, invalidating asicboost has only a very
limited number of potential detractors.  Only a mining farm that
self-mined and used custom software would be able to exploit this.
Every other mining farm on the planet, plus any users wishing for more
transactions to be included in blocks would be in favor of this,
assuming the theory that it favors fewer transactions is correct.
That makes it less contentious than many other alternatives.  It might
even force the mining operation(s) in question to flip and support SW
in order to avoid losing face and/or appearing guilty.
As an additional plus, nearly all of the BU crowd and most BU
supporting miners would have little reason to object to Asicboost -
Based on philosophy alone, but not based on any practical
On Wed, Apr 5, 2017 at 8:23 PM, David Vorick via bitcoin-dev

@_date: 2017-04-06 10:31:13
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
To me, all of these miss the main objection.  If a miner found an
optimization and kept it for themselves, that's their prerogative.
But if that optimization also happens to directly discourage the
growth and improvement of the protocol in many unforseen ways, and it
also encourages the miner to include fewer transactions per block,
that directly hurts Bitcoin and its future.  Something should clearly
be done about it when the latter is at issue.  I agree with you that
the former is a relative nonissue.
On Wed, Apr 5, 2017 at 11:24 PM, Jonathan Toomim via bitcoin-dev

@_date: 2017-04-06 13:21:12
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
Envisioning it in my head and trying to read the white paper, it
sounds like the process for a non-stratum mining farm would be this:
On primary server with sufficient memory, calculate ~4k-6k valid
left-side merkle tree roots and ~4k-6k right-side merkle tree roots.
Then try hashing every left-side option with every right-side option.
I'm not sure if modern asic chips are sufficiently generic that they
can also sha256-double-hash those combinations, but it seems logical
to assume that the permutations of those hashes could be computed on
an asic, perhaps via additional hardware installed on the server.
Hashing these is easier if there are fewer steps, i.e., fewer
Out of this will come N(2-16 at most, higher not needed) colliding
merkle roots where the last 4 bytes are identical.  Those N different
merkle combinations are what can be used on the actual mining devices,
and those are all that needs to be sent for the optimization to work.
On the actual mining device, what is done is to take the identical
(collision) right 4 bytes of the merkle root and hash it with one
nonce value.  Since you have N(assume 8) inputs that all work with the
same value, calculating this single hash of once nonce is equivalent
to calculating 8 nonce hashes during the normal process, and this step
is 1/4th of the normal hashing process.  This hash(or mid-value?) is
then sent to 8 different cores which complete the remaining 3 hash
steps with each given collision value.  Then you increment the nonce
once and start over.
This works out to a savings of (assuming compressor and expander steps
of SHA2 require computationally the same amount of time) 25% * (7 / 8)
where N=8.
Greg, or someone else, can you confirm that this is the right
understanding of the approach?
As above, it doesn't require such a massive change.  They just need to
retrieve N different sets of work from the central server instead of 1
set of work.  The central server itself might need substantial
bandwidth if it farmed out the merkle-root hashing computational space
to miners.  Greg, is that what you're assuming they are doing?  Now
that I think about it, even that situation could be improved.  Suppose
you have N miners who can do either a merkle-tree combinatoric
double-sha or a block-nonce double-sha.  The central server calculates
the left and right merkle treeset to be combined and also assigns each
miner each a unique workspace within those combinatorics.  The miners
compute each hash in their workspace and shard the results within
themselves according to the last 16 bits.  Each miner then needs only
the memory for 1/Nth of the workspace, and can report back to the
central server only the highest number of collisions it has found
until the central server is satisfied and returns the miners to normal
(collided) mining.
Seems quite workable in a large mining farm to me, and would allow the
collisions to be found very, very quickly.
That said, it strikes me that there may be some statistical method by
which we can isolate which pools seem to have used this approach
against the background noise of other pools.  Hmm...
On Wed, Apr 5, 2017 at 7:10 PM, Jonathan Toomim via bitcoin-dev

@_date: 2017-04-09 14:16:26
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] A Small Modification to Segwit 
I can speak from personal experience regarding another very prominent
altcoin that attempted to utilize an asic-resistant proof of work
algorithm, it is only a matter of time before the "asic resistant"
algorithm gets its own Asics.  The more complicated the algorithm, the more
secretive the asic technology is developed.  Even without it,
multi-megawatt gpu farms have already formed in the areas of the world with
low energy costs.  I'd support the goal if I thought it possible, but I
really don't think centralization of mining can be prevented.
On Apr 9, 2017 1:16 PM, "Erik Aronesty via bitcoin-dev" <

@_date: 2017-06-02 12:40:36
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hypothetical 2 MB hardfork to follow BIP148 
I think this is exactly the right direction to head.  There are
arguments to be made for various maximum sizes... Maybe the limit
could be set to 1mb initially, and at a distant future block
height(years?) automatically drop to 500kb or 100kb?  That would give
anyone with existing systems or pre-signed transactions several years
to adjust to the change.  Notification could (?possibly?) be done with
a non-default parameter that must be changed to continue to use 100kb
- <1mb transactions, so no one running modern software could claim
they were not informed when that future date hits.
I don't see any real advantages to continuing to support transactions
larger than 100kb excepting the need to update legacy use cases /
already signed transactions.
On Tue, May 30, 2017 at 8:07 PM, Jacob Eliosoff via bitcoin-dev

@_date: 2017-06-02 13:13:58
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Compatibility-Oriented Omnibus Proposal 
it's what most users had/have in mind when they discuss a "2MB+SegWit"
results in ~2 or 2.1MB blocks.
increased to 2000000 bytes and Total Weight is increased to 8000000 bytes.
With normal usage, the expected results would then be ~4 or 4.2MB blocks.
I think Calvin is correct here, the secondary limit is not what people
anticipated with the segwit + 2mb agreement.  It would not kill the
agreement for me, but it might for others.
What is the justification for the secondary limitation?  Is there hard data
to back this?  The quadratic hashing problem is frequently brought up, but
that is trivially handled with a hard 1mb transaction limit and on the
other thread there's talk/suggestions of an even lower limit.  Are there
any other reasons for this limitation, and is there data to justify those
concerns?  If not, this should be left out in favor of a transaction size
limit.  If so, hard data would go a long way to dealing with the conversy
this will create.
release a new deployment.
?flag-day activation?) serve to prevent unnecessary delays in the network
upgrade process, addressing a common criticism of the Scaling Agreement and
providing an opportunity for cooperation and unity instead.
This is likely to cause more controversy and unfortunately has the tightest
timelines.  Unlike the SW2mb working group's timelines, a hard-coded
timeline couldn't be changed with mutual agreement from the signers.
Given the chance of bit1 accidental activation without clear signaling for
the required bit4 2mb hard fork, I don't think the fair or acceptable
tradeoff is for flag day to require bit1 signaling only.  *Flag day should
be modified to accept either bit1 signaling, OR to accept bit4 signaling IF
the 80% threshold hasn't been met.*  In this way the anti-segwit working
group members are not in danger of an activated bit1 segwit without also
getting their portion of the compromise, the bit4 signaled HF.  If flag day
accepts bit4 OR bit1, AND bit4 requires both bit1 and bit4 once 80% is
reached, flag day is nearly guaranteed to get its stated desire within 1750
blocks (bit4 accepted until block 800; bit4+bit1 signaled afterwards until
95%), but without the chance that the WG signers won't get what they agreed
*That seems like a minor compromise for BIP148.  Thoughts on this change to
flag day / BIP148?*
In addition, the aggressiveness of the timelines and the complexity of the
merged COOP proposal may require the BIP148 flag day to be pushed back.  I
would think some day in September is achievable, but I'm not sure if August
1st will be.
On Tue, May 30, 2017 at 3:20 PM, CalvinRechner via bitcoin-dev <

@_date: 2017-06-07 14:09:04
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
FYI, even if very successful, this deployment and change may have a
severe negative impact on a small group of miners.  Any miners/pools
who are not actively following the forums, news, or these discussions
may be difficult to reach and communicate with in time, particularly
with language barriers.  Of those, any who are also either not
signaling segwit currently or are running an older software version
will have their blocks continuously and constantly orphaned, but may
not have any alarms or notifications set up for such an unexpected
failure.  That may or may not be a worthy consideration, but it is
definitely brusque and a harsh price to pay.  Considering the
opposition mentioned against transaction limits for the rare cases
where a very large transaction has already been signed, it seems that
this would be worthy of consideration.  For the few miners in that
situation, it does turn segwit from an optional softfork into a
punishing hardfork.
I don't think that's a sufficient reason alone to kill the idea, but
it should be a concern.
On Wed, Jun 7, 2017 at 7:10 AM, Erik Aronesty via bitcoin-dev

@_date: 2017-06-07 14:29:55
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
I think this BIP represents a gamble, and the gamble may not be a good
one.  The gamble here is that if the segwit2x changes are rolled out
on time, and if the signatories accept the bit4 + bit1 signaling
proposals within BIP91, the launch will go smoother, as intended.  But
conversely, if either the segwit2x signatories balk about the Bit1
signaling OR if the timelines for segwit2mb are missed even by a bit,
it may cause the BIP148 chainsplit to be worse than it would be
without.  Given the frequent concerns raised in multiple places about
the aggressiveness of the segwit2x timelines, including the
non-hardfork timelines, this does not seem like a great gamble to be
The reason I say it may make the chainsplit be worse than it would
otherwise be is that it may provide a false sense of safety for BIP148
that currently does not currently exist(and should not, as it is a
chainsplit).  That sense of safety would only be legitimate if the
segwit2x signatories were on board, and the segwit2x code effectively
enforced BIP148 simultaneously, neither of which are guaranteed.  If
users and more miners had a false sense that BIP148 was *not* going to
chainsplit from default / segwit2x, they might not follow the news if
suddenly the segwit2x plan were delayed for a few days.  While any
additional support would definitely be cheered on by BIP148
supporters, the practical reality might be that this proposal would
take BIP148 from the "unlikely to have any viable chain after flag day
without segwit2x" category into the "small but viable minority chain"
category, and even worse, it might strengthen the chainsplit just days
before segwit is activated on BOTH chains, putting the BIP148
supporters on the wrong pro-segwit, but still-viable chain.
If Core had taken a strong stance to include BIP148 into the client,
and if BIP148 support were much much broader, I would feel differently
as the gamble would be more likely to discourage a chainsplit (By
forcing the acceleration of segwit2x) rather than encourage it (by
strengthening an extreme minority chainsplit that may wind up on the
wrong side of two segwit-activated chains).  As it stands now, this
seems like a very dangerous attempt to compromise with a small but
vocal group that are the ones creating the threat to begin with.
On Tue, Jun 6, 2017 at 5:56 PM, James Hilliard via bitcoin-dev

@_date: 2017-06-07 14:43:14
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
after that happens it becomes optional for miners again.
I missed that, that does effectively address that concern.  It appears
that BIP148 implements the same rule as would be required to prevent a
later chainsplit as well, no?
This comment did bring to mind another concern about BIP148/91 though,
which I'll raise in the pull request discussion.  Feel free to respond
to it there.
On Wed, Jun 7, 2017 at 2:21 PM, James Hilliard

@_date: 2017-06-07 14:50:18
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
Could this risk mitigation measure be an optional flag?  And if so,
could it+BIP91 signal on a different bit than bit4?
The reason being, if for some reason the segwit2x activation cannot
take place in time, it would be preferable for miners to have a more
standard approach to activation that requires stronger consensus and
may be more forgiving than BIP148.  If the segwit2x activation is on
time to cooperate with BIP148, it could be signaled through the
non-bit4 approach and everything could go smoothly.  Thoughts on that
idea?  It may add more complexity and more developer time, but may
also address your concerns among others.
The concern I'm raising is more about the psychology of giving BIP148
a sense of safety that may not be valid.  Without several more steps,
BIP148 is definitely on track to be a risky chainsplit, and without
segwit2x it will almost certainly be a small minority chain. (Unless
the segwit2x compromise falls apart before then, and even in that case
it is likely to be a minority chain)
On Wed, Jun 7, 2017 at 2:42 PM, James Hilliard

@_date: 2017-06-07 15:53:14
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
hashpower either of which is enough to make a permanent chain split
unlikely, miners will mine whichever chain is most profitable(see
ETH/ETC hashpower profitability equilibrium for an example of how this
works in practice)
That's not a comparable example.  ETC did not face potentially years of
slow blocktimes before it normalized, whereas BIP148 is on track to do
exactly that.  Moreover, ETC represented a fundamental break from the
majority consensus that could not be rectified, whereas BIP148 represents
only a minority attempt to accelerate something that an overwhelming
majority of miners have already agreed to activate under segwit2x.  Lastly
ETC was required to add replay protection, just like any minority fork
proposed by any crypto-currency has been, something that BIP148 both lacks
and refuses to add or even acknowledge the necessity of.  Without replay
protection, ETC could not have become profitable enough to be a viable
minority chain.  If BIP148's chain is not the majority chain and it does
not have replay protection, it will face the same problems, but that
required replay protection will turn it into a hardfork.  This will be a
very bad position for UASF supporters to find themselves in - Either
hardfork and hope the price is higher and the majority converts, or die as
the minority chain with no reliable methods of economic conversion.
I believe, but don't have data to back this, that most of the BIP148
insistence comes not from a legitimate attempt to gain consensus (or else
they would either outright oppose segwit2mb for its hardfork, or they would
outright support it), but rather from an attempt for BIP148 supporters to
save face for BIP148 being a failure.  If I'm correct, that's a terrible
and highly non-technical reason for segwit2mb to bend over backwards
attempting to support BIP148's attempt to save face.
is takes too long to activate unless started ahead of the existing
segwit2x schedule and it's unlikely that BIP148 will get pushed back
any further.
Even if I'm not correct on the above, I and others find it hard to accept
that this timeline conflict is segwit2x's fault.  Segwit2x has both some
flexibility and broad support that crosses contentious pro-segwit and
pro-blocksize-increase divisions that have existed for two years.  BIP148
is attempting to hold segwit2x's timelines and code hostage by claiming
inflexibility and claiming broad support, and not only are neither of those
assertions are backed by real data, BIP148 (by being so inflexible) is
pushing a position that deepens the divides between those groups.  For
there to be technical reasons for compatibility (so long as there are
tradeoffs, which there are), there needs to be hard data showing that
BIP148 is a viable minority fork that won't simply die off on its own.
On Wed, Jun 7, 2017 at 3:23 PM, James Hilliard

@_date: 2017-06-07 16:43:57
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
This statement is misleading.  Wipeout risks don't apply to any consensus
changes; It is a consensus change, it can only be abandoned.  The BIP148
chain carries just as many risks of being abandoned or even more with
segwit2x on the table.  No miner would consider "wipeout risk" an advantage
when the real threat is chain abandonment.
Higher transaction fees suffers the same problem as exchange support does.
Without replay protection, it is very difficult for any average user to
force transactions onto one chain or the other.  Thus, without replay
protection, the UASF chain is unlikely to develop any viable fee market;
Its few miners 99% of the time will simply choose from the highest fees
that were already available to the other chain, which is basically no
advantage at all.
ETC replay protection was added because they were already a hardfork and
without it they would not have had a viable chain.  BIP148 is not supposed
to be a hardfork, and if it added replay protection to remain viable it
would lose the frequently touted "wipeout advantage" as well as the ability
to call itself a softfork.  And are you seriously suggesting that what
happened with ETC and ETH is a desirable and good situation for Bitcoin,
and that UASF is ETC?
For a miners blowing through six million dollars a day in mining
operational costs, that's a pretty crappy reason.  Serious miners can't
afford to prop up a non-viable chain based on philosophy or maybes.  BIP148
is based entirely upon people who aren't putting anything on the line
trying to convince others to take the huge risks for them.  With
deceptively fallacious logic, in my opinion.
Even segwit2x is based on the assumption that all miners can reach
consensus.  Break that assumption and segwit2x will have the same problems
as UASF.
They are bundled.  Segwit alone doesn't have the desired overwhelming
consensus, unless core wishes to fork 71% to 29%, and maybe not even that
high.  That's the technical reason, and they can't be unbundled without
breaking that consensus.
On Wed, Jun 7, 2017 at 4:11 PM, James Hilliard

@_date: 2017-06-07 17:20:23
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
Ah, so the BIP148 client handles this on behalf of its less technical users
on their behalf then, yes?
Sure, Exchanges are going to dedicate hundreds of developer hours and
thousands of support hours to support something that they've repeatedly
told everyone must have replay protection to be supported.  They're going
to do this because 8% of nodes and <0.5% of miners say they'll be rewarded
richly.  Somehow I find that hard to believe.
Besides, if the BIP148 client does it for them, they wouldn't have to
dedicate those hundreds of developer hours.  Right?
I can't imagine how this logic is getting you from where the real data is
to the assumption that an economic majority will push BIP148 into being
such a more valuable chain that switching chains will be attractive to
enough miners.  There's got to be some real data that convinces you of this
Wipeout risk is a serious issue when 45% of the miners support one chain
and 55% support the other chain.  Segwit doesn't even have 35% of the
miners; There's no data or statements anywhere that indicate that UASF is
going to reach the point where wipeout risk is even comparable to
abandonment risk.
To convince miners you would have to have some data SOMEWHERE supporting
the economic majority argument.  Is there any such data?
It doesn't have those issues during the segwit activation, ergo there is no
reason for segwit-activation problems to take priority over the very real
hardfork activation problems.
In a consensus system they are frequently the same, unfortunately.
Technical awesomeness without people agreeing = zero consensus.  So the
choice is either to "technically" break the consensus without a
super-majority and see what happens, or to go with the choice that has real
data showing the most consensus and hope the tiny minority chain actually
dies off.
On Wed, Jun 7, 2017 at 5:01 PM, James Hilliard

@_date: 2017-06-07 18:01:38
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] User Activated Soft Fork Split Protection 
There's quite a few hard numbers that are available that are of varying
use.  Mining commitments are a major one because of the stalled chain
problem.  Node signaling represents some data because while it can be
sybiled, they are cheap but not free to run.  Upvotes and comments on
reddit and other forums might be of some use, but there's not a clear
supermajority driving every pro-uasf comment up and every anti-uasf comment
down, and Reddit obscures the upvote/downvotes pretty well.  It could be a
gleaned datapoint if someone pulled the comments, manually evaluated their
likely position on the matter(neutrally), and then reported on it, but that
is a lot of work and I think it is unlikely to show anything except how
deep the rifts in the community are.  Of the two main statistics available,
they do not support the idea that UASF has any chance of success.  Of the
third, it at least shows that there is deep opposition that is nearly equal
to the support amongst the forums most likely to support UASF.
So I'll take anything, any statistic that actually indicates UASF has a
chance in hell of succeeding, at least that would be worth something.
Otherwise it's all much ado about nothing.
What markets?  Where?  How would we know?
is no
Because it is not segwit that has appears to have the supermajority
Well, then we have a point of agreement at least. :)
On Wed, Jun 7, 2017 at 5:44 PM, James Hilliard

@_date: 2017-06-13 12:35:13
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Proposal: Demonstration of Phase in Full Network 
accept a particular chain is an important part of bitcoins security
What you're describing is effectively the same as BU.
Nodes follow chains, they do not decide the victor.  The average user
follows the default of the software, which is to follow the longest valid
chain.  Forcing the average user to decide which software to run is far
more valuable than allowing "the software" to decide things, when in fact
all it will do is decide the previous default.
This is true and a good point.  A false signal from miners could trick the
honest miners into forking off prematurely with a minority.
This is the job of the stratum server and the pool operator.  These are
distinct responsibilities; Miners should choose a pool operator in line
with their desires.  Solo mining is basically dead, as it will never again
be practical(and has not been for at least 2 years) for the same hardware
that does the mining to also do full node operation.
If the pool operator/stratum server also does not do validation, then any
number of problems could occur.
On Mon, Jun 12, 2017 at 10:44 PM, James Hilliard via bitcoin-dev <

@_date: 2017-06-13 18:08:49
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Proposal: Demonstration of Phase in Full Network 
Right, but that's my point.  Any level of control the fullnodes believe
they have is effectively a placebo, unless the opposition to the miners is
essentially unanimous (and maybe not even then, if the chainsplit doesn't
have any miners to get to the next difficulty change or gets attacked
We're derailed from the main thread at this point, but just wanted to state
that I agree in part.  The part I don't agree with is when a single
transaction begins to cost more than a month's worth of full validation,
which has already happened at least once last week, the full validation is
on its way to becoming worthless.  The two costs have to be balanced for
the coin to have utility for its users.
I agree with the rest.
On Tue, Jun 13, 2017 at 5:23 PM, James Hilliard

@_date: 2017-03-29 01:45:14
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
could simply do something based on historical transaction growth (which
is somewhat linear, with a few inflection points),
Where do you get this?  Transaction growth for the last 4 years averages to
+65% per year and the last 2 is +80% per year.  That's very much not linear.
On Tue, Mar 28, 2017 at 10:13 AM, Matt Corallo via bitcoin-dev <

@_date: 2017-03-29 02:16:43
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
bitcoin/papers/CDE+16.pdf  It may help you form opinions based in science
rather than what appears to be nothing more than a hunch.  It shows that
even 4MB is unsafe.  SegWit provides up to this limit.
I find this paper wholly unconvincing.  Firstly I note that he assumes the
price of electricity is 10c/kwh in Oct 2015.  As a miner operating and
building large farms at that time, I can guarantee you that almost no large
mines were paying anything even close to that high for electricity, even
then.  If he had performed a detailed search on the big mines he would have
found as much, or could have asked, but it seems like it was simply made
up.  Even U.S. industrial electricity prices are lower than that.
Moreover, he focuses his math almost entirely around mining, asserting in
table 1 that 98% of the "cost of processing a transaction" as being
mining.  That completely misunderstands the purpose of mining.  Miners
occasionally trivially resolve double spend conflicts, but miners are
paid(and played against eachother) for economic security against
attackers.  They aren't paid to process transactions.  Nodes process
transactions and are paid nothing to do so, and their costs are 100x more
relevant to the blocksize debate than a paper about miner costs.  Miner's
operational costs relate to economic protection formulas, not the cost of a
He also states: "the top 10% of nodes receive a 1MB block 2.4min earlier
than the bottom 10% ? meaning that depending on their access to nodes, some
miners could obtain a significant and unfair lead over others in solving
hash puzzles."
He's using 2012-era logic of mining.  By October 2015, no miner of any size
was in the bottom 10% of node propagation.  If they were a small or medium
sized miner, they mined shares on a pool and would be at most 30 seconds
behind the pool.  Pools that didn't get blocks within 20 seconds weren't
pools for long.  If they were a huge miner, they ran their own pool with
good propagation times.  For a scientific paper, this is reading like
someone who had absolutely no idea what was really going on in the mining
world at the time.  But again, none of that relates to transaction "costs."
 Transactions cost nodes money; protecting the network costs miners money.
Miners are rewarded with fees; nodes are rewarded only by utility and price
On Tue, Mar 28, 2017 at 10:53 AM, Alphonse Pace via bitcoin-dev <

@_date: 2017-03-29 12:07:15
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
be controversial among some users [..] I don't think it's very interesting
to discuss further size increases.
I think the reason for this is largely because SegWit as a blocksize
increase isn't very satisfying.  It resolves to a one-time increase with no
future plans, thus engendering the same objections as people who demand we
just "raise the number to N."  People can argue about what N should be, but
when N is just a flat number, we know we'll have to deal with the issue
In that light I think it is even more essential to continue to discuss the
blocksize debate and problem.
Segwit harms them,
largely comes down to the rumor that has a deathgrip on the BU community -
That Core are all just extensions of Blockstream, and blockstream wants to
restrict growth on-chain to force growth of their 2nd layer
services(lightning and/or sidechains).
I believe the tone of the discussion needs to be changed, and have been
trying to work to change that tone for weeks now.  There's one faction that
believes that Bitcoin will rarely, if ever, benefit from a blocksize
increase, and fees rising is a desired/unavoidable result.  There's a
different faction that believes Bitcoin limits are arbitrary and that all
people worldwide should be able to put any size transactions, even
microtransactions, on-chain.  Both factions are extreme in their viewpoints
and resort to conspiracy theories to interpret the actions of
Core(blockstream did it) or BU(Jihan controls everything and anyone who
says overwise is a shill paid by Roger Ver!)
It is all very unhealthy for Bitcoin.  Both sides need to accept that
microtransactions from all humans cannot go on-chain, and that never
increasing the blocksize doesn't mean millions of home users will run
nodes.  The node argument breaks down economically and the microtransaction
argument is an impossible mountain for a blockchain to climb.
On Wed, Mar 29, 2017 at 2:37 AM, Jorge Tim?n via bitcoin-dev <

@_date: 2017-03-29 12:10:42
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
In order for any blocksize increase to be agreed upon, more consensus is
needed.  The proportion of users believing no blocksize increases are
needed is larger than the hardfork target core wants(95% consensus).  The
proportion of users believing in microtransactions for all is also larger
than 5%, and both of those groups may be larger than 10% respectively.  I
don't think either the Big-blocks faction nor the low-node-costs faction
have even a simple majority of support.  Getting consensus is going to be a
big mess, but it is critical that it is done.
On Wed, Mar 29, 2017 at 12:49 AM, Martin L?zner via bitcoin-dev <

@_date: 2017-03-29 12:46:50
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
bitcoin in the background on affordable, non-dedicated home-hardware should
be a top consideration.
Why is that a given?  Is there math that outlines what the risk levels are
for various configurations of node distributions, vulnerabilities, etc?
How does one even evaluate the costs versus the benefits of node costs
versus transaction fees?
being the second most significant problem, and finally bandwidth
consumption as the third most important consideration. I believe that v0.14
is already too expensive on all three fronts, and that block size increases
shouldn't be considered at all until the requirements are reduced (or until
consumer hardware is better, but I believe we are talking 3-7 years of
waiting if we pick that option).
Disk space is not the largest cost, either today or in the future.  Without
historical checkpointing in some fashion, bandwidth costs are more than 2
orders of magnitude higher cost than every other cost for full listening
nodes.  With historical syncing discounted(i.e. pruned or nonlistening
nodes) bandwidth costs are still higher than hard drive costs.
Today: Full listening node, 133 peers, measured 1.5 TB/mo of bandwidth
consumption over two multi-day intervals.  1,500 GB/month @ ec2 low-tier
prices = $135/month, 110 GB storage = $4.95.  Similar arguments extend to
consumer hardware - Comcast broadband is ~$80/mo depending on region and
comes with 1.0 TB cap in most regions, so $120/mo or even $80/mo would be
in the same ballpark.  A consumer-grade 2GB hard drive is $70 and will last
for at least 2 years, so $2.93/month if the hard drive was totally
dedicated to Bitcoin and $0.16/month if we only count the percentage that
Bitcoin uses.
For a non-full listening node, ~25 peers I measured around 70 GB/month of
usage over several days, which is $6.3 per month EC2 or $5.6 proportional
Comcast cost.  If someone isn't supporting syncing, there's not much point
in them not turning on pruning.  Even if they didn't, a desktop in the $500
range typically comes with 1 or 2 TB of storage by default, and without
segwit or a blocksize cap increase, 3 years from now the full history will
only take up the 33% of the smaller, three year old, budget-range PC hard
drive.  Even then if we assume the hard drive price declines of the last 4
years hold steady(14%, very low compared to historical gains), 330gb of
data only works out to a proportional monthly cost of $6.20 - still
slightly smaller than his bandwidth costs, and almost entirely removable by
turning on pruning since he isn't paying to help others sync.
I don't know how to evaluate the impacts of RAM or CPU usage, or
consequently electricity usage for a node yet.  I'm open to quantifying any
of those if there's a method, but it seems absurd that ram could even
become a signficant factor given the abundance of cheap ram nowadays with
few programs needing it.  CPU usage and thus electricity costs might become
a factor, I just don't know how to quantify it at various block scales.
Currently cpu usage isn't taxing any hardware that I run a node on in any
way I have been able to notice, not including the syncing process.
good move, even as little as SegWit does.
The consequence of your logic that holds node operational costs down is
that transaction fees for users go up, adoption slows as various use cases
become impractical, price growth suffers, and alt coins that choose lower
fees over node cost concerns will exhibit competitive growth against
Bitcoin's crypto-currency market share.  Even if you are right, that's
hardly a tradeoff not worth thoroughly investigating from every angle, the
consequences could be just as dire for Bitcoin in 10 years as it would be
if we made ourselves vulnerable.
And even if an altcoin can't take Bitcoin's dominance by lower fees, we
will not end up with millions of home users running nodes, ever.  If they
did so, that would be orders of magnitude fee market competition, and
continuing increases in price, while hardware costs decline.  If
transaction fees go up from space limitations, and they go up even further
in real-world terms from price increases, while node costs decline,
eventually it will cost more to send a transaction than it does to run a
node for a full month.  No home users would send transactions because the
fee costs would be higher than anything they might use Bitcoin for, and so
they would not run a node for something they don't use - Why would they?
The cost of letting the ratio between node costs and transaction costs go
in the extreme favor of node costs would be worse - Lower Bitcoin
usability, adoption, and price, without any meaningful increase in security.
How do we evaluate the math on node distributions versus various attack
On Wed, Mar 29, 2017 at 8:57 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-29 13:32:05
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
single 512GB SSD. Lots of consumer hardware has that little storage.
That's very poor logic, sorry.  Restricted-space SSD's are not a
cost-effective hardware option for running a node.  Keeping blocksizes
small has significant other costs for everyone.  Comparing the cost of
running a node under arbitrary conditons A, B, or C when there are far more
efficient options than any of those is a very bad way to think about the
costs of running a node.  You basically have to ignore the significant
consequences of keeping blocks small.
If node operational costs rose to the point where an entire wide swath of
users that we do actually need for security purposes could not justify
running a node, that's something important for consideration.  For me, that
translates to modern hardware that's relatively well aligned with the needs
of running a node - perhaps budget hardware, but still modern - and
above-average bandwidth caps.
You're free to disagree, but your example only makes sense to me if
blocksize caps didn't have serious consequences.  Even if those
consequences are just the threat of a contentious fork by people who are
mislead about the real consequences, that threat is still a consequence
On Wed, Mar 29, 2017 at 9:18 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-29 13:53:40
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
configuration then I think you would see far more users running a pruned
Default configurations aren't a big enough deal to factor into the critical
discussion of node costs versus transaction fee cost.  Default
configurations can be changed, and if nodes are negatively affected by a
default configuration, there will be an abundance of information about how
to correct that effect by turning on pruning.  Bitcoin can't design with
the assumption that people can't google - If we wanted to cater to that
population group right now, we'd need 100x the blocksize at least.
This is already a big problem from the measurements I've been looking at.
There are alternatives that need to be considered there as well.  If we
limit ourselves to not changing the syncing process for most users, the
blocksize limit debate changes drastically.  Hard drive costs, CPU costs,
propagation times... none of those things matter because the cost of sync
bandwidth is so incredibly high even now ($130ish per month, see other
email).  Even if we didn't increase the blocksize any more than segwit,
we're already seeing sync costs being shifted onto fewer nodes - I.e., Luke
Jr's scan finding ~50k nodes online but only 7k of those show up on sites
like bitnodes.21.co.  Segwit will shift it further until the few nodes
providing sync limit speeds and/or max out on connections, providing no
fully-sync'd nodes for a new node to connect to. Then wallet providers /
node software will offer a solution - A bundled utxo checkpoint that
removes the need to sync.  This slightly increases centralization, and
increases centralization more if core were to adopt the same approach.
The advantage would be tremendous for such a simple solution - Node costs
would drop by a full order of magnitude for full nodes even today, more
when archival nodes are more restricted, history is bigger, and segwit
blocksizes are in effect, and then blocksizes could be safely increased by
nearly the same order of magnitude, increasing the utility of bitcoin and
the number of people that can effectively use it.
Another, much more complicated option is for the node sync process to
function like a tor network.  A very small number of seed nodes could send
data on to only other nodes with the highest bandwidth available(and good
retention policy, i.e. not tightly pruning as they sync), who then spread
it out further and so on.  That's complicated though, because as far as I
know the syncing process today has no ability to exchange a selfish syncing
node for a high performing syncing node.  I'm not even sure - will a
syncing node opt to sync from a different node that, itself, isn't fully
sync'd but is farther ahead?
At any rate, syncing bandwidth usage is a critical problem for future
growth and is solvable.  The upsides of fixing it are huge, though.
On Wed, Mar 29, 2017 at 9:25 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-29 15:08:33
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
That's not true unless miners are thought of as the identical to nodes,
which is has not been true for nearly 4 years now.  Nodes arbitrating a
consensus the BU theory - that nodes can restrain miners - but it doesn't
work.  If miners were forked off from nonminers, the miner network could
keep their blockchain operational under attack from the nodes far better
than nodes could keep their blockchain operational under attack from the
miners.  The miners could effectively grind the node network to a complete
halt and probably still run their own fork unimpeded at the same time.
This would continue until the the lack of faith in the network drove the
miners out of business economically, or until the node network capitulated
and followed the rules of the miner network.
The reason BU isn't a dire threat is that there's a great rift between the
miners just like there is between the average users, just as satoshi
intended, and that rift gives the user network the economic edge.
to trust and rely on other, more powerful nodes to represent them. Of
course, the more powerful nodes, simply by nature of having more power, are
going to have different opinions and objectives from the users.
I think you're conflating mining with node operation here.  Node users only
power is to block the propagation of certain things.  Since miners also
have a node endpoint, they can cut the node users out of the equation by
linking with eachother directly - something they already do out of
practicality for propagation.  Node users do not have the power to
arbitrate consensus, that is why we have blocks and PoW.
5,000,000 users. Users running full nodes is important to prevent political
hijacking of the Bitcoin protocol.  [..] that changes you are opposed to
are not introduced into the network.
This isn't true.  Non-miner nodes cannot produce blocks.  Their opinion is
not represented in the blockchain in any way, the blockchain is entirely
made up of blocks.  They can commit transactions, but the transactions must
follow an even stricter set of rules and short of a user activated PoW
change, the miners get to decide.  It might be viable for us to introduce
ways for transactions to vote on things, but that also isn't nodes voting -
that's money voting.
Bitcoin is structured such that nodes have no votes because nodes cannot be
trusted.  They don't inherently represent individuals, they don't
inherently represent value, and they don't commit work that is played
against eachother to achieve a game theory equilibrium.  That's miners.
nodes. For home users, 200 GB of bandwidth and 500 GB of bandwidth largely
have the exact same cost.
Your assumption is predicated upon the idea that users pay a fixed cost for
any volume of bandwidth.  That assertion is true for some users but not
true for others, and it is becoming exceedingly less true in recent years
with the addition of bandwidth caps by many ISP's.  Even users without a
bandwidth cap can often get a very threatening letter if they were to max
their connection 24/7.  Assuming unlimited user bandwidth in the future and
comparing that with limited datacenter bandwidth is extremely short
sighted.  Fundamentally, if market forces have established that datacenter
bandwidth costs $0.09 per GB, what makes you think that ISP's don't have to
deal with the same limitations?  They do, the difference is that $0.09 per
GB times the total usage across the ISP's customer base is far, far lower
than $80 times the number of customers.  The more that a small group of
customers deviating wildly becomes a problem for them, the more they will
add bandwidth caps or send threatening letters or even rate-limit or stop
serving those users.
Without that assumption, your math and examples fall apart - Bandwidth
costs for full archival nodes are nearly 50 times higher than storage costs
no matter whether they are at home or in a datacenter.
costs you are citing by quoting datacenter prices.
No, they really aren't without your assumption.  Yes, they are somewhat
different - If someone has a 2TB hard drive but only ever uses 40% of it,
the remaining hard drive space would have a cost of zero.  Those specific
examples break down when you average over several years and fifty thousand
users.  If that same user was running a bitcoin node and hard drive space
was indeed a concern, they would factor that desire into the purchase of
their next computer, preferring those with larger hard drives.  That
reintroduces the cost with the same individual who had no cost before.  The
cost difference doesn't work out to the exact same numbers as the
datacenter costs, who have a better economy of scale but also have profit
and business overhead, but all of the math I've done indicates that over
thousands of individuals and several years of time, the costs land in the
same ballpark.  For example - Comcast bandwidth cap = 1000gb @ ~$80/month.
 $0.08/GB.  Amazon's first tier is currently $0.09.  Much closer than I
even expected before I worked out the math.  I'm open to being proven wrong.
I'm running 0.13.2 and only see 300 mb of ram.  Why is 0.14 using three
times the ram?
seeing users choose not to run nodes simply
Again, while I sympathize with the concept, I don't believe holding the
growth of the entire currency back based on minimum specs is a fair
tradeoff.  The impact on usecases that depend on a given fee level is total
obliteration.  That's unavoidable for things like microtransactions, but a
fee level of $1/tx allows for hundreds of opportunities that a fee level of
$100/tx does not.  That difference may be the deciding factor in the
network effect between Bitcoin and a competitor altcoin.  Bitcoin dying out
because a better-operated coin steals its first-mover advantage is just as
bad as bitcoin dying out because an attacker halted tx propagation and
killed the network.  Probably even worse - First mover advantages are
almost never retaken, but the network could recover from a peering attack
with software changes and community/miner responses.
start to be the case.
I calculated this out.  If blocksizes aren't increased, but price increases
continue as they have in the last 3-5 years, per-node operational costs for
one month drop from roughly $10-15ish (using datacenter numbers, which you
said would be higher than home user numbers and might very well be when
amortized thoroughly) down to $5-8 in less than 8 years.  If transaction
fees don't rise at all due to blockspace competition (i.e., they offset
only the minimum required for miners to economically protect Bitcoin),
they'll be above $10 in less than 4 years.  I believe that comparing
1-month of node operational costs versus 1 transaction fee is a reasonable,
albeit imperfect, comparison of when users will stop caring.
That's not very far in the future at all, and fee-market competition will
probably be much, much worse for us and better for miners.
your government defaults, hyperinflates, seizes citizen assets, etc. etc.
(situations that many Bitcoin users today have to legitimately worry about),
So I don't mean to be rude here, but this kind of thinking is very poor
logic when applied to anyone who isn't already a libertarian Bitcoin
supporter.  By anyone outside the Bitcoin world's estimation, Bitcoin is an
extremely high risk, unreliable store of value.  We like to compare it to
"digital gold" because of the parameters that Satoshi chose, but saying it
does not make it true.  For someone not already a believer, Bitcoin is a
risky, speculative investment into a promising future technology, and gold
is a stable physical asset with 4,000 years of acceptance history that has
the same value in nearly every city on the planet.  Bitcoin is difficult to
purchase and difficult to find someone to exchange for goods or services.
Could Bitcoin become more like what you described in the future?  A lot of
us hope so or we wouldn't be here right now.  But in the meantime, any
other crypto currency that choses parameters similar to gold could eclipse
Bitcoin if we falter.  If their currency is more usable because they
balance the ratio of node operational costs/security versus transaction
fees/usability, they have a pretty reasonable chance of doing so.  And then
you won't store your $10k+ in bitcoin, you'll store in $altcoin.  The
market doesn't really care who wins.
think it continues to make sense to be considering the home nodes as the
target that we want to hit.
That's nothing, we've never had any fee competition at all until basically
November of last year.  From December to March transaction fees went up by
250%, and they doubled from May to December before that.  Transactions per
year are up 80% per year for the last 4 years.  Things are about to get
On Wed, Mar 29, 2017 at 1:28 PM, David Vorick via bitcoin-dev <

@_date: 2017-03-29 15:17:40
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
relationships with to start including the root hash of the (lagging) UTXO
set in their coinbase transactions, in order to begin transforming this
idea into reality.
By itself, this wouldn't work without a way for a new node to differentiate
between a false history and a true one.
controlled by known people that include the same root hash in an OP_RETURN
output, which would allow cross-checking against the miners? UTXO
commitments, as part of this initial ?prototype?
This might work, but I fail to understand how a new node could verify an
address / transaction without a blockchain to back it.  Even if it could,
it becomes dependent upon those addresses not being compromised, and the
owners of those addresses would become targets for potential government
Having the software silently attempt to resolve the problem is risky unless
it is foolproof.  Otherwise, users will assume their software is showing
them the correct history/numbers implicitly, and if the change the utxo
attacker made was small, the users might be able to follow the main chain
totally until it was too late and the attacker struck with an address that
otherwise never transacted.  Sudden, bizarre, hard to debug fork and
potentially double spend against people who picked up the fraudulent utxo.
Users already treat wallet software with some level of suspicion, asking if
they can trust x or y or z, or like the portion of the BU community
convinced that core has been compromised by blockstream bigwigs.  Signed
releases could provide the same thing but would encourage both open-source
security checks of the signed utxo's and potentially of users to check
download signatures.
Either approach is better than what we have now though, so I'd support
On Wed, Mar 29, 2017 at 1:28 PM, Peter R via bitcoin-dev <

@_date: 2017-03-30 09:44:21
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
paid to miners to make a block.
There's a formula to this as well, though going from that to a blocksize
number will be very difficult.  Miner fees need to be sufficient to
maintain economic protection against attackers.  There is no reason that
miner fees need to be any higher than "sufficient."  I believe that
"sufficient" value can be estimated by considering a potential attacker
seeking to profit from short-selling Bitcoin after causing a panic crash.
If they can earn more profit from shorting Bitcoin than it costs to buy,
build/deploy, and perform a 51% attack to shut down the network, then we
are clearly vulnerable.  The equation for the profit side of the equation
can be worked out as:
(bitcoin_price * num_coins_shortable * panic_price_drop_percentage)
The equation for the cost side of the equation depends on the total amount
of miner hardware that the network is sustainably paying to operate,
factoring in all costs of the entire bitcoin mining lifecycle(HW cost,
deployment cost, maintenance cost, electricity, amortized facilities cost,
business overheads, orphan losses, etc) except chip design, which the
attacker may be able to take advantage of for free.  For convenience I'm
simplifying that complicated cost down to a single number I'm calling
"hardware_lifespan" although the concept is slightly more involved than
(total_miner_payouts * bitcoin_price * hardware_lifespan)
Bitcoin_price is on boths ides of the equation and so can be divided out,
Unsafe point = (num_coins_shortable * panic_price_drop_percentage) <
* hardware_lifespan)
Estimating the total number of shortable coins an attacker of nearly
unlimited funds is tricky, especially when things like high leverage levels
or naked short selling may be offered by exchanges.  The percent of damage
the resulting panic would cause is also tricky to estimate, but on both
numbers we can make some rough guesses and see how they play out.  With
more conservative numbers like say, 2 year hardware lifespan, 10% short,
70% panic drop you get: 1,300k coins profit, 1800 BTC/day in fees minimum
needed to make the attack cost more than it profits.
Using various inputs and erring on the side of caution, I get a minimum
BTC/day fee range of 500-2000.  Unfortunately if the blocksize isn't
increased, a relatively small number of transactions/users have to bear the
full cost of the minimum fees, over time increasing the minimum "safe"
average fee paid to 0.008 BTC, 30x the fees people are complaining about
today, and increasing in real-world terms as price increases.  All that
said, I believe the costs for node operation are the number that gets hit
first as blocksizes are increased, at least past 2020.  I don't think
blocksizes could be increased to such a size that the insufficient-fee
vulnerability would be a bigger concern than high node operational costs.
The main thing I don't have a good grasp on at the moment is any math to
estimate how many nodes we need to protect against the attacks that can
come from having few nodes, or even a clear understanding of what those
attacks are.
This is also totally true.  A system that tried to eliminate the fee
markets would be flawed, and fortunately miners have significant reasons to
oppose such a system.
The reverse is also a problem - If miners as a large group sought to lower
blocksizes to force fee markets higher, that could be a problem.  I don't
have solutions for the issue at this time, but something I've turned over
in my mind.
On Thu, Mar 30, 2017 at 3:30 AM, Tom Zander via bitcoin-dev <

@_date: 2017-03-30 10:16:41
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Nodes don't do politics.  People do, and politics is a lot larger with a
lot more moving parts than just node operation.
which they do not agree with.
Full nodes protect from nothing if the chain they attempt to use is
arguably what gives Bitcoin most of it's value
maintain monetary sovereignty
This power is far more complicated than just nodes.  You're implying that
node operation == political participation.  Node operation is only a very
small part of the grand picture of the bitcoin balance of power.
prevents them from rewriting any of Bitcoin's rules.
No, it isn't.  Nodes disagreeing with miners is necessary but not
sufficient to prevent that.  Nodes can't utilize a nonfunctional chain, nor
can they utilize a coin with no exchanges.
Only two things - 1. Node propagation being strong enough that a target
node can't be surrounded by attacker nodes (or so that attacker nodes can't
segment honest nodes), and 2. Miners being distributed in enough countries
and locations to avoid any single outside attacker group from having enough
leverage to prevent transaction inclusion, and miners also having enough
incentives(philosophical or economic) to refuse to collude towards
transaction exclusion.
Being able to run a node yourself has no real effect on either of the two.
Either we have enough nodes that an attacker can't segment the network or
we don't.
What you're describing would result in a fork war.  The opposition to this
would widespread and preventing an attempt relies upon mutual destruction.
If users refused to get on board, exchanges would follow users.  If miners
refused to get on board, the attempt would be equally dead in the water.
It would require a majority of users, businesses and miners to change the
limit; Doing so without an overwhelming majority(90% at least) would still
result in a contentious fork that punished both sides(in price, confidence,
adoption, and possibly chain or node attacks) for refusing to agree.
Nodes have absolutely no say in the matter if they can't segment the
network, and even if they could their impact could be repaired.  Users !=
Err, this makes me worry that you don't understand how blockchains work...
This is because miners are severely punished for attempting to mine on
anything but the longest chain.  Nodes have absolutely no say in the
matter, they always follow the longest chain unless a hardfork was
applied.  If the hardfork has overwhelming consensus, i.e. stopping a 51%
attack, then the attack would be handled.  If the hardfork did not have
overwhelming consensus it would result in another fork war requiring users,
businesses, and miners to actively decide which to support and how, and
once again would involve mutual destruction on both forks.
Nodes don't decide any of these things.  Nodes follow the longest chain,
and have no practical choices in the matter.  Users not running nodes
doesn't diminish their power - Mutual destruction comes from the market
forces on the exchanges, and they could give a rats ass whether you run a
node or not.
all it's risks because it is useful for everyday transactions, that is a
solved problem in every part of the world (Cash/Visa/etc..).
This is just the "bitcoin is gold" argument.  Bitcoin is not gold.  For
someone not already a believer, Bitcoin is a risky, speculative investment
into a promising future technology, whereas gold is a stable physical asset
with 4,000 years of acceptance history that has the same value in nearly
every city on the planet.  Bitcoin is difficult to purchase and difficult
to find someone to exchange for goods or services.  Literally the only
reason we have 10s of billions of dollars of value is because speculation,
which includes nearly all Bitcoin users/holders and almost all businesses
and miners.  While Bitcoin borrows useful features from gold, it has more
possible uses, including uses that were never possible before Bitcoin
existed, and we believe that gives it huge potential.
The ability of other systems to do transactions, like visa or cash, come
with the limitations of those systems.  Bitcoin was designed to break those
limitations and STILL provide the ability to do transactions.  We might all
agree Bitcoin isn't going to ever solve the microtransaction problem, at
least not on-chain, but saying Bitcoin doesn't need utility is just
foolish.  Gold doesn't need utility, gold has 4,000 years of history.  We
picture, transaction capacity will still be too low for global usage in the
medium-long term.
Which is why it needs to be a formula or a continuous process, not a single
market innovating solutions when there is money to be made.
That's like saying it would be better to do nothing so someone else solves
our problem for us than it would be for us to do what we can to solve it
ourselves.  Someone else solving our problem may very well be Ethereum, and
"solving it for us" is pulling Bitcoin investments, users and nodes away
into Ethereum.
with very large systemic costs compared with the userbase and usage which
is growing exponentially.
The capacity increases do not have to be linear.  The increases in utility
are linear with blocksize increases, but so are the costs.  There's no
reason those blocksize increases can't be tied to or related to usage
increases, so long as the concerns about having too few nodes (or too few
fees) for security are handled.

@_date: 2017-03-30 13:51:45
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
I agree in concept with everything you've said here, but I think there's a
frequent misconception that there's a certain level of miner payouts that
miners "deserve" and/or the opposite, that miners "deserve" as little as
possible.  The 51% attacks that PoW's shields us from are relatively well
defined, which can be used to estimate the minimum amount of sustainable
fees for shielding.  Beyond that minimum amount of fees, the best amount of
fees for every non-miner is the lowest.
Unfortunately miners could arbitrarily decide to limit blocksizes, and
there's little except relay restrictions that everyone else could do about
it.  Fortunately miners so far have pushed for blocksize increases at least
as much as anyone else, though the future when Bitcoin adoption stabilizes
would be an unknown.
FYI, I don't see this happening again ever, barring brief exceptions,
unless there was a sudden blocksize change, which ideally we'd avoid ever
happening.  The stable average value of the transaction fee determines what
kind of business use-cases can be built using Bitcoin.  An average fee of
$0.001 usd enables a lot more use cases than $0.10 average fees, and $50.00
average fees still have far more possible use cases than a $1000 average
fee.  If fees stabilize low, use cases will spring up to fill the
blockspace, unless miners arbitraily seek to keep the fees above some level.
On Thu, Mar 30, 2017 at 3:30 AM, Tom Zander via bitcoin-dev <

@_date: 2017-03-30 14:42:31
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
sufficient hashrate can leverage large blocks to exacerbate selfish mining.
Can you give me a link to this?  Having done a lot of mining, I really
really doubt this.  I'm assuming the theory relies upon propagation times
and focuses on small miners versus large ones, but that's wrong.
Propagation times don't affect small miners disproportionately, though they
might affect small POOLS disproportionately, that isn't the same thing at
all.  No miner since at least 2014 has operated a full node directly with
each miner - it is incredibly impractical to do so.  They retrieve only the
merkle root hash and other parameters from the stratum server, which is a
very small packet and does not increase with the size of the blocks.  If
they really want to select which transactions to include, some pools offer
options of that sort(or can, I believe) but almost no one does.  If they
don't like how their pool picks transactions, they'll use a different pool,
that simple.
If there's some other theory about a miner exploiting higher blocksizes
selfishly then I'd love to read up on it to understand it.  If what
you/others actually meant by that was smaller "pools," that's a much much
smaller problem.  Pools don't earn major profits and generally are at the
mercy of their miners if they make bad choices or can't fix low
performance.  For pools, block propagation time was a major major issue
even before blocks were full, and latency + packet loss between mining
units and the pool is also a big concern.  I was seeing occasional block
propagation delays(over a minute) on a fiber connection in 2013/4 due to
minute differences in peering.  If a pool can't afford enough bandwidth to
keep propagation times down, they can't be a pool.  Bigger blocksizes will
make it so they even more totally-can't-be-a-pool, but they already can't
be a pool, so who cares.  Plus, compact blocks should have already solve
nearly all of this problem as I understand it.
So definitely want to know more if I'm misunderstanding the attack vector.
transactions) can be leveraged in ways that both damages the network and
increases miner profits.
Maybe you're meaning an attack where other pools get stuck on validation
due to processing issues?  This is also a nonissue.  The smallest viable
pool has enough difficulties with other, non-hardware related issues that
buying the largest, beefiest standard processor available with ample RAM
won't even come up on the radar.  No one cares about $600 in hardware
versus $1000 in hardware when it takes you 6 weeks to get your peering and
block propagation configuration just right and another 6 months to convince
miners to substantially use your pool.
If you meant miners and not pools, that's also wrong.  Mining hardware
doesn't validate blocks anymore, it hasn't been practical for years.  They
only get the merkle root hash of the valid transaction set.  The pool
handles the rest.
Bitcoin has by far the strongest development team, and also is by far the
most decentralized.
Markets only care a little bit what your development team is like.
Ethereum has Vitalik, who is an incredibly smart and respectable dude,
while BU absolutely hates the core developers right now.  Markets are more
likely to put more faith in a single leader than core right now if that
comparison was really made.
"Most decentralized" is nearly impossible to quantify, and has almost no
value to speculators.  Since all of these markets are highly speculative,
they only care about future demand.  Future demand relies upon future use.
Unsubstantiated?  Ethereum is already 28% of Bitcoin by cap and 24% by
trading.  Four months ago that was 4%.  Their transaction volume also
doubled.  What world are you living in?
that's okay. Ethereum has very different properties and it's not something
I would trust as a tool to provide me with political sovereignty.
Well great, I guess so long as you're ok with it we'll just roll with it.
Wait, no.  If Bitcoin loses its first-mover network effect, a small cadre
of die-hard libertarians are not going to be able to keep it from becoming
a page in the history books.  Die hard libertarians can barely keep a voice
in the U.S. congress - neither markets nor day-to-day users particularly
care about the philosophy, they care about what it can do for them.
superior to Bitcoin.
The markets have literally told us why Ethereum is shooting up.  Its
because the Bitcoin community has fractured around a debate with nearly no
progress on a solution for the last 3 years, and especially because BU
appears to be strong enough to think they can fork and the markets know
full well what a contentious fork will do to Bitcoin's near-term future.
Then it would have happened not when the BU situation imploded but when
Microsoft announced they were working with Ethereum on things like that.
No one cared about Microsoft's announcement.  You don't seriously believe
what you're saying, do you?
I agree with you, but Bitcoin becoming a page in the history books because
a few die-hard libertarians didn't think price or adoption was important is
a big, big concern, especially when they almost have veto power.  Markets
don't care about philosophy, they care about future value.  Bitcoin has
value because we think it may be the most useful new innovation in the
future.  If we screw that future usefulness up, philosophy gives us no more
value than Friendster has today.
On Thu, Mar 30, 2017 at 4:19 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-30 14:52:01
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] High fees / centralization 
high enough to block home users from using the network.
This depends entirely on the usecase entirely.  Most likely even without a
blocksize increase, home purchases will be large enough to fit on the
blocksize in the forseeable future.  Microtransactions(<$0.25) on the other
hand aren't viable no matter what we try to do - There's just too much data.
Most likely, transaction fees above $1 per tx will become unappealing for
many consumers, and above $10 is likely to be niche-level.  It is hard to
say with any certainty, but average credit card fees give us some
indications to work with - $1.2 on a $30 transaction, though paid by the
business and not the consumer.
Without blocksize increases, fees higher than $1/tx are basically
inevitable, most likely before 2020.  Running a node only costs $10/month
if that.  If we were going to favor node operational costs that highly in
the weighting, we'd better have a pretty solid justification with
mathematical models or examples.
pursuit of supporting 0.1% of the world's daily transactions.
If we can easily have both, why not have both?
An altcoin with both will take Bitcoin's monetary sovereignty crown by
default.  No crown, no usecases, no Bitcoin.
On Thu, Mar 30, 2017 at 9:14 AM, David Vorick via bitcoin-dev <

@_date: 2017-03-30 19:01:49
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] High fees / centralization 
That would be blockchain sharding.
Would be amazing if someone could figure out how to do it trustlessly.  So
far I'm not convinced it is possible to resolve the conflicts between the
shards and commit transactions between shards.
On Thu, Mar 30, 2017 at 6:39 PM, Vladimir Zaytsev <

@_date: 2017-03-30 22:28:33
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Err, no, that's what happens when you double click the Ethereum icon
instead of the Bitcoin icon.  Just because you run "Bitcoin SPV"
instead of "Bitcoin Verify Everyone's Else's Crap" doesn't mean you're
somehow going to get Ethereum payments.  Your verification is just
different and the risks that come along with that are different.  It's
only confusing if you make it confusing.
If every block that is mined for them is deliberately empty because of
an attacker, that's nonfunctional.  You can use whatever semantics you
want to describe that situation, but that's clearly what I meant.
As above, if someone operates Bitcoin in SPV mode they are not
magically at risk of getting Dashcoins.  They send and receive
Bitcoins just like everyone else running Bitcoin software.  There's no
confusion about it and it doesn't have anything to do with hashrates
of anyone.  It is just a different method of verification with
corresponding different costs of use and different security
We're already fucked, China has 61% of the hashrate and the only thing
we can do about it is to wait for the Chinese electrical
supply/demand/transmission system to rebalance itself.  Aside from
that little problem, mining distributions and pool distributions don't
significantly factor into the blocksize debate.  The debate is a
choice between nodes paying more to allow greater growth and adoption,
or nodes constraining adoption in favor of debatable security
Do you really consider it choosing when there is only a single option?
 And even if there was, the software would choose it for you?  If it
is a Bitcoin client, it follows the Bitcoin blockchain.  There is no
BU blockchain at the moment, and Bitcoin software can't possibly start
following Ethereum blockchains.
Yes you do, if the segment options are known (and if they aren't,
running a node likely won't help you choose either, it will choose by
accident and you'll have no idea).  You would get to choose whose
verifications to request/check from, and thus choose which segment to
follow, if any.
This is only true for the small minority that actually need that added
level of security & confidence, and the paranoid people who believe
they need it when they really, really don't.  Some guy on reddit
spouted off the same garbage logic, but was much quieter when I got
him to admit that he didn't actually read the code of Bitcoin that he
downloaded and ran, nor any of the code of the updates.  He trusted.
The average person doesn't need that level of security.  They do
however need to be able to use it, which they cannot right now if you
consider "average" to be at least 50% of the population.
Demand comes from usage and adoption.  Neither can happen us being
willing to give other people the option to trade security features for
lower costs.
Great.  Somehow I think Bitcoin's future involves very few more people
like you, and very many people who aren't paranoid and just want to be
able to send and receive Bitcoins.
No, it has its value for many, many reasons, trustless properties is
only one of them.  What I'm suggesting doesn't involve giving up
trustless properties except in your head (And not even then, since you
would almost certainly be able to afford to run a node for the rest of
your life if Bitcoin's value continues to rise as it has in the past).
And even if it did, there's a lot more reasons that a lot more people
than you would use it.
Are you really this dense?  If the cost of on-chain transactions
rises, numerous use cases get killed off.  At $0.10 per tx you
probably won't buy in-game digital microtransactions with it, but you
might buy coffee with it.  At $1 per tx, you probably won't buy coffee
with it but you might pay your ISP bill with it.  At $20 per tx, you
probably won't pay your ISP bill with it, but you might pay your rent.
At $300 per tx you probably won't use it for anything, but a company
purchasing goods from China might.  At $4000 per tx that company
probably won't use it, but international funds settlement for
million-dollar transactions might use it.
At each fee step along the way you kill of hundreds or thousands of
possible uses of Bitcoin.  Killing those off means fewer people will
use it, so they will use something else instead.
No they don't.  They only give people the option to pay more for
higher security or to accept lower security and use Bitcoin anyway.
So far as anyone has presented actual numbers, there's no reason to
believe larger blocksizes endanger anything of the sort, even if I
agreed that that was Bitcoin's primary proposition.  And I don't
believe we need an insignificant capacity increase, I used to think
that way though.  I strongly believe we can handle massive increases
by adjusting our expectations of what nodes do, how they operate, how
they justify the price of their services, and what levels of security
are available and appropriate for various levels of transaction risk.
Segwit is a miniscule blocksize increase and wholly inadequate
compared to the scope of the problem.  Good for other reasons, though.
Lightning is not Bitcoin, it is something different(but not bad IMO)
that has different features and different consequences.  I guess you
think it is ok that if your lightning node goes offline at the wrong
time, you could lose funds you never transacted with in the first
place?  No?  Oh, then you must be ok with lightning hub centralization
then as well as paying a monthly fee to lightning hubs for their
services.  Wait, that sounds an awful lot like visa....
I have no idea what you're referring to with the pre-loaded wallets point.

@_date: 2017-03-31 08:59:19
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
Correct me if I'm wrong, but nothing  possible if the client software
was electrum-like and used two independent sources for verification.
This and the next point are just reductio ad absurdem, since no one is
suggesting anything of the sort. Even in that situation, I can't think
of anything miners could do if clients used more than one independent
source for verification, ala electrum question above.
No one is suggesting anything like this.  The cost of running a node
that could handle 300% of the 2015 worldwide nonbitcoin transaction
volume today would be a rounding error for most exchanges even if
prices didn't rise.

@_date: 2017-03-31 09:46:10
@_author: Jared Lee Richardson 
@_subject: [bitcoin-dev] Hard fork proposal from last week's meeting 
I guess I should caveat, a rounding error is a bit of exaggeration -
mostly because I previously assumed that it would take 14 years for
the network to reach such a level, something I didn't say and that you
might not grant me.
I don't know why paypal has multiple datacenters, but I'm guessing it
probably has a lot more to do with everything else they do -
interface, support, tax compliance, replication, redundancy - than it
does with the raw numbers of transaction volumes.
What I do know is the math, though.  WW tx volume = 426,000,000,000 in
2015.  Assuming tx size of ~500 bytes, that's 669 terabytes of data
per year.  At a hard drive cost of 0.021 per GB, that's $36k a year or
so and declines ~14% a year.
The bandwidth is the really big cost.  You are right that if this
hypothetical node also had to support historical syncing, the numbers
would probably be unmanagable.  But that can be solved with a simple
checkpointing system for the vast majority of users, and nodes could
solve it by not supporting syncing / reducing peer count.  With a peer
count of 25 I measured ~75 Gb/month with today's blocksize cap.  That
works out to roughly 10 relays(sends+receives) per transaction
assuming all blocks were full, which was a pretty close approximation.
The bandwidth data of our 426 billion transactions per year works out
to 942 mbit/s.  That's 310 Terabytes per month of bandwidth - At
today's high-volume price of 0.05 per GB, that's $18,500 a month or
$222,000 a year.  Plus the $36k for storage per year brings it to
~$250k per year.  Not a rounding error, but within the rough costs of
running an exchange - a team of 5 developers works out to ~$400-600k a
year, and the cost of compliance with EU and U.S. entities (including
lawyers) runs upwards of a million dollars a year.  Then there's the
support department, probably ~$100-200k a year.
The reason I said a rounding error was that I assumed that it would
take until 2032 to reach that volume of transactions (Assuming
+80%/year of growth, which is our 4-year and 2-year historical average
tx/s growth).  If hard drive prices decline by 14% per year, that cost
becomes $3,900 a year, and if bandwidth prices decline by 14% a year
that cost becomes $1800 a month($21,600 a year).  Against a
multi-million dollar budget, even 3x that isn't a large concern,
though not, as I stated, a rounding error.  My bad.
I didn't approximate for CPU usage, as I don't have any good estimates
for it, and I don't have significant reason to believe that it is a
higher cost than bandwidth, which seems to be the controlling cost
compared to adding CPU's.
Care to respond to the math?
Well, we agree on something at least.
