
@_date: 2015-08-06 13:43:43
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fwd: Block size following technological growth 
How many nodes are necessary to ensure sufficient network reliability? Ten,
a hundred, a thousand? At what point do we hit the point of diminishing
returns, where adding extra nodes starts to have negligible impact on the
overall reliability of the system?
On Thu, Aug 6, 2015 at 10:26 AM, Pieter Wuille via bitcoin-dev <

@_date: 2015-08-11 13:46:43
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Hi Jorge: Many people would like to participate in a global consensus
network -- which is a network where all the participating nodes are aware
of and agree upon every transaction. Constraining Bitcoin capacity below
the limits of technology will only push users seeking to participate in a
global consensus network to other solutions which have adequate capacity,
such as BitcoinXT or others. Note that lightning / hub and spoke do not
meet requirements for users wishing to participate in global consensus,
because they are not global consensus networks, since all participating
nodes are not aware of all transactions.
On Tue, Aug 11, 2015 at 12:47 PM, Jorge Tim?n <

@_date: 2015-08-11 13:55:56
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
It generally doesn't matter that every node validate your coffee
transaction, and those transactions can and will probably be moved onto
offchain solutions in order to avoid paying the cost of achieving global
consensus. But you still don't get to set the cost of global consensus
artificially. Market forces will ensure that supply will meet demand there,
so if there is demand for access to global consensus, and technology exists
to meet that demand at a cost of one cent per transaction -- or whatever
the technology-limited cost of global consensus happens to be -- then
that's what the market will supply.
It would be like if Amazon suddenly said that they were going to be
charging $5 / gb / month to store data in s3. Can't do it. Technology
exists to bring about cloud storage at $0.01 / GB / month, so they don't
just get to set the price different from the capabilities of technology or
they'll get replaced by a competitor. Same applies to Bitcoin.
On Tue, Aug 11, 2015 at 1:48 PM, Mark Friedenbach

@_date: 2015-08-11 13:59:26
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Lightning *depends* on global consensus in order to function. You can't use
it without a global consensus network at all. So given that there is
absolutely a place for a global consensus network, we need to decide
whether the cost to participate in that global consensus will be limited
above or below the capability of technology. In a world where anybody can
step up and fork the code, it's going to be hard for anyone to artificially
set the price of participating in global consensus at a rate higher than
what technology can deliver...

@_date: 2015-08-11 14:26:48
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
All things considered, if people want to participate in a global consensus
network, and the technology exist to do it at a lower cost, then is it
sensible or even possible to somehow arbitrarily set the price of
participating in a global consensus network to be expensive? Can someone
please walk me through how that's expected to play out because I'm really
having a hard time understanding how it could work.
On Tue, Aug 11, 2015 at 2:00 PM, Mark Friedenbach via bitcoin-dev <

@_date: 2015-08-11 14:37:01
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Jorge, As long as Bitcoin remains the best global consensus network -- and
part of being best means being reasonably priced -- then no I don't think
people will be pushed into altcoins. Better money ultimately displaces
worse money, so I don't see a driving force for people to move to other
altcoins as long as Bitcoin remains competitive.
Hitting the limit in and of itself is not necessarily a bad thing. The
question at hand is whether we should constrain that limit below what
technology is capable of delivering. I'm arguing that not only we should
not, but that we could not even if we wanted to, since competition will
deliver capacity for global consensus whether it's in Bitcoin or in some
other product / fork.

@_date: 2015-08-11 15:56:45
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
I'm not sure whether removing the limit at the protocol-level would lead to
government by miners who might reject blocks which were too big, but I
probably wouldn't want to take that risk. I think we should probably keep a
block size limit in the protocol, but that we should increase it to be as
high as "technology can provide." Toward that: I don't necessarily think
that node-count in and of itself should be the metric for evaluating what
technology can provide, as much as the goal that the chain be inexpensive
to validate given the capabilities of present technology -- so if I can
lease a server in a datacenter which can validate the chain and my total
cost to do that is just a few dollars, then we're probably ok.
Of course there's also the issue that we maintain enough geographic /
political distribution to keep the network reliable, but I think we're far
from being in danger on the reliability front. So maybe my criteria that
the chain be validated at low cost is the wrong focus, but if it is than
what's the appropriate criteria for deciding whether it's safe by standards
of "today's technology" to raise the limit at the protocol level?

@_date: 2015-08-11 16:18:49
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
The only reason why Bitcoin has grown the way it has, and in fact the only
reason why we're all even here on this mailing list talking about this, is
because Bitcoin is growing, since it's "better money than other money". One
of the key characteristics toward that is Bitcoin being inexpensive to
transact. If that characteristic is no longer true, then Bitcoin isn't
going to grow, and in fact Bitcoin itself will be replaced by better money
that is less expensive to transfer.
So the importance of this issue cannot be overstated -- it's compete or die
for Bitcoin -- because people want to transact with global consensus at
high volume, and because technology exists to service that want, then it's
going to be met. This is basic rules of demand and supply. I don't
necessarily disagree with your position on only wanting to support
uncontroversial commits, but I think it's important to get consensus on the
criticality of the block size issue: do you agree, disagree, or not take a
side, and why?
On Tue, Aug 11, 2015 at 2:51 PM, Pieter Wuille

@_date: 2015-08-11 16:31:49
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Re: "In my opinion the main source of disagreement is that one: how the
maximum block size limits centralization."
I generally agree with that, but I would add that centralization is only a
goal insofar as it serves things like reliability, transaction integrity,
capacity, and accessibility. More broadly: how do you think that moving the
block size from 1MB to 8MB would materially impact these things?
Re: "That's why I cannot understand the urgency to rise the maximum size."
This issue is urgent because the difference between bitcoin being a success
and it being forgotten hinges on it being "better money" than other money.
If people want a money that can process lots and lots of transactions at
low cost, they're going to get it so long as technology can give it to
them. While it's not critical we raise the block size this very moment
since we're not hitting the capacity wall right now, based on the way
growth spikes in Bitcoin have occurred in the past we, may hit that
capacity wall soon and suddenly. And the moment we do, then Bitcoin may no
longer be "better money" since there's a big opportunity for other money
with higher throughput and lower fees to take its place.

@_date: 2015-08-11 16:35:52
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Bitcoin would be better money than current money even if it were a bit more
expensive to transact, simply because of its other great characteristics
(trustlessness, limited supply, etc). However... it is not better than
something else sharing all those same characteristics but which is also
less expensive. The best money will win, and if Bitcoin doesn't increase
capacity then it won't remain the best.

@_date: 2015-08-11 16:39:33
@_author: Michael Naber 
@_subject: [bitcoin-dev] Fees and the block-finding process 
Sure, most people probably would be happy with cheaper off-chain systems.
There already are and will probably continue to be more transactions
happening off-chain partly for this very reason. That's not the issue we're
trying to address though: The main chain is the lynch-pin to the whole
system. We've got to do a good job meeting demand that people have for
wanting to utilize the main-chain, or else we'll risk being replaced by
some other main-chain solution that does it better.

@_date: 2015-08-15 18:30:14
@_author: Michael Naber 
@_subject: [bitcoin-dev] Bitcoin XT 0.11A 
Bitcoin has no elections; it has no courts. If not through attempting a
hard-fork, how should we properly resolve irreconcilable disagreements?
On Sat, Aug 15, 2015 at 6:07 PM, Eric Lombrozo via bitcoin-dev <

@_date: 2015-07-01 03:15:15
@_author: Michael Naber 
@_subject: [bitcoin-dev] Reaching consensus on policy to continually increase 
This is great: Adam agrees that we should scale the block size limit
discretionarily upward within the limits of technology, and continually so
as hardware improves. Peter and others: What stands in the way of broader
consensus on this?
We also agree on a lot of other important things:

@_date: 2015-06-27 10:39:51
@_author: Michael Naber 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Demand to participate in a low-fee global consensus network will likely
continue to rise. Technology already exists to meet that rising demand
using a blockchain with sufficient block size. Whether that blockchain is
Bitcoin Core with an increased block size, or whether it is a fork, market
forces make it almost certain that demand will be met by a blockchain with
adequate capacity. These forces ensure that not only today?s block size
will be increased, but also that future increases will occur should the
demand arise.
In order to survive, Bitcoin Core must remain the lowest-fee,
highest-capacity, most secure, distributed, fastest, overall best solution
possible to the global consensus problem. Attempting to artificially
constrain the block size below the limits of technology for any reason is a
conflict with this objective and a threat to the survival of Bitcoin Core.
At the same time, scheduling large future increases or permitting unlimited
dynamic scaling of the block size limit raises concerns over availability
of future computing resources. Instead, we should manually increase the
block size limit as demand occurs, except in the special case that
increasing the limit would cause an undue burden upon users wishing to
validate the integrity of the blockchain.
Compromise: Can we agree that raising the block size to a static 8MB now
with a plan to increase it further should demand necessitate except in the
special case above is a reasonable path forward?

@_date: 2015-06-27 12:09:16
@_author: Michael Naber 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
The goal of Bitcoin Core is to meet the demand for global consensus as
effectively as possible. Please let's keep the conversation on how to best
meet that goal.
The off-chain solutions you enumerate are are useful solutions in their
respective domains, but none of them solves the global consensus problem
with any greater efficiency than Bitcoin does.

@_date: 2015-06-27 12:19:04
@_author: Michael Naber 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
That test seems like a reasonable suggestion; 840GB is not prohibitive
given today's computing costs. What other than the successful result of
that test would you want to see before agreeing to increase the block size
to 8MB?

@_date: 2015-06-27 13:25:14
@_author: Michael Naber 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Global network consensus means that there is global network recognition
that a particular transaction has occurred and is irreversible. The
off-chain solutions you describe, while probably useful for other purposes,
do not exhibit this characteristic and so they are not global network
consensus networks.
Bitcoin Core scales as O(N), where N is the number of transactions. Can we
do better than this while still achieving global consensus?

@_date: 2015-06-28 17:23:51
@_author: Michael Naber 
@_subject: [bitcoin-dev] A Proposed Compromise to the Block Size Limit 
Bitcoin Core exists to solve the global consensus problem. Global network
consensus means that there is global network recognition that a particular
transaction has occurred and is irreversible. Systems like hub-and-spoke,
payment channels, Lightning, etc. are useful, but they are not solutions to
the global consensus problem, because they do not meet this definition of
global consensus.
Let us focus our efforts on the goal of making Bitcoin Core the best
solution to the global consensus problem. Let us address Peter Todd?s
requirements to raise the block size limit to 8MB:
1) Run a successful test-net with 8MB blocks and show that the network
works and small miners are not unduly disadvantaged
2) Address Peter Todd's concern: ?without scarcity of blockchain space
there is no reason to think that transaction fees won?t fall to the
marginal cost of including a transaction, which doesn?t leave anything to
pay for proof-of-work security?
Regarding 1: This is not done yet, though it seems reasonable enough to do.
Regarding 2: It is a fallacy to believe that artificially constraining
capacity of Bitcoin Core below the limits of technology will lead to
increased fees and therefore lead to sufficient security in the far-future.
Constraining capacity below the limits of technology will ultimately only
drive users seeking global consensus to solutions other than Bitcoin Core,
perhaps through a fork.
Demand for user access to high-capacity global consensus is real, and the
technology exists to deliver it; if we don't meet that demand in Bitcoin
Core, it's inevitably going to get met through some other product. Let's
not let that happen. Let's keep Bitcoin Core the best solution to the
global consensus problem.
Thoughts? Is there anything else not mentioned above which anyone would
like done in order to raise the block size to a static 8 MB?
On Sun, Jun 28, 2015 at 5:05 PM, Gavin Andresen

@_date: 2015-06-29 04:33:21
@_author: Michael Naber 
@_subject: [bitcoin-dev] Bitcoin Core: The globally aware global consensus 
Bitcoin is globally aware global consensus.
It means every node both knows about and agrees on every transaction.
Do we need global awareness of every transaction to run a worldwide payment network? Of course not! In fact the limits of today's technology probably would not even allow it.
Global awareness is a finite resource. That's okay; hub and spoke, or other clever designs are going to ensure we can run worldwide transactions without requiring global awareness. That's great!
But even if we have hub and spoke, we still need a "hub" at the center. Bitcoin Core needs to focus on being that hub. It needs to be the best globally aware global consensus network that we can build. Let's put aside our differences and focus on this goal. Part of building the best network means ensuring that network can operate at the highest capacity technology can allow. If we run the test-net to show that hardware exists today to safely increase block size to a static 8 MB, then we will have broad developer support to make this happen.
Let's get that done. We can continue to adjust the block size upward in the future as technology permits.

@_date: 2015-06-30 11:34:33
@_author: Michael Naber 
@_subject: [bitcoin-dev] Block size increase oppositionists: please clearly 
and help do it
As you know I'm trying to lobby for a block size increase to a static 8MB.
I'm happy to try to get the testing done that people want done for this,
but I think the real crux of this issue is that we need to get consensus
that we intend to continually push the block size upward as bounded only by
Imagine an engineer (Gavin) at Boeing (Bitcoin Core) said he was going to
build an airplane (block) that was going to move 8x as many people
(transactions) as today?s planes (blocks), all while costing about the same
amount to operate. Imagine he then went on to tell you that he expects to
double the plane?s (block's) capacity every two years!
Without full planes (blocks), will the airlines (miners) go out of
business, since planes (blocks) will never be full and the cost to add
people (transactions) to a plane (block) will approach zero? Probably not.
Airlines (miners) still have to pay for pilots, security screening staff,
fuel, etc (engineers, hash rate, electricity, etc) so even if their
airplanes (blocks) can hold limitless people (transactions), they would
still have to charge sufficient fees to stay in business.
What tests do you need done to move to 8MB? Pitch in and help get those
tests done; agree that we'll run more tests next year or the year after
when technology might allow for 16 MB blocks. Do you really want to be the
guy holding back bigger planes? Do you really want to artificially
constrain block size below what technology allows?
In the face of such strong market demand for increased capacity in globally
aware global consensus, do you really think you can prevent supply from
meeting demand when the technology exists to deliver it? Do you really want
to force a fork because you and others won't agree to a simple raise to a
static 8MB?
Do what's best for Bitcoin and define what needs to get done to agree to a
simple block size increase to a static 8MB.

@_date: 2015-06-30 12:35:12
@_author: Michael Naber 
@_subject: [bitcoin-dev] Block size increase oppositionists: please 
and help do it
Re: Why bother doubling capacity? So that we could have 2x more network
participants of course.
Re: No clear way to scaling beyond that: Computers are getting more capable
aren't they? We'll increase capacity along with hardware.
It's a good thing to scale the network if technology permits it. How can
you argue with that?

@_date: 2015-05-08 03:18:51
@_author: Michael Naber 
@_subject: [Bitcoin-development] Suggestion: Dynamic block size that updates 
Why can't we have dynamic block size limit that changes with difficulty, such as the block size cannot exceed 2x the mean size of the prior difficulty period? I recently subscribed to this list so my apologies if this has been addressed already.
