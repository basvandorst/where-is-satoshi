
@_date: 2015-12-04 08:26:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Blockchain verification flag (BIP draft) 
For discussion,
A significant fraction of hashrate currently mines blocks without
verifying them for a span of time after a new block shows up on the
network for economically rational reasons. This otherwise harmful
behavior can be made a beneficial to the whole network; but only if it
is communicated.
This BIP proposal suggests a communication channel and describes its
use and the motivations for it.  I wrote it in response to suggestions
that Bitcoin Core add explicit support for this kind of mining, which
could also implement best in class risk mitigations. I believe
signaling the behavior is a necessary component for risk mitigation
  BIP: draft-maxwell-flagverify
  Title: Blockchain verification flag
  Author: Greg Maxwell   Status: Draft
  Type: Standards Track
  Created: 2015-12-02
This BIP describes a flag that the authors of blocks can use to voluntarily
signal that they have completely validated the content of their
block and the blocks before it.
Correct use of this signaling is not enforced internally to the network
but if used it can act as a hint allowing more intelligent risk analysis.
If deployed and adhered to, this mechanism turns otherwise harmful
validation skipping by miners into a behavior which benefits the public.
The version field in a Bitcoin block header is a 32-bit signed integer.
The most significant bit (30) of the block version is defined to signal that
the author of the block has validated the whole chain up to and including the
content of the block.
Conforming miners MUST NOT set this flag when they have not completely
validated the prior block(s) or the content of their own block.  Miners
should continue to try to minimize the amount of time spent mining
on a non-validated chain.  Blocks which extend an invalid chain will
continue to be rejected and ultimately orphaned as validation catches up.
It is recommended, but not required, that miners also not set the flag on blocks
created by the same device which created the block immediately prior.  This
will reduce the incorrect implication of independent validation when the two
most recent blocks are both the product of the same, single, faulty system.
The set state for the bit is defined as verified so that that
un(der)maintained systems do not falsely signal validation.
Non-verifying clients of the network may check this bit (e.g. checking
that the version is >= 1073741824) and use it as an input to their risk
modeling.  It is recommended that once this BIP is widely accepted by the
network that non-full-node wallets refrain from counting confirmations on
blocks where the bit is not set.
The authors of non-verifying clients should keep in mind that this flag
is only correct with the cooperation of the block author, and even then
a validating miner may still accidentally accept or produce an invalid
block due to faulty hardware or software.  Additionally, any miner which
correctly uses this flag could stop doing so at any time, and might
do so intentionally in order to increase the effectiveness of an attack.
As a result of misunderstanding, misconfiguration, laziness, or other
human factors some miners may falsely set the flag.  Because invalid
blocks are rare it may take a long time to detect misuse of the flag.
As such, the accuracy of this field MUST NOT be strongly relied upon.
Especially due to the non-enforceability of the flag, the user community
should keep in mind that both setting the flag correctly and mining
without verification (for brief periods of time) are healthy for the
network.  If participants are punished for following this specification
they will simply lie, and its utility will be diminished.
Some applications of the Bitcoin system such as thin-client wallets make
a strong assumption that all the authors of the blocks have faithfully
verified the blockchain.  Because many of these applications also take
irreversible actions based on only one or two confirmations and the time
between blocks is often very short, these clients are vulnerable to
even small and short-duration violations of this assumption.
Processing and propagation delays resulting from increased transaction
load contribute to block orphaning when multiple blocks are found at
close to the same time. This has caused some miners to work on extending
the chain with the most proof-of-work prior to validating the latest
Although this validation skipping undermines the security assumptions
of thin clients, it also has a beneficial effect: these delays also
make the mining process unfair and cause increased rewards for the
largest miners relative to other miners, resulting in a centralization
pressure.  Deferring validation can reduce this pressure and improve
the security of the Bitcoin system long term.
This BIP seeks to mitigate the harm of breaking the thin client
assumption by allowing miners to efficiently provide additional
information on their level of validation.  By doing so the
network can take advantage of the benefits of bypassed
validation with minimal collateral damage.
Because there is no consensus enforced behavior there is no special
deployment strategy required.  [BIP 9 will need to be updated.]
Thanks goes to Jeremy Rubin for his two-phase mining suggestion
which inspired this simplified proposal.
This document is placed in the public domain.

@_date: 2015-12-06 05:13:15
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Blockchain verification flag (BIP draft) 
I think this is orthogonal: You should only tell SPV clients* about
blocks you've fully validated yourself.  The bit in the header doesn't
matter with respect to that. (Effectively, the wallet risk model gets
as input the fact that they got given the block in the first place as
well as the flag where the miner said they were validating things or
Whatever the ideal behavior is for network nodes towards lite clients;
it's insanely cheap to just spin up a lot of 'nodes' that have
arbitrary behavior; so it shouldn't be relied on too much; but
absolutely they should be filtering to things they've verified
themselves... unlike the mining case, there is no reason not to.
[Specific attacks to consider: You get a broken miner to include both
your payment, and some invalid transaction. Other miners extend it
without verifying. To avoid the fact that nodes sensibly filter
invalid blocks from their lite clients, you simply just run a lot of
'nodes' so that virtually every lite client has a connection to you]
(More specifically, peers should be able to specify that they want to
know about pre-validated blocks and you should be able to fetch blocks
from them which haven't been validated... but no one should get fed
unverified blocks by surprise.)

@_date: 2015-12-06 06:26:15
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Blockchain verification flag (BIP draft) 
On Sun, Dec 6, 2015 at 2:47 AM, James Hilliard via bitcoin-dev
An interesting potential use for the flag suggested in this draft
would be allowing non-monotone mining for non-verified blocks.
I could add a recommendation for that as well.

@_date: 2015-12-07 22:02:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating
proposals were presented. I think this would be a good time to share my
view of the near term arc for capacity increases in the Bitcoin system. I
believe we?re in a fantastic place right now and that the community
is ready to deliver on a clear forward path with a shared vision that
addresses the needs of the system while upholding its values.
I think it?s important to first clearly express some of the relevant
principles that I think should guide the ongoing development of the
Bitcoin system.
Bitcoin is P2P electronic cash that is valuable over legacy systems
because of the monetary autonomy it brings to its users through
decentralization. Bitcoin seeks to address the root problem with
conventional currency: all the trust that's required to make it work--

@_date: 2015-12-08 05:21:18
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
On Tue, Dec 8, 2015 at 4:58 AM, Anthony Towns via bitcoin-dev
Actually being able to compute fees for your transaction: If there are
multiple limits that are "at play" then how you need to pay would
depend on the entire set of other candidate transactions, which is
unknown to you. Avoiding the need for a fancy solver in the miner is
also virtuous, because requiring software complexity there can make
for centralization advantages or divert development/maintenance cycles
in open source software off to other ends... The multidimensional
optimization is harder to accommodate for improved relay schemes, this
is the same as the "build blocks" but much more critical both because
of the need for consistency and the frequency in which you do it.
These don't, however, apply all that strongly if only one limit is
likely to be the limiting limit... though I am unsure about counting
on that; after all if the other limits wouldn't be limiting, why have
It can seem that way, but all limiting schemes have pathological cases
where someone runs up against the limit in the most costly way.  Keep
in mind that casual pathological behavior can be suppressed via
IsStandard like rules without baking them into consensus; so long as
the candidate attacker isn't miners themselves. Doing so where
possible can help avoid cases like the current sigops limiting which
is just ... pretty broken.

@_date: 2015-12-08 19:31:27
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
On Tue, Dec 8, 2015 at 3:55 PM, Justus Ranvier via bitcoin-dev
Pieter was originally putting it in a different location; so it's no
big deal to do so.
But there exists deployed mining hardware that imposes constraints on
the coinbase outputs, unfortunately.

@_date: 2015-12-08 23:59:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
It's nearly complexity-costless to put it in the coinbase transaction.
Exploring the costs is one of the reasons why this was implemented
We already have consensus critical enforcement there, the height,
which has almost never been problematic. (A popular block explorer
recently misimplemented the var-int decode and suffered an outage).
And most but not all prior commitment proposals have suggested the
same or similar.  The exact location is not that critical, however,
and we do have several soft-fork compatible options.
Yes, it will make them larger by log2() the number of transaction in a
block which is-- say-- 448 bytes.
With the coinbase transaction thats another couple kilobytes, I think
this is negligible.
perform the primary change in a backwards compatible manner, and pick
up the data reorganization in a hardfork if anyone even cares.
I think thats generally a nice cadence to split up risks that way; and
avoid controversy.
The witness data is never an input to sighash, so no, I don't agree
that this holds for "any" increase.
The fraud proof data is deterministic, full nodes could skip sending
it between each other, if anyone cared; but the overhead is pretty
tiny in any case.
My message lays out a plan for several different complementary
capacity advances; it's not referring to the current situation--
though the current capacity situation is no emergency.
I believe it already reflects the emerging consensus in the Bitcoin
Core project; in terms of the overall approach and philosophy, if not
every specific technical detail. It's not a forever plan, but a
pragmatic one that understand that the future is uncertain no matter
what we do; one that trusts that we'll respond to whatever
contingencies surprise us on the road to success.

@_date: 2015-12-09 00:23:27
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
By contrast it does not reduce the safety factor for the UTXO set at
all; which most hold as a much greater concern in general; and that
isn't something you can say for a block size increase.
With respect to witness safety factor; it's only needed in the case of
strategic or malicious behavior by miners-- both concerns which
several people promoting large block size increases have not only
disregarded but portrayed as unrealistic fear-mongering. Are you
concerned about it?  In any case-- the other improvements described in
my post give me reason to believe that risks created by that
possibility will be addressable.

@_date: 2015-12-09 01:31:51
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Witness size comes out of the 1MB at a factor of 0.25. It is not
possible to make a block which has signatures with the full 1MB of
data under the sighash while also having signatures externally.  So
every byte moved into the witness and thus only counted as 25% comes
out of the data being hashed and is hashed nInputs (*checksigs) less
We are designing for success; including the success of being able to
adapt and cope with uncertainty-- which is the most critical kind of
success we can have in a world where nothing is and can be
I agree, but nothing I have advocated creates significant technical
debt. It is also a bad engineering practice to combine functional
changes (especially ones with poorly understood system wide
consequences and low user autonomy) with structural tidying.
That isn't my perspective. I believe we've suffered delays because of
a strong desire to be inclusive and hear out all ideas, and not
forestall market adoption, even for ideas that eschewed pragmatism and
tried to build for forever in a single step and which in our hear of
hearts we knew were not the right path today. It's time to move past
that and get back on track with the progress can make and have been
making, in terms of capacity as well as many other areas. I think that
is designing for success.

@_date: 2015-12-09 06:29:53
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
Nothing being discussed would move something from consensus critical
code to not consensus critical.
What was being discussed was the location of the witness commitment;
which is consensus critical regardless of where it is placed. Should
it be placed in an available location which is compatible with the
existing network, or should the block hashing data structure
immediately be changed in an incompatible way to accommodate it in
order to satisfy an ascetic sense of purity and to make fraud proofs
somewhat smaller?
I argue that the size difference in the fraud proofs is not
interesting, the disruption to the network in an incompatible upgrade
is interesting; and that if it really were desirable reorganization to
move the commitment point could be done as part of a separate change
that changes only the location of things (and/or other trivial
adjustments); and that proceeding int this fashion would minimize
disruption and risk... by making the incompatible changes that will
force network wide software updates be as small and as simple as
I am speaking from our engineering experience in a  public,
world-wide, multi-vendor, multi-version, inter-operable, distributed
system which is constantly changing and in production contains private
code, unknown and assorted hardware, mixtures of versions, unreliable
networks, undisclosed usage patterns, and more sources of complex
behavior than can be counted-- including complex economic incentives
and malicious participants.
Even if we knew the complete spectrum of possible states for the
system the combinatioric explosion makes complete testing infeasible.
Though testing is essential one cannot "unit test" away all the risks
related to deploying a new behavior in the network.

@_date: 2015-12-09 07:17:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
I didn't comment on the transaction output. I have commented on
coinbase outputs and on a hard-fork.
Using an output in the last transaction would break the assumption
that you can truncate a block and still have a valid block. This is
used by some mining setups currently, because GBT does not generate
the coinbase transaction and so cannot know its size; and you may have
to drop the last transaction(s) to make room for it.
That a block can be truncated and still result in a valid block also
seems like a useful property to me.
If the input for that transaction is supposed to be generated from a
coinbase output some blocks earlier, then this may again run into
hardware output constraints in coinbase transactions. (But it may be
better since it wouldn't matter which output created it.). This could
likely be escaped by creating a zero value output only once and just
rolling it forward.

@_date: 2015-12-09 08:03:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capacity increases for the Bitcoin system. 
I think it would be logical to do as part of a hardfork that moved
commitments generally; e.g. a better position for merged mining (such
a hardfork was suggested in 2010 as something that could be done if
merged mining was used), room for commitments to additional block
back-references for compact SPV proofs, and/or UTXO set commitments.
Part of the reason to not do it now is that the requirements for the
other things that would be there are not yet well defined. For these
other applications, the additional overhead is actually fairly
meaningful; unlike the fraud proofs.

@_date: 2015-12-10 08:26:04
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
Thanks, I agree there.
A point to keep in mind:  Segregated Witness was specifically designed
to make script changes / improvements / additions / total rewrites no
harder to do _after_ SW then they would be do do along with it.  For
many people the "ah ha! lets do this" was realizing it could be a
pretty clean soft-fork.  For me, it was realizing that we could
structure Segwit in a way that radically simply future script updates
... and in doing so avoid a getting trapped by a rush to put in every
script update someone wants.
This is achieved by having the 'version' byte(s) at the start of the
witness program. If the witness program prefix is unrecognized it
means RETURN TRUE.  This recaptures the behavior that seems to have
been intended by OP_VER in the earliest versions of the software, but
actually works instead of giving every user the power to hardfork the
system at any time. :)  This escapes much of the risk in script
changes, as we no longer need to worry about negation, or other
interactions potentially breaking things.  A new version flag can have
its whole design crafted as if it were being created on a clean slate.
Optimizing layout and such I think makes sense, but I think we should
consider any script enhancements completely off the table for SW;
otherwise the binding will delay deployment and increase complexity. I
want most of those things too (a couple I disagree with) and a few of
them we could do quite quickly-- but no need to bind them up; post SW
and esp with version bits we could deploy them quite rapidly and on
their own timeframes.
Operations like these make sense with fixed with types, when they are
over arbitrary bignums, they're a complexity nightmare...  as
demonstrated by Bitcoin. :)
RE: OP_DUPTOALTSTACK  yea, I've wanted that several times (really I've
been sad that there isn't just a stack flag on every manipulation

@_date: 2015-12-10 09:51:23
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Segregated Witness features wish list 
I should have also commented on this: the block can indicate how many
sum criteria there are; and then additional ones could be soft-forked
in. Haven't tried implementing it yet, but there you go. :)

@_date: 2015-12-13 02:07:36
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
I believe you have misunderstood jl2012's post.  His post does not
cause the outputs to become discarded. They are still spendable,
but the transactions must carry a membership proof to spend them.
They don't have to have stored the data themselves, but they must
get it from somewhere-- including archive nodes that serve this
purpose rather than having every full node carry all that data forever.
Please be conservative with the send button. The list loses its
utility if every moderately complex idea is hit with reflexive
opposition by people who don't understand it.
Peter Todd has proposed something fairly similar with "STXO
commitments". The primary argument against this kind of approach that
I'm aware of is that the membership proofs get pretty big, and if too
aggressive this trades bandwidth for storage, and storage is usually
the cheaper resource. Though at least the membership proofs could be
omitted when transmitting to a node which has signaled that it has
kept the historical data anyways.

@_date: 2015-12-13 08:18:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
You already are in that boat. If your paper wallet has only the
private key (as 100% of them do today). You'll have no idea what coins
have been assigned to it, or what their TXids are. You'll need to
contact a public index (which isn't a service existing nodes provide)
or synchronize the full blockchain history to find it. Both are also
sufficient for jl2012's (/Petertodd's STXO), they'd only be providing
you with somewhat more data.  If instead, you insist that you'd
already be running a full node and not have to wait for the sync, then
again you'd also be your own archive. In none of these cases do you
lose anything.

@_date: 2015-12-13 09:24:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin 
Not every node is an archive node; that's even the case today.
Lowering the resource requirements to independently enforce the rules
of the system is highly virtuous.
They are not printed now with UTXO data
(txid:vout:scriptpubkey:amount), and unless you start and fully
synchronize (or are running a full node) you already cannot author a
transaction without that data. The private key is already not enough,
and no Bitcoin node will just give you what you need to know.
The only additional information JL2012's scheme would add would be the
hash tree fragments to show membership; and the same places that
currently give you what is required to author a transaction could
provide it for you.
The system already inhibits the rate new UTXO can be added; but we're
still left with the perpetually growing history that contains many
lost and otherwise unspendable outputs.

@_date: 2016-08-08 17:42:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Authentication BIP 
I don't see any reason that it couldn't also accept a DNS name there.
The purpose of that table is so the client knows which server ID to expect.
And the design seeks to preserve that privacy.
The client must know the identity of the server it is expecting. The
server does not announce itself. If it did then your changing of IPs
would provide you with no privacy at all.
If the design is to provide any protection against MITM you need to
know who you expected to connect to in any case.
Huh. No. Almost the opposite. The system is designed to inhibit
fingerprinting. You can't tell what identity key(s) a node has unless
you already know them. This means that if you don't publish your node
pubkey, no one can use it to track your node around the network.
Then they're just not listed in the file. The client can ask the server to
authenticate without authenticating itself.
No. OpenSSH doesn't make an effort to protect the privacy of its users.
It's not a issue because we're not aware of any usecase where a node
would have a large list of authenticated peers.
network interface (IPv4, IPv6, tor).
I'm not aware of any reason for this limitation to exist. A node
should be able to have as many listening identities as it wants, with
a similar cost to having a large authorized keys list.

@_date: 2016-08-10 02:14:42
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
With DSP hat on, your decoder for noisy/distorted channels will be 99.9% of
the complexity and will completely control the design of the encoder.
It's not a proposal yet without a decoder, it's just an idea.  FSK modems
microphone-channel (terrible multipath) is quite challenging and several
other parties have tried to do bitcoin info over audio in the past without
It's very interesting, but I think you do need to go through and get the
whole thing working to really gauge viability.

@_date: 2016-08-12 00:36:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Number Request: Addresses over Audio 
Which would take a quad core laptop about 8 hours with competent software
And presumably you're not using the whole 2^31 space else the receiver
also has to do that computation...

@_date: 2016-08-17 00:18:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF 
On Tue, Aug 16, 2016 at 10:52 PM, Russell O'Connor via bitcoin-dev
Relay rules are quite fragile-- people build programs or protocols not
expecting them to be violated, without proper error handling in those
cases... and then eventually some miner rips them out because they
simply don't care about them: not enforcing them won't make their
blocks invalid.
It's my general view that we should avoid blocking things with relay
rules unless we think that someday they could be made invalid... not
necessarily that they will, but that it's plausible. Then the
elimination at the relay level is just the first exploratory step in
that direction.
One should also consider adversarial behavior by miners.  For example,
I can mine blocks with mutated witnesses with a keyed mac that chooses
the mutation. The key is shared by conspirators or customers, and now
collectively we have a propagation advantage (since we know the
mutated version before it shows up).  Not the _biggest_ concern, since
parties doing this could just create their own new transactions to
selectively propagate; but doing that would require leaving behind fee
paying public transactions, while using malleability wouldn't.

@_date: 2016-08-24 08:34:20
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Status updates (including to Active/Final 
On Tue, Aug 23, 2016 at 8:54 PM, Kenneth Heutmaker via bitcoin-dev
There are almost no NODE_BLOOM supporting bloom-off nodes on the
network currently. So, while supporting this is important, I am
doubtful that its the current problem you've suffered.
There are a great many fake nodes which appear to exist purely to
monitor transactions. Many do not implement enough of the protocol to
support scanning or transaction relay. (and, in fact, relaying
transactions would make monitoring less effective).
You can't count on peers on a peer to peer network to be honest and
cooperative. Implementations need to work hard to be robust to abusive
peers. Unfortunately, the design of the bloom filtering is such that
it isn't always easy (or even possible) to be robust.

@_date: 2016-08-24 23:38:30
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
On Wed, Aug 24, 2016 at 11:03 PM, Chris Priest via bitcoin-dev
Working as designed in that case:  You know  is compromised, it
doesn't tell you if it was an insider or an outsider, but in both
cases someone unauthorized or without integrity got access to the

@_date: 2016-08-25 18:26:21
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capital Efficient Honeypots w/ "Scorched Earth" 
On Thu, Aug 25, 2016 at 2:27 PM, Christian Decker via bitcoin-dev
Depends on the value of their activity compared to the value of the coins.
Spamming doesn't pay much.
Covert tripwires would obviously be better, but if shared tripwires
allow you to have 100x the funds available it could be a good

@_date: 2016-02-04 19:36:58
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Hardfork bit BIP 
I think this is a good idea, and I've independently proposed it in the past.
I agree with most of luke's language nitpicks.
It could, however, be pointed out that the version number flag is not
sufficient in the deployed network, because many clients also do not
validate the version field, due to a disinterest in security great
enough to not implement anything around height-in-coinbase.
So to fully achieve the intended effect using the highest bit of prev
would currently be much more effective.

@_date: 2016-02-17 02:43:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [BIP Proposal] New "feefilter" p2p message 
I did think it might be interesting to do a priorityrate filter. but
since it seems no one is even working on adding an index for ancestor
priorityrate... or working on a space limited priority mempool... if
that extension would be needed it could just be a new "priorityfilt"
Technically fees per byte could be greater than 32 bits (e.g. a 9000
BTC fee is enough). Values are normally 64 bits already.
I think your reasoning is that you want to learn of your own
transactions even if they don't meet the filter?
I'm not sure this reasoning plays out though-- regardless of what your
own feefilter is, if a tx has too low a rate for your peers to relay
it, they won't and you won't learn of it.
I might wave my hands at a use case for OR "I will relay very high fee
txn ... or my own"; but considering that performance/privacy disaster
that bloom filters are-- and that to relay third party txn at all you
need to be able to validate them (or will get yourself banned).  Also,
if you really want the OR behavior you could make two connections...
the same cannot be said for and.
Maybe one argument I could add is that if we added a priorityrate
filter, that one would be an OR

@_date: 2016-02-26 01:32:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] SIGHASH_NOINPUT in Segregated Witness 
The design of segwit was carefully constructed to make it maximally
easy and safe to soft-fork in future script enhancements after its
deployment with the specific goal of avoiding indefinite delays in its
deployment from inevitable scope creep from additional things that are
"easy" to deploy as part of segwit.  I think to be successful we must
be absolutely ruthless about changes that go in there beyond the
absolute minimum needed for the safe deployment of segwit... so I
think this should probably be constructed as a new segwit script type,
and not a base feature.
The exact construction you're thinking of there isn't clear to me...
one thing that comes to mind is that I think it is imperative that we
do not deploy a without-inputs SIGHASH flag without also deploying at
least a fee-committing sighash-all. The reason for this is that if
hardware wallets are forced to continue transferring input
transactions to check fees or to use without-inputs, they may choose
the latter and leave the users needlessly exposed to replay attacks.
When you do write a BIP for this its imperative that the vulnerability
to replay is called out in bold blinking flaming text, along with the
necessary description of how to use it safely. The fact that without
input commitments transactions are replayable is highly surprising to
many developers... Personally, I'd even go so far as to name the flag
SIGHASH_REPLAY_VULNERABLE. :)

@_date: 2016-02-26 05:56:56
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] INV overhead and batched INVs to reduce full node 
Copying my response to you from BitcoinTalk
Uh. Bitcoin has done this since the very early days. The batching was
temporarily somewhat hobbled between 0.10 and 0.12 (especially when
you had any abusive frequently pinging peers attached), but is now
fully functional again and it now manages to batch many transactions
per INV pretty effectively. Turn on net message debugging and you'll
see the many INVs that are much larger than the minimum. The average
batching size (ignoring the trickle cut-through) is about 5 seconds
long-- and usually gets about 10 transactions per INV. My measurements
were with these fixes in effect; I expect the blocksonly savings would
have been higher otherwise.
2016-02-26 05:47:08 sending: inv (1261 bytes) peer=33900
2016-02-26 05:47:08 sending: inv (109 bytes) peer=32460
2016-02-26 05:47:08 sending: inv (37 bytes) peer=34501
2016-02-26 05:47:08 sending: inv (217 bytes) peer=33897
2016-02-26 05:47:08 sending: inv (145 bytes) peer=41863
2016-02-26 05:47:08 sending: inv (37 bytes) peer=35725
2016-02-26 05:47:08 sending: inv (73 bytes) peer=20567
2016-02-26 05:47:08 sending: inv (289 bytes) peer=44703
2016-02-26 05:47:08 sending: inv (73 bytes) peer=13408
2016-02-26 05:47:09 sending: inv (649 bytes) peer=41279
2016-02-26 05:47:09 sending: inv (145 bytes) peer=42612
2016-02-26 05:47:09 sending: inv (325 bytes) peer=34525
2016-02-26 05:47:09 sending: inv (181 bytes) peer=41174
2016-02-26 05:47:09 sending: inv (469 bytes) peer=41460
2016-02-26 05:47:10 sending: inv (973 bytes) peer=133
2016-02-26 05:47:10 sending: inv (361 bytes) peer=20541
Twiddling here doesn't change the asymptotic efficiency though; which
is what my post is about.
[I'm also somewhat surprised that you were unaware of this; one of the
patches "classic" was talking about patching out was the one restoring
the batching... due to a transaction deanonymization service (or
troll) claiming it interfered with their operations.]

@_date: 2016-02-26 21:42:26
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] The first successful Zero-Knowledge Contingent Payment 
I am happy to announce the first successful Zero-Knowledge Contingent
Payment (ZKCP) on the Bitcoin network.
ZKCP is a transaction protocol that allows a buyer to purchase
information from a seller using Bitcoin in a manner which is private,
scalable, secure, and which doesn?t require trusting anyone: the
expected information is transferred if and only if the payment is
made. The buyer and seller do not need to trust each other or depend
on arbitration by a third party.
Imagine a movie-style ?briefcase swap? (one party with a briefcase
full of cash, another containing secret documents), but without the
potential scenario of one of the cases being filled with shredded
newspaper and the resulting exciting chase scene.
An example application would be the owners of a particular make of
e-book reader cooperating to purchase the DRM master keys from a
failing manufacturer, so that they could load their own documents on
their readers after the vendor?s servers go offline. This type of sale
is inherently irreversible, potentially crosses multiple
jurisdictions, and involves parties whose financial stability is
uncertain?meaning that both parties either take a great deal of risk
or have to make difficult arrangement. Using a ZKCP avoids the
significant transactional costs involved in a sale which can otherwise
easily go wrong.
In today?s transaction I purchased a solution to a 16x16 Sudoku puzzle
for 0.10 BTC from Sean Bowe, a member of the Zcash team, as part of a
demonstration performed live at Financial Cryptography 2016 in
Barbados. I played my part in the transaction remotely from
The transfer involved two transactions:
Almost all of the engineering work behind this ZKCP implementation was
done by Sean Bowe, with support from Pieter Wuille, myself, and Madars
Read more, including technical details at
[I hope to have a ZKCP sudoku buying faucet up shortly. :) ]

@_date: 2016-02-26 23:23:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd: The first successful Zero-Knowledge Contingent 
Yes, the secrecy is information theoretic (assuming no implementation
bugs); beyond the truth of the outcome. This holds even if the
initialization is malicious.
The soundness of this scheme is computational-- we're trusting a deep
stack of cryptographic assumptions that the proofs cannot be forged.

@_date: 2016-02-26 23:45:03
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] The first successful Zero-Knowledge Contingent 
One might wonder why anyone would want to own coins that couldn't keep
up technologically, but to each his own. (especially one defunct
enough that it can't even update IsStandard rules...)
I don't think it's infeasible to do the EC multiply in a snark, but an
efficient implementation would be a lot of work. You'd probably want
to build a circuit for the field operations using 128 bit operations.
Fortunately the overall operation is pretty easy to directly convert
into a circuit (e.g. no branching).
Why not use the single-show-signature scheme I came up with a while
back on the Bitcoin side to force the bitcoin side to reveal a private

@_date: 2016-07-27 20:59:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: derived mnemonics 
Jochen, two alternatives were raised in public discussion:
Use a scheme which supports delegatable hardening-- (there are two
broad classes proposed, one where the delegated party learns
information that would let them bypass the part of the hardening they
perform but only that part, and another where the delegation is
information theoretically private.)
Eschew the pretextual 'hardening' that serves no purpose but to cause
users to think the scheme is more secure than it is, and which makes
the system more complex to implement.
Both were rejected by the authors of that spec.
This ignores the history of that spec and the widespread use. Because
of the design, the check value can't be computed without a fixed
dictionary, and many people do use it as a brainwallet-- which is what
that BIP originally specified, in fact.

@_date: 2016-06-09 01:24:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 151 MITM 
Reduction to plaintext isn't an interesting attack vector for an
active attacker: they can simply impersonate the remote side.
This is addressed via authentication, where available, which is done
by a separate specification that builds on this one.
Without authentication this only provides protection against passive attackers.

@_date: 2016-06-28 19:55:37
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 151 
This is a bit of a strawman, you've selected a single narrow usecase
which isn't proposed by the BIP and then argue it is worthless. I
agree that example doesn't have much value (and I believe that
eventually the BIP37 bloom filters should be removed from the
Without something like BIP151 network participants cannot have privacy
for the transactions they originate within the protocol against
network observers. Even if, through some extraordinary effort, their
own first hop is encrypted, unencrypted later hops would rapidly
expose significant information about transaction origins in the
Without something like BIP151 authenticated links are not possible, so
manually curated links (addnode/connect) cannot be counted on to
provide protection against partitioning sybils.
Along the way BIP151 appears that it will actually make the protocol faster.
This is untrue. The proposal is an ephemerally keyed opportunistic
encryption system. The privacy against a network observer does not
depend on authentication, much less "identity".  And when used with
authentication at all it makes interception strongly detectable after
the fact.
Because it does not propose any "identity system" or authorization
(also, I object to your apparent characterization of authentication as
as an 'identity system'-- do you also call Bitcoin addresses an
identity system?).
That said, manually maintaining adds nodes to your own and somewhat
trusted nodes is a recommend best practice for miners and other high
value systems which is rendered much less effective due to a lack of
authentication, there is no significant key distribution problem in
that case, and I expect the future auth BIP (Jonas had one before, but
it was put aside for now to first focus on the link layer encryption)
to address that case quite well.

@_date: 2016-06-28 21:36:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 151 
On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
It has a session ID for this purpose.
One might wonder how you ever use a Bitcoin address, or even why we
might guess these emails from "you" aren't actually coming from the

@_date: 2016-06-29 01:01:50
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 151 
It cites SPV as an example, doesn't mention bloom filters.. and sure--
sounds like the bip text should make the
Not passively, undetectable, and against thousands of users at once at low cost.
Not against Bitcoin Core, transactions are batched and relayed in
sorted order.  (obviously there are limits at what this provides;
ironically, the lack of link encryption has been used to argue against
privacy preserving relay behavior)
Huh? The first and subsequent hops obscures the origin and timing.
Don't need and want them for what?  For _partitioning_ resistance,
you are not partitioned if you have one honest connection to the
functional network. Additional peers purely reduce your partition
vulnerability-- so long as an active network attacker isn't
itercepting all your connections out.
For privacy, you have improve transaction privacy so long as your
transaction isn't initially relayed to a malicious peer-- but
malicious peers can lie further out because transit nodes obscure the
order of message creation.  Bitcoin Core currently relays transactions
first and more frequently to outbound and whitelisted peers.
I understood that, but my point was that Bitcoin cannot be used at
all_unless users have secure communication channels to share
They're not required to _only_ connect with anonymous peers. And
partition resistance requires that you have any one good link.
Not via passive observation.
Glad you agree.
We seem to be looping now. Feel free to not implement this proposal,
no one suggests making it mandatory.

@_date: 2016-03-01 00:57:58
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Open Bitcoin Privacy Protect Privacy 
Better late than never, I should correct things here. In the future it
would probably be more productive to open an issue. Otherwise there is
no mechanism for someone to take ownership of a response.
On Sun, Aug 30, 2015 at 7:45 PM, Kristov Atlas via bitcoin-dev
The ordering used by Bitcoin-QT is cryptographically randomized. This
provides the greatest privacy possible.
The BIP 69 recommendation would currently be equally as private if
universally used, but today would reduce privacy by making the
software more distinguishable.  It is unclear if BIP69 will be equal
in privacy in the future, because external infrastructure may impose
ordering requirements that are incompatible with it.
BIP 62 is withdrawn. The useful mechanisms in it for standardness
rules are, of course, implemeted in Bitcoin Core-- were invented
there, and have been there for years.
It's unclear to me precisely what is meant here. I'll answer broadly.
Bitcoin Core is compatible with and can be used with the joinmarket
module to include coinjoins. The raw transaction functionality in
Bitcoin Core was also created specifically to facilitate coinjoins.
Beyond joinmarket there have been several other coinjoin modules
created for Bitcoin Core though today JM is by far the most common,
This functionality is not directly implemented for a number of reasons
including the non-existence of decenteralized tools for this that
don't harm the user's privacy in other ways.
Skipped as these are specific to the implementation in use.
As Kristov noted, Bitcoin Core does not implement anti-features like donations.
Optionally, but in all cases the user's privacy is indistinguishable
from keeping all the data locally.
It would be more correct to say that Bitcoin Core always has the
highest possible FP rate.  It uses the only currently available tool
to avoid leaking private address information to indexing services.  As
several academic studies have shown, bloom filters are completely
inadequate for protecting user privacy.
I user that has downloaded a bootstrap.dat is indistinguishable from
any other user on the network; their transaction anonymity set is not
reduced in any way by doing this.  By running bitcoin at all they are
distinguished from other people who do not, but thousands of hosts run
Bitcoin without even having a wallet.
To be clear: This is N/A because there are no queries that would leak
private information about the user's wallet.
  >> d.      Does the user?s device query addresses individually in a manner
Yes. Because the Bitcoin Core downlaods all information, the third
parties cannot correlate responses.
Bitcoin Core does make remote queries to obtain "balance information",
but it can be directed to perform all commmunications via tor, before
starting it as noted.
Bitcoin Core can simultaneously connect to both Tor hidden services
and the public IPv4 network for improved partitioning resistance (and
has been able to for years). Instead of setting the socks proxy, the
user configures onion=.
As of 0.12 inbound tor HS is also auto-configured by default when tor
is installed.
All network connections are independent via Tor by default, no manual
change is required there. Separate "identities" do require separate
wallets, as noted.
No it does not and cannot. Freedom from this kind of leak is one of
the benefits of the current design that doesn't allow intermixing
"identities" in wallets.
However, unlikely most other wallets, Bitcoin nodes forward
transactions for third parties and do not make external queries for
private information. Because of this the ability to correlate a
particular node connection multiple times does not necessarily leak
anything about wallet usage.
This assumption is incorrect. All the private wallet state is stored
in the wallet. If the wallet is changed the node does know any of them
anymore.  There is no ability to open a new wallet without restarting
the software.
That said, Bitcoin Core normally relays transactions for third
parties-- unlikely virtually all other wallets. This means that where
observation of a transaction from another wallet would give a nearly
guaranteed identification that the system on the other end of the link
is the source, with Bitcoin Core sending a transaction is merely
potentially suggestive of origination.
After review and testing we've determined that reliable deletion of
private data is not very feasible on current hardware/OSes. Techniques
which used to work, like overwriting are defeated by write balancing.
We recommend users use OS level encryption to protect their privacy
Not for normal transactions. Bitcoin Core currently supports payment
URIs and BIP70, and if a user follows a payment URI it may instruct
the user to make a connection to a location requested by the payee.
Kind of.  If a Bitcoin Core node already knows of peers through prior
operation and is able to get at least two network connections within
11 seconds, it will make no further queries.
If a node is completely new and hasn't been otherwise configured; it
will perform four DNS queries to determine lists of candidate nodes.
These queries are frequently answered by caching name servers and do
not go all the way back to their origins. Only if both of these step
fail does it consult a hardcoded list of several hundred nodes to
attempt initialization.
That said, Bitcoin traffic is easily identifiable regardless of how
peers are found. We recommend users run Tor, and if tor is used no
identifiable traffic should happen, except for timing/volume analysis.
And many parties run Bitcoin Core nodes without running wallets; so
the use of Bitcoin does not identify a user as even having a wallet at
Bitcoin Core is this questions _definition_ of an unremarkable useragent.
But yes, the useragent notes the major/minor version. Concealing this
would have little to no privacy advantage, as functional/behavioral
analysis would easily reveal the version with at least that level of
If uninstall deleted the wallet it would reliably result in massive
funds loss for users.
To conceal their user of Bitcoin users should at a minimum do a
security erase of their system.
Other wallets who claim to "delete" private information which was
previously stored on disk are likely giving their users a false sense
of security. Doubly so in that many other wallets are written in
dynamic lanaguages which make it impossible to prevent highly secret
data from being written to system swap.
I believe any software which claimed to do this would have to meet a
rather high burden of proof.
Correct. And unlike some other Wallets the KDF used to harden the
users key takes 100ms with efficient native code; this substantially
limits attacker brute for performance.
We recommend users use full disk encryption. Encrypting the public
data in the wallet would require the wallet to enter their key at
every use and increase the probability that their key was leaked (or
if two keys were used, that they'd forget their spending key).
Even if the public key information were encrypted, other data on their
computer (browser cache, swap, logs) would likely compromise the
user's privacy, thus the full disk encryption recommendation. Full
disk encryption is a common, easily used tool; and I don't believe any
wallet software that stores data locally can provide strong privacy in
practice without it.
Right. Each wallet file can have it's own single password which
protect spending.
No telemetry data.
Yes, and a large portion of our user base does their own builds. Our
determinstic build process is also actively audited by a good dozen
parties who post cryptographic signatures of their duplicated builds.

@_date: 2016-03-02 17:53:46
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Hardfork to fix difficulty drop algorithm 
What you are proposing makes sense only if it was believed that a very
large difficulty drop would be very likely.
This appears to be almost certainly untrue-- consider-- look how long
ago since hashrate was 50% of what it is now, or 25% of what it is
now-- this is strong evidence that supermajority of the hashrate is
equipment with state of the art power efficiency. (I've also heard
more directly-- but I think the this evidence is more compelling
because it can't be tainted by boasting). If a pre-programmed ramp and
drop is set then it has the risk of massively under-setting
difficulty; which is also strongly undesirable (e.g. advanced
inflation and exacerbating existing unintentional selfish mining)...
and that is before suggesting that miners voluntarily take a loss of
inflation now.
So while I think this concern is generally implausible; I think it's
prudent to have a difficulty step patch (e.g. a one time single point
where a particular block is required to lower bits a set amount) ready
to go in the unlikely case the network is stalled. Of course, if the
alternative is "stuck" from a large hashrate drop the deployment would
be both safe and relatively uncontroversial. I think the
unfavorability of that approach is well matched to the implausibility
of the situation, and likely the right coarse of action compared to
risky interventions that would likely cause harm. The cost of
developing and testing such a patch is low, and justified purely on
the basis of increasing confidence that an issue would be handled (a
fact _I_ am perfectly confident in; but apparently some are not).
With respect what Luke was suggesting; without specifics its hard to
comment, but most altcoin "tolerate difficulty drop" changes have made
them much more vulnerable to partitioning attacks and other issues
(e.g. strategic behavior by miners to increase inflation), and have
actually been exploited in practice several times (solidcoin's being
the oldest I'm aware of). Many survived a fairly long time before
being shown to be pretty broken, simply because they were deployed in
cases where no one cared to attack. I'm currently doubtful that
particular path would be fruitful.

@_date: 2016-03-07 20:51:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Services bit for xthin blocks 
Does this functionality change peer selection?  If not, the preferred
signaling mechanism is probably the one in BIP 130.
Otherwise, I think the standard method for getting numbers has been to
write a BIP documenting the usage. I don't know if that is intentional
or just how things have previously happened; and I don't have much of
an opinion on it.

@_date: 2016-03-08 06:09:53
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd:  Services bit for xthin blocks 
Not communicated in address messages, so useless for discovery.
I think any feature which could do this could use the BIP130 approach instead.

@_date: 2016-05-01 16:21:40
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] segwit subsidy and multi-sender (coinjoin) 
This does not appear to be the case.
Coinjoin doesn't necessitate any particular behavior that is relevant
here -- normal transactions spend a coin and create a payment of an
externally specified amount and change; CoinJoins are not special
in this regard.
Users may sometimes split up their outputs in an effort to improve
privacy, which would have the "more outputs" effect you're describing,
but more outputs in and of itself would not increase costs under segwit:
The total cost to a user for creating an output paying themselves is both
the cost of the creation and the cost of eventually spending it.
Segwit's cost calculation improvements shifts some relative cost from
spending to creation, but in these cases same user is paying both.

@_date: 2016-05-03 05:02:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Thanks Matt!
I've been testing this for a couple weeks (in various forms).  I've
been getting over 96% reduction in block-bytes sent. I don't have a
good metric for it, but bandwidth spikes are greatly reduced. The
largest blocktxn message I've seen on a node that has been up for at
least a day is 475736 bytes. 94% of the blocks less than 100kb must be
sent in total.
In the opportunistic mode my measurements are showing 73% of blocks
transferred with 0.5 RTT even without prediction, 87% if up to 4
additional transactions are predicted, and 91% for 30 transactions (my
rough estimate for the 10k maximum prediction suggested in the BIP.

@_date: 2016-05-08 11:09:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposal to update BIP-32 
On Sun, May 8, 2016 at 10:07 AM, Pavol Rusnak via bitcoin-dev
AFAIK Sipa has not been on this list for some time.

@_date: 2016-05-09 08:57:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Committed bloom filters for improved wallet 
I think this is a fantastic idea.
Some napkin work shows that it has pretty good communications
bandwidth so long as you assume that the wallet has many keys (e.g.
more than the number of the outputs in the block)-- otherwise BIP37
uses less bandwidth, but you note its terrible privacy problems.
You should be aware that when the filter is transmitted but not
updated, as it is in these filtering applications, the bloom filter is
not the most communication efficient data structure.
The most efficient data structure is similar to a bloom filter, but
you use more bits and only one hash function. The result will be
mostly zero bits. Then you entropy code it using RLE+Rice coding or an
optimal binomial packer (e.g.
  This is about 45%
more space efficient than a bloom filter. ... it's just a PITA to
update, though that is inapplicable here.  Entropy coding for this can
be quite fast, if many lookups are done the decompression could even
be faster than having to use two dozen hash functions for each lookup.
The intuition is that this kind of simple hash-bitmap is great, but
space inefficient if you don't have compression since most of the bits
are 0 you end up spending a bit to send less than a bit of
information. A bloom filter improve the situation by using the
multiple filters to increase the ones density to 50%, but the
increased collisions create overhead. This is important when its a
in-memory data-structure that you're updating often, but not here.
One thing to do with matching blocks is after finding the matches the
node could potentially consult some PIR to get the blocks it cares
about... thus preventing a leak of which blocks it was interested in,
but not taking PIR costs for the whole chain or requiring the
implementation of PIR tree search (which is theoretically simple but
in practice hard to implement).

@_date: 2016-05-09 10:43:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev
Not required. It may. If it chooses fortunately, latency is reduced--
to 0.5 RTT in many cases. If not-- nothing harmful happens.
Testing on actual nodes in the actual network (not a "lab") shows that
blocks are normally requested from one of the last three peers they
were requested from 70% of the time, with no special affordances or
skipping samples when peers disconnected.
(77% for last 4, 88% for last 8)
This also _increases_ robustness. Right now a single peer failing at
the wrong time will delay blocks with a long time out. In high
bandwidth mode the redundancy means that node will be much more likely
to make progress without timeout delays-- so long at least one of the
the selected opportunistic mode peers was successful.
Because the decision is non-normative to the protocol, nodes can
decide based on better criteria if better criteria is discovered in
the future.
"High bandwidth" mode uses somewhat more bandwidth than low
bandwidth... but still >>10 times less than an ordinary getdata relay
which is used ubiquitously today.
If a node is trying to minimize bandwidth usage, it can choose to not
request the "high bandwidth" mode.
The latency bound cannot be achieved without unsolicited data. The
best we can while achieving 0.5 RTT is try to arrange things so that
the information received is maximally useful and as small as
reasonably possible.
If receivers implemented joint decoding (combining multiple
comprblocks in the event of faild decoding) 4 byte IDs would be
completely reasonable, and were what I originally suggested (along
with forward error correction data, in that case).
Service bits are not generally a good mechanism for negating optional
peer-local parameters.
The settings for compactblocks can change at runtime, having to
reconnect to change them would be obnoxious.
This is one of the two variable length encodings used for years in
Bitcoin Core. This is just the first time it's shown up in a BIP.
[It's a little disconcerting that you appear to be maintaining a fork
and are unaware of this.]
The similarity with UTF-8 is that both are variable length and some
control information is in the high bits. The similarity ends there.
UTF-8 is more complex and less efficient for this application (coding
small numbers), as it has to handle things like resynchronization
which are critical in text but irrelevant in our framed, checksummed,
reliably transported binary protocol.
Then you expose it to a trivial collision attack:  To find two 64 bit
hashes that collide I need perform only roughly 2^32 computation. Then
I can send them to the network.  You cannot reason about these systems
just by assuming that bad things happen only according to pure chance.
This issue is eliminated by salting the hash.  Moreover, with
per-source randomization of the hash, when a rare chance collision
happens it only impacts a single node at a time, so the propagation
doesn't stall network wide on an unlucky block; it just goes slower on
a tiny number of links a tiny percent of the time (instead of breaking
everywhere an even tinyer amount of the time)-- in the non-attacker,
chance event case.

@_date: 2016-05-09 12:12:14
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd:  Compact Block Relay BIP 
I cannot parse this sentence.
A node implementing this does not have to ask peers to send blocks
without further solicitation.
If they don't, their minimum transfer time increases to the current
1.5 RTT (but sending massively less data).
My measurements were made in the real world, on a collection of nodes
around the network which were not setup for this purpose and are
running standard configurations, over many weeks of logs.
This doesn't guarantee that they're representative of everything-- but
they don't need to be.
That is incorrect.
If a header shows up and a compact block has not shown up, a compact
block will be requested.
If compactblock shows up reconstruction will be attempted.
If any of the requested compact blocks show up (the three in advance,
if high bandwidth mode is used, or a requested one, if there was one)
then reconstruction proceeds without delay.
The addition of the unsolicited input causes no additional timeouts or
delays (ignoring bandwidth usage). It does use some more bandwidth
than not having it, but still massively less than the status quo.
It is massively more efficient than the current protocol, even under
fairly poor conditions. In the absolute worst possible case (miner
sends a block of completely unexpected transactions, and three peers
send compact blocks, it adds about 6% overhead)
With this kind of unsubstantiated axiomatic assertion, I don't think
further discussion with you is likely to be productive-- at least I
gave a reason.
UTF-8 would be a poor fit here for the reasons I explained and others
less significant ones (including the additional error cases that must
be handled resulting from the inefficient encoding; -- poor handing of
invalid UTF-8 have even resulted in security issues in some
I am a bit baffled that you'd suggest using UTF-8 as a general compact
integer encoding in a binary protocol in the first place.
What are you talking about? You seem profoundly confused here. There
is no proof of work involved anywhere.
I obtain some txouts. I write a transaction spending them in malleable
form (e.g. sighash single and an op_return output).. then grind the
extra output to produce different hashes.  After doing this 2^32 times
I am likely to find two which share the same initial 8 bytes of txid.
I send one to half the nodes, the other to half the nodes.  When a
block shows up carrying one or the other of my transactions
reconstruction will fail on half the nodes in the network in a
protocol with a simple truncated hash.
Of course, doing this is easy, so I can keep it going persistently. If
I am a miner, I can be sure to filter these transactions from my own
blocks-- causing all my competition to suffer higher orphaning.
The salted short-ids do not have this easily exploited, and gratuitous
vulnerability. This was obvious enough that it this feature was in the
very earliest descriptions of these techniques in 2013/2014. The
salted short-ids cannot be collided in pre-computation, and cannot be
collided with respect to multiple nodes at once.

@_date: 2016-05-10 02:12:03
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
$ echo -n Perhaps. 00000000f2736d91 |sha256sum
$ echo -n Perhaps. 0000000011ac0388 |sha256sum
Try search term "collision", or there may be an undergrad Data
structures and algorithms coarse online-- you want something covering
"cycle finding".
(Though even ignoring efficient cycle finding, your factorial argument
doesn't hold... you can simply sort the data... Search term
"quicksort" for a relevant algorithm).

@_date: 2016-05-10 10:07:27
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Compact Block Relay BIP 
Hm. 12 bits sounds very small even giving those figures. Why failure
rate were you targeting?
I've mostly been thing in terms of 3000 txn, and 20k mempools, and
blocks which are 90% consistent with the remote mempool, targeting
1/100000 failure rates (which is roughly where it should be to put it
well below link failure levels).
If going down the path of more complexity, set reconciliation is
enormously more efficient (e.g. 90% reduction), which no amount of
packing/twiddling can achieve.
But the savings of going from 20kb to 3kb is not interesting enough to
justify it*.  My expectation is that later we'll deploy set
reconciliation to fix relay efficiency, where the savings is _much_
larger,  and then with the infrastructure in place we could define
another compactblock mode that used it.
(*Not interesting because it mostly reduces exposure to loss and the
gods of TCP, but since those are the long poles in the latency tent,
it's best to escape them entirely, see Matt's udp_wip branch.)
Doing this would greatly increase the cost of a collision though, as
it would happen in many places in the network at once over the on the
network at once, rather than just happening on a single link, thus
hardly impacting overall propagation.
(The downside of the nonce is that you get an exponential increase in
the rate that a collision happens "somewhere", but links fail
"somewhere" all the time-- propagation overall doesn't care about
Using the same nonce means you also would not get a recovery gain from
jointly decoding using compact blocks sent from multiple peers (which
you'll have anyways in high bandwidth mode).
With a nonce a sender does have the option of reusing what they got--
but the actual encoding cost is negligible, for a 2500 transaction
block its 27 microseconds (once per block, shared across all peers)
using Pieter's suggestion of siphash 1-3 instead of the cheaper
construct in the current draft.
Of course, if you're going to check your whole mempool to reroll the
nonce, thats another matter-- but that seems wasteful compared to just
using a table driven size with a known negligible failure rate.
64-bits as a maximum length is high enough that the collision rate
would be negligible even under fairly unrealistic assumptions-- so
long as it's salted. :)
The band between "no collisions" and "infeasible many" is fairly
narrow.  You can add a small amount more space to the ids and
immediately be in the no collision zone.
Some earlier work we had would send small amount of erasure coding
data of the next couple bytes of the IDs.  E.g. the receiver in all
the IDs you know, mark totally unknown IDs as erased and the let the
error correction fix the rest. This let you algebraically resolve
collisions _far_ beyond what could be feasibly bruteforced. Pieter
went and implemented... but the added cost of encoding and software
complexity seem not worth it.

@_date: 2016-05-11 22:58:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
On Wed, May 11, 2016 at 10:42 PM, Timo Hanke via bitcoin-dev
There are already many altcoins out there, we could not prevent that
even if we wanted to. New ones are created all the time.
A 20% inherent advantage, in perfect competition, is likely to lead to
an eventual monopoly of mining if monopoly patent right prohibit
competitions-- if mining profits go are under the level of that
enhancement everyone without it would be operating at a loss.
Preserving a vulnerability that will ultimately harm the system's
decentralization for just the betterment of some miners does not seem
like a rational decision for the users of Bitcoin-- no more than it
would reasonable to add a rule that all blocks must be signed by a
particular private key.
As an altcoin the "asicboost" altcoin would be one of the least
interesting altcoins ever created... after all, no other altcoin has
ever been created that required licensing in order to mine.
I don't know if forking it out is the best move here and now, but I'm
happy some people are thinking carefully about what it would take to
do that.

@_date: 2016-05-12 00:02:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Making AsicBoost irrelevant 
On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev
Taking that a step further, the old POW could continue to be accepted
but with a 20% target penalty. (or vice versa, with the new POW having
a 20% target boost.)

@_date: 2016-10-02 22:58:46
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] About ASICBoost 
Sergio was concerned about centralization pressure in private. He
reached out to the BCF on 2013-11-23 and asked if they would license
the patent from him so they could make it equally available to all
under "fair" terms.  BCF responded that they didn't think it (a
proprietary patent encumbered enhancement that would make its user(s)
30% more effective than others) would be a big deal and basically
encouraged him to go ahead and seek the patent.

@_date: 2016-10-02 23:27:20
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] About ASICBoost 
We can't change the past (besides, would you want BCF to have owned
that patent? I didn't)-- only the future.
To do so requires collaboration, so lets focus on that.

@_date: 2016-09-10 00:42:30
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
The alert system was a centralized facility to allow trusted parties
to send messages to be displayed in wallet software (and, very early
on, actually remotely trigger the software to stop transacting).
It has been removed completely in Bitcoin Core after being disabled for a while.
While the system had some potential uses, there were a number of
problems with it.
The alert system was a frequent source of misunderstanding about the
security model and 'effective governance', for example a years ago a
BitcoinJ developer wanted it to be used to control fee levels on the
network and few months back one of Bloq's staff was pushing for a
scheme where "the developers" would use it to remotely change the
difficulty-- apparently with no idea how abhorrent others would find
The system also had a problem of not being scalable to different
software vendors-- it didn't really make sense that core would have
that facility but armory had to do something different (nor would it
really make sense to constantly have to maintain some list of keys in
the node software).
It also had the problem of being unaccountable. No one can tell which
of the key holders created a message. This creates a risk of misuse
with a false origin to attack someone's reputation.
Finally, there is good reason to believe that the key has been
compromised-- It was provided to MTGox by a developer and MTGox's
systems' were compromised and later their CEO's equipment taken by the
Japanese police.
In any case, it's gone now in Core and most other current software--
and I think it's time to fully deactivate it.
I've spent some time going around the internet looking for all
software that contains this key (which included a few altcoins) and
asked them to remove it. I will continue to do that.
One of the facilities in the alert system is that you can send a
maximum sequence alert which cannot be overridden and displays only a
static key compromise text message and blocks all other alerts. I plan
to send a triggering alert in the not-distant future (exact time to be
announced well in advance) feedback on timing would be welcome.
There are likely a few production systems that automatically shut down
when there is an alert, so this risks some small one-time disruption
of those services-- but none worse than if an alert were sent to
advise about a new system upgrade.
At some point after that, I would then plan to disclose this private
key in public, eliminating any further potential of reputation attacks
and diminishing the risk of misunderstanding the key as some special
trusted source of authority.

@_date: 2016-09-10 01:48:40
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
Yes, this was one of my motivations for doing this soon.
It would only require about 2 LOC to have Bitcoin Core vomit out a
blob containing the final alert to any old protocol version peers that
connect.  I don't know how other people would feel about that, but I
wouldn't mind implementing it, and it would greatly improve the
likelihood that they continue to to get once propagation of it is
gone. This could be left in the codebase for a couple years or until
other changes made those old versions p2p incompatible for other

@_date: 2016-09-10 15:36:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Completing the retirement of the alert system 
Because if you were offline at the time of the final alert, the alert
you may see instead is "Urgent security problem! Upgrade to
UltraBitcoin NOW!  among other similar reasons.

@_date: 2016-09-21 18:01:30
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Requesting BIP assignment; Flexible Transactions. 
This document does not appear to be concretely specified enough to
review or implement from it.
For example, it does not specify the serialization of "integer" (is it
a 32 bit word in network byte order or?) nor does it specify how the
presence of the optional fields are signaled nor the cardinality of
the inputs or outputs. For clearly variable length elements
('bytearray') no mention is made of their length encoding. etc.
Without information like this, I don't see how someone could
realistically begin reviewing this proposal.
The motivation seems unclear to me as well: The scheme is described as
'flexible' but it appears to remove flexibility from the existing
system. The "schema" appears to be hardcoded and never communicated.
If the goal is to simply have a more compact on the wire
representation, this could be done without changing the serialization
used for hashing or the serialization used for costing.

@_date: 2016-09-23 18:57:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
The reorg safety impact of this proposal could be eliminated and the
mempool handling complexity greatly reduced if the transaction was
required to be locktimed at least 100 blocks after the block its
This would also resolve a rather severe DOS weakness that the spec has
with the suggestion that nodes would relay this rule without
validating it. With the depth restriction nodes could relay one (or a
couple) blocks early without creating a situation where someone can
consume relay resources with near zero odds of paying a fee for them.
Irritatingly, applications of this rule would really want to be
applied at signing time (like locktime is), not as part of a
scriptpubkey. With it part of a scriptpubkey two moves are required. I
think solving this is important.
FWIW, this scheme more has been proposed before for another reason--
effectively allowing users to 'vote against' long reorgs by making
sure their transactions can't be included in them. Though for that
application it was only needed to use 32 bits of the block hash.

@_date: 2016-09-23 23:43:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP draft: OP_CHECKBLOCKATHEIGHT 
On Fri, Sep 23, 2016 at 10:20 PM, Luke Dashjr via bitcoin-dev
There is a fungibility hit... right now, absent double spends (and
privacy issues), every coin you might get paid is equal.
With this script feature as described, you could get paid a coin which
has one of these in its recent past, pinning the block immediately
before it. A reorg long enough to remove that block-- due to an
attack, or an ordinary block race, or some kind of consensus glitch
(like we had in March 2013 or around the activation of BIP65)-- is
_guaranteed_ to invalidate those coins, even without any double spend.
If the scheme doesn't do as I suggest and prevent over-eager usage
(perhaps 100 is too much, I just decided to match coinbases); then it
should probably have a consensus enforced explicit "maximum survivable
reorg" that is traced along with the outputs, so that someone who
received exposed coins could handle it sensibly.
Just for plain engineering reasons, I still think it is important to
now allow overly short back references. If the reference has to be a
few blocks back we don't need to worry about short forks breaking
propagation, and simple mempool handling like purging all CBAH
transactions on a large reorg would work fine.  It need not be so long
as to implicate Petertodd's concern that you could only use it where
it wouldn't matter.  (Though I also disagree that a depth of 100
achieves that, consider persistent chain forks).

@_date: 2016-09-24 00:21:16
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposed BIP-1 change removing OPL licensing option. 
I've proposed a revision to BIP-1 that removes the option to license
the work under the OPL:  The OPL contains troublesome terms where the licensor can elect to
prohibit print publication of the work as well as the creation of
modified versions without their approval.
"Distribution of substantively modified versions of this document is
prohibited without the explicit permission of the copyright holder."
"Distribution of the work or derivative of the work in any standard
(paper) book form is prohibited unless prior permission is obtained
from the copyright holder."
Additionally, even without these optional clauses the specific
construction of this licenses' attribution requirements are
restrictive enough that Debian does not consider it acceptable for
works included in their distribution
I can't find any discussion that indicates anyone involved with the
project was aware of these clauses at the time this text was added...
and I believe they are strongly incompatible with having a
transparent, public, collaborative process for the development of
standard for interoperablity. I certainly wasn't aware of it, and
would have argued against it if I was.
Moreover, the project that created this license has recommended people
use creative commons licenses instead since 2007.
The only BIPs that have availed themselves of this are BIP145 (which
is dual licensed under the permissive 2-clause BSD, which I wouldn't
object to adding as an option-- and which doesn't active the
objectionable clauses) and the recently assigned BIP134.

@_date: 2017-04-05 21:37:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which
is exploited by ASICBOOST and the various steps which could be used to
block it in the network if it became a problem.
While most discussion of ASICBOOST has focused on the overt method
of implementing it, there also exists a covert method for using it.
As I explained one of the approaches to inhibit covert ASICBOOST I
realized that my words were pretty much also describing the SegWit
commitment structure.
The authors of the SegWit proposal made a specific effort to not be
incompatible with any mining system and, in particular, changed the
design at one point to accommodate mining chips with forced payout
Had there been awareness of exploitation of this attack an effort
would have been made to avoid incompatibility-- simply to separate
concerns.  But the best methods of implementing the covert attack
are significantly incompatible with virtually any method of
extending Bitcoin's transaction capabilities; with the notable
exception of extension blocks (which have their own problems).
An incompatibility would go a long way to explain some of the
more inexplicable behavior from some parties in the mining
ecosystem so I began looking for supporting evidence.
Reverse engineering of a particular mining chip has demonstrated
conclusively that ASICBOOST has been implemented
in hardware.
On that basis, I offer the following BIP draft for discussion.
This proposal does not prevent the attack in general, but only
inhibits covert forms of it which are incompatible with
improvements to the Bitcoin protocol.
I hope that even those of us who would strongly prefer that
ASICBOOST be blocked completely can come together to support
a protective measure that separates concerns by inhibiting
the covert use of it that potentially blocks protocol improvements.
The specific activation height is something I currently don't have
a strong opinion, so I've left it unspecified for the moment.
  BIP: TBD
  Layer: Consensus
  Title: Inhibiting a covert attack on the Bitcoin POW function
  Author: Greg Maxwell   Status: Draft
  Type: Standards Track
  Created: 2016-04-05
  License: PD
This proposal inhibits the covert exploitation of a known
vulnerability in Bitcoin Proof of Work function.
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
Due to a design oversight the Bitcoin proof of work function has a potential
attack which can allow an attacking miner to save up-to 30% of their energy
costs (though closer to 20% is more likely due to implementation overheads).
Timo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,
which they have so far not licensed for free and open use by the public.
They have been marketing their patent licenses under the trade-name
ASICBOOST.  The document takes no position on the validity or enforceability
of the patent.
There are two major ways of exploiting the underlying vulnerability: One
obvious way which is highly detectable and is not in use on the network
today and a covert way which has significant interaction and potential
interference with the Bitcoin protocol.  The covert mechanism is not
easily detected except through its interference with the protocol.
In particular, the protocol interactions of the covert method can block the
implementation of virtuous improvements such as segregated witness.
Exploitation of this vulnerability could result in payoff of as much as
$100 million USD per year at the time this was written (Assuming at
50% hash-power miner was gaining a 30% power advantage and that mining
was otherwise at profit equilibrium).  This could have a phenomenal
centralizing effect by pushing mining out of profitability for all
other participants, and the income from secretly using this
optimization could be abused to significantly distort the Bitcoin
ecosystem in order to preserve the advantage.
Reverse engineering of a mining ASIC from a major manufacture has
revealed that it contains an undocumented, undisclosed ability
to make use of this attack. (The parties claiming to hold a
patent on this technique were completely unaware of this use.)
On the above basis the potential for covert exploitation of this
vulnerability and the resulting inequality in the mining process
and interference with useful improvements presents a clear and
present danger to the Bitcoin system which requires a response.
The general idea of this attack is that SHA2-256 is a merkle damgard hash
function which consumes 64 bytes of data at a time.
The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while
incriminating a 32-bit nonce which is at the end of this header data. This
means that the processing of the header involves two runs of the compression
function run-- one that consumes the first 64 bytes of the header and a
second which processes the remaining 16 bytes and padding.
The initial 'message expansion' operations in each step of the SHA2-256
function operate exclusively on that step's 64-bytes of input with no
influence from prior data that entered the hash.
Because of this if a miner is able to prepare a block header with
multiple distinct first 64-byte chunks but identical 16-byte
second chunks they can reuse the computation of the initial
expansion for multiple trials. This reduces power consumption.
There are two broad ways of making use of this attack. The obvious
way is to try candidates with different version numbers.  Beyond
upsetting the soft-fork detection logic in Bitcoin nodes this has
little negative effect but it is highly conspicuous and easily
The other method is based on the fact that the merkle root
committing to the transactions is contained in the first 64-bytes
except for the last 4 bytes of it.  If the miner finds multiple
candidate root values which have the same final 32-bit then they
can use the attack.
To find multiple roots with the same trailing 32-bits the miner can
use efficient collision finding mechanism which will find a match
with as little as 2^16 candidate roots expected, 2^24 operations to
find a 4-way hit, though low memory approaches require more
An obvious way to generate different candidates is to grind the
coinbase extra-nonce but for non-empty blocks each attempt will
require 13 or so additional sha2 runs which is very inefficient.
This inefficiency can be avoided by computing a sqrt number of
candidates of the left side of the hash tree (e.g. using extra
nonce grinding) then an additional sqrt number of candidates of
the right  side of the tree using transaction permutation or
substitution of a small number of transactions.  All combinations
of the left and right side are then combined with only a single
hashing operation virtually eliminating all tree related
With this final optimization finding a 4-way collision with a
moderate amount of memory requires ~2^24 hashing operations
instead of the >2^28 operations that would be require for
extra-nonce  grinding which would substantially erode the
benefit of the attack.
It is this final optimization which this proposal blocks.
==New consensus rule==
Beginning block X and until block Y the coinbase transaction of
each block MUST either contain a BIP-141 segwit commitment or a
correct WTXID commitment with ID 0xaa21a9ef.
(See BIP-141 "Commitment structure" for details)
Existing segwit using miners are automatically compatible with
this proposal. Non-segwit miners can become compatible by simply
including an additional output matching a default commitment
value returned as part of getblocktemplate.
Miners SHOULD NOT automatically discontinue the commitment
at the expiration height.
The commitment in the left side of the tree to all transactions
in the right side completely prevents the final sqrt speedup.
A stronger inhibition of the covert attack in the form of
requiring the least significant bits of the block timestamp
to be equal to a hash of the first 64-bytes of the header. This
would increase the collision space from 32 to 40 or more bits.
The root value could be required to meet a specific hash prefix
requirement in order to increase the computational work required
to try candidate roots. These change would be more disruptive and
there is no reason to believe that it is currently necessary.
The proposed rule automatically sunsets. If it is no longer needed
due to the introduction of stronger rules or the acceptance of the
version-grinding form then there would be no reason to continue
with this requirement.  If it is still useful at the expiration
time the rule can simply be extended with a new softfork that
sets longer date ranges.
This sun-setting avoids the accumulation of technical debt due
to retaining enforcement of this rule when it is no longer needed
without requiring a hard fork to remove it.
== Overt attack ==
The non-covert form can be trivially blocked by requiring that
the header version match the coinbase transaction version.
This proposal does not include this block because this method
may become generally available without restriction in the future,
does not generally interfere with improvements in the protocol,
and because it is so easily detected that it could be blocked if
it becomes an issue in the future.
==Backward compatibility==
This document is placed in the public domain.

@_date: 2017-04-06 00:17:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
On Wed, Apr 5, 2017 at 11:05 PM, theymos via bitcoin-dev
Not 0.14.1 because that is in RC already and will hopefully be out in a week.
I think the speed of adoption depends a lot of the level of support
from the community. I don't believe there are any technical hurdles to
implementing this relatively quickly (and I specifically propose using
the users choice of the segwit commitment or a modified form in order
to lower the technical complexity and risk).
This is the default behavior as of 0.13.2, but I haven't gone out to
measure this which is why the backwards compatibility section of the
BIP isn't written yet.
While I'm posting, I've had a dozen off-list emails that presented me
with some FAQ:
Many people asked what other protocol upgrades beyond segwit could run
into the same incompatibility.
Many proposed improvements to Bitcoin require additional
transaction-dependent commitment data.
Examples include:
(1) Segwit.
(2) UTXO commitments. (non-delayed, at least)
(3) Committed Bloom filters
(4) Committed address indexes
(5) STXO commitments (non-delayed).
(6) Weak blocks
(7) Most kinds of fraud proofs

@_date: 2017-04-06 01:32:03
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
I apologize for the glib talk on chat and I hope you understand that
the tone in such venues is significantly informal; and that my remark
was a causal one among friends which was not intended in a spirit as
seriously as you've taken it.
That said, two days ago you participated in a highly unusual
announcement of a protocol change that-- rather than being sent for
community review in any plausible venue for that purpose-- was
announced as a done deal in embargoed media announcements.  This
proposed protocol change seemed custom tailored to preserve covert
boosting, and incorporated direct support for lightning -- and the
leading competing theory was that a large miner opposed segwit
specifically because they wanted to block lightning. Moreover, I have
heard reports I consider reliable that this work was funded by the
miner in question.
In the time since, when people asked for revisions to the proposal to
not block segwit they received responses from the Bcoin account on
twitter that "there would be no amendments", and I was sent leaked
chatlogs of you making considerably hostile statements, claiming that
if your extension block proposal is "a litmus test for corruption",
and claimed (before AFAIK anyone had had a chance to comment on it)
that the Bitcoin project contributors opposed it for "nonsense
It is with this in mind that when you tried to pull me into an off the
record conversation that I responded stating:
"[...] I am disinclined to communicate with you except in email where I can
get third party transferable proof of our communication.  I'm
concerned that you may now be involved in a conspiracy which I do not
want to be implicated in myself.
It is my estimation that, for that above reason, it would be in my
best interest to not communicate with you at all.  But in all your
prior interactions you appeared to have integrity and sense, so out of
respect for that history I'm willing to communicate with you, but only
in public or in email where my end is on gmail."
This was two days ago and you did not respond further.
With that in mind I hope you do not find some casual crap-talking on
chat to be especially surprising.
I understand that you didn't intend for the initial message to be
posted in public, so I'm sorry for continuing the thread here-- but I
thought it was useful for people to understand the context behind that
glib remark: Including the point that I do not know for a fact that
you are complicit in anything, but I consider your recent actions to
be highly concerning.

@_date: 2017-04-06 21:38:31
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the 
It was just pointed out to me that the proposed ID (which I just
selected to be above the segwit one) collides with one chosen in
another non-BIP proposal.  This wasn't intentional, and I'll happily
change the value when I update the document.

@_date: 2017-04-07 01:09:26
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
How do you deal with validity rules changing based on block height?
So it sounds like to work the software still needs an analog of a
(U)TXO database? I am confused by the earlier comments about thinking
the the resource consumption of the (U)TXO database is not a
consideration in your design.
If you get a transaction claiming to spend 0xDEADBEEFDEADBEEF, an
output that never existed how does your spent index reject this spend?

@_date: 2017-04-07 18:18:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
I'm still lost on this-- AFAICT your proposals long term resource
requirements are directly proportional to the amount of unspent output
data, which grows over time at some fraction of the total transaction
volume (plus the rate of spending which is more or less a constant).
Can you help out my understanding here?

@_date: 2017-04-07 19:42:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
On Fri, Apr 7, 2017 at 6:52 PM, Tom Harding via bitcoin-dev
Only with the additional commitment structure such as those proposed
by Peter Todd in his stxo/txo commitment designs, e.g.

@_date: 2017-04-08 00:44:50
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
Then I think you may want to retract the claim that "As this solution,
reversing the costs of outputs and inputs, [...] updates to the
protocol addressing the UTXO growth, might not be worth considering
*protocol improvements* "
As you note that the output costs still bound the resource
requirements. Short of radical protocol changes like TXO-proofs the
UTXO data remains a driving unavoidable long term resource cost, not
an implementation detail.  Implementation optimizations like improving
locality further or keeping spentness in memory do not change this
Latency related costs in Bitcoin Core also do not depend on the number
of outputs in transactions in a block. When a transaction is handled
it goes into an in-memory buffer and only gets flushed later if isn't
spent before the buffer fills.  A block will take more time to
validate with more inputs, same as you observer, but the aggregate
resource usage for users depends significantly on outputs (so, in fact
there is even further misaligned incentives than just the fact that
small outputs have a outsized long term cost).

@_date: 2017-04-08 22:12:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Using a storage engine without UTXO-index 
In Bitcoin Core the software _explicitly_ and intentionally does not
exploit mempool pre-validation because doing that very easily leads to
hard to detect consensus faults and makes all mempool code consensus
critical when it otherwise is not. There have been bugs in the past
which would have split the network if this optimization had been used.
(in particular, I believe I recall one related to correctly removing
coinbase spends from the mempool during reorganization that made them
immature; and with the optimization and without the CNB post-test
would have resulted in nodes that saw the reorg creating and accepting
an invalid block, while nodes that didn't rejecting it; but because of
prudent design it was largely harmless).
Because signature validation is cached, and takes the majority of the
block validation time the speed up from the risky optimization isn't
that considerable, and there are other lower hanging fruity with
bigger payouts like Pieter's change to the per-txout management model
and the new non-atomic flushing logic.... and these things don't make
more of the system consensus critical.

@_date: 2017-04-14 07:56:31
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
I do not support the BIP148 UASF for some of the same reasons that I
do support segwit:  Bitcoin is valuable in part because it has high
security and stability, segwit was carefully designed to support and
amplify that engineering integrity that people can count on now and
into the future.
I do not feel the the approach proposed in BIP148 really measures up
to the standard set by segwit itself, or the existing best practices
in protocol development in this community.
The primary flaw in BIP148 is that by forcing the activation of the
existing (non-UASF segwit) nodes it almost guarantees at a minor level
of disruption.
Segwit was carefully engineered so that older unmodified miners could
continue operating _completely_ without interruption after segwit
Older nodes will not include segwit spends, and so their blocks will
not be invalid even if they do not have segwit support. They can
upgrade to it on their own schedule. The only risk non-participating
miners take after segwit activation is that if someone else mines an
invalid block they would extend it, a risk many miners already
frequently take with spy-mining.
I do not think it is a horrible proposal: it is better engineered than
many things that many altcoins do, but just not up to our normal
standards. I respect the motivations of the authors of BIP 148.  If
your goal is the fastest possible segwit activation then it is very
useful to exploit the >80% of existing nodes that already support the
original version of segwit.
But the fastest support should not be our goal, as a community-- there
is always some reckless altcoin or centralized system that can support
something faster than we can-- trying to match that would only erode
our distinguishing value in being well engineered and stable.
"First do no harm." We should use the least disruptive mechanisms
available, and the BIP148 proposal does not meet that test.  To hear
some people-- non-developers on reddit and such-- a few even see the
forced orphaning of 148 as a virtue, that it's punitive for
misbehaving miners. I could not not disagree with that perspective any
more strongly.
Of course, I do not oppose the general concept of a UASF but
_generally_ a soft-fork (of any kind) does not need to risk disruption
of mining, just as segwit's activation does not.  UASF are the
original kind of soft-fork and were the only kind of fork practiced by
Satoshi. P2SH was activated based on a date, and all prior ones were
based on times or heights.  We introduced miner based activation as
part of a process of making Bitcoin more stable in the common case
where the ecosystem is all in harmony.  It's kind of weird to see UASF
portrayed as something new.
It's important the users not be at the mercy of any one part of the
ecosystem to the extent that we can avoid it-- be it developers,
exchanges, chat forums, or mining hardware makers.  Ultimately the
rules of Bitcoin work because they're enforced by the users
collectively-- that is what makes Bitcoin Bitcoin, it's what makes it
something people can count on: the rules aren't easy to just change.
There have been some other UASF proposals that avoid the forced
disruption-- by just defining a new witness bit and allowing
non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I
think they are vastly superior. They would be slower to deploy, but I
do not think that is a flaw.
We should have patience. Bitcoin is a system that should last for all
ages and power mankind for a long time-- ten years from now a couple
years of dispute will seem like nothing. But the reputation we earn
for stability and integrity, for being a system of money people can
count on will mean everything.
If these discussions come up, they'll come up in the form of reminding
people that Bitcoin isn't easily changed at a whim, even when the
whims are obviously good, and how that protects it from being managed
like all the competing systems of money that the world used to use
were managed. :)
So have patience, don't take short cuts.  Segwit is a good improvement
and we should respect it by knowing that it's good enough to wait for,
and for however its activated to be done the best way we know how.

@_date: 2017-04-14 20:59:55
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
On Fri, Apr 14, 2017 at 8:34 PM, Tom Zander via bitcoin-dev
Please stop abusing participants on this list. Your activity is
actively driving people off this list.
James Hilliard should be commended for correcting your misinformation.
Anyone can modify their software to produce invalid blocks at any
time. If they want to be stupid, they can be stupid.
The fact remains that miners who haven't gone and wreaked their
software internals will not mine segwit incompatible blocks. Right now
_no_ observable has broken node in this way.

@_date: 2017-04-14 21:12:47
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
On Fri, Apr 14, 2017 at 9:10 PM, James Hilliard via bitcoin-dev
IIRC-- If you do it accidentally you'll fail the tests, though there
have been a couple reckless alternative implementations that have just
ripped out most of the tests...
In any case there is no need to speculate or guess-- invalid segwit
spends are not being mined today...

@_date: 2017-04-15 03:29:10
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
Wow. Where did you get that idea? That is _absurd_ and untrue, and I
struggle a bit to even comprehend how someone could believe it.  It
would continue until something clearly better came along or people
lost interest in it, why would it be anything else?
There is a definitional issue there. There isn't much of "the core
team" there is a lot of amorphous public collaboration; everything
ends up being retroactively defined as the core team.  With open
participation and hundreds of contributors and software running
everywhere in the network, its unlikely that someone would advance to
the point of being able to make a credible proposal without at some
point making some improvement to the project or without the help of
someone who has.
In some sense you are coming very close to asking for a list of people
who have contributed to Bitcoin without contributing to Bitcoin.
CLTV was a proposal by Peter Todd whom has done a number of other
things in core but AFAIR had no involvement in any prior soft-fork
(though perhaps I'm forgetting one?), though he subsequently
contributed to BIP66 (which activated before CLTV), and he contributed
mostly after-the fact review of segwit. CSV was mostly the work of
Mark Friedenbach whom I believe was not involved in any prior or
subsequent soft-fork (and whos total contributions to Bitcoin core
weigh in at 14 commits over 5 years).
I am not suggesting slow. I am suggesting that we not be outright
reckless. Some people are expecting changes which are effectively
orders of magnitude faster than changes in centralized systems
elsewhere which are far easier and safer to take quickly.
(Some more comparatives here:
By all means, take risks-- but you don't get to choose to make other
peoples things fail; you certainly don't get to demand their support,
though you could try to earn it if you care, by figuring out how to
meet their concerns.

@_date: 2017-04-15 04:47:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
There is a technical requirement that BIP 9 bit allocations must have
a timeout so that a bit is not forever burned if a proposal is ever
abandoned (e.g. because something better came along before it
activated).  This isn't a timeout for the proposal, but for the bit
assignment.  If a proposal hasn't activated but there is still
interest it will just get a new bit (and can alternate back and forth
between a pair). This is a timeout of the bit, not the proposal.
It has to be setup this way because there is no real way to
communicate abandonment to old software, so a timeout must be set in
"Core" doesn't plan on much of anything beyond the immediate pipeline
of activities, similar to other large open source collaboration, or
open standards development organizations. It isn't a company.
Individuals have plans about their own work which they may collaborate
in one place or another.
But allocating a new bit is how BIP9 works.
What is a "core defined process"?  BIP _itself_ was created by someone
who, AFAICT, has never made a commit to Bitcoin Core.  Numbers are
currently assigned, a nearly judgement-less administrative task, by
someone that authors competing fork of the software (Knots).
Yet it was proposed on this list, had a BIP defined... if it got
eventually used it would presumably end up in the Bitcoin Core project
eventually... so what exactly is your definition of outside? Above you
seemed to be saying a BIP was not outside, but here you are saying
something documented as a BIP is outside?
If your preference is to not insult then it may be advisable to not
disregard distinctions which you do not understand as semantics. :) I
am not prone to arguing over semantics-- the continually binning in
almost all public collaboration as the work of some centralized entity
is really harmful to our community. The distinction is real, and not
Sure, and I said so directly in my message.  I believe I was
adequately clear that my complaint about BIP148 is specifically that
it has forced orphaning of passive participants which can be easily
avoided but at the expense of actually needed users to adopt the
For clarity, it could be summarized as: I would not classify BIP148 as
a user activated soft-fork but instead as "user enforced miner
soft-fork activation". The consequence of this is that it likely
cannot achieve low disruptiveness-- this limitation would be excusable
if we weren't aware of any alternative, but in this case we are and
the only relative downside of it is that users will need to upgrade
for it-- which should not be a problem in principle if we believe a
UASF is really user activated.

@_date: 2017-04-15 07:04:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
And as a result we ultimately got a clearly inferior solution (520
byte script limit; 80-bit security; months of orphaned blocks-- and
two of those were not issues in BIP17).  I went along for the cram
fest on 16 after 12 caught fire, and I was mistaken to do so.
Doubly so because it took years for P2SH to achieve any kind of mass
deployment due to issues far away from consensus.  An extra two months
spent on some ground-work (including communications and documentation)
could have pulled forward practical deployment by a year and given
time to find and fix some of the flaws in the design of P2SH.
It seems I lost a word in my comment: that should have been "almost
guarantees at _least_ a minor level of disruption". A minor level of
disruption is the _minimum_ amount of disruption, and for no good
reason except an unprecedented and unjustified level of haste.
Considering that you did not spare a single word about the specific
property that I am concerned about-- that the proposal will reject the
blocks of passive participants, due to avoidable design limitations--
I can't help but feel that you don't even care to understand the
concern I was bringing up. :(
How many people barely reviewed the specifics of the proposal simply
because they want something fast and this proposal does something
By now competitors and opponents to Bitcoin have surely realized that
they can attack Bitcoin by stirring up drama.
As a result, the only way that we will ever be free from "war" is if
we choose to not let it impact us as much as possible. We must be
imperturbable and continue working at the same level of excellence as
if virtual shells weren't flying overhead-- or otherwise there is an
incentive to keep them flying 24/7. Internet drama is remarkably cheap
to generate. "The only thing we have to fear is fear itself".
The alternative is that we hand opponents a ready made formula for
disruption: astroturf enough drama up that Bitcoiners "sacrifice
correctness" themselves right off a cliff in a futile attempt to make
it go away. :)

@_date: 2017-04-15 18:50:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
I do not follow the argument that a critical design feature of a particular
"user activated soft fork" could be that it is users don't need to be
involved.  If the goal is user activation I would think that the
expectation would be that the overwhelming majority of users would be
upgrading to do it, if that isn't the case, then it isn't really a user
activated softfork-- it's something else.
So it has to be supported by the public but I can't say why I don't support
it? This seems extremely suspect to me.

@_date: 2017-04-21 20:38:36
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Small Nodes: A Better Alternative to Pruned Nodes 
Hi David,
I've been thinking about this subject for a long time (Tier Nolan
linked some of the threads; see also the coloration part of
 and
about using FEC with the history for the last year or so. (we use FEC
in practice in fibre for relay at the tip now, and that has a design
history going back to 2013).
As Jonas points out, we're all set to also offer the 'recent blocks'
separately, so that is obviously going to happen and will help. The
free design parameter is the definition of recent, but we have good
measurements for setting that now. But about history...
I think I might have designed myself into a corner and perhaps you've
shown a way out-- I think there are some serious limits in your
proposal but perhaps we can fix them.  Let me give you what I had been
thinking about, then hand you at least a couple improvements to your
As you hopefully now know (per Tier Nolan's) post: I'd previously been
thinking about nodes keeping a deterministic random, independently
selected subset which is self-leveling based on a small seed.   The
connection overhead can can be mitigated by working with chunks of
blocks rather than single blocks.
But as you've observed, the failure probabilities are rather high,
especially if an active attacker targets nodes carrying less commonly
available blocks.  So I thought, okay we can use FEC to help improve
the recovery odds.
So I considered using a sparse random code (E.g. an LDPC erasure code
like in RFC 5170) the advantage of these codes is that they are very
fast to decode, and they support having enormous codewords, so you can
a high probability of every peer having a unique ID, so there would
effectively never need to be any duplication.
But then I ran into the problem that I had no reasonable way to
recover from bad data, short of pulling a single group from an archive
then banning whatever peers gave you bad chunks.
So that is kind of where I got stuck.
I didn't even consider the advantage of only being able to use a few
peers total, as I still assumed that it would be doing the random
groups thing as well. That's a great point.
So on your design:
Being able to have a lower bound of 20% seems like a serious
limitation to me: it would be fine today, but the chain will quite
possibly be twice the current size in a year.... and in four years
we're back to where we are now. What I'd been thinking would be able
to scale arbitrarily low.
20% is small, but is it small enough? -- today that would get us back
into common SSDs being able to hold the whole chain, but that property
will probably be lost again in a couple years. I think with any fixed
fraction we'll constantly be fighting against the opportunity cost of
upgrading storage, if not the cost of the storage itself. (and I agree
this is the vast majority of the response from actual users,  sync
time and ongoing bandwidth usage seeming to tie for second; the latter
of which can be mitigated in other ways but for the former see my
remarks at the end)
The fact that having only a few required blocks needed lets you brute
force out the decode is a great point.  I hadn't considered that, and
the schemed I'd been considering are not communications efficient with
only a few blocks required e.g. they sometimes require a couple extra
blocks to successfully decode, which is a lot of overhead if you're
only splitting 5 ways.
Actually RS codes are _the_ codes you want to use for with decoding
with errors but the 'computationally efficient' error correcting
decoding (which is itself heinously slow) requires 2*errors + data
number of blocks to decode. Not so useful if you're only looking at 5
RS decoding is pretty slow generally, esp compared to binary sparse
codes.  We were unable to make RS coding make sense for use in fast
block relay for this reason-- decoding time bottlenecked
reconstruction. The most highly optimized RS codes make a special
optimization which is not useful for your proposal: They're much
faster to decode if you're decoding from the first couple correction
blocks.  Without these optimizations speeds from from 1GB/s to more
like 100MB/s or worse.  Though I suppose with assumevalid initial sync
now is pretty cpu-idle on most hardware.  (If 100MB/s sounds fast,
keep in mind that time spent decoding is time that can't be spent
hashing/verifying/etc.. and on a lot hardware with fast broadband with
signature validation enabled we're cpu bound already)
This was a complete non-starter when thinking about using these sparse
codes where the number of possible blocks is effectively unlimited.
Your 4kb assumption isn't correct though: How you do it is that after
downloading a block you compute the 255 fragments (as an aside, you're
saying 256 but the most you can get is 255 for an 8-bit RS code).
then you compute a hashtree over them. Every node remembers the root,
and the membership proofs for their chunk(s)... this is 256 bytes of
extra storage.
When you decode you use a majority to decide what root you are trying
to decode. If it fails to result in valid blocks, you blacklist that
root, ban all those peers, and try the next.  Worst case cost is the
number of invalid roots rather than peers choose 5.
I'm not sure if combinitoral decode or the above would be easier to implement.
The RS coder stuff is small, even doing the fancy w/ error decodes it
isn't huge. I expect complexity mostly will show up in the garbage
input handling.
A couple other thoughts:
The coded blocks are not useful for things like bloom scanning or
other lookup.  With the committed filter proposals you could still
keep the committed filters (even 5 way shared themselves...) so
perhaps not that much of a concern.
Coding the blocks will make their encoding normative. The current P2P
one we use is by no means more efficient,  Pieter and I have an
encoding that works on a transaction by transaction basis and
decreases the size of the whole chain by ~28%, block-wide entropy
encoding could reduce the size even further.  We'd hoped to at least
start using this per transaction coding locally, converting on the fly
to the original serialization where needed (the idea would be to
eventually support the compacted serialization on the wire too).
Maybe the answer there is to just get in whatever improvements we
think are reasonable before doing the coded block.
I think the 20% floor is still too high, and too many nodes will be
forced to just do the recent blocks things. perhaps not today, but
eventually.   I suppose we could just assume that the block is split
10 ways, and the default is two indexes, but there is flexibility to
go down to one in the future, at the cost of needing more peers...
could run numbers on the decode failure odds for the 10 of 255 case...
But that would only improve it to 10%.   I suppose the proposal could
be combined with sparse chain storage like I was thinking years ago,
but much less sparsity would be needed so the downsides would be less
E.g. if you want to store <10% of the chain you'd keep some 10% blocks
for random groups.  such a feature could be introduced later when it
was more important to keep less than 10 or 20 percent.
Recovery odds could be improved with a second level of coding. E.g. if
your ID was not a constant but a seed into PRNG(height)%250+5  then
you also have a fraction of nodes store the xor between adjacent pairs
then you can fill in unrecoverable groups.  the limit of this thinking
is exactly the sparse random code ECC schemes) Probably not worth the
complexity, but just a thing to keep in mind...
Probably the next step is to do some more focused benchmarks. I have
some code I can recommend offline.
I can't help but feel that this might be a little bit of a waste of
time. I think the long term survival of the system is likely going to
ultimately depend on doing an assumevalid like gated sync of a UTXO
set.  Ethereum already does something far more reckless (they let
miners pick the state and blindly trust it with almost no depth
required) and no one cares.  If that is done then all these concerns
are greatly reduced, along with the 100(+++?)GB/history-year transfer
costs.  You'd still want to have archives, but do you care about 20%
Replying to some other comments in the thread:
This is almost if not quite entirely pointless. There are already many
nodes on AWS and DO, adding more does not improve your security (you
must trust DO/AWS), does not improve your reliability (DO/AWS), does
not improve the network's capacity, etc.  About the only arguable
benefit I can see there beyond making more money for these hosts is
feel good points for (incorrectly) thinking you're helping the
network, and negligibly reducing the density of spy nodes (but wait:
AWS/DO may well just be spying on your connections too..). and it it
isn't like fast storage on these services is cheap.
We have outbound transmission limits though not by default. Setting
them sensibly is something of a black art. Obviously these will
improve over time... and are more reasons that I agree with your
relative cost assumptions except for the sync-time part.
Unavoidable, but should be made inconsequential by making transaction
announcement more private independent of any of this. There are
already _MANY_ ways to fingerprint nodes, please don't think that
existing software has any real immunity here. We avoid adding new
fingerprinting where we can... but they're very fingerprintable
already. Transaction announcement privacy MUST be fixed.  I assume if
any of this is worth doing, it will also be worth the increase in
fingerprinting. And at least this proposal would 'only' create 255
node classes.
As I pointed out above, Vorick's idea is totally compatible with
committed filters, and the filters can even be shared like the blocks
The SNR on this list really sucks.  If you aren't spending a couple
hours thinking about your responses or at least citing research that
took days of effort or more then you are probably wasting people's
time. Please respect the other users of the list.
Absolutely, I assume if Vorick's proposal were implemented that nodes
would have the follow options: Pruned [UTXO + recent two weeks of
blocks], 20%, 40%, 60%, 80%, 100% (archive).

@_date: 2017-04-25 18:28:14
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] I do not support the BIP 148 UASF 
I have not reviewed it carefully yet, but I agree that it addresses my
main concern!  I think this is a much better approach. Thanks.

@_date: 2017-08-15 05:12:11
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal, Pay to Contract BIP43 Application 
This construction appears to me to be completely insecure.
Say my pubkey (the result of the derivation path) is P.
We agree to contract C1.   A payment is made to P + G*H(C1).
But in secret, I constructed contract C2 and pubkey Q and set P = Q + G*H(C2).
Now I can take that payment (paid to Q + G*(C1) + G*H(C2)) and assert
it was in act a payment to P' + G*H(C2).   (P' is simply Q + G*H(C1))
I don't see anything in the proposal that addresses this. Am I missing it?
The applications are also not clear to me, and it doesn't appear to
address durability issues (how do you avoid losing your funds if you
lose the exact contract?).
On Mon, Aug 14, 2017 at 6:05 AM, omar shibli via bitcoin-dev

@_date: 2017-08-28 17:06:04
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd:  P2WPKH Scripts, P2PKH Addresses, 
Absolutely not. You can only pay people to a script pubkey that they
have specified.
Trying to construct some alternative one that they didn't specify but
in theory could spend would be like "paying someone" by putting a
cheque in a locked safe labeled "danger radioactive" that you quietly
bury in their back yard.  Or taking the payment envelope they gave you
stuffing it with cash after changing the destination name to pig latin
and hiding it in the nook of a tree they once climbed as a child.
There have been technical reasons why some wallets would sometimes
display some outputs they didn't generate but could spend, but these
cases are flaws-- they're not generic for all cases they could in
theory spend, and mostly exist because durability to backup recovery
makes it impossible for it to tell what it did or didn't issue.
So regardless of your query about uncompressed keys, you cannot do
what you described: Wallets will not see the payment and may have no
mechanism to recover it even if you tell the recipient what you've
done. And yes, the use of an uncompressed yet could later render it

@_date: 2017-08-28 17:12:15
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd:  "Compressed" headers stream 
You are leaving a lot of bytes on the table.
The bits field can only change every 2016 blocks (4 bytes per header),
the timestamp can not be less than the median of the last 11 and is
usually only a small amount over the last one (saves 2 bytes per
header), the block version is usually one of the last few (save 3
bytes per header).
But all these things improvements are just a constant factor. I think
you want the compact SPV proofs described in the appendix of the
sidechains whitepaper which creates log scaling proofs.

@_date: 2017-12-11 21:04:01
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] "Compressed" headers stream 
In the last proposal I recall writing up, there was a one byte flag on
each header to indicate what was included.
Nbits _never_ needs to be sent even with other consensus rules because
its more or less necessarily a strict function of the prior headers.
This still holds in every clone of Bitcoin I'm aware of; sending it
with the first header in a group probably makes sense so it can be
checked independently.
another >18% reduction in size beyond the removal of prev. is not
insubstantial by any means.  I don't think it should lightly be
Prev omission itself is not, sadly, magically compatible:  I am quite
confident that if there is a bitcoin hardfork it would recover the
nbits/4-guarenteed always-zero bits of prev to use as extra nonce for
miners. This has been proposed many times, implemented at least once,
and the current requirement for mining infrastructure to reach inside
the coinbase txn to increment a nonce has been a reliable source of
failures.  So I think we'd want to have the encoding able to encode
leading prev bits.
Many altcoins also change the header structures. If the better thing
is altcoin incompatible, we should still do it. Doing otherwise would
competitively hobble Bitcoin especially considering the frequent
recklessly incompetent moves made by various altcoins and the near
total lack of useful novel development we've seen come out of the
Probably the most important change in a new header message wouldn't be
the encoding, but it would be changing the fetching mechanism so that
header sets could be pulled in parallel, etc.
I would rather not change the serialization of existing messages,
nodes are going to have to support speaking both messages for a long
time, and I think we already want a different protocol flow for
headers fetching in any case.

@_date: 2017-12-11 23:11:24
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] "Compressed" headers stream 
That provides no security without additional consensus enforced
commitments, so I think pretty off-topic for this discussion.

@_date: 2017-12-13 00:01:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] "Compressed" headers stream 
Yes, that is what I was thinking last time we discussed it, just with
each header include a one byte flag that lets you express:
bit: meaning
(0) if nbits is the same as last,
(1) if timestamp is a full field or a small offset, (e.g. two bytes
defined as unsigned offset between the last time - 7200 and the new
(2,3,4) if version is the same as the last distinct value .. 7th last,
or a new 32bit distinct value;
(5,6) if prev is entirely there, entirely gone, first 4 bytes
provided, or first 8 bytes provided. (if provided they override the
computed values).
That would be 7 bits in total; the 8th could be reserved or use to
signal "more headers follow" to make the encoding self-delimiting.
The downside with nbits the same as last as the optimization is that
if we ever change consensus rules to ones where difficulty management
works differently it may be the case that nbits changes every block.
Alternatively, nbits could get a differential encoding that could be
opted into for small differences-- though I haven't thought much about
it to see if a one byte difference would be that useful (e.g. can bch
differences usually be expressed with one byte?)
I'm kind of dubious of the consensus layer anti-dos separation:  nbits
minimum is so low compared to the speed of a mining device, virtually
any attack that you might do with invalid headers could still be done
with headers at the minimum difficulty. But I'm fully willing to
accept that simpler is better...

@_date: 2017-12-13 00:12:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] "Compressed" headers stream 
The question becomes there-- how should it work.
In an ideal world, we'd decide what peers to fetch headers from based
on a compact proof of the total work in their chains... but we cannot
construct such proofs in Bitcoin today.
I think that instead of that a weak heuristic is to fetch first from
the peers with the tip at the highest difficulty. Then work backwards.
See: Which is the inspiration for the current headers first sync, but
without the reverse part because the protocol didn't permit it: you
can't request a linear wad of headers that come before a header. This
is the thing I was mostly thinking of when I mentioned that we may
want to change the interface.

@_date: 2017-12-18 20:42:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why not witnessless nodes? 
Because it would make no meaningful difference now, and if you are not
going to check the history there are much more efficient things to
do-- like not transfer it at all.
On Mon, Dec 18, 2017 at 8:32 AM, Kalle Rosenbaum via bitcoin-dev

@_date: 2017-12-21 22:44:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Total fees have almost crossed the block reward 
Personally, I'm pulling out the champaign that market behaviour is
indeed producing activity levels that can pay for security without
inflation, and also producing fee paying backlogs needed to stabilize
consensus progress as the subsidy declines.
I'd also personally prefer to pay lower fees-- current levels even
challenge my old comparison with wire transfer costs-- but we should
look most strongly at difficult to forge market signals rather than
just claims-- segwit usage gives us a pretty good indicator since most
users would get a 50-70% fee reduction without even considering the
second order effects from increased capacity.
As Jameson Lopp notes, more can be done for education though-- perhaps
that market signal isn't efficient yet. But we should get it there.
But even independently of segwit we can also look at other inefficient
transaction styles: uncompressed keys, unconfirmed chaining instead of
send many batching, fee overpayment, etc... and the message there is
I've also seen some evidence that a portion of the current high rate
congestion is contrived traffic. To the extent that it's true there
also should be some relief there soon as the funding for that runs
out, in addition to expected traffic patterns, difficulty changes,
On Thu, Dec 21, 2017 at 9:30 PM, Melvin Carvalho via bitcoin-dev

@_date: 2017-12-22 01:15:46
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Total fees have almost crossed the block reward 
On Fri, Dec 22, 2017 at 12:30 AM, Mark Friedenbach via bitcoin-dev
The distinction is does a next fee replacement hit the next block 99%
of the time or does it do so with 10% probability each successive
block that the original remains unconfirmed; eventually converging to
the same 99% but only after a non-trivial additional delay.  As a
result it's still useful to flip it on.
I believe electrum has been defaulting to opt-in without any big problems.
There was discussion in the bitcoin core weekly irc meeting today
about defaulting it on.  Some expressed the view that perhaps it
should be left off by default for the RPC because some industrial
users but I'm of the view that those users are both most likely to
want it on and also the most able to see it in the release notes and
change their settings.

@_date: 2017-07-02 20:56:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: No chaining off replaceable 
I don't really see how this is desirable:  Just replace it-- the
receiver foolishly spent it at its own peril, spending a unconfirmed
payment from a third party is something that Core never does, it's
reckless unless you're doing something like CPFPing it to yourself,
which is harmless (either it'll work, or it'll fail and you'll be fine
with that).
Beyond being paternalistic the issue I see with your proposal is that
its contrary to miner income-- you're asking miners to ignore these
spends that otherwise they could accept.  This seems unstable-- some
people would ignore your rule even if it were otherwise widely
adopted, leading to the network behavior having higher volatility.
Instead, perhaps a BIP that very strongly advises parties to not spend
unconfirmed outputs from third parties while making a payment to third
parties would achieve your end?

@_date: 2017-07-03 02:28:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: No chaining off replaceable 
Perhaps I am not following what you're saying here.
If the receiver is paying a higher feerate than your replacement,
he'll get it confirmed as fast or faster than your replacement in any

@_date: 2017-07-05 08:06:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Height based vs block time based thresholds 
These proposals for gratuitous orphaning are reckless and coersive.
We have a professional obligation to first do no harm, and amplifying
orphaning which can otherwise easily be avoided violates it.
It is not anyones position to decide who does and doesn't need to be
"woken up" with avoidable finical harm, nor is it any of our right to
do so at the risk of monetary losses by any and all users users from
the resulting network instability.
It's one thing to argue that some disruption is strictly needed for
the sake of advancement, it's another to see yourself fit as judge,
jury, and executioner to any that does not jump at your command.
(which is exactly the tone I and at least some others extract from
your advocacy of these changes and similar activity around BIP148).
I for one oppose those changes strongly.
I have seen no evidence or case for this.

@_date: 2017-07-07 23:22:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] A Segwit2x BIP 
I'm happy to see that someone has begun writing a specification. But I
am appalled to see one just being written now for change it's authors
expect to be irreversibly applied to the network in less than 30 days.
The timeline of this proposal is recklessly short to such an extreme
level that we have never, to the best of my knowledge, seen a prior
proposal so hasty.  Nowhere does this specification provide
justification or assurance that this is at all safe.  The time line of
it violates the most minimal of responsible engineering practices, by
being shorter than even a fast development and release candidate
timeframe.   This proposal carries an extreme risk for parties to lose
money due to transaction reversals at two distinct points in time and
provides no proposed countermeasures to avoid these losses.
The proposal adds another gratuitous limit to the system: A maximum
transaction size where none existed before, yet this limit is almost
certainly too small to prevent actual DOS attacks while it is also
technically larger than any transaction that can be included today
(the largest possible transaction today is 1mb minus the block
overheads).  The maximum resource usage for maliciously crafted 1MB
transaction is enormous and permitting two of them greatly exacerbates
the existing vulnerability.
But in a worst case the result would be 8MB, which this document fails
to mention.
The claim that the document's [2] says that these increases are "safe"
is incorrect and is a matter which has been previously corrected by
the authors of the document:
The cited paper does an approximate best case analysis considering
only a couple of risk factors (in particular, block relay time, but
ignoring durability to dos attacks, robustness against state
intervention, and initial synchronization time) and concluded that 4MB
was the largest they could argue was safe. The paper goes on to then
argue that even if you crank Bitcoin's parameters to the maximum in
those dimensions that it doesn't result in a truly meaningful increase
in scalablity-- in effect, it's a weak argument against your proposal
and ones like it.
This means that BIP-91 and your proposal are indistinguishable on the
network, because the string "segsignal" is merely a variable name used
in the software.
The proposal is unable to distinguish itself from BIP-91. Does this
mean if segwit2x or BIP91 activates ?
Considering that we just spent the whole weekend with the mempool
having ~1 block or less worth of transactions most of the time, it
seems highly likely that just activating segwit will substantially
disrupt the fee market; to say nothing for the further doubling that
isn't even tempered by new wallet adoptions.  There seems to be no
consideration given to avoiding this disruption and preventing further
emergency events when the new capacity is eventually used and software
is again left unprepared for having to pay market fees.
In effect, the document admits that it isn't a solution that
meaningfully improves the scale or scalablity but rather it's just a
bailout to temporarily lower/negate transaction fees.  It doesn't seem
to make any argument (or even acknowledge) that the risks and
disruption are worth its benefit, and it exacerbates those risks by
being the product of a closed process and having a timeline shorter
than basically any software update for production software (much less
the timeframe for any consensus update previously). Kudos for being
frank here, but it's not exactly selling itself.
It seems to me that the document doesn't really even make an effort to
justify the bailout at all and don't explain how it will result in
anything except an endless series of additional fee bailouts.
Moreover, it doesn't discuss any remediation against the replay
exposure that the proposed hardfork is sure to create. ( I can
guarantee to you, I will not adopt this hardfork; especially given
that is has been made completely clear that the terms of it were set
in its closed door meetings and the input of non-supporters was not
welcome. )

@_date: 2017-07-07 23:25:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] A Segwit2x BIP 
Indeed, their code previously did not increase the blocksize but it
was adjusted at the last minute to do so-- so it may actually do that
now. Because they don't appear to have implemented any tests for it, I
wouldn't be too surprised if it still didn't work at all but also
wouldn't be surprised if it did.
You are correct that the specification text appears to refer to the
prior change that did not. (In my response I just assumed that it
meant what they actually did-- good catch).

@_date: 2017-07-07 23:38:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] A Segwit2x BIP 
I think it's very clear that they will in the very short term
(  note the rate drops
when demand falls below supply). But I agree with you if you mean a
somewhat longer term e.g. a year out.

@_date: 2017-07-11 21:11:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
I think it's great that people want to experiment with things like
drivechains/sidechains and what not, but their security model is very
distinct from Bitcoin's and, given the current highly centralized
mining ecosystem, arguably not very good.  So positioning them as a
major solution for the Bitcoin project is the wrong way to go. Instead
we should support people trying cool stuff, at their own risk.
So, given that although the vast majority of the things in the document
are things I've been supporting for months (Please see Note 1 way down
at the bottom) I cannot support your document.
I think you may have have missed how much work I put into what I published
before talking with people who actually work on the project to find out
what they wouldn't object to before publishing the prior document--
and how much I left out that I would have loved to have in; and why
I specifically held back from describing it as a roadmap or prompting
people to sign onto it (though they did of their own accord).
On a more meta-subject, I think grandly stated "top down" roadmaps
in highly collaborative development are of minimal utility at best and
actively misleading at worst. Fundamentally, it misunderstands the nature
of peer collaboration. It's kind of like asking for a roadmap for the
development of fusion power; individual practitioners have their own
roadmaps, but the collaboration of science does not.
Consider an example,
The Linux kernel is one of the largest and best funded open source
projects, which produces the most widely used operating system kernel
in the world and one of the most widely used pieces of software of all
time, and it produces _no_ roadmaps.
Quoting Andrew Morton, "Instead of a roadmap, there are technical
guidelines. Instead of a central resource allocation, there are
persons and companies who all have a stake in the further development
of the Linux kernel, quite independently from one another: People like
Linus Torvalds and I don?t plan the kernel evolution. We don?t sit
there and think up the roadmap for the next two years, then assign
resources to the various new features. That's because we don?t have
any resources. The resources are all owned by the various corporations
who use and contribute to Linux, as well as by the various independent
contributors out there. It's those people who own the resources who
Linus remarked, "I look at the current release and the next one, as I
don't think planning 10 years ahead is sane."
Yet the Linux kernel still has every advantage over us:  They have far
more contributing resources from far more sources, they have a fairly
centralized model and control over their own destiny because they have
a much more functional pathway to disagreement than we have in Bitcoin
for consensus rules.
IMO the way to do "roadmaps" in Bitcoin is to roadmap the finalization
and release process once the basic technology is done; because it's
only past that point that guarantees can really start being made.
But that isn't what your document does-- it names a lot of things which
are still various shades of research at this point (much of it research
I'm working on and strongly support, in fact--)
We can also talk in a visionary sense about the sorts of things we're
exploring, but it just isn't possible to nail down when they'll be
ready until they are: If it's not something the linux kernel can do,
it's not something we can do.  These are neat and existing ideas,
but not a roadmap.
At least not as a group. Individually contributing parties have a lot
more visibility and control into their own activities, and there is
virtue in forecasting at that level. Redhat, for example, has
roadmaps. They primarily deal with stabilization and support of
already existing technology that you could get in the raw from the
various upstream sources (fedora, kernel, etc.). (see for an example,
Separately, what we can do stably in Core is have timely reliable
releases.  Juniper made it 10 years with regular timed releases that
did not slip by more than IIRC three days and which were _all_ production
deployable (things changed later, but thats another story).
This was an incredible benefit to our customers, but the only way it was
possible was because _features_ were not guaranteed in a release.
If a feature failed during the final testing and it needed more than the
most trivial of fixes, it was _removed_ or disabled.  As soon as there
are multiple in-flight deliverables it becomes more important to be
timely over-all even that that significantly delays single deliverables.
This is effectively at odds with hard deadlines on functionality, even
before getting into the fact that for consensus features delivery in
software is irrelevant until activation which is a public election.
(E.g. we shipped segwit almost a year ago, but it still hasn't arrived
for users).
asking for us to do is to (help) drive the Bitcoin Story-- the actual
technology and its timelines is usually not that relevant: if it were,
they'd be stepping up with resources to contribute to it. There are
many ways to do that, -- we don't have to use highly authoritarian
methods that wouldn't even work for the Linux kernel.
[And all these projects you listed, your help would be more than welcome!]
This can be done by a mixture of talking about research _as research_
not a done deal, and by moving discussions of non-research things to
where they're actually more forecastable.  98% of the public
discussion about segwit was before the pull request, -- there were
solid political reasons for this-- but it was the wrong timing.  On
the case of CSV and CLTV, the general public didn't hear about them
until they were merged -- pretty much-- and the timing then was much
more compatible with 'roadmapping' +/- activation uncertainty.
Some of this is driven by competitive pressure with things like
Ethereum or other altcoins (e.g. dash, god save us) that have a lot
lower commitment to engineering integrity or even logical consistency.
We can't compete with technobabble bullshit, and we shouldn't try and
as a side effect back ourselves into a corner where we're making
remarkable accomplishments but regarded as failures in payment
(because we either forecast it taking N years, which is the best we
could promise, or because we forecast the best we might achieve and it
was both still too long and the timeframe got missed).
One of the big screwups with segwit handling was people sticking
some random unrealistic date on it it which was done AFAIK,
by well meaning folks who took some random numbers from people
working on it for when they'd be done with the development-- not the
testing, not the integration, and certainly not deployment and published
it as The Date.  Then even though the development was actually done
by them, segwit was portrayed as massively delayed, because what
matters to the users is deployment-- which we can't control.
I see you've done this yourself with signature aggregation, sticking Q4 2016
right on it, which as far as I can tell is a figure that comes from mars.
(Well not quite mars, see Note 1)
Probably we'll have the research and an implementation done by then, but
with so much uncertainty around segwit activation, I doubt anyone can be
about when users will be able to use it (which is what they care about!)
It's also not really appropriate to ask people to sign onto stuff when they
can't even review it.  Perhaps the signature aggregation stuff is insecure,
patent encumbered, or practically useless... (It won't be but no one could
tell that from your post; because it doesn't even exist as a concrete proposal)
I think people would rightly protest about a number of these things-- especially
things like the the agg sigs and tx compaction, "wtf, I've not heard
of this. Who
are you to insist this goes into Bitcoin?"
In any case; I can repeat the same story for major RFCs, FWIW.  Collaborative
innovation is _very_ hard to stick to a tight schedule.  And road-maps
of totally prospective technology that no one has the actual authority to make
happen aren't a productive thing for the industry.
In a few weeks you'll start seeing articles on the major new things
coming in Bitcoin Core 0.15,

@_date: 2017-07-11 21:31:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
A fine intention, but I've checked with many of the top contributors
and it sounds like the only regular developer you spoke with was
Luke-Jr.  Next time you seek to represent someone you might want to
try talking to them!
I think the project is not philosophically against hardforks, at least
not in an absolute sense.
Part of the reason they were discussed in the capacity document was to
make it clear that I wasn't, and to invite other project members to
expose disagreement (though I'd mostly checked in advance...)
But these recently proposed ultra-hasty highly contentious changes,
that seem to be being suggested on shorter and shorter timeframes; I
do think the project is actually opposed to those in a very strong
But if you were instead to talk about things like fixing timewarp,
recovering header bits, etc. It would clearly be the other way.
At least at the moment computers and bandwidth are improving; I don't
think any regular developers are opposed to hardforks that change
capacity well tech improvements and protocol improvements have made it
obviously not much of a trade-off.
Personally, I wish the project had previously adopted a license that
requires derived works to not accept any block the derived-from work
wouldn't accept for at least two years, or otherwise the derivative
has to be clearly labeled not-bitcoin. :P
In any case, I think it's safe to say that people's opinions on
hardforks are complicated. And all the smoke right now with unusually
poorly executed proposals probably clouds clear thinking.

@_date: 2017-07-11 21:40:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
To make it clear, since I munged the English on this: Most of my post
is just copied straight out of a private thread where I explained my
perspective on 'roadmaps' as they apply to projects like Bitcoin.

@_date: 2017-07-12 00:07:51
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
That might be your impression, then you've misunderstood what I
intended-- What I wrote was carefully constructed as a personal view
of how things might work out. It never claimed to be a a project
roadmap. Though as usual, I work hard to propose things that I believe
will be successful... and people are free to adopt what they want.
And to the extent that it got taken that way I think it was an error
and that it has harmed progress in our community; and created more
confusion about control vs collaboration.
With perfect hindsight I wouldn't have posted it; especially since
we've learned that the demand for increased capacity from many people
was potentially less than completely earnest. (The whole, can't double
capacity until we quadruple it thing...)
and the concept of mining centralization on this list in the recent
I don't agree that you have; but for the purpose of this thread
doesn't really matter if I (specifically) do or don't agree.  It's an
objective truth that many people do not yet believe these concerns
have been addressed.
That Adam asked me to write publish a new "roadmap" for Bitcoin as
you've done here, with particular features and descriptions, which I
declined; and explained why I didn't believe that was the right
approach.  And Adam worked with you on the document, and provided
content for it (which I then recognized in the post).
Set aside what you know to be true for a moment and consider how this
might look to an outsider who found out about it.  It could look a
like Blockstream was trying to influence the direction of Bitcoin by
laundering proposals through an apparently unaffiliated third party.
Doubly so considering who participated in your drafting and who didn't
(more below).
I don't think in actuality you did anything remotely improper
(ill-advised, perhaps, since your goal to speak for developers but you
didn't speak to them, IMO--) but I think transparency is essential to
avoid any appearance of misconduct.
I did--
As Bryan pointed out... with the exception of the intro and end and a
couple sentences in the middle my entire response to your post, with
the position on "roadmaps" was written a long time ago, specifically
to complain about and argue against that particular approach.
I think you may be mistaking a roadmap with "communications"-- people
taking about research they are exploring is great! and we should have
more of it.  But like with RedHat and kernel features, we can't really
roadmap things that are unresourced and currently just unimplemented
exploration or test code-- without seeing things which are more or
less done the community can't rightfully decide if they'd want to
support something or not.  Perhaps they'll be good things perhaps
awful-- the devil is in the details, and until an effort is fairly
mature, you cannot see the details.
There have, for example, been signature aggregation proposals in the
past that required address reuse (could only aggregate if they're
reused).  I would strongly oppose such a proposal, and I hope everyone
else here would too.  So if I were a third party looking at your
message, rather than the person who initially proposed the agg sig
thing you're talking about, I wouldn't know if I could love it or hate
it yet; and probably couldn't be expected to have much of an opinion
on it until there is a BIP and/or example implementation.
To the extent that a roadmap differs from communications in general,
it is in that it also implies things that none of us can promise
except for our own efforts; Completion of implementations, success of
experiments, adoption-- basically assuming a level of top down control
that doesn't exist in a wide public collaboration.
One of the great challenges in our industry is that we don't have
rings of communication: We don't have much in the way of semi-experts
to communicate to semi-semi-experts to communicate to media pundits to
communicate to the unwashed masses-- at each level closing the
inferential gap and explaining things in terms the audience
understands. We don't have things like LWN.   We'll get there, but it
it took the Linux world decades to build to what it has now. I think
various forces in the industry work against us-- e.g. we lose a mot of
mid tier technical people to the allure of striking it rich printing
money in their own altcoins.
It might be attractive to try to end-run the slow development
appropriate web of reliable communications by deploying high authority
edicts, but it ultimately can't work because it doesn't map to how
things are accomplished, not in true collaborative open source, and
certainly not in an effort whos value comes substantially from
decentralization. Doing so, I fear, creates a false understanding of
(It's a common misunderstanding, for example, that people do what I
want (for example); but in reality, I just try to avoid wasting my
time advocating things that I don't think other people are already
going to do; :) otherwise, if _I_ want something done I've got to do
it myself or horse trade for it, just like anyone else.)
One of the great things about general communications is anyone can do
it.  Of course, unless they're talking about their own work, they
can't promise any of it will be completely-- but that is always true.
 If someone wants some guarantee about work, they have to be willing
to get it done themselves (and, of course, if it's a consensus
feature-- that much is necessary, but still not sufficient to get a
[from your other reply]
Come now, this is needlessly insulting. I would have made the same
comment if you had talked to me because you didn't talk to most/all of
Matt Corallo, Wladimir, Pieter Wuille, Alex Morcos, etc.... e.g. the
people doing most of the work of actually building the system.  Before
making that comment I went and checked with people to find out if only
I was left out.  Talking to Adam (who isn't involved in the project)
and Luke-jr (who is but is well known for frustratingly extreme
minority positions and also contracts for Blockstream sometimes) isn't
a great approach for the stated goal of speaking for developers!

@_date: 2017-07-12 03:33:47
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updating the Scaling Roadmap 
You've complained in this thread that Tao Effects' specific criticisms
were off-topic for the thread. I agree.
Yes, but apparently none of the most active developers or people
working on the proposals you named.  But you're fully entitled to
write about whatever you want while talking to whomever you want or
even without talking to anyone at all.
But, strategically it seems a little ill-advised.
For example, had you spoken to me I would have advised against the
dates offered for signature agg-- which would be more realistic for
publication of a complete proposal and implementation that the
community could finally have an opinion on, not for actual deployment;
and I probably would have discouraged highlighting compaction since we
haven't worked on that much since December due to other priorities.
(I also would have forwarded you my general concern about 'roadmaps'
as a communication tool.)
Maybe this could saved a bit of time and discussion, maybe not!
I apologize for causing you to feel anything was used against you.  I
don't support the roadmap-approach you propose here-- but my failure
to support it is definitionally non-personal according to the laws of
time and space: I wrote that opposition to other peoples similar
proposal some nine months ago, in private-- it has nothing to do with
you in a rather profound and physical sense.
To the extent that I criticize whom you talked to, it was intended to
be merely a remark on strategy: You yourself stated that "wrote the
roadmap to try to be representative of a Core / developer position",
but you didn't talk to the major developers or the authors of the
things you put on the roadmap--  there is /nothing improper/ or bad
about that... but I don't think it was good strategy. Feel free to
disagree, it was-- perhaps-- unsolicited advice.
It seems to me that your goal is creating more communication around
the clear set of obvious shared intentions; sounds great. Dressing it
as an official "roadmap" I think works counter to that purpose, and to
really be successful with the communications goal  I think it would be
best to go around privately polling major actors to find out what
they'd add or remove then take the intersection then spare everyone
the debate.  Not that debate isn't good, but we shouldn't shouldn't be
debating over an omnibus bill that needlessly ties things together,
people can debate each initiative on its own merits in its own
threads... the purpose was to communicate, right?  I do support that
goal, even though I don't think I support the current approach.
As before-- that is more unsolicited advice, feel free to ignore it.
Just keep in mind that no one owes anyone a response. I did take the
time to read and understand your message. I'm sorry that my response
isn't more to your liking. I'm thankful that you read and responded to
my reply.

@_date: 2017-07-13 01:04:19
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] how to disable segwit in my build? 
On Wed, Jul 12, 2017 at 6:17 AM, Dan Libby via bitcoin-dev
It is not simple to do so correctly, I couldn't tell you off the top
of my head; a number of things must be changed it isn't as simple as
disabling the activiation because of the segwit P2P changes.  Nor is
it a supported configuration. Even if it were now, it wouldn't be one
we'd continue to support in the future after segwit is active, as
we're likely to drop deployment/compat code.
Can you explain why you wish to do this?  It should have absolutely no
adverse impact on you-- if you don't use segwit, you don't use it-- it
may be the case that there is some confusion about the implications
that I could clear up for you... or suggest alternatives that might
achieve your goals.
Having a node that supports it won't make it more likely to activate,
you can mine without signaling segwit even on a node that supports it.
Your own transactions will not use segwit just because your node
supports it.
Effectively the only reason I'm aware of to intentionally not run with
segwit support beyond just not having upgraded yet, is a desire to try
to temporarily have as your tip block a block that was actively
stealing the segwit transactions of a third party. Which doesn't seem
either personally or publicly desirable; but I fully admit I may be
missing some good reason which I am not aware of.

@_date: 2017-06-02 02:28:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Proposal: Compact Client Side Filtering for 
On Fri, Jun 2, 2017 at 2:15 AM, Chris via bitcoin-dev
Really bad for privacy. Data for transactions at the tip is only
14kb/s-- potentially less if segwit is in use and you're not getting
witnesses. Is that really that burdensome?
FWIW, leaving a mobile browser just running while pointed at some
websites seems to use more traffic than that just loading advertising.

@_date: 2017-06-06 23:02:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable 
Please don't insult our community-- the issues with replay were
pointed out by us to Ethereum in advance and were cited specifically
in prior hardfork discussions long before Ethereum started editing
their ledger for the economic benefit of its centralized
The lack of extensive discussion on these issues you're seeing is
rather symptomatic of engineers that take stability seriously not
taking BIP148 seriously; not symptomatic of people not knowing about
them. The same concerns also applies to all these HF proposals (which
for some reason you don't mention), arguably even stronger.  The same
basic pattern exists: There are people that just don't care about the
technical issues who have made up their minds, and so you don't see
technical discussion.  Those people who do see the issues already
called out the proposals as being ill-advised.   Replay isn't even the
largest of the technical issues (network partitioning, for example, is
a much larger one).
BIP149 is arguably something of another matter in particular because
it has a time-frame that allows dealing with replay and other issues--
and particularly because it has a time-frame that can allow for the
avoidance of a meaningful fork at all.

@_date: 2017-06-07 21:41:36
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Proposal: Compact Client Side Filtering for 
I see the inner loop of construction and lookup are free of
non-constant divmod. This will result in implementations being
needlessly slow (especially on arm, but even on modern x86_64 a
division is a 90 cycle-ish affair.)
I believe this can be fixed by using this approach
   which has the same non-uniformity as mod but needs only a multiply
and shift.
Otherwise fast implementations will have to implement the code to
compute bit twiddling hack exact division code, which is kind of
complicated. (e.g. via the technique in "{N}-bit Unsigned Division via
{N}-bit Multiply-Add" by Arch D. Robison).
Shouldn't all cases in your spec where you have N=transactions be
n=indexed-outputs? Otherwise, I think your golomb parameter and false
positive rate are wrong.

@_date: 2017-06-13 01:00:50
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving 
I'm glad to see this out now, so I'm not longer invading the git repo
uninvited. :)
The description in the BIP appears inadequate:
For example, it's not clear if I can query for the existence of a
transaction by sending a conflict.  If this doesn't seem problematic,
consider the case where I, communicating with you over some private
channel, send you a payment inside a payment protocol message. You
announce it to the network and I concurrently send a double spend.
Only nodes that were part of the stem will reject my double spend, so
I just learned a lot about your network location.
It's also appears clear that I can query by sending an inv and
noticing that no getdata arrives.  An example attack in the latter is
that when I get a stem transaction I rapidly INV interrogate the
entire network and by observing who does and doesn't getdata I will
likely learn the entire stem path upto permutation.
The extra network capacity used by getdata-ing a transaction you
already saw via dandelion would be pretty insignificant.
I believe the text should be simplified and clarified so just say:
"With the exception of her behavior towards the peer sending in the
stem transaction and the peer sending out the transaction Alice's
behavior should be indistinguishable from a node which has not seen
the transaction at all until she receives it via ordinary forwarding
or until after the timeout." -- then its up to the implementation to
achieve indistinguishably regardless of what other protocol features
it uses.
I think avoiding the is the most sensible way; and from a software
maintenance perspective I expect that anything less will end up
continually suffering from serious information leaks which are hard to
avoid accidentally introducing via other changes.
The primary functionality should be straightforward to implement,
needing just a flag to determine if a transaction would be accepted to
the mempool but for the flag, but which skips actually adding it.
Handling chains of unconfirmed stem transactions is made more
complicated by this and this deserves careful consideration. I'm not
sure if its possible to forward stem children of stem transactions
except via the same stem path as the parent without leaking
information, it seems unlikely.
This approach would mostly take additional complexity from the need to
limit the amplification of double spends. I believe this can be
resolved by maintaining a per-peer map of the not yet expired vin's
consumed by stem fowards sent out via that peer. E.g. vin->{timeout,
feerate}.  Then any new forward via that stem-peer is tested against
that map and suppressed if it it spends a non-timed-out input and
doesn't meet the feerate epsilon for replacement.
Use of the orphan map is not indistinguishable as it is used for block
propagation, and itself also a maintenance burden to make sure
unrelated code is not inadvertently leaking the stem transactions.
The BIP is a bit under-specified on this transition, I think-- but I
know how it works from reading the prior implementation (I have not
yet read the new implementation).
The way it works (assuming I'm not confused and it hasn't changed) is
that when a new stem transaction comes in there is a chance that it is
treated as coming in as a normal transaction.
An alternative construction would be that when a stem transaction goes
out there is a random chance that the stem flag is not set (with
suitable adjustment to keep the same expected path length)
For some reason I believe this would be a superior construction, but I
am only able to articulate one clear benefit:  It allows non-dandelion
capable nodes to take on the role of the last stem hop, which I
believe would improve the anonymity set during the transition phase.
Has any work been given to the fact that dandelion propagation
potentially making to measure properties of the inter-node connection
graph?  e.g.  Say I wish to partition node X by disconnecting all of
its outbound connections, to do that it would be useful to learn whom
is connected to X.  I forward a transaction to X, observe the first
node to fluff it,  then DOS attack that node to take it offline.  Will
I need to DOS attack fewer or more nodes  to get all of X's outbounds
if X supports rapid stem forwarding?

@_date: 2017-06-13 18:11:39
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposal: Demonstration of Phase in Full Network 
Your English is much better than my Chinese.  Thank you for taking the
time to write this.
I am still reading and trying to completely understand your proposal
but I wanted to make one observation:
This is not true. Non-mining wallet nodes were considered, and their
upgrade practices are not usually slower than miners.
Even in the very first version of the software it did not mine unless
the user went into the settings and explicitly turned it on or used a
command-line option.  By default, every installation of Bitcoin was a
non-mining wallet node.
The enforcement of the system's rules by users broadly, and not just
miners, is specifically described in the white paper (section 8,
paragraph 2, it especially clear in the last sentence).  This is
critical for the security of Bitcoin especially with the current
degree of centralization in pools.  Without it, Bitcoin's security
would look a lot more like the Ripple system.
Frequently it is the miners that are "passive and lazy" in upgrading.
In some cases when new versions have had major improvements specific
to mining (such as for 0.8) miners upgraded much faster than other
nodes. But often, it is the other way around and miners adopt new
versions much slower than other nodes. If you look at block
construction today you will see that many miners are running highly
outdated node software which is more than one or even two years old.
(and as a result, they lose out on a considerable amount of
transaction fees.)
In fact, many miners have the most severe form of passive behavior:
they do not run a node at all but simply sell their hash power to
pools (which themselves are often slow to upgrade).  By comparison,
 95%
of reachable nodes are running software now from the last year and a
I do not, however, believe that it is a problem that anyone is slow to upgrade.
Reliability cannot be maintained in infrastructure if it is rapidly
changing.  A normal deployment process for major systems
infrastructure outside of Bitcoin usually takes months because time
must be given to test and find bugs.
Miners depend on their income from mining and interruptions can be
very costly.  Many pools are also involved with altcoins which are
constantly breaking and they have their attention directed elsewhere
and cannot quickly spare the time required to upgrade their software.
These delays are the natural consequence of a decentralized system
where no one has the power to force other people to adopt their
If you look at the deployment processes of major internet protocols,
HTTP2, new versions of SSH, BGP,  or IP itself you will find that
upgrades often happen slower than the entire life of Bitcoin so far--
and none of these protocols have the difficult consistency challenges
of Bitcoin or as much risk of irreparable financial loss if things go
Because many people in the Bitcoin community appears to expect
upgrades much faster than even centralized ISP backbones upgrade their
router software I think they have unrealistic expectations with how
fast upgrading can occur while preserving stability, security, and
decentralization and unrealistic expectations of how fast upgrading
will occur so long as no one has the ability to force other people to
run their upgrades.
I look forward to competing my understanding of your proposal.

@_date: 2017-06-19 22:41:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Proposal: Compact Client Side Filtering for 
On Mon, Jun 19, 2017 at 12:26 PM, bfd--- via bitcoin-dev
Sending just the output addresses of each transaction would use about
1 kilobit/s of data. Sending the entire transactions would use
~14kbit/sec data.  These don't seem to be a unsustainable tremendous
amount of data to use while an application is running.
Doubly so for SPV wallets which are highly vulnerable to unconfirmed
transactions, and many which last I heard testing reports on became
pretty severely corrupted once given a fake transaction.
Can someone make a case why saving no more than those figures would
justify the near total loss of privacy that filtering gives?
"Because they already do it" isn't a good argument when talking about
a new protocol feature; things which already do BIP37 will presumably
continue to already do BIP37.

@_date: 2017-06-20 21:49:50
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
Miners can simply continuing signaling segwit, which will leave them
at least soft-fork compatible with BIP148 and BIP91 (and god knows
what "segwit2x" is since they keep changing the actual definition and
do not have a specification; but last I saw the near-term behavior the
same as BIP91 but with a radically reduced activation window, so the
story would be the same there in the near term).
Ironically, it looks like most of the segwit2x signaling miners are
faking it (because they're not signaling segwit which it requires).
It'll be unfortunate if some aren't faking it and start orphaning
their own blocks because they are failing to signal segwit.
I don't think the rejection of segwit2x from Bitcoin's developers
could be any more resolute than what we've already seen:
On Tue, Jun 20, 2017 at 5:22 PM, Mark Friedenbach via bitcoin-dev
I think this is somewhat naive and sounds a lot like the repeat of the
previously debunked "XT" and "Classic" hysteria.
There is a reason that segwit2x is pretty much unanimously rejected by
the technical community.  And just like with XT/Classic/Unlimited
you'll continue to see a strong correlation with people who are
unwilling and unable to keep updating the software at an acceptable
level of quality-- esp. because the very founding on their fork is
predicated on discarding those properties.
If miners want to go off and create an altcoin-- welp, thats something
they can always do,  and nothing about that will force anyone to go
along with it.
As far as prevent a chain split goes, all those things
(148/91/segwit2x(per today)) effectively guarantee a chainsplit-- so I
don't think that holds.

@_date: 2017-06-20 22:34:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Miners forced to run non-core code in order to 
All versions of Bitcoin Core since 0.13.1 signal segwit, 0.14.1+ even
when downstream mining software doesn't support it.
I think it would be useful for there to exist a useful and trivial
patch against current (0.14.2) software to engage in the BIP91-like
orphaning, like people have provided for BIP148-- but right now I
don't see any specification of the behavior so it's unclear to me
_exactly_ what it would need to implement to be consistent.

@_date: 2017-06-28 04:07:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP: OP_BRIBVERIFY - the op code needed for Blind 
This is an absurd restriction-- I hope it was not your intent to
directly ban P2Pool and probably any other form of decentralized or
less centralized mining pooling... but thats what doing that does.
This removes important flexibility that was intentionally preserved.
What happens when an additional commitment is needed for bitcoin?
must some sidechain be irreparably destroyed? looks like it in  your
And what happens if index 1 isn't present? if index 35 is used must
there be 34 dummy outputs?
This is not monotone/reorg safe. It means that the output coins (if
any) are not equivalently fungible with other bitcoins (for, e.g. 100
blocks) because if there is a reorg this transaction cannot be
restored to the chain.  It's also impure and not compatible with
caching, which would be unfortunate and slow block propagation.

@_date: 2017-05-03 21:53:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Full node "tip" function 
The primary result would be paying people to sybil attack the network.
It's far cheaper to run one node behind thousands of IPs than it is to
run many nodes.
Suggestions like this have come up many times before.

@_date: 2017-05-08 23:56:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
Sergio, You've provided absolutely no information to qualify your
"should be".  It sounds like you are only measuring how much data is
witness vs non-witness while completely ignoring the relative cost of
UTXO bloat?  It's perfectly acceptable to increase the worst case in
one dimension while decreasing it in another-- and thats what segwit
This sounds like a misunderstanding of what the factors should have
accomplish. The non-witness factor should be as large as possible
because the prunable witness data has little to no long term cost to
the system, no cost to lite clients, etc-- as eventually the system's
survival will require transitioning to starting from a state snapshot.
But it cannot be too large because of the hyperbolic increase in worst
case bandwidth.   Also, when starting from a state snapshot security
will require starting from an old one-- otherwise the whole system
becomes much closer to SPV security, so the cost of witness data
between there and the tip will still matter.
If I had any leaning to adjust it, it would be towards five-- not
towards even lower values.
Where is it? (I have to say, I haven't found bitcoinj based things at
all readable but it would be worth seeing.)

@_date: 2017-05-09 19:30:52
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Some real-world results about the current Segwit 
And the UTXO bloat potential is twice as large and the cost of that
UTXO bloat is significantly reduced.  So you're basically gutting the
most of the gain from weight, making something incompatible, etc.
I'm not sure what to explain-- even that page on segwit.org explains
that the values are selected to balance worst case costs not to
optimize one to the total exclusion of others. Raw size is not very
relevant in the long run, but if your goal were to optimize for it
(which it seems to be), then the limit should be pure size.

@_date: 2017-05-11 18:17:19
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: NODE_NETWORK_LIMITED service bits 
It probably should be stated in terms of what you're promising to do--
 288 and 1152 blocks, not what we hope it will accomplish. Then advise
clients to use peers with headroom because their estimates could be
wrong and reorgs.
Reorgs aren't the only concerns that drive larger numbers:  The peak
at syncing is at ~24 hours, but sometimes there are quite a few more
than 144 blocks in 24 hours. Also, new blocks show up in the chain:
you think you're 144 behind but by the time you connect you find
you're 146 behind from that peer's perspective.
I think it's a bit ambiguous what it's saying about the headers,
especially because it goes into detail about address relay. I believe
nodes with any of these settings should be willing to serve headers
for their entire best chain.  Perhaps you could say that this is
equivalent to NODE_NETWORK except that they aren't necessarily willing
to server historical blocks.
I'm unsure about the third depth level. Perhaps that should be left
undefined for sending right now and treated as least 1152 blocks by
receivers-- I don't have any reason to think 7056 is a particularly
useful choice, and we'll need another (longer) level for UTXO based
sync.   You could probably go further and say that nodes shouldn't
send it now, but if sent it means they intend to keep 2016*2 blocks.
(Not sending because the requirement for sending it may be that the
node is able to send you a UTXO data feed.)
That isn't grammatical, you want "switching".  But I think it would be
better to say that when a node believe it is in sync enough to use
NODE_NETWORK_LIMITED_X it should just treat them identically to
NODE_NETWORK in peer selection.   We don't really need any more
topology distortion than that.   In particular, I don't want to be in
a case where NODE_NETWORK peers suddenly find themselves far less well
In terms of making room, a node network peer could choose to
disconnect the least useful peers that aren't syncing from them to
make more room for ones that are.  This lets them decide what
connections they want, based on how full they are and what is useful
to them, rather than finding themselves all lonely because nodes
decided to avoid them to be "helpful", and we already use
disconnections to manage fullness.
On Thu, May 11, 2017 at 3:13 PM, Jonas Schnelli via bitcoin-dev

@_date: 2017-05-12 02:22:15
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal: NODE_NETWORK_LIMITED service bits 
The instructions for relay addresses should not instruct you to relay
these addresses but rather that you should relay addresses you would
connect to, under the generalized assumption that if it is useful to
you it will be useful to others.
This avoids instructing someone who might not consider
non-node-network peers useful from being directed by the BIP to relay
things that they don't find useful. (In particular, it means that the
obvious implementation of just throwing out the 'useless' information
works fine.)  I think would better reflect what people are likely to
actually do.

@_date: 2017-05-15 23:30:59
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Validationless mining without transactions 
Today someone showed up on IRC suggesting a scheme for to improve the
ability of miners to mine without validation while including transactions
by shipping around an approximate sketch of the txins that were used by a
I pointed out that what sounded like the exact same scheme had been
previously proposed by Anthony Towns over a year ago,  that it turned out
that it didn't need any consensus changes, but also wasn't very attractive
because the actual transmission of the block (at least with FBRP or Fibre)
didn't really take any longer...  And, of course, mining without validating
does a real number on SPV security assumptions.
But then realized the the conversation between Anthony and I was offlist.
So-- for posterity...
I think the most interesting thing about this thread is that it gives a
concrete proof that a restriction on collecting transaction fees does not
discourage validationless mining; nor to other proposed consensus changes
make it any easier to include transactions while mining without validation.
Forwarded conversation
On Fri, Dec 04, 2015 at 08:26:22AM +0000, Gregory Maxwell via bitcoin-dev
Two thoughts related to this. Are they obvious or daft?
Would it make sense to require some demonstration that you've validated
prior blocks? eg, you could demonstrate you've done part of the work
to at least verify signatures from the previous block by including the
sha256 of the concatenation of all the sighash values in the coinbase
transaction -- if you'd already done the sig checking, calculating that
as you went would be pretty cheap, I think. Then make the rule be that
if you set the "validated" bit without including the demonstration of
validation, your block is invalid.
I guess this is more or less what Peter Todd proposed in:
It occurred to me while emailing with Matt Corallo, that it's probably
possible to make it easy to generate actually useful blocks while doing
validationless mining, rather than only creating empty blocks.
When creating a block, you:
   - calculate a fixed size (7168 bytes?) bloom filter of the
     prevouts that the block is spending
   - include the sha256 of the final bloom filter as the last output
     in the coinbase
   - enforce the inclusion of that sha256 by soft-fork
   - as part of fast relaying, transmit:
       - 80 byte block header
       - 7168 byte bloom filter
       - < 416 (?) byte merkle path to the coinbase
       - 64 byte sha256 midstate for coinbase up to start of the
         final transaction
       - < 128 byte tail of coinbase including bloom commitment
     (total of 7856 bytes, so less than 8kB)
I think that would be enough to verify that the proof-of-work is
committing to the bloom filter, and the bloom filter will then let
you throw out any transactions that could have been included in a block
built on block n-1, but can't be included in block n+1 -- whether they're
included in the new block, or would be double spends. So given that
information you can safely build a new block that's actually full of
transactions on top of the new block, even prior to downloading it in
full, let alone validating it.
I've run that algorithm over the last couple of weeks' worth of
transactions (to see how many of the next block's transaction would have
been thrown away using that approach) and it appeared to work fine --
it throws away maybe a dozen transactions per block compared to accurate
validation, but that's only about a couple of kB out of a 1MB block,
so something like 0.2%.  (I'm seeing ~4500 prevouts per block roughly,
so that's the error rate you'd expect; doubling for 2MB's worth of txes
with segwit predicts 3.5%, doubling again would presumably result in 14%
of transactions being falsely identified as double spends prior to the
block actually validating)
I haven't checked the math in detail, but I think that could reasonably
give an immediate 20% increase in effective blocksize, given the number of
empty blocks that get mined... (There were only ~1571MB of transactions
in the last 2016 blocks, so bumping the average from 780kB per block to
940kB would be a 20% increase; which would bring the 1.7x segwit increase
up to 2x too...)
Also, as far as I can see, you could probably even just have bitcoin core
transmit that 8kB of data around as part of propogating headers first.
Once you've got the new header and bloom filter, the only extra bit
should be passing both those into getblocktemplate to update the
previousblockhash and transaction selection. Both those together and it
seems like you could be mining on top of the latest block seconds after
it was found, just by naively running a bitcoin node?
I saw the "Bitcoin Classic" roadmap includes:
  "Implement "headers-first" mining. As soon as a valid 80-byte block
   header that extends the most-work chain is received, relay the header
   (via a new p2p network message) and allow mining an empty block on top
   of it, for up to 20 seconds."
which seems like the same idea done worse...
Any thoughts? Pointers to the bitcointalk thread where this was proposed
two years ago? :)
That information is easily shared/delegated... so it just creates
another centralized information source, and another source of
unfairness producing latency in the mining process. Without actually
preventing parties from mining. Doubly so in the context of how
validationless mining is actually done; the miners pull from other
miner's stratum servers; so they'll just see the commitments there.
So I don't see there being too much value there.
Pretty good incentive to not adopt the scheme, perhaps?
Moreover, this creates another way for a block to be invalid which has
no compact fraud proof. :(
I agree but:
I'm basically tired of repeating to people that there is no need for a
validationless block to be empty. So Yes, I agree with you on that
fact; it's possible for miners to do this already, with no protocol
changes (yes, it requires trusting each other but inherently
validationless mining already requires that). Miners only don't bother
right now because the funds left behind are insubstantial.
Its absolutely untrue that an empty block is not useful. Every block,
empty or not, mined against the best tip you know contributes to the
resolution of consensus and collapsing the network onto a single
state. Every block that was mined only after validating a block
amplifies security; by helping leave behind an invalid chain faster. A
block doesn't need to contain transactions to do these things.
FWIW, thats significantly larger than the amount of data typically
needed to send the whole block using the fast block relay protocol.
Your estimates are assuming the empty blocks come purely from
transmission and verification, but because most verification is cached
and transmission compressed-- they don't. There are numerous latency
sources through the whole stack, some constant some
size-proportional... the mining without validation achieves its gains
not from skipping validation (at least not most of the time); but
mostly from short cutting a deep stack with many latency sources;
including ones that have nothing to do with bitcoin core or the
Bitcoin protocol.
High hardware latency also amplifies short periods of empty block
mining to longer periods.
Perhaps most importantly, VFM mining avoids needing to identify and
characterize these other delay sources, by short cutting right at the
end no one needs to even figure out that their pool server is
performing a DNS request before every time it contacts their bitcoind
RPC or whatnot.
This BIP draft resulted in me relieving some pretty vicious attacks
from that community... funny.
Relevant to your interests: Lots of discussion on IRC.
Yeah, I thought about that. It's a tradeoff -- you definitely want the
validation to be easily "shared" in the sense that you want one validation
run to suffice for billions of mining attempts; and you probably want
it to be easy to compute when you receive a block, so you don't have
to revalidate the previous one to validate the new one... But you don't
want it to be so easily shared that one person on the planet calculates
it and everyone else just leeches from them.
I think you could make it hostile to accidental sharing by having it be:
   ;
  sha256(
      sha256( current block's first +1 coinbase outputs ;
               previous block's nonce )
      sha256( previous block's sighash values )
  )
If you skipped the internal sha256's (or just moved the nonce into the
final sha256), you'd be half-way forced to revalidate the previous block
every time you found a new block, which might be worthwhile.
Well, my theory was once you have validated the block, then the
demonstration is trivially easy to provide.
I was thinking that you could add a positive incentive by making validated
blocks count for something like 1.6x the chainwork for choosing which
chain to build on; so if you have a chain with 3 unvalidated blocks in
a row, then a chain with 2 validated blocks in a row instead would be
preferred for building your next block.
Hmmm. That's true. Is it true by definition though? If you're proving
you've validated 100% of a block, then is it even conceptually possible
to check that proof with less work than validating 100% of a block?
Sounds kind of SNARK-ish.
Oh, don't SNARKs (theoretically) give you a compact fraud proof, provided
the block size and sigops are bounded? The "secret" input is the block
data, public input is the block hash and the supposed validation proof
hash, program returns true if the block hash matches the block data,
and the calculated validation hash doesn't match the supposed validation
hash. Shudder to think how long generating the proof would take though,
or how hard it'd be to generate the circuit in the first place...
If you're only mining an empty block, the only way someone else can
cause you to waste your time is by wasting their own time doing PoW on
an invalid block. If you're mining a block with transactions in it, and
they can mine a valid block, but trick you into mining something that
double spends, then they can make you waste your time without wasting
their own, which seems like a much worse attack to me.
The advantage of the consensus enforced bloom filter is you don't have
to trust anything more than that economic incentive. However if you just
sent an unverifiable bloom filter, it'd be trivial to trick you into
mining an invalid block.
(If you already have the 1MB of block data, then extracting the prevouts
for use as a blacklist would probably be plenty fast though)
(Of course, maybe 90% of current hashpower does trust each other
anyway, in which case requiring trust isn't a burden, but that's not
very decentralised...)
(Paragraphs deleted. My maths is probably wrong, but I think it is
actually economically rational to mine invalid blocks as chaff to distract
validationless miners? The numbers I get are something like "if 40% of
the network is doing validationless mining for 20 seconds out of every
10 minutes, then it's profitable to devote about 2% of your hashpower to
mining invalid blocks". Probably some pretty dodgy assumptions though,
so I'm not including any algebra. But having actual invalid blocks with
real proof of work appear in the wild seems like it'd be a good way to
encourage miners to do validation...)
Hey, fees are almost 1% of the block payout these days -- that's within
an order of magnitude of a rounding error!
Yeah, I deleted "useless" for that reason then put it back in anyway...
Really? Hmm, if you have 2-byte indexes into the most likely to be mined
60k transactions, by 2000 transactions per block is about 4000 bytes. So
I guess that makes sense. And weak blocks would make that generalisable
and only add maybe a 32B index to include on the wire, presumably.
It'd only take a dozen missed transactions to be longer though.
Hmm, so my assumption is the "bitcoin core" side of the stack looks
something like:
   block header received by p2p or relay network
     V
   block data received by p2p or relay network
     V
   validation, UTXO set updates
     V
   getblocktemplate (possible tx ordering recalculation)
     V
   block header to do PoW on!
     V
   vary and push to miners over the network
     V
   push to ASICs
and the validationless "shortcut" just looks like:
   block header received by p2p or relay network
     V
   hack hack
     V
   new block header to do PoW on!
     V
   vary and push to miners over the network
     V
   push to ASICs
and so making the bitcoin core parts able to provide an unvalidated
header to push to miners/ASICs against "instantly" would be a win as
far as getting bitcoin proper back into the loop all the time... That
would mean removing validation from the critical path, and possibly more
optimisation of getblocktemplate to make it effectively instant too. But
those seem possible?
Having it be:
  header received by bitcoin core
    V
  new block header to do (unverified) PoW on!
    V
  ...
  header received by bitcoin core
    V
  block data received by bitcoin core
    V
  block data validated
    V
  new block header to do (verified) PoW on!
    V
  ...
with mining tools being able to just reliably and efficiently leave
bitcoin core in the loop seems like it ought to be a win to me...
At least with longpoll, doing a DNS query before connection shouldn't
I'm guessing you meant "receiving", which makes that a kinda weird
freudian slip? :) But yeah, technical consistency isn't something I've
seen much of from that area...
Tsk, 2 != 4...
Hmm, I'm not sure where this leaves my opinion on either of those ideas.
So I think there's two levels of withholding adversarial miners could
 - block withholding, so they have more time to build on top of their
   own block, maybe increasing their effective hashrate if they have
   above average connectivity
 - transaction withholding, so an entire block can be invalidated
   after the fact, hitting SPV nodes. if there are SPV miners, this can
   invalidate their work (potentially profitably, if you've accidently
   orphaned yourself)
You could solve transaction withholding for miners just by saying
"a PoW isn't valid unless the merkle tree is valid", that way you
can't retroactively invalidate a block, but then you need fast relay
before starting to mine, not just the header and some hint as to what
transactions might be included, and therefore the bloom filter idea
is pointless...
Having actually tried the relay network now, it seems like:
 a) it gets less coding gain than it theoretically could; the day or
    so's worth of blocks from Lightsword only seemed to be ~8x less data,
    rather than ~125x-250x, and what I'm seeing seems similar. So still
    room for improvement?
 b) using "weak blocks" as a way of paying for adding "non-standard"
    transactions (large, low fee, actually non-standard, etc) to the
    mempool seems workable to me; so long as the only reason you're doing
    weak blocks is so miners can ensure the transactions they're mining
    are in mempools, and thus that their blocks will relay quickly, the
    incentives seem properly aligned. (I think you'd want to distinguish
    txns only relayed because they have a weak block, just to be nice to
    SPV clients -- weak block txns might only be mined by one miner, while
    standard, fee paying transactions are being mined by all/most miners)
 c) it seems like it would be possible to adapt the relay protocol into
    a p2p environment to me? I'm thinking that you provide a bidirectional
    mapping for (a subset of) your mempool for each connection you
    have, so that you can quickly go to/from a 2-byte index to a
    transaction. If you make it so that whoever was listening gets to
    decide what transactions are okay, then you'd just need 9 of these
    maps -- 1 for each of your outgoing connections (ie, 8 total), plus
    another 1 that covers all your incoming connections, and each map
    should only really need to use up to about a meg of memory, which
    seems pretty feasible.  Maybe it means up to 8x5MB of your mempool
    is controlled by other people's policies rather than your own,
    but that doesn't seem to bad either.
 d) I'm a bit confused how it compares to IBLT; it seems like IBLT has
    really strong ordering requirements to work correctly, but if you
    had that you could compress the fast relay protocol really well,
    since you could apply the same ordering to your shared mempool, and
    then just send "next tx, next tx, skip 1 tx, next tx, next tx, skip
    3 tx, next tx, here's one you missed, ...", which with compression
    would probably get you to just a few /bits/ per (previously seen)
    transaction...  [0] [1]
 e) for p2p relay, maybe it would make sense to have the protocol only
    allow sending blocks where all the transactions are "previously
    seen". that way if you get a block where some txes haven't been
    seen before, you stall that block, and start sending transactions
    through. if another block comes in in the meantime, that doesn't
    have any new transactions, you send that block through straight away.
    that encourages sending weak blocks through first, to ensure your
    transactions are already in mempools and no one else can sneak
    in first.
Hmm... So that all seems kind of plausible to me; in how many ways am I
mistaken? :)
[0] A hard-fork change to have the block merkle tree be ordered by txid,
    and have the transactions topologically sorted before being validated
    would be kind-of interesting here -- apart from making sorting
    obvious, it'd make it easy to prove that a block doesn't contain a
    transaction. Bit altcoin-y though...
[1] Maybe having the shared mempool indexes be sorted rather than FIFO
    would make the data structures hard; I don't think so, but not sure.
Also called "selfish mining".
Right, this is how Bitcoin Core works (won't extend a chain it hasn't
validated)-- but some miners have shortcutted it to reduce latency.
(And not just bypassing validation, but the whole process, e.g.
transaction selection; which historically has taken more time than
It's pretty variable.  It depends a lot on consistency between the
transactions the server side selects and the client. When spam attacks
go on, or miners change their policy compression falls off until the
far end twiddles.
Go look at the distribution of the results.
That is a bit kludgy, but yes-- it would work.
But the key thing about latency minimization is that you _must_ send a
block with no request; because otherwise the RTT for just the request
alone will totally dominate the transfer in most cases.  And having N
peers send you the whole block redundantly ends up hurting your
performance (esp because packet losses mean more round trips) even if
the compression is very high.
All these problems can be avoided; at least in theory. Optimal latency
mitigation would be achieved by something like block network coding
With these techniques peers could blindly send you data without you
requesting it, while every byte they send would usefully contribute to
your reconstruction. With extra effort and opportunistic forwarding
the entire network could, in theory, receive a block in the time it
took the original host to send only one block, while making use of a
significant fraction of the network's whole bisection bandwidth.
Latency of block relay easily ends up CPU bound; even when not doing
anything too smart (this is why Matt's relay protocol stuff has AVX
sha2 code in it). Prior IBLT implementation attempts have performance
so low that their decode time ends up dwarfing transmission time, and
plain uncoded blocks are faster for common host/bandwidth
The ordering requirements stuff is not that relevant in my view; you
likely believe this because Gavin rat-holed himself on it trying to
spec out ordering requirements for miners...  The reality of it is
that a uniform permutation of, say, 4000 transactions can be stored in
log2(4000!)/8 bytes, or about 5.2kbytes (and this is easily achieved
just by using range coding to optimally pack integers in the range
[0..n_unpicked_txn) to pick transactions out of a lexagraphically
sorted list) ... and this is without any prediction at all-- randomly
ordered txn in the block would work just as well.
[E.g. using the uint coder from the daala video codec project can code
these values with about 1% overhead, and runs at about 12MB/sec doing
so on my slow laptop]
Recently some folks have been working privately on a block network
coding implementation... earlier attempts (even before IBLT became
trendy) were thwarted by the same thing that thwarts IBLT: the
decoding was so slow it dominated the latency. We've found some faster
coding schemes though... so it looks like it might be practical now. I
could send you more info if you read the block network coding page and
are interested in helping.
Both IBLT and BNC would both be more useful in the weakblocks model
because there the decode speed isn't latency critical-- so if it needs
100ms of cpu time to decode an efficiently encoded block, that is no
big deal.
Yes, it's perfectly reasonable to do that for bandwidth minimization--
though it doesn't minimize latency.  "Seen" is complex, you have no
guarantee a peer will accept any transaction you've sent it, or even
that it will retain any it sent you. So multiple round trips are
required to resolve missing transactions.
We haven't bothered implementing this historically because the
bandwidth reduction is small overall, and it's not the right strategy
for reducing latency-- the vast majority of bandwidth is eaten by
relay. Right now maybe 15% is used by blocks... so at most you'd get a
15% improvement here.
I did some fairly informal measurements and posted about it:
I also point out there that the existing blocksonly mode achieves
bandwidth optimal transport already (ignoring things like transaction
format compression)... just so long as you don't care about learning
about unconfirmed transactions. :)
If you sort by data (or ID) without requiring the verifier to
topologically sort then an efficient permutation coder would only
spend bits on places where dependencies push things out of the
expected order... which is fairly rare.
Seems like a reasonable cost for avoiding the hardfork, no? The
receiver topo sort requirement would also require more memory in a
block verifier; and would be more complex to fraud proof, I think.
Engineering wise it's not quite so simple. It's helpful for miners to
have blocks sorted by feerate so that later stages of the mining
process can drop the least profitable transactions simply by
truncating the block.
I tried to get Matt to do that for his stuff previously; pointing out
the sorted indexes would be easier to efficiently code. His
counterargument was that for 2000 txn, the two bytes indexes take 4kb,
which is pretty insignificant... and that his time would be better
spent trying to get the hit-rate up. I found that hard to argue with.
If the block can be encoded fully, then it's up to maybe 10kB per block
max (at 1MB blocksize); I don't think multiple transmissions matter much
in that case? Hmm, maybe it does...
Ugh, patents. Interesting that the patents on turbo codes have expired,
last time I looked they hadn't.
Yeah, that makes sense I think. Pretty complicated though. The "someone
sent corrupt data" seems a little bit problematic to deal with too,
especially in the "optimistically forward stuff before you can validate
it" phase. At least if you're using error correcting codes anyway,
that's probably a self-solving problem.
What's with the switch from 32 bit faux ids in the original section
to 63 bits in the reimagination? I guess you use most of that for the
additional encoded length though...
Keying with the previous block's hash seems kind-of painful, doesn't it?
Once you receive the ids, you want to lookup the actual transactions
from your mempool, but since you can't decrypt anything useful with
only the first 50/60 bits of cyphertext, the only way to do that is
to have already cycled through all the transactions in your mempool
and pre-calculated what their network coded id for that block is, and
you have to do that everytime you receive a block (including orphans,
I guess). It'd make reorgs more expensive too, because you'd have to
reindex all the mempool then as well?
Maybe if you're only doing that predictively it's not so bad? The 5MB-20MB
of txes with highest fees get coded up, and you just download any other
transactions in full? If you're downloading large coinbase txes regularly
anyway, that's probably no big deal.
Yeah, that seemed a little odd to me; there shouldn't be that much
hashing to validate a block (1MB of transactions, then maybe 128kB to
get to sha256d, then another 2*128kB for the rest of the merkle tree?).
Matt's code seems like it's doing a linear search through the tx index
to find each tx though, which probably doesn't help.
Right, but 5.2 kB is a lot of overhead; at least compared to the cases
where Matt's stuff already works well :)
Sure. (Though, fair warning, I've already failed a few times at doing
anything useful with erasure coding...)
The "p2p relay" in my head has "seen" meaning "the 5MB of transactions
the listening peer thinks is most likely to be mined", odds on both
peers have actually seen something like 145MB of additional transactions
too. You don't do round trips; you just start sending the "unseen"
transactions automatically (by id or in full?), then you send the
compressed block. The only round trip is if you sent the id, but they
actually needed the full tx.
In my head, you get good latency if you do weak blocks beforehand,
and somewhat poorer latency if you don't. Even in my head, I'm not sure
that's actually feasible, though: I'm not sure weak blocks for coinbase
transactions really work, and comparatively high latency on 5% of blocks
that didn't get any weak blocks beforehand isn't very attractive...
Yeah, I'm assuming a non-trivial increase in bandwidth usage compared
to current relay. Compared to relaying spam transactions (that don't
get mined prior to expiry), not sure it's significant though.
Really? I was seeing a lot of transaction chains in the couple of blocks I
looked at. Also, you wouldn't get short proofs that a transaction isn't
present in a block that way either afaics.
Hmm, I think it'd be easy to fraud proof -- just show adjacent merkle
paths where the results are in the wrong order. Maybe the same's true
with the id-order-but-toposorted too -- just show adjacent merkle paths
where the results are in the wrong order, and the later doesn't depend
on the former. I'm not sure that gives a unique sort though (but maybe
that doesn't actually matter).
Yeah; not having ordering requirements seems far more practical.
Yeah. Having the bitcoin mempool and fee info (and heck, priority info)
more readily available when seeing new transactions and choosing what to
include seems like it'd be helpful here. Seems relatively painful to do
that outside of bitcoin though.
I ran into someone proposing the same thing as you. Can I share this
discussion with them? (with the public?)
Yes, go ahead on both counts.
Sent from my phone.

@_date: 2017-05-15 23:59:58
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
Ya, lite nodes with UTXO sets are one of the the oldest observed
advantages of a commitment to the UTXO data:
But it requires a commitment. And for most of the arguments for those
you really want compact membership proofs.  The recent rise in
interest in full block lite clients (for privacy reasons), perhaps
complements the membership proofless usage.
Pieter describes some uses for doing something like this without a
commitment.  In my view, it's more interesting to first gain
experience with an operation without committing to it (which is a
consensus change and requires more care and consideration, which are
easier if people have implementation experience).
For audibility and engineering reasons it would need to be be in
addition to rather than rather than, because the proof of work needs
to commit to the witness data (in that kind of flip, the transactions
themselves become witnesses for UTXO deltas) or you get trivial DOS
attacks where people provide malleated blocks that have invalid

@_date: 2017-05-16 18:20:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Rolling UTXO set hashes 
The serialization of the txout itself is normative, but very minimal.

@_date: 2017-05-23 17:50:12
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP149 timeout-- why so far in the future? 
Based on how fast we saw segwit adoption, why is the BIP149 timeout so
far in the future?
It seems to me that it could be six months after release and hit the
kind of density required to make a stable transition.
(If it were a different proposal and not segwit where we already have
seen what network penetration looks like-- that would be another

@_date: 2017-11-02 23:37:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin Cash's new difficulty algorithm 
This is the bitcoin development mailing list, not the "give free
review to the obviously defective proposals of adversarial competing
systems" mailing list. Your posting is off-topic.

@_date: 2017-11-03 00:00:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin Cash's new difficulty algorithm 
I urge my colleagues here to not fall for the obvious xkcd386 bait.
The competitive advantage of prudence and competence is diminished if
competitors are able to divert our efforts into reviewing their

@_date: 2017-11-14 01:21:14
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updates on Confidential Transactions efficiency 
Jump to "New things here" if you're already up to speed on CT and just
want the big news.
Back in 2013 Adam Back proposed that Bitcoin and related systems could
use additive homomorphic commitments instead of explicit amounts in
place of values in transactions for improved privacy.     (
 )
We've since adopted the name 'confidential transactions' for this
particular approach to transaction privacy.
This approach makes transaction amounts private--known only to the
sender, the receiver, and whichever parties they choose to share the
information with through sharing watching keys or through revealing
single transactions. While that, combined with pseudonymous addresses,
is a pretty nice privacy improvement in itself, it turns out that
these blinded commitments also perfectly complement CoinJoin (
 ) by avoiding the
issue of joins being decoded due to different amounts being used. Tim
Ruffing and Pedro Moreno-Sanchez went on to show that CJ can be
dropped into distributed private protocols for CoinJoin (
 ) which
achieve the property where no participant learns which output
corresponds to which other participant.
The primary advantage of this approach is that it can be constructed
without any substantial new cryptographic assumptions (e.g., only
discrete log security in our existing curve), that it can be high
performance compared to alternatives, that it has no trusted setup,
and that it doesn't involve the creation of any forever-growing
unprunable accumulators.  All major alternative schemes fail multiple
of these criteria (e.g., arguably Zcash's scheme fails every one of
I made an informal write-up that gives an overview of how CT works
without assuming much crypto background:
The main sticking point with confidential transactions is that each
confidential output usually requires a witness which shows that the
output value is in range.  Prior to our work, the smallest range
proofs without trusted setup for the 0-51 bit proofs needed for values
in Bitcoin would take up 160 bytes per bit of range proved, or 8160
bytes needed for 51 bits--something like a 60x increase in transaction
size for a typical transaction usage.
I took Adam's suggestion and invented a number of new optimizations,
and created a high performance implementation. (
). With these optimizations the size is reduced to 128 bytes per two
bits plus 32 bytes; about 40% of the prior size.  My approach also
allowed a public exponent and minimum value so that you could use a
smaller range (e.g., 32 bits) and have it cover a useful range of
values (though with a little privacy trade-off). The result could give
proof sizes of about 2.5KB per output under realistic usage.  But this
is still a 20x increase in transaction size under typical usage--
though some in the Bitcoin space seem to believe that 20x larger
blocks would be no big deal, this isn't a view well supported by the
evidence in my view.
Subsequent work has been focused on reducing the range proof size further.
In our recent publication on confidential assets (
 ) we reduce the size to
96*log3(2)*bits + 32, which still leaves us at ~16x size for typical
usage. (The same optimizations support proofs whose soundness doesn't
even depend on the discrete log assumption with the same size as the
original CT publication).

@_date: 2017-11-14 10:38:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updates on Confidential Transactions efficiency 
I very strongly disagree with your strong disagreement.
Its important that people know and understand what properties a system has.
Perhaps one distinction you miss is that perfectly hiding systems
don't even exist in practice: I would take a bet that no software on
your system that you can use with other people actually implements a
perfectly hiding protocol (much less find on most other people's
system system :)).
In the case of practical use with CT perfect hiding is destroyed by
scalability-- the obvious construction is a stealth address like one
where a DH public key is in the address and that is used to scan for
your payments against a nonce pubkey in the transactions.   The
existence of that mechanism destroys perfect hiding.  No scheme that
can be scanned using an asymmetric key is going to provide perfect
Now, perhaps what you'd like is a system which is not perfect hiding
but where the hiding rests on less "risky" assumptions.  That is
something that can plausibly be constructed, but it's not itself
incompatible with unconditional soundness.
As referenced in the paper, there is also the possibility of having a
your cake and eating it too-- switch commitments for example allow
having computational-hiding-depending-on-the-hardness-of-inverting-hashes
 (which I would argue is functionally as good as perfect hiding, esp
since hiding is ultimately limited by the asymmetric crypto used for
discovery)  and yet it retains an option to upgrade or block spending
via unsound mechanisms in the event of a crypto break.
Sounds like you are assuming that you know when there is a problem, if
you do then the switch commitments scheme works and doesn't require
any selling of anything. Selling also has the problem that everyone
would want to do it at once if there was a concern; this would not
have good effects. :) Without switch commitments though, you are just
hosed.  And you cannot have something like switch commitments without
abandoning perfect hiding ( though you get hiding which is good enough
(tm), as mentioned above).
Miners to reduce coin supply, enhancing the value of their own
holdings, by simply not letting near-expiry ones get spent...
(This can be partially mediated by constructing proofs to hide if a
coins in near expiration or not.)
Yes, that they can do-- though with the trade-offs inherent in that.
It is worse than what you were imagining in the Bitcoin case because
you cannot use one or two time-ordered trees, the spent coins list
needs search-insertion; so maintaining it over time is harder. :(  The
single time ordered tree trick doesn't work because you can't mutate
the entries without blowing anonymity.
I think it's still fair to say that ring-in and tree-in approaches
(monero, and zcash) are fundamentally less scalable than
CT+valueshuffle, but more private-- though given observations of Zcash
behavior perhaps not that much more private.  With the right smart
tricks the scalablity loss might be more inconvenient than fatal, but
they're still a loss even if they're one that makes for a good
As an aside, you shouldn't see Monero as entirely distinct now that
we're talking about a framework which is fully general:  Extending
this to a traceable 1 of N input for monero is simple-- and will add
size log() in the number of ring inputs with good constant factors.
One could also store inputs in a hash tree, and then have a
bulletproof that verified membership in the tree.  This would provide
tree-in style transactions with proofs that grow with the log() of the
size of the tree (and a spent coins accumulator); the challenge there
would be choosing a hash function that had a compact representation in
the arithmetic circuit so that the constant factors aren't terrible.
Effectively that's what bulletproofs does:   It takes a general scheme
for ZKP of arbitrary computation, which could implement a range proof
by opening the commitments (e.g. a circuit for EC point scalar
multiply) and checking the value, and optimizes it to handle the
commitments more directly. If you're free to choose the hash function
there may be a way to make a hash tree check ultra efficient inside
the proof, in which case this work could implement a tree-in scheme
like zcash-- but with larger proofs and slower verification in
exchange for much better crypto assumptions and no trusted setup.
This is part of what I meant by it opening up "many other interesting
But as above, I think that the interactive-sparse-in (CJ) has its own
attractiveness, even though it is not the strongest for privacy.

@_date: 2017-11-14 10:51:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Updates on Confidential Transactions efficiency 
While I'm enumerating private transaction topologies there is fourth
one I'm aware of (most closely related to ring-in):
take N inputs,  write >= N outputs,  where some coins are spent and
replaced with a new output, or an encrypted dummy... and other coins
are simply reencrypted in a way that their owner can still decode.
Provide a proof that shows you did this faithfully. So this one avoids
the spent coins list by being able to malleiate the inputs.
We never previously found an efficient way to construct that one in a
plain DL setting, but it's probably possible w/ bulletproofs, at least
for some definition of efficient.

@_date: 2017-11-20 18:59:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why SegWit Anyway? 
That is effectively what segwit does, upto engineering minutia and
compatibility details.
Segwit does not serialize transactions in to a data structure where
signatures are separated from the rest of the transaction data; this
is a misunderstanding.  The "segregated" refers to them being excluded
from the TXID.   The serialization of segwit on the p2p network in
transactions and in blocks encodes the witness field inside the
transactions, immediately prior to the nlocktime field.

@_date: 2017-11-21 18:45:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP159 - NODE_NETWORK_LIMITED service bits, 
With the way pruning works today my expirence is that virtually no one
sets any parameter other than the minimum, though even with that set a
few more blocks can be available.
In the future we would set further pruning identifying bits, with
those set node would (obviously) answer for their blocks.  An earlier
version of this BIP had such a bit defined but it appeared that we
lacked sufficient experience from practice to usefully specify what
height it should mean exactly and the proposals sounded like they
would likely interact poorly with other future proposals, so we
thought it better to delay defining any additional levels for the
Part of your concern is mooted by the logistics of actually fetching
those additional blocks.  At least in the network today we have a
superabundance of nodes that serve anything, to handle them being rare
will require very different approaches than we have now.  We have no
reason to believe that "like the pruning thing but more blocks" is
actually all that useful-- and some reason to expect that its not:
once you go back more than a handful of weeks the probably of fetching
get pretty close to uniform, those fetches are only be newly
initializing nodes that need all the blocks.
On Tue, Nov 21, 2017 at 2:03 PM, Sjors Provoost via bitcoin-dev

@_date: 2017-11-30 09:12:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Idea: Marginal Pricing 
This idea presumes that the protocol has any ability to regulate fees. I
believe the locally optimal strategy for both miners and payers alike is to
accept (pay) zero fees natively in the protocol and instead accept (pay)
their actual fees out-of-band or via OP_TRUE outputs which the miner can
simply collect.  Then the miner sets the fee threshold to ~0 and selects
transactions on the basis of out of band fees.
Miners today already accept out-of-band fees, and as far back as at least
2011 there were miners that would also accept fees in the form of
additional transaction outputs which they were able to spend.

@_date: 2017-11-30 11:40:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP Idea: Marginal Pricing 
Out of band fees are a reality even today-- and have for most of
Bitcoin's life--, without a system that has any particular incentive
for them.
This kind of analysis seems to imagine that a single decision maker is
making a globally optimal decision and that also people are somehow
forced to only make one choice. Both are untrue in this case (and most
other economic circumstances). Instead, participants can take the best
of multiple choices and will often act locally for their own best
interest even when it reduces revenue for their industry in total.
Concretely: as a user with competent wallet software, I would be
automatically drafting two transactions-- one paying OOB and one
paying min fee, at equivalent expected rates.  Miners would construct
blocks which locally maximized their revenue.   It is far from clear
that use of the minfee scheme is an equilibrium-- in fact I think it
is clearly not an equilibrium: a user that writes both transactions
will always pay equal or less fees for the transactions where they do
both (even if all users doing this causes users collectively to pay
higher fees), a miner who considers both will always make equal or
greater fee income on a block by block basis (even if it lowers miner
income collectively when all do this).
(If it were in fact preferred by users and miners alike: why doesn't
it already exist?  Since the existence of OOB fees cannot be
eliminated, as far as we know, any use of MINFEE would be inherently
voluntary-- so in one sense we already 'have' voluntary minfee, but no
one uses it.)
Ignoring the possibility of evasion, there are some other concerns
that you might want to consider:
I believe the idea converts variance in fee willingness into variance
in capacity for the network.  If rich uncle bill wants to waste money
with uneconomically high fees, with a constant flow of transactions,
he'll effectively knock out a large number of participants.  You could
argue that bill could spend those same fees in spam to displace the
same amount of transactions while also using more capacity; but I
UncleBill isn't trying to attack the capacity of the system. It's just
collateral damage.  I worry also about related strategies that arise
in that world: For example, lets imagine that world consisted of a
couple unclebill who will pay high fees, and the unwashed masses that
will not and pay a much lower consistent feerate.   Honest conformance
with your protocol would result in miners either processing only the
UncleBill txn or processing all of them at the lower rate, whichever
is more profitable.  Super-rational behavior might be for a majority
of hashpower to collude to only permit high fee-rate transactions
every other block and only permit low feerate in the others, and then
the network processes all unclebills in one block (at full rate), and
all the unwashed in the others.  From a fee perspective it arguably
isn't any worse than today, but I believe it significantly handicaps
your capacity limiting argument.
Nor does a MINFEE system; since the user can near costlessly construct
as many variations of their transaction as they like.
That is unfortunately not the reality of Bitcoin today.

@_date: 2017-10-30 23:29:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Simplicity: An alternative to Script 
For some framing-- I think we're still a long way off from proposing
something like this in Bitcoin, and _how_ it's ultimately proposed is
an open question.
There are many ways to use simplicity, for an extreme example:  one
could define a collection of high level operations and combinators at
the level of things in Bitcoin Script (op_sha256, op_equal, op_cat,
etc.)  and make an interpreter that implements these operations as
discounted jets and ONLY these operations at all.
At that point you have a system which is functionally like Bitcoin
Script-- with the same performance characteristics-- but with a pretty
much perfectly rigorous formal specification and which is highly
amenable to the formal analysis of smart contracts written in it.
At the other extreme, you expose a full on Bitmachine and allow
arbitrary simplicity--  But this is probably slow enough to not be
very useful.  Simplicity itself is so simple that it doesn't natively
have a concept of a _bit_, library code programs the concept of a bit,
then the concept of a half adder ... and so on.   As a result a
completely unjetted implementation is slow (actually remarkably fast
considering that it's effectively interpreting a circuit constructed
from pure logic).
The most useful way of using it would probably be in-between: a good
collection of high level functions, and mid-level functions (e.g.
arithmetic and string operations) making a wide space of useful but
general software both possible and high performance.  But to get there
we need enough experience with it to know what the requisite
collection of operations would be.
One challenge is that I don't think we have a clear mental model for
how nominal validation costs are allowed to be before there is a
negative impact.  It's probably safe to assume 'pretty darn nominal'
is a requirement, but there is still a lot that can be done within
that envelope.
As far as consensus discounted jets goes:
Is a particular script-root jetted or not in an implementation?
 -- In and of itself this is not of consensus consequence; esp.
because a major design feature of simplicity is that it should be
possible using to prove that an optimized C implementation of a
simplicity program is complete and correct (using VST+COQ).
Is a particular script-root 'standard and known' in the P2P network:
 -- This means that you can skip communicating it when sending
witnesses to peers; but this is something that could be negotiated on
a peer by peer basis-- like compressing transactions, and isn't at all
consensus normative.
Is a particular jet discounted and what are the discounts:
 -- This is inherently a consensus question; as the bitmachine costing
for a program is consensus normative (assuming that you allow
arbitrary simplicity code at all).
A script-versioning like mechanism can provide for a straight-forward
way to upgrade discounted cost tables in a compatible way--  if you're
running old software that doesn't have the required jets to justify a
particular discount collection -- well that's okay, you won't validate
those scripts at all. (so they'll be super fast for you!)
Another potential tool is the idea of sunsetting cost limits that
sunset; e.g. after N years, the limits go away with an assumption that
updated limits have been softforked in that ativate at that time and
themselves expire in N years.  Old software would become slower
validating due to newly discounted code they lack jets for... but
would continue validating (at least until they run out of performance
This is theoretically attractive in a number of regards, but
unfortunately I think our industry hasn't shown sufficient maturity
about engineering tradeoffs to make this a politically viable choice
in the mid-term-- I known I'm personally uncomfortable with the
outspokenness of parties that hold positions which I think can fairly
be summarized "We should remove all limits and if the system crashes
and burns as a result, we'll just make a new one! YOLO.". But it's
interesting to think about in the long term.
There are also hybrid approaches where you can imagine this decision
being made by node operators, e.g. continuing to validate code that
exceeds your effort limits on probabilistic and best effort basis;
even more attractive if there were a protocol for efficiently showing
others that an operation had an invalid witness. Though there is a lot
to explore about the brittleness to partitioning that comes from any
expectation that you'd learn about invalid updates by exception.
In any case, these are all options that exist completely independently
of simplicity.  I think we should think of simplicity as a rigorous
base which we could _potentially_ use to build whatever future
direction of script we like out of... by itself it doesn't mandate a
particular depth or level of adoption.
And for the moment it's still also mostly just a base-- I don't
anticipate typical smart contracting end users programming directly w/
simplicity even if Bitcoin did support arbitrary simplicity--  I
expect they'd program in user friendly domain specific languages which
are formally tied to their implementations in simplicity that allow-
but do not force- closed loop formal reasoning about their contracts
all the way from their high level business rules straight through to
the machine code implementing the interpreter(s) that run in the
But to get there we'll have to prove in practice that this is actually
workable. We have some evidence that it is,  e.g. Roconnor's SHA2
implementation in simplicity is proven to implement the same function
that a C implementation implements (via the compcert formalization of
C).  but there will need to be more.

@_date: 2017-09-11 18:29:46
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
On Mon, Sep 11, 2017 at 5:43 PM, Daniel Stadulis via bitcoin-dev
This assumes the states are discernible.  They often aren't cleanly.
You obviously know how bad it is in the best case, but the worst could
be much worse.
I've multiple time seen a hard to exploit issue turn out to be trivial
when you find the right trick, or a minor dos issue turn our to far
more serious.
Simple performance bugs, expertly deployed, can potentially be used to
carve up the network--- miner A and exchange B go in one partition,
everyone else in another.. and doublespend.
And so on.  So while I absolutely do agree that different things
should and can be handled differently, it is not always so clear cut.
It's prudent to treat things as more severe than you know them to be.
In fact, someone pointed out to me a major amplifier of the
utxo-memory attack thing today that Bitcoin Core narrowly dodges which
would have made it very easy to exploit against some users, and which
it seems no one previously considered.
I also think it's somewhat incorrect to call this thread anything
about disclosure, this thread is not about disclosure. Disclosure is
when you tell the vendor.  This thread is about publication and that
has very different implications. Publication is when you're sure
you've told the prospective attackers.

@_date: 2017-09-12 17:41:42
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
On Tue, Sep 12, 2017 at 3:37 AM, Anthony Towns via bitcoin-dev
For embargoed fixes we test the specific fixes against experienced
developers inside the project, handing them the proposed commit and
informing them that it fixes a vulnerability and asking them to
identify it.
This does not guarantee that the fix won't leak the issue, but in
virtually all cases in the past the issues we've dealt with would not
be made worse off being leaked in that way vs just making it public
If we had an issue that would be-- e.g. an RCE that could lead to
private key theft, we would likely handle it differently (e.g. making
a public notice to take sensitive systems offline before attempting
any fix).
History does not support your assumptions.
Not really.  Any forced change still creates centralization,
dependence, and an opportunity for insecurity.
That is a concern too, but our bar for backport fixes is low enough
that they're often able to include more serious fixes without calling
attention to them.
This is true even outside of the consensus critical parts.  In the P2P
network other people upgrading can be protective.
Sure, a few have. Most do not because they are either not focused on
software quality or consider themselves as having an adversarial
relationship with Bitcoin.
If you'd like to provide the sort of valuable information to the
market which may get you sued or targeted for harassment of physical
attack-- feel free. Don't ask the rest of us to do so.

@_date: 2017-09-12 17:57:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Responsible disclosure of bugs 
On Tue, Sep 12, 2017 at 4:49 AM, Sergio Demian Lerner via bitcoin-dev
I agree with your post, but wanted to make a point of clarification on
the use of "can't".
If someone wants to report something to the Bitcoin project we're
obviously at your mercy in how we handle it. If we disagree on the
handling approach we may try to talk you into a different position
based with a rational judgement based on our experience (or, if
justified, advice that we're likely to whine about your approach in
public). But if you still want to go also report a common issue to
something else with a different approach then you can. Even our
ire/whining can be avoided by a sincere effort to communicate and give
us an opportunity to mitigate harm.
That said, as mentioned, we'd encourage otherwise for issues that
warrant it-- and I think with cause enough that the reporter will
agree. So that is a different kind of "cant". :)
In Bitcoin the overwhelming majority of serious issues we've
encountered have been found by people I'd consider 'inside the
project' (frequent regular contributors who aren't seriously involved
in other things).  That hasn't been so obviously the case for other
open source projects that I've been involved with; but Bitcoin is
pretty good from a basic security perspective and finding additional
issues often requires specialized experience that few people outside
of the project regulars have (though some, like Sergio, clearly do).
I know through direct experience that both Mozilla and the Chrome
project fix _serious_ (like RCE bugs) issues based on internal
discoveries which they do not make public (apparently ever), though
they may coordinate with distributors on some of them.   (Some of
these experiences are also why I give the advice that you should not
consider any computer which has ever run a web browser to be strongly

@_date: 2017-09-13 09:34:37
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] SF proposal: prohibit unspendable outputs with 
On Wed, Sep 13, 2017 at 9:24 AM, Peter Todd via bitcoin-dev
I agree that dropping zero value outputs is a needless loss of
flexibility.  In addition to the CT example, something similar could
be done for increased precision (nanobitcoin!).
Maybe if in the future the value of 1e-8 btc is high enough then an
argument could be made that requiring one is a meaningful reduction in
a miner's ability to spam up the network... but the argument doesn't
fly today... the cost in lost fee income from the spam just totally
dwarfs it.

@_date: 2017-09-13 09:39:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Minutia in CT for Bitcoin. Was: SF proposal: prohibit 
Can we solve the problem that pool inputs are gratuitously non-reorg
safe, without creating something like a maturity limit for shielded to
So far the best I have is this:  Support unshielded coins in shielded
space too. So the only time you transition out of the pool is paying
to a legacy wallet.  If support were phased in (e.g. addresses that
say you can pay me in the pool after its enabled), and the pool only
used long after wallets supported getting payments in it, then this
would be pretty rare and a maturity limit wouldn't be a big deal.
Can better be done?

@_date: 2017-09-27 22:06:58
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin Assistance 
As the author of that comment: the reference there is unrelated to the
code but just a found-on-the-internet explanation for an trivial, old,
and well known technique (which I've seen in code since at least the
80s) that manages to sometimes surprise people who aren't familiar
with fixed point signal processing.  I wrote and submitted the comment
after encountering people confused by our code in another project,
long after it was written.

@_date: 2017-09-27 22:21:12
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Bitcoin Assistance 
On Wed, Sep 27, 2017 at 9:54 PM, Tim Ruffing via bitcoin-dev
Relicensed by the copyright holder:
(For future reference, git log -p  makes it easy to go find
where some string was last in a file, so you can look at the commit
that changed it.)

@_date: 2017-09-28 00:22:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Address expiration times should be added to 
So every wallet needs all the addresses ever used and a fast index into them?
This seems pretty harmful for scalability.
So you propose a best practice that requires contacting a service and
telling them what addresses you're planning on paying?  This seems
pretty harmful for privacy.

@_date: 2017-09-28 00:58:30
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Address expiration times should be added to 
When Pieter and I were working on Bech32 we specifically designed for
error correcting codes that had good performance for longer lengths
than we technically needed specifically to incorporate things like
dates and explicit amounts.
(explicit amounts so that typos and bit flips in amounts displayed or
in memory couldn't result in sending the wrong amount)
But we also thought that also adding those features at the same time
would retard adoption-- both due to debating over the encodings and
because handling would result in different software requirements and
layering, so you couldn't just drop them in.
Doubly unfortunately, people have even deployed BIP173 already (prior
to it even having much peer review or being adopted by its own
authors), so I think a rethink now wouldn't be timely (I mean as a
replacement to BIP173 rather than an additional format). :(
But I do support the idea.
One thing to keep in mind is that address format linked fields are
most efficient if they're multiples of 5 bits.  Perhaps use 1 bit to
indicate an embedded amount and 19 bits of 1 day precision, resulting
in a 1435 year span.
Keep in mind that high precision of the expiration times is asking the
sender to have a higher precision of idea of the time, date only is
kinda nice.  I think shorter expiration times are unlikely to be
useful due to clock skew-- you can't assume a signer has any access to
the Bitcoin network at all.

@_date: 2017-09-29 02:06:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Address expiration times should be added to 
I ask you to pay 1 Bitcoin to bc1blahblah.
...you make a typo, or a poorly placed cosmic ray switches it in your
ram to bc1blohblahbah.   No problem, it'll get rejected. (even if the
cosmic ray happens just before signing... if the software is robust
it'll reencode from the signed transaction and check against the
original input.
But if instead the typo converts it to 2 Bitcoins or the cosmic ray
converts it to 2.34217728... the payment will happily go through,
assuming your wallet had enough, and you're stuck asking me to refund
you the excess.
Sure, you can put amounts in URIs and whatnot, but they're not error
protected... so there will always be unprotected poritons where a
glitch can radically change the amount.
In many cases you know exactly what amount you're asking for when you
generate an address. There isn't any reason the amount couldn't be
covered by the addresses checksum in those cases.
There are a couple ways of doing that... e.g. adding it explicitly,
where the checksum includes it but not the address itself; so it
errors out if you get it wrong. But this is unfortunate because it
can't tell you the expected amount when its wrong.   Another way would
be to embed the amount in the address, and then the software can tell
you the amount the address was expecting and not let you proceed until
they match.

@_date: 2017-09-29 17:25:18
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Address expiration times should be added to 
On Fri, Sep 29, 2017 at 12:45 PM, Andreas Schildbach via bitcoin-dev
Who's payment protocol SSL cert was expired for months without even
generating a post on reddit.  Not exactly convincing there.
The fact that someone supports it doesn't mean its being used.

@_date: 2017-09-30 00:47:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
If I'm not mistaken that is is nothing new or interesting: You can
delay some transaction by paying more than it offered by every block
you delay it from. E.g. if the next full block would pay 0.8 BTC in
fees, you just need to make transactions paying more than that. But
you'll pay it for each delay and the people you push out only pay once
(when they are successful), so it gets awfully expensive fast.
(Arguably the monopoly price model is better because outbidding party
doesn't need to bloat the chain to do their thing; arguable its
somewhat worse because its harder to do by accident.)
My thought on this was the same as PT's initial: miners and users can
arrange OOB payments (and/or coinjoin rebates) and bypass this. I
don't see why it wouldn't be in their individual best interest to do
so, and if they do that would likely be a centralizing effect.

@_date: 2017-09-30 08:54:57
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Rebatable fees & incentive-safe fee markets 
On Sat, Sep 30, 2017 at 3:55 AM, Jorge Tim?n via bitcoin-dev
I think this is not relevant: By paying the same amount you can delay
the same transactions today.
The difference is that your 'attack' wastes less capacity-- it can be
a simple 150 weight txn rather than a collection that add up to almost
4 million weight; but it costs exactly the same.  To the extent that
this difference matters at all, I think it's an improvement.
The only argument that I see for it not being one is that it's easier
to do accidentally.  But part of the purpose of this alternative
market is to achieve an equilibrium other than the ultrabloating one;
so yes, you're going to find outcomes where the blocks are not
maximally full.
I wonder how the economics would respond if there is a PI controller
on the maximum size, so that 'lost space' in single blocks with bogon
fee transactions could be recovered if doing so didn't change the
medium timescale total. I think the paper's analysis assumes there is
no limit, but that is impractical for technical reasons (e.g. making
it impossible to budget communications and storage capacity for

@_date: 2018-08-06 02:15:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Capping the size of locators [trivial protocol change 
Coinr8d posted on bct that the node software would process large
locators limited only by the maximum message size yet sensible usage
of locators only results in messages of log2(n_blocks) size. He was
concerned that it might be a DOS vulnerability but quick measurements
indicated to me that it likely wasn't worse than many other protocol
messages.  It still seems silly to allow absurd locators. So I propose
that the size of locators be limited.
However, capping them is a P2P change that could potentially result in
network splits if older nodes would potentially produce larger
locators (esp if triggered to produce unexpectedly large ones by
forks).  A quick survey of node software indicated that no software I
could find would ever produce a locator with more than 42 hashes
before encountering other limits, so I think a limit of 64 will be
automatically compatible with all or virtually all nodes on the
I'm bothering writing a BIP because there might be some naive
implementation lurking out there that sends a crazy number due to
sub-exponential backoff that would be broken by nodes enforcing a
limit... particularly since the correct use of locators was never
previously mandated and might not be obvious to all developers.
I take the opportunity to also specify that the locators be correctly
ordered in terms of total work, but  don't specify that they all come
from the same chain.
This document proposes limiting the locator messages used in the getblocks
and getheaders to 64 entries and requiring that be ordered by total
This document is licensed under the 2-clause BSD license.
The Bitcoin P2P protocol uses a simple and efficient data structure
to reconcile blockchains between nodes called a locator.  A locator
communicates a list of known hashes which allows a peer to find a
recent common ancestor between the best chains on two nodes.  By
exponentially increasing the space between each entry, the locator
allows a log() sized message to find the difference between two nodes
with only a constant factor overhead.
Because short forks are much more common than long forks the typical
usage of the locator includes a small number of topmost hashes before
switching to exponential spacing.
The original Bitcoin implementation provided no explicit limit to the
number of hashes in a locator message, allowing for absurd and
wasteful uses like including
all hashes in a chain.
Although locators are very inexpensive for existing node software to
process there is no known utility for sending very large locators.
To reduce the worst case cost of processing a locator message it would
be useful if the size of locator messages were strictly
bounded to sensible levels.
Common implementations have implicit limitations of 2^32 blocks and an
exponent of 2 after the first 10 locators and so could never request
more than 42 hashes in any case.
== Specification ==
A locator included in a getblock or getheaders message may include no more
than 64 hashes, including the final hash_stop hash. Additionally, the blocks
referenced by the locator must be in order of equal or decreasing total
Sending a locator that violates these requirements may result in normal
processing, the message being ignored, a disconnection, or a ban.
Implementations that seek to handle larger numbers of blocks than afforded
by this limit with an exponent of 2 can adaptively switch to a larger
exponent as required to stay within the limit.
== Acknowledgements ==
Thanks to Coinr8d on bitcointalk for pointing out that node software would
process and respond to locators with about 125,000 hashes in them.

@_date: 2018-08-13 20:39:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Witness serialization in PSBT non-witness UTXOs 
An alternative is to require reading either or but also require
writing without the witness.  It's likely that two years from now,
nothing will write the witnesses, and the requirement to support
reading them could be dropped.
On Mon, Aug 13, 2018 at 8:32 PM Achow101 via bitcoin-dev

@_date: 2018-08-20 20:14:50
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Getting around to fixing the timewarp attack. 
Since 2012 (IIRC) we've known that Bitcoin's non-overlapping
difficulty calculation was vulnerable to gaming with inaccurate
timestamps to massively increase the rate of block production beyond
the system's intentional design. It can be fixed with a soft-fork that
further constraints block timestamps, and a couple of proposals have
been floated along these lines.
I put a demonstration of timewarp early in the testnet3 chain to also
let people test mitigations against that.  It pegs the difficulty way
down and then churned out blocks at the maximum rate that the median
time protocol rule allows.
I, and I assume others, haven't put a big priority into fixing this
vulnerability because it requires a majority hashrate and could easily
be blocked if someone started using it.
But there haven't been too many other network consensus rules going on
right now, and I believe at least several of the proposals suggested
are fully compatible with existing behaviour and only trigger in the
presence of exceptional circumstances-- e.g. a timewarp attack.  So
the risk of deploying these mitigations would be minimal.
Before I dust off my old fix and perhaps prematurely cause fixation on
a particular approach, I thought it would be useful to ask the list if
anyone else was aware of a favourite backwards compatible timewarp fix
proposal they wanted to point out.

@_date: 2018-08-31 00:06:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Testnet3 Reest 
On Thu, Aug 30, 2018 at 11:21 PM Johnson Lau via bitcoin-dev
I would much rather have a signed blocks testnet, with a predictable
structured reorg pattern* (and a config flag so you can make your node
ignore all blocks that are going to get reorged out in a reorg of nth
or larger).  There are many applications where the mined testnet just
doesn't give you anything useful... it's too stable when you want it
to be a bit unstable and too wildly unstable when you want a bit of
stability-- e.g. there are very few test cases where a 20,000 block
reorg does anything useful for you; yet they happen on testnet.
We looked at doing this previously in Bitcoin core and jtimon had some
patches,  but the existing approach increased the size of the
blockindex objects in memory  while not in signed testnet mode.   This
could probably have been fixed by turning one of the fields like the
merkel root into a union of it's normal value and a pointer a
look-aside block index that is used only in signed block testnet mode.
Obviously such a mode wouldn't be a replacement for an ordinary
testnet, but it would be a useful middle ground between regtest (that
never sees anything remotely surprising and can't easily be used for
collaborative testing) and full on testnet where your attempts to test
against ordinary noise require you cope your entirely universe being
removed from existence and replaced by something almost but not quite
entirely different at the whim of some cthulhuian blind idiot god.

@_date: 2018-02-05 05:58:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate scripts 
In my post on taproot I showed a simple commitment scheme for scripts
that is very efficient that there exists some collection of pubkeys
(like an M-of-N or even N-of-N) whos authorization is an acceptable
alternative to whatever other conditions we might want to impose on a
coin.  If this holds then when spends happen via the plain signature
path the existence of the alternative is never revealed, providing
privacy with improved efficiency compared to not being private at all.
Taproot suffers from a limitation that it only natively provides for
one alternative.  Trees or cascades of taproots can be done, but they
have less privacy and efficiency than just a single level. E.g. a tree
commitment has overhead that grows with the log of the number of
However, under the taproot assumption-- that there exists some
monotone function on plain public keys and nothing else that is
sufficient to authorize a transaction-- we can do even better.
With graftroot, the participants establish a threshold key, optionally
with a taproot alternative, just as they do with taproot.  At any
time, they can delegate their ability to sign to a surrogate script by
signing that script (and just the script) with their taproot key, and
sharing that delegation with whomever they choose.   Later, when it
comes time to spend the coin, if the signers aren't available and the
script must be used,  the redeeming party does whatever is required to
satisfy the script (e.g. provides their own signature and a timelock,
or whatnot)  and presents that information along with the signer's
signature of the script.
The result is that instead of allowing only one alternative an
unlimited number of alternatives can be provided. All are executed
with equal efficiency to a single alternative, and the number of them
is hidden without overhead.  Alternatives can be provided for existing
coins too, without requiring they get moved-- movement is only
required to destroy the ability to use alternatives by changing keys.
Allowing this kind of delegation makes sense because the same signers
could have just signed the transaction outright. The new script simply
stands in for them, if they're not available or cooperating. No
special conditions are needed outside of the surrogate script on when
the surrogate is allowed, because they can be written inside the
We've discussed delegation in script back to at least 2012-- with
speculation that enabling it may have been an original motivation
behind codeseperator. ... but these design discussions have gotten
mired in how to express and connect the levels of delegation.  But the
case where delegation is accomplished with a simple unconditional
signature is an especially simple case, and under the taproot
assumption the only case that is ever needed.
A naive implementation of this idea requires a complete signature
every time a surrogate is used, which means 64 bytes of data (assuming
128 bit ECC). This is higher overhead than taproot.
However,  the non-interactive schnorr aggregation trick[1] can be
applied to merge the S values of all graftroots and signatures in a
transaction into a single aggregate.  With this approach only a single
R value for each graftroot need be published, lowering the overhead to
~32 bytes-- the same as taproot. This has a side benefit of binding
the published grafts to a particular transaction, which might help
avoid some screwups.
In cases where the taproot assumption doesn't hold, taproot can still
be used by setting the public key to a NUMS point, which preserves
privacy (e.g. you can't distinguish txn where the key could never have
been used.)  A similar thing can be done for graftroot if the
signature is not a proof of knowledge (commits to the public key): you
select the signature in a NUMS manner, and then recover the applicable
public key.  Though this can't be done if the signature is a PoK, and
it's probably a pretty good idea to make it a PoK.
The primary limitation of this approach compared to taproot
alternatives and trees is that it requires that anyone who wants to
make use of a particular surrogate to interact with the participants
and store the resulting signature because a single party couldn't
compute it again on their own from public data. For trees and taproot
alternatives, the alternatives can be setup without any interaction
with the participants. The primary advantage is that it scales to any
number of alternatives with small constant overhead, can be delegated
after the fact, and can still be spent by the participants without
Summarizing:   A coin's authorizing contract is decomposed into a top
level OR between a monotone function of pubkeys (such as N of N) and
any number of arbitrary surrogate scripts which are acceptable
authorizations.  A key aggregate (see [2]) is formed, and is used to
sign each of the the surrogates.  Participants save these signatures.
Later, when it comes time to spend the coin, if the pubkey holders are
unwilling or unavailable, the spender presents and satisfies the
relevant surrogate along with it's signature R-value and
non-interactively aggregates the S-value into the transaction's
overall aggregate signature.  The result is 0-overhead if the signers
cooperate, or ~32-byte overhead (plus the script) if they don't.  This
avoids the log() overhead of tree based schemes, and allows delegation
to take place before or after the fact but requires storage.  The
potential for unexpected surrogate replay if keys are reused in
foolish ways also needs to be kept in mind, though it may be somewhat
mitigated by aggregation. The existence of unused surrogates is
completely hidden.
I believe this general design is simple and powerful enough that it
avoids the rathole that earlier delegation discussions have suffered.
[1] And the secure construction at:
[2]

@_date: 2018-02-05 19:58:24
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
Yes, though I'd avoid the word rotation because as you note it doesn't
invalidate the interests of any key, the original setup remains able
to sign.  You could allow a new key of yours (plus everyone else) to
sign, assuming the other parties agree... but the old one could also
still sign.

@_date: 2018-02-14 22:20:31
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Amend the BIP 123 process to include buried 
On Wed, Feb 14, 2018 at 10:11 PM, Luke Dashjr via bitcoin-dev
In that sense, no but they help people understand the system (e.g. so
they don't go look at implementations and confuse that the activations
they expect are simply not there); and they aid other implementations
in understanding what other people have already analyzed and concluded
was safe. You could certainly get an analysis wrong for one of these

@_date: 2018-02-17 02:33:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Alternative way to count sigops 
We have a related policy rule in Bitcoin Core for some time now, the
weight of the transaction for the purpose of mining is
max(weight,lambda*sigops), though we set lambda a bit lower than makes
sense due to how checkmultisig.  This policy rule replaced an earlier
one which was almost equivalent to your proposal: it rejected
transactions with too many sigops per the byte count, but we found it
block actual more or less sensible transactions.
Going forward I don't think this is a great framework.  It works if
the only expensive operations all involve large input data, but I
think many proposals people have made for new operations would have
computational cost which requires relatively small amounts of
additional input-- aggregation is just one fairly minor example.

@_date: 2018-02-24 18:58:59
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Graftroot: Private and efficient surrogate 
On Thu, Feb 22, 2018 at 7:44 PM, Daniel Edgecumbe via bitcoin-dev
That would require that you know the txid in advance. Sometimes you
do-- and a graftroot sighash flag could handle that... but usually you
wouldn't.  The case where you already do know it can sort of be
covered today without using the graftroot:  Sign a transaction
spending the multisig coin to the graft.  This isn't a strict
alternative however, because it's not atomic: you could imagine that
txn being announced and then the graft not being spent, while someone
would like to spend a different graft.  That non-atomiticity could be
addressed by making the graft spends an OR of all the other graft
spends but that isn't scalable or private.  Regardless, still doesn't
work if the graft isn't created after the fact.
The aggregation bit has the property of working just in time, even on
grafts created in advance.

@_date: 2018-01-06 00:44:20
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] =?utf-8?q?Bech32_and_P2SH=C2=B2?= 
P2SH^2 wasn't a serious proposal-- I just suggested it as a thought
experiment. I don't think it offers much useful in the context of
Bitcoin today. Particularly since weight calculations have made output
space relatively more expensive and fees are at quite non-negligible
rates interest in "storing data" in outputs should at least not be
Moreover, unfortunately, people already rushed bech32 to market in
advance of practically any public review-- regrettable but it is what
it is... I don't think adding more address diversity at this time
wouldn't be good for the ecosystem.
What we might want to do is consider working on an address-next
proposal that has an explicit timeframe of N years out, and very loud
don't deploy this--- layered hashing is just one very minor slightly
nice to have... things like coded expiration times, abilities to have
amounts under checksum, etc. are probably more worth consideration.
On Thu, Jan 4, 2018 at 2:23 PM, Luke Dashjr via bitcoin-dev

@_date: 2018-01-08 04:22:43
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
This specification forces the key being used through a one way
function, -- so you cannot take a pre-existing key and encode it with
this scheme.  The KDF it specifies is unconfigurable and fairly weak
(20000xhmac-sha2-- which can be cracked at about 0.7M passwords a
second on a single motherboard GPU cracker).  The construction also
will silently result in the user getting a different private key if
they enter the wrong passphrase-- which could lead to funds loss. It
is again, unversioned-- so it kinda of seems like it is intentionally
constructed in a way that will prevent interoperable use, since the
lack of versioning was a primary complaint from other perspective
users.  Of course, it fine if you want to make a trezor only thing,
but why bother BIPing something that was not intended for
interoperability?  Even for a single vendor spec the lack of
versioning seems to make things harder to support new key-related
features such as segwit.
The 16-bit "checksum" based on sha2 seems pretty poor since basing
small checksums on a cryptographic hash results in a fairly poor
checksum that is surprisingly likely to accept an errored string. Your
wordlist is 10 bits and you have much less than 1023*10 bits of input,
so you could easily have a 20 bit code (two words) which guaranteed
that up to two errored words would always be detected, and probably
could choose one which catches three words much more often 1:2^20
(sipa's crc tools can help find codes like this).
The metadata seems to make fairly little affordance to help users
avoid accidentally mixing shares from distinct sharings of the same
key. Is it the idea that this is the only likely cause of a checksum
error? (1:2^16 chance of silently returning the wrong key seems kinda
bad). -- I'm not sure much could be done here, though, since
additional payload is precious.
As an aside, your specification might want to give some better advice
about the SSS since my experience virtually everyone gets it wrong in
ways that degrade or destroy its properties e.g. many fail to generate
the additional coefficients of the polynominal randomly which results
in insecurity (see armory for an example).   Oh, also, I believe it is
normally refereed to as "SSS" (three S)-- four S is the name of a
linux program for secret sharing.
I'm happy to see that there is no obvious way to abuse this one as a
brainwallet scheme!

@_date: 2018-01-08 23:47:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
You can use a large block cipher. E.g. CMC cipher mode.
Though I am doubtful that this is a very relevant concern: What
consequence is it if someone with partial access to more than a
threshold of the shares can recover part of the seed?  This doesn't
seem like a very interesting threat.   A large block mode would be
more complete, but this isn't something that would keep me up at night
in the slightest.
Perhaps I'm missing something, -- but the only real attack I see here
is that a enduser mistakenly shows the first or couple words of all
their shares on national television or what not... but doing so would
not really harm their security unless they showed almost all of them,
and in that case an attacker could simply search the remaining couple
Also, if we are going to assume that users will leak parts, the
mnemonic encoding ends up being pretty bad... since just leaking a
letter or two of each word would quite likely give the whole thing
In any case, to whatever extent partial leaks are a concern, using a
large block cipher would be the obvious approach.
Under this constraint it might be arguably to just eliminate the KDF.
I think it provides false security and makes the implementation much
more complicated.
Have you considered using blind host-delegated KDFs, where the KDF
runs on the user's computer instead of the hardware wallet, but the
computer doesn't learn anything about they keys?
I don't believe you can justify this design decision with any kind of
rigorous threat model.
The probability that a user loses funds because they have at some
point recovered with the wrong key and don't know it would almost
certainly dwarf the probability that the user face some kind of
movie-plot threat where someone is attempting to forcibly extract a
key and yet somehow has no information about the user's actual
wallet-- through, for example, leaked data on the users computers, the
users past payments to online accounts, or through a compromise or
lawful order to satoshilab's web service which the users send private
information to-- which would allow them to determine the key they were
given was not correct.
But even there, given the weak level of false input rejection that you
have used (16 bits), it would be fairly straight forward to grind out
an alternative passphrase that also passed the test.  Might that not
make for a better compromise?
Another thing to consider is that the main advantage of SSS over
ordinary computational secret sharing is that it's possible to
generate alternative shares to an sub-threshold set of primary shares
that decodes to arbitrarily selected alternative data-- but it seems
the proposal makes no use of this fact.
The end result is no better-- I think.  If you compromise
functionality or security (e.g. pretextual KDF) because your product
doesn't yet support -- say, aggregate signatures-- or won't ever
support a strong KDF; then other software will just not be
interoperable.  In cases were you won't ever support it, that doesn't
matter-- but presumably you would later support new signature styles
and the loss of interoperability would potentially be gratitious.
That said, I'm generally skeptical of key interoperability to begin
with. Wallets can't share keys unless their functionality is
identical, half-interoperability can lead to funds loss. Identical
functionality would mean constraining to the least common denominator.
But even if we exclude cross vendor interoperability entirely,
wouldn't you want your next version of your firmware to be able to
support new and old key styles (e.g. aggregate signatures vs plain
segwit) without having to define a whole new encoding?
That sounds like a kind of hand-wave and cargo cult argument-- pleas
be more specific, because that just sounds like amateur block cipher
There isn't any difference in "entropy" in either of these cases.
As an aside, using "n bits of a longer CRC" usually results in a low
quality code for error detection similar to using a cryptographic
Not meaningfully more than the truncated cryptographic hash.
The best possible code of that length would allow you to list decode
to around two errors with a lot of computation.
With the cryptographic hash the attacker need only check the 2^28
two-error candidates to do exactly the same thing.
So the attacker there is no real difference-- he can brute force
search to the same radius as correction would allow, but for the
honest users and software the probability of undetected error is
greater.  Similarly, while 2^28 operations is nothing to an attacker,
if user software wants to use the correction for error hinting,
running a hash 2^28 times would lead to a somewhat unfriendly user
experience so non-attack tools would be pretty unlikely to implement

@_date: 2018-01-10 23:47:23
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Yes, this scheme.
I believe that can be avoided by having the computer do somewhat more
work and checking the consistency after the fact.
(or for decode time, having a check value under the encryption...)

@_date: 2018-01-16 03:27:26
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 117 Feedback 
On Tue, Jan 16, 2018 at 1:06 AM, Rusty Russell via bitcoin-dev
That is my view, generally.  Like any other principle, its
applicability is modulated by the specific facts.
For low-s the most critical mitigating specific facts were (in order
of importance):  Any third party could malleate non-conforming
transactions to make them conform and that code to do this was written
and run,  that S-value malleation was being actively attacked at the
time, and that the intention to eventually enforce lowS had been made
clear a long time ahead and the vast majority of transactions were
already conforming.
In particular these facts meant that the change could not result in
the confiscation of funds except in the case of a key-destroyed
unconfirmed chain of timelock transactions which was already highly
vulnerable due to the malleation attacks -- and even there, the
non-standardness step itself wouldn't destroy the funds esp. given the
malleation risk redemption of that sort of chain would probably be
best accomplished with the collaboration of a miner.

@_date: 2018-01-17 15:31:44
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
I'm sorry but I must not be following your message. I read the above
as "these are similar because they are based on math"...
Shamir secret sharing, correctly implemented (which indeed seems to be
many parties problem...) achieves information theoretic security. In
this critical sense it is utterly unrelated to RSA.
In fact this applies generally given any fixed threashold-1 set of
shares there is an value of the final remaining share which decodes to
every possible message. So without knowing of an extra share you know
nothing of the message.
The simplest demonstration is the 2 of 2 case, which can most simply
be constructed over GF(2) as in the traditional "one time pad":
message = share1 xor share2.  For any given share1 or given share2
there exist a value of share2 or share1 respectively which yields
every possible message.
If the generalization isn't obvious, it might be helpful to make a
little test utility that tries all possible one byte messages with all
possible share values using the GF(256) sharing scheme proposed in the
draft-- in this case information theory is why we can know SSS (and
similar) have (within their limited scope) _perfect_ security, rather
than it being a reason to speculate that they might not turn out to be
secure at all. (or, instead of a test utility just work through some
examples on paper in a small field).
This doesn't change when you add additional conditionals on it-- e.g.
Say you a 2-of-3 sharing where you have your choice of any of the
three shares but do not know the others and assume you know every bit
of the plaintext save one bit or any linear or non-linear relationship
between plaintext bits (excepting for actually knowing the secret)...
In these case there can still be no attack arising out of this
charitably bad plaintext structure because-- as pointed out above--
all possible plaintexts are equal-probable you know nothing of which
of the two possible solutions is correct without knowing about the
other shares because for each possible value there exists a value for
the unknown shares which would cause that decoding-- there is no
leakage at all, the share doesn't teach you anything you didn't
already know.
In my view any SSS tool should also include a forgery utility which
demonstrates this property, both as a critical test-- but also because
being able to forge an alternative answer to deceive an attacker which
has compromised some of your shares is one of the (at least
theoretical) arguments for using SSS over computational secret
Complicated does not mean secure. And from an information theoretic
perspective the hash does almost nothing (other then some small
destruction of entropy due to its lack of perfect uniformity which is
information theoretically equivalent to using a smaller perfect code).
There are many cases where I too am more comfortable using a hash --
where it may destroy some structure which I cannot _prove_ would be
safe to retain, but this is not one of those cases.
into the message
The discussion of using a proper code was primarily related to the
outer check value which protects the shares themselves and is sitting
unprotected in plaintext; not so much the one inside the sharing in
any case; since its the outer one which could be structured to provide
perfect detection of errors that align with words (e.g. transposing
two words).

@_date: 2018-01-17 15:36:25
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
Obligatory repeated point: if the scheme being used actually is SSS
and not a Shamir-Shaped-Sharing instead. This should go without
mention by my experience is that a great many things which claim to be
SSS aren't. Sometimes precisely because they stuck in some hashes in
arbitrary places and destroyed the properties (in fact, the really old
broken armory implementation effectively did that, and in fact
resulted in a real weakness not just a theoretical one).

@_date: 2018-01-18 14:34:24
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
On Thu, Jan 18, 2018 at 1:50 PM, Ond?ej Vejpustek
If being secure against partial share leakage is really part of your
threat model the current proposal is gratuitously insecure against it.
And the choice of check algorithm really doesn't matter for that.
For example,  in a 2-of-3 share  say I have the first half of shares
1,2 and the second half of shares 2,3  with the current proposal the
secret is directly revealed, even though I didn't have any single
complete share.
If partial share disclosure were an actual concern, I would recommend
that after sharing and before encoding for transmission (e.g. before
applying check values and word encoding to the share) the individual
shares be passed through a large block unkeyed cryptographic
permutation.  Under reasonable-ish assumptions about the difficulty of
inverting the permutation with partial knowledge, this transformation
would prevent attacks from leaks of partial share information.

@_date: 2018-01-18 18:58:14
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
On Thu, Jan 18, 2018 at 4:59 PM, Ond?ej Vejpustek
My post provided a concrete example. I'd be happy to answer any
questions about it, but otherwise I'm not sure how to make it more
Quite the opposite-- a large block cipher is a standard
construction... and the off-label application of a KDF that you've
used here doesn't provide any protection against the example I gave.

@_date: 2018-01-18 19:30:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] ScriptPubkey consensus translation 
A common question when discussing newer more efficient pubkey types--
like signature aggregation or even just segwit-- is "will this thing
make the spending of already existing outputs more efficient", which
unfortunately gets an answer of No because the redemption instructions
for existing outputs have already been set, and don't incorporate
these new features.
This is good news in that no one ends up being forced to expose their
own funds to new cryptosystems whos security they may not trust.  When
sigagg is deployed, for example, any cryptographic risk in it is borne
by people who opted into using it.
Lets imagine though that segwit-with-sigagg has been long deployed,
widely used, and is more or less universally accepted as at least as
good as an old P2PKH.
In that case, it might be plausible to include in a hardfork a
consensus rule that lets someone spend scriptPubkey's matching
specific templates as though they were an alternative template.  So
then an idiomatic P2PKH or perhaps even a P2SH-multisig could be spent
as though it used the analogous p2w-sigagg script.
The main limitation is that there is some risk of breaking the
security assumptions of some complicated external protocol e.g. that
assumed that having a schnorr oracle for a key wouldn't let you spend
coins connected to that key.  This seems like a pretty contrived
concern to me however, and it's one that can largely be addressed by
ample communication in advance.  (E.g. discouraging the creation of
excessively fragile things like that, and finding out if any exist so
they can be worked around).
Am I missing any other arguments?

@_date: 2018-01-19 02:59:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Change in contact info 
Not really all that on-topic, but since it was suggested to me that
this would be an efficient venue to reach others who might care to
In order to spend more time working independently on deep protocol
work, especially new cryptographic privacy and security technology for
Bitcoin, I resigned from Blockstream last November. It took until the
end of December to wind down my involvement there.
Back when we founded the company I was concerned that there was
significant underinvestment in Bitcoin technology: Bitcoin had a
healthy technical community just as it does today, but lacked the kind
of industry support that projects like Linux have. Without sustained
financial support, some kinds of bigger projects seemed really hard to
pull off with developers needing to share time between non-bitcoin
employment, their families, and their other interests. For the most
part, back then early Bitcoin companies weren't investing in public
technology, at least not effectively.
We hoped that Blockstream could help act as an anchor of support for
technology development, and in doing so help grow the community. I
think that has been a big success. The Bitcoin industry has matured a
lot and today Bitcoin Core gets significant regular contributions from
many organizations (including Chaincode, DCI, Blockstream, Coinbase,
Bitmain, Blockchain, and probably others that I am forgetting or not
even aware of) and a volunteer community much larger and more active
than it has ever been before.  From what I've been told Blockstream
plans to continue to contribute to awesome technology in Bitcoin--as
demonstrated by their Lightning webstore this week--but if they
didn't, that wouldn't be a problem for Bitcoin.
So for me this means that I can go back to working on the things I
find most exciting ... without the overhead of managing staff or
dealing with the many non-Bitcoin blockchain applications which are
important to Blockstream's business. The maturing Bitcoin industry
means I don't need to worry that Bitcoin development could be left
with inadequate financial support.
I'm very excited about all the new and interesting technology that is
coming to production--Bulletproofs / CT, signature aggregation,
improved propagation and synchronization--as well as the continuing
maturation of Bitcoin as a viable subject matter for academic
researchers. I'll be spending more time helping with these and other
things, and will no longer have insight into Blockstream's activities
or a Blockstream email address (I can continue to be reached at my
xiph.org and gmail email addresses as I've used here in the past), but
otherwise this shouldn't change anything for anyone here.

@_date: 2018-01-23 01:05:44
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Satoshilabs secret shared private key scheme 
On Mon, Jan 22, 2018 at 7:21 PM, Russell O'Connor
I expect it would be especially since operations must be implemented
in sidechannel resistant manners.
Also, binary extension fields are doing to have linear subgroup
properties where leaking part of elements wouldn't be good. Not as
obviously broken as the example I gave above, but still in the domain
of "get chunks of a lot of a supra threshold set of shares, and setup
a latices basis problem that can provide an efficient subspace to

@_date: 2018-01-23 00:30:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Interest in merkelized scriptPubKeys (e.g. MAST) is driven by two main
areas: efficiency and privacy. Efficiency because unexecuted forks of
a script can avoid ever hitting the chain, and privacy because hiding
unexecuted code leaves scripts indistinguishable to the extent that
their only differences are in the unexecuted parts.
As Mark Friedenbach and others have pointed out before it is almost
always the case that interesting scripts have a logical top level
branch which allows satisfaction of the contract with nothing other
than a signature by all parties.  Other branches would only be used
where some participant is failing to cooperate. More strongly stated,
I believe that _any_ contract with a fixed finite participant set
upfront can be and should be represented as an OR between an N-of-N
and whatever more complex contract you might want to represent.
One point that comes up while talking about merkelized scripts is can
we go about making fancier contract use cases as indistinguishable as
possible from the most common and boring payments. Otherwise, if the
anonymity set of fancy usage is only other fancy usage it may not be
very large in practice. One suggestion has been that ordinary
checksig-only scripts should include a dummy branch for the rest of
the tree (e.g. a random value hash), making it look like there are
potentially alternative rules when there aren't really.  The negative
side of this is an additional 32-byte overhead for the overwhelmingly
common case which doesn't need it.  I think the privacy gains are
worth doing such a thing, but different people reason differently
about these trade-offs.
It turns out, however, that there is no need to make a trade-off.  The
special case of a top level "threshold-signature OR
arbitrary-conditions" can be made indistinguishable from a normal
one-party signature, with no overhead at all, with a special
delegating CHECKSIG which I call Taproot.
Let's say we want to create a coin that can be redeemed by either
Alice && Bob   or by CSV-timelock && Bob.
Alice has public A, Bob has pubkey B.
We compute the 2-of-2 aggregate key C = A + B.  (Simplified; to
protect against rogue key attacks you may want to use the MuSig key
aggregation function [1])
We form our timelock script S =  " OP_CSV OP_DROP B OP_CHECKSIGVERIFY"
Now we tweak C to produce P which is the key we'll publish: P = C + H(C||S)G.
(This is the attack hardened pay-to-contract construction described in [2])
Then we pay to a scriptPubKey of [Taproot supporting version] [EC point P].
Now Alice and Bob-- assuming they are both online and agree about the
resolution of their contract-- can jointly form a 2 of 2 signature for
P, and spend as if it were a payment to a single party (one of them
just needs to add H(C||S) to their private key).
Alternatively, the Taproot consensus rules would allow this script to
be satisfied by someone who provides the network with C (the original
combined pubkey), S, and does whatever S requires-- e.g. passes the
CSV check and provides Bob's signature. With this information the
network can verify that C + H(C||S) == P.
So in the all-sign case there is zero overhead; and no one can tell
that the contract alternative exists. In the alternative redemption
branch the only overhead is revealing the original combined pubkey
and, of course, the existence of the contract is made public.
This composes just fine with whatever other merkelized script system
we might care to use, as the S can be whatever kind of data we want,
including the root of some tree.
My example shows 2-of-2 but it works the same for any number of
participants (and with setup interaction any threshold of
participants, so long as you don't mind an inability to tell which
members signed off).
The verification computational complexity of signature path is
obviously the same as any other plain signature (since its
indistinguishable). Verification of the branch redemption requires a
hash and a multiplication with a constant point which is strictly more
efficient than a signature verification and could be efficiently fused
into batch signature validation.
The nearest competitor to this idea that I can come up with would
supporting a simple delegation where the output can be spent by the
named key, or a spending transaction could provide a script along with
a signature of that script by the named key, delegating control to the
signed script. Before paying into that escrow Alice/Bob would
construct this signature. This idea is equally efficient in the common
case, but larger and slower to verify in the alternative spend case.
Setting up the signature requires additional interaction between
participants and the resulting signature must be durably stored and
couldn't just be recomputed using single-party information.
I believe this construction will allow the largest possible anonymity
set for fixed party smart contracts by making them look like the
simplest possible payments. It accomplishes this without any overhead
in the common case, invoking any sketchy or impractical techniques,
requiring extra rounds of interaction between contract participants,
and without requiring the durable storage of other data.
[1] [2]  Appendix A

@_date: 2018-01-23 13:15:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
You're reading too much into a description of the idea. It's not a BIP
or a spec; I tried to provide enough details to make the general idea
concrete. I didn't dive into details or optimizations (for example,
you can use this with a "no EC redemption path" by special casing
empty C as the point at infinity, and you'd have an output that was
indistinguishable until spend... yadda yadda).
Considering the considerable level of address reuse -- I recall prior
stats that a majority of circulating funds are on addresses that had
previously been used, on top of the general race limitations-- I am
now dubious to the idea that hashing provides any kind of meaningful
quantum resistance and somewhat regret introducing that meme to the
space in the first place. If we considered quantum resistance a
meaningful concern we should address that specifically.  --- so I
don't think that should be a factor that drives a decision here.
When collision resistance is needed (as I think it clearly is for
taproot) you don't get a space savings in the txout from hashing, so
there is an argument to use the public key directly at least... but
it's worth considering.  Direct SPK use is also adventitious for being
able to efficiently ZKP over the UTXO set, e.g. for private solvency
proofs, but it isn't absolutely mandatory for that (one can hash
inside the proof, but it's slower).

@_date: 2018-01-23 21:31:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
It's quite easy to get no change with a not-dumb algorithm selecting
coins if you have a decent number of outputs well under the value
you're paying.
The number of ways n choose m combines grows exponentially, and you
only need to get close enough over the right value so that you're
paying excess fees equal or less than the cost of the change (which
should include the current cost output itself as well as estimated
cost of the future signature to spend it).
Achow101 and Murch have code to implement an efficient algorithm for
finding these solutions for Bitcoin core which will hopefully get in

@_date: 2018-01-23 21:38:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
On Tue, Jan 23, 2018 at 9:23 PM, Matt Corallo via bitcoin-dev
Even if to someone who didn't care about anyone's privacy at all,
non-taproot is simply inefficient.  In the (I argue) overwhelmingly
common case of everyone-agrees simple hash based branching requires a
30% overhead to communicate the commitment to the untaken branch (and
worse in the case of extensive aggregation).  I don't think an
argument can be sustained in favor of that kind of communications

@_date: 2018-01-23 22:45:06
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Can you show me a model of quantum computation that is conjectured to
be able to solve the discrete log problem but which would take longer
than fractions of a second to do so? Quantum computation has to occur
within the coherence lifetime of the system.
By using scriptpubkeys with actual security against quantum computers
instead of snake-oil.
Indeed, that doesn't work.
They are. But I don't believe that is relevant; the attacker would
simply steal the coins on spend.

@_date: 2018-01-23 22:49:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Transaction Merging (bip125 relaxation) 
On Tue, Jan 23, 2018 at 10:19 PM, Rhavar via bitcoin-dev
BIP125 replacement requires that the fee rate increases.  The text of
the BIP document is written in a confusing way that doesn't make this

@_date: 2018-01-24 04:25:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why is deriving public key from the signature not 
It is slow to verify, incompatible with batch validation, doesn't save
space if hashing isn't used, and is potentially patent encumbered.

@_date: 2018-01-24 10:31:35
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why is deriving public key from the signature not 
No such behaviour ever existed, you are simply mistaken.

@_date: 2018-01-24 11:35:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Why is deriving public key from the signature not 
Because the pubkey is in the scriptPubKey of vout 0 of
40872a376e98a1f8b285827c2ad8c5b3eec7d779d752dc3a4adda5d9bb70f3b5 which
it is spending.

@_date: 2018-01-26 21:34:39
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Taproot: Privacy preserving switchable scripting 
Keeping in mind that a single public point can stand in for any
monotone function of public keys, a taproot branch is only needed for
accountability (being able to tell from public data which branches
were used) or when conditions other than public keys are required e.g.
CSV + a monotone function of keys.
I believe that with scriptless-scripts most of hash preimages can be
accomplished without an actual hash pre-image condition.
Are there other simple and very useful/general preconditions that
would be useful ANDed with a monotone function of public keys like is
the case for CSV?
I ask because recursive taproot by itself isn't very interesting,
since (other than accountability) there is no gain to not just merging
the alternative, but if there are additional conditions then it can be
useful. E.g.
      \-[pubkey]&&CSV
             \-[fancy script]
So it might make sense to support a taproot construction that can
nest, where interior nested keys have a CSV/CLTV predicate. But are
there other simple predicates that cover a lot of cases?
[Aside: _please_ change the subject lines for further discussion about
quantum computers;]

@_date: 2018-01-27 19:06:41
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposal: rewarding fees to next block miner 
Not incentive compatible. Miners would prefer to include transactions
paying fees via alternative mechanisms (anyone can spend outputs,
direct pay to miner outputs, or completely out of band), if they even
paid attention to internal fees at all they would give a lot more
weight to direct payment fees. Users would accordingly pay much lower
fees if they used these alternatives instead of directly, so the
equlibrium state is almost everyone bypassing.   Bypass fee mechenisms
have been supported by miners since 2011 too, so it isn't just
On Sat, Jan 27, 2018 at 8:45 AM, Nathan Parker via bitcoin-dev

@_date: 2018-01-29 21:22:25
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposal: rewarding fees to next block miner 
On Mon, Jan 29, 2018 at 4:49 AM, Eric Voskuil via bitcoin-dev
I agree.
Steel-manning it, I guess I could argue that empty blocks are slightly
more conspicuous and might invite retaliation especially given the
high levels of mining centralization creates retaliation exposure. ...
but dummy transactions are hardly less conspicuous, many nodes log now
when blocks show up containing txn that they've never seen before.
Moreover, inexplicably underfilled blocks are produced (e.g. by
bitmain's antpool) and no retaliation seems to be forthcoming.

@_date: 2018-01-29 21:54:23
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] How accurate are the Bitcoin timestamps? 
It would be more accurate to say that the median is not used for
improved accuracy but to mitigate a consensus incompatibility:
If the block's own timestamp were used for nlocktime and time based
nlocks were common on the network each miner would maximize their fee
income by setting the value as high as they could get away with.  What
concerned us wasn't so much that this would make the times less
accurate (though it would) but rather that it would create an
incentive for a runaway situation that could harm network stability
(e.g. with all miners cranking times against the 2hr window, then
creating pressure for miners to accept further and further in the
future; each responding to his own local incentives).
This incentive incompatibility could have been addressed e.g. by using
the prior block's time, but since the protocol doesn't require times
to be monotone (and for good reason!) the simple implementation of
that wouldn't have been a soft-fork.  The 11 block MTP worked out
nicely because the protocol already required new times to be larger
than that.
The timestamps in Bitcoin aren't intended to be particularly accurate.
They're used only for controlling the difficulty, and the adjustment
window is large enough that there isn't much distortion that can be
accomplished there.  It's not clear to me that much better can really
be done... if there were tighter time requirements in the protocol
miners would address them by running NTP which as an _astounding_ lack
of security in terms of how it is commonly deployed.  As far as I
know, I'm the only person whos ever mined blocks with their own
stratum 1 time source.
If times need to be accurate Bitcoin would need to use a rather
different design (e.g. each block would commit to the observation time
of the prior N blocks, and an iterative algorithm would solve for each
blocks time and each miners local offset).
IIRC open-timestamp calendar servers provide more precise
time-stamping under the assumption that the calendar server is
behaving correctly.

@_date: 2018-01-30 01:59:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Proposal: rewarding fees to next block miner 
That case would be stronger when there is no more subsidy, but we
collectively the uses of Bitcoin are currently paying miners around
$130k USD per block in the form of inflation for the job of honestly
complying with the Bitcoin protocol.
I don't think you can argue that they have any more right to do that
than any of us have a right to run software that invalidates their
coinbase outputs when they do; which would be the sort of retaliation
they might get targeted with.

@_date: 2018-07-02 18:11:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP sighash_noinput 
I know it seems kind of silly, but I think it's somewhat important
that the formal name of this flag is something like
"SIGHASH_REPLAY_VULNERABLE" or likewise or at least
"SIGHASH_WEAK_REPLAYABLE". This is because noinput is materially
insecure for traditional applications where a third party might pay to
an address a second time, and should only be used in special protocols
which make that kind of mistake unlikely.   Otherwise, I'm worried
that wallets might start using this sighash because it simplifies
handling malleability without realizing that when a third party reuses
a script pubkey, completely outside of control of the wallet that uses
the flag, funds will be lost as soon as a troublemaker shows up (but
not, sadly, in testing).  This sort of risk is magnified because the
third party address reuser has no way to know that this sighash flag
has (or will) be used with a particular scriptpubkey.
So, one could even argue that the possibility that someone might use
this flag means that it's generally unsafe to reuse a scriptpubkey.  I
don't think the same argument applies for NONE or the single-bug
because they render even a single use insecure...  The best mitigation
I can think of is defence in depth to ensure that anyone who uses this
sighash flag understands the consequences.

@_date: 2018-07-02 18:23:48
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] SIGHASH2 for version 1 witness programme 
This seems fairly complicated and yet-- if I don't misunderstand-- it
doesn't capture the one special output masking case that I've seen
actual applications want (which itself, is one out of the only two
special sighash cases I've seen applications want-- the other being
The case I think this is missing is SIGHASH_SINGLE |
SIGHASH_LAST_OUTPUT   e.g. "Sign the matching output, and the last
output regardless of its index". The application for this style is
"kickstarter" joint-payment transactions where you wish to sign both
your change output (SIGHASH_SINGLE)  and the joint-payment output
(SIGHASH_LAST_OUTPUT).  Without it, this kind of usage requires
usually a chain of depth two for each input to split off the change.
I came back around to your post at Sipa's recommendation because I was
musing on is there a _simple_ set of enhanced sighash flags that
capture real useful behaviour without falling down a rathole of
specifying a totally general behaviour.

@_date: 2018-07-03 23:45:22
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP sighash_noinput 
I believe that making the signature replayable is 1:1 with omitting
the identification of the specific coin being spent from it.

@_date: 2018-07-06 22:00:28
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
There is a minor design preference to have message before nonce when
H() is a MD-style hash function.  Say the attacker knows some weakness
in H and can find pairs of messages m and m' so that the compression
function results in the same midstate.  He could then ask you to sign
m but get a signature that also works for m'.   If the signer
controlled R value comes first, then this doesn't work.    The pubkey
being where it is in the current design just follows from the idea
that it is just logically prepended on the message.  I don't think the
pubkey is sufficiently attacker controlled that the above argument
would apply,  so H(P || R.x || m) would be okay.
BUT, the sha256 compression function reads 64 bytes at a time. PRM
would not let you precompute a whole compression function run, but
instead would just let you hardwire part of the expander in a pubkey
dependant way-- an optimization I'm pretty confident virtually no one
would use.  (Hardwiring to a constant, yes. Hardwiring to a reused
dynamic value that comes in from the network, no)
If instead the hash function were defined as using 31 zeros then
P||R||m (or P || 31 zeros bytes || R || m, I'm not sure what would be
better), an entire midstate could be cached for different pubkeys. m
is often 32 bytes, sadly- - but the final compression run in that case
would only be the constant update with the length.... and
almost-all-zeros + constant length, is an easy optimization. (Bitcoin
core even has it for computing sha256(sha256())).
[I'm not really sure if I was clear, so I'll try TLDRing it:  I think
optimizing sha256 where part of the input is constant is realistic,
optimizing midstate reuse is realistic, optimizing where part is
reused is less realistic.  If we insert padding, and put P first, we
can make it possible to midstate cache P,  and the 'extra' compression
function run ends up with all constant input, so it could be made

@_date: 2018-07-06 22:01:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
::sigh:: to NOT have the message before the nonce.

@_date: 2018-07-08 21:01:36
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Multiparty signatures 
As also described in "Multisignatures and Threshold Signatures" in the BIP.

@_date: 2018-07-09 16:21:59
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Multiparty signatures 
This is isomorphic to the insecure musig variant where keys are
blinded by H(g*x) instead of a commitment to all keys. It is insecure
because it vulnerable to an attacker knowing a victim pubkey P  who
uses wagner's algorithim to solve a random modular subset sum problem:
-1H(P) = H(aP)/a + H(bP)/b + H(cP)/c + ... for some a,b,c...  then
claiming to be participants with keys aP, bP, cP, ..., xG (their own
key) and canceling out key P, allowing the value to just be signed for
with their key alone.
AFAICT your suggestion is using simple multiplication in the place of
a cryptographic hash.  E.g.  you have just suggested a schnorr
signature where H() is  just r*m in the field of size n. It doesn't
have any new properties about how you can use it. The same linearities
do and don't apply as the normal schnorr construction, but for any of
the security proofs to hold we'd have to believe that multiplication
in the field of n is a suitable random oracle-- which is not very

@_date: 2018-07-09 16:58:38
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Multiparty signatures 
Musig is instructions on using the original schnorr construction for
multiparty signing which is secure against participants adaptively
choosing their keys, which is something the naive scheme of just
interpolating keys and shares is vulnerable to. It works as
preprocessing on the keys, then you continue on with the naive
protocol. The verifier (e.g. network consensus rules) is the same.
Now that you're back to using a cryptographic hash, I think what
you're suggesting is "use naive interpolation of schnorr signatures"

@_date: 2018-07-11 20:05:32
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 174 thoughts 
On Wed, Jul 11, 2018 at 6:27 PM, Pieter Wuille via bitcoin-dev
Do we really want the specification to permit conforming
implementations to refuse to sign because there is extra metadata?
ISTM this would make it very hard to implement new features that
require extra data. For example, say you have a checkmultisig with one
key in it using schnorr multisignature which require the extra rounds
to establish an R, and the other keys are legacy stuff.  If the other
signer(s) suddenly stop working when there are extra fields irrelevant
to them, then this will create a substantial pressure to not extend
the PSBT in the intended way, but to instead find places to stuff the
extra data where it won't interfere with already deployed signers.
This would be really unfortunate since PSBT was created specifically
to avoid field stuffing (otherwise we could have accomplished all the
intended goals by field stuffing a bitcoin transaction encoding).
Obviously no signer should be signing data they don't understand,  but
extra data that they ignore which doesn't change their signature
should not stop them.  Another way of looking at it, perhaps somewhere
someplace some idiot defined signatures starting with 0xdead to give
away all the users funds or whatever.  That's something you "can't
understand" either, ... but no one would conclude because something
could happen somewhere that you don't know about that you just can't
sign at all... yet it is possible. :)
If someone wants to make a non-conforming signer, that is cool too and
they may have good reason to do so-- but I think it would be sad if
new applications get gunked up, slowed down or forced to use ugly
hacks, due to the intentional extension support in the protocol being
blocked by things claiming to support the spec.  The whole reason the
spec doesn't lock in exactly the possible fields and allow no others
is to allow extensions without breaking compatibility.

@_date: 2018-06-01 04:15:13
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
On Fri, Jun 1, 2018 at 2:52 AM, Olaoluwa Osuntokun via bitcoin-dev
Only if you make a very strong assumption about the integrity of the
nodes the client is talkign to. A typical network attacker (e.g.
someone on your lan or wifi segmet, or someone who has compromised or
operates an upstream router) can be all of your peers.
The original propsal for using these kinds of maps was that their
digests could eventually be commited and then checked against the
commitment, matching the same general security model used otherwise in
Unfortunately, using the scripts instead of the outpoints takes us
further away from a design that is optimized for committing (or, for
that matter, use purely locally by a wallet)...

@_date: 2018-06-02 00:22:25
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I wish that were the true, but absent commitments that wouldn't be the
case unless you were always downloading all the blocks-- since you
wouldn't have any sign that there was something wrong with the
filter-- and downloading all the blocks would moot using the filters
in the first place. :)
Or have I misunderstood you massively here?
For segwit originally I had proposed adding additional commitments
that would make it possible to efficiently prove invalidity of a
block; but that got stripped because many people were of the view that
the "assume you have at least one honest peer who saw that block and
rejected it to tell you that the block was invalid" security
assumption was of dubious value. Maybe it's more justifiable to make
use of a dubious assumption for a P2P feature than for a consensus
feature?  Perhaps,  I'd rather have both filter types from day one so
that things not implementing the comparison techniques don't get the
efficiency loss or the extra work to change filter types for a
consensus one.
[I think now that we're much closer to a design that would be worth
making a consensus committed version of than we were a few months ago
now, since we are effectively already on a second generation of the
design with the various improvements lately]

@_date: 2018-06-03 00:28:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
On Sat, Jun 2, 2018 at 10:02 PM, Tamas Blummer via bitcoin-dev
pretty much us that all these filter things are a total waste of time.
BIP37 use is nearly dead on the network-- monitor your own nodes to
see the actual use of the filters: it's very low.  I see under average
of 1 peer per day using it.
Moreover the primary complaint from users about BIP37 vs the
alternatives they're choosing over it (electrum and web wallets) is
that the sync time is too long-- something BIP158 doesn't improve.
So if we were going to go based on history we wouldn't bother with on
P2P at all.   But I think the history's lesson here may mostly be an
accident, and that the the non-use of BIP37 is  due more to the low
quality and/or abandoned status of most BIP37 implementing software,
rather than a fundamental lack of utility.   Though maybe we do find
out that once someone bothers implementing a PIR based scanning
mechanism (as electrum has talked about on and off for a while now)
we'll lose another advantage.
BIP37 also got a number of things wrong-- what went into the filters
was a big element in that (causing massive pollution of matches due to
useless data), along with privacy etc.  This kind of approach will
have the best chances if it doesn't repeat the mistakes... but also
it'll have the best chances if it has good security, and getting SPV-
equivalent security will require committing the filters, but
committing them is a big step because then the behaviour becomes
consensus normative-- it's worth spending a few months of extra
iteration getting the design as good as possible before doing that
(which is what we've been seeing lately).

@_date: 2018-06-05 17:52:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
As an important point of clarification here. If scripts are used to
identify inputs and outputs, then no use is required for that savings.
Each coin spent was created once, so in an absurd hypothetical you can
get a 2:1 change in bits set without any reuse at all.   I don't know
what portion of coins created are spent in the same 144 block

@_date: 2018-06-08 16:14:41
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
On Fri, Jun 8, 2018 at 5:03 AM, Olaoluwa Osuntokun via bitcoin-dev
It seems to me that you're making the argument against your own case
here: I'm reading this as a "it's hard to switch so it should be done
the inferior way".  That in argument against adopting the inferior
version, as that will contribute more momentum to doing it in a way
that doesn't make sense long term.
I don't agree at all, and I can't see why you say so.
This is inherent in how e.g. the segwit commitment is encoded, the
initial bytes are an identifying cookies. Different commitments would
have different cookies.
What was previously proposed is that the commitment be required to be
consistent if present but not be required to be present.  This would
allow changing whats used by simply abandoning the old one.  Sparsity
in an optional commitment can be addressed when there is less than
100% participation by having each block that includes a commitment
commit to the missing filters ones from their immediate ancestors.
Additional optionality can be provided by the other well known
mechanisms,  e.g. have the soft fork expire at a block 5 years out
past deployment, and continue to soft-fork it in for a longer term so
long as its in use (or eventually without expiration if its clear that
it's not going away).
Absolutely, but given the failure of BIP37 on the network-- and the
apparent strong preference of end users for alternatives that don't
scan (e.g. electrum and web wallets)-- supporting making this
available via P2P was already only interesting to many as a nearly
free side effect of having filters for local scanning.  If it's a
different filter, it's no longer attractive.
It seems to me that some people have forgotten that this whole idea
was originally proposed to be a committed data-- but with an added
advantage of permitting expirementation ahead of the commitment.
You can still scan blocks directly when peers disagree on the filter
content, regardless of how the filter is constructed-- yes, it uses
more bandwidth if you're attacked, but it makes the attack ineffective
and using outpoints considerably increases bandwidth for everyone
without an attack.  These ineffective (except for increasing
bandwidth) attacks would have to be common to offset the savings. It
seems to me this point is being overplayed, especially considering the
current state of non-existing validation in SPV software (if SPV
software doesn't validate anything else they could be validating, why
would they implement a considerable amount of logic for this?).

@_date: 2018-06-09 15:45:54
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
An example of that cost is you arguing against specifying and
supporting the design that is closer to one that would be softforked,
which increases the time until we can make these filters secure
because it slows convergence on the design of what would get
We have an extensible commitment style via BIP141 already. I don't see
why this in particular demands a new one.
Great point, but it should probably exclude coinbase OP_RETURN output.
This would exclude the current BIP141 style commitment and likely any
Should I start a new thread on excluding all OP_RETURN outputs from
BIP-158 filters for all transactions? -- they can't be spent, so
including them just pollutes the filters.
If 384 bytes is a concern, isn't 3840 bytes (the filter size
difference is in this ballpark) _much_ more of a concern?  Path to the
coinbase transaction increases only logarithmically so further
capacity increases are unlikely to matter much, but the filter size
increases linearly and so it should be much more of a concern.
I think it's a fairly ugly hack. esp since it requires that mining
template code be able to stuff the block if they just don't know
enough actual transactions-- which means having a pool of spendable
outputs in order to mine, managing private keys, etc... it also
requires downstream software not tinker with the transaction count
(which I wish it didn't but as of today it does). A factor of two
difference in capacity-- if you constrain to get the smallest possible
proof-- is pretty stark, optimal txn selection with this cardinality
constraint would be pretty weird. etc.
If the community considers tree depth for proofs like that to be such
a concern to take on technical debt for that structure, we should
probably be thinking about more drastic (incompatible) changes... but
I don't think it's actually that interesting.
Yes, maybe it isn't.  But then that just means we don't have good information.
When a lot of people were choosing electrum over SPV wallets when
those SPV wallets weren't abandoned, sync time was frequently cited as
an actual reason. BIP158 makes that worse, not better.   So while I'm
hopeful, I'm also somewhat sceptical.  Certainly things that reduce
the size of the 158 filters make them seem more likely to be a success
to me.
::shrugs:: Above you're also arguing against fetching down to the
coinbase transaction to save a couple hundred bytes a block, which
makes it impossible to validate a half dozen other things (including
as mentioned in the other threads depth fidelity of returned proofs).
There are a lot of reasons why things don't get implemented other than
experience! :)

@_date: 2018-06-20 14:30:55
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
On Wed, Jun 20, 2018 at 12:12 PM, ZmnSCPxj via bitcoin-dev
If it isn't possible to make a graftroot signature independent of the
outpoint then the functionality is _greatly_ reduced to the point of
largely mooting it-- because you could no longer prepare the grafts
before the coins to be spent existed, and meaning you must stay online
and sign new grafts as coins show up. In my view graft's two main
gains are being able to delegate before coins exist and making the
conditional transfer atomic (e.g. compared to just pre-signing a
transaction).  Making outpoint binding optional, so that you could
choose to either sign for particular outputs or in a blanket way would
be a lot more useful.
Though I had assumed outpoint binding could best be achieved by
checking the outpoint in the graft-script-- this is general for
whatever kinds of arbitrary graft conditions you might want to specify
(e.g. locktimes, value checks, or conditions on subsequent outputs)...
but perhaps binding a particular outpoint is enough of a special case
that it's worth avoiding the overhead of expressing a match condition
in the script, since that would probably end up blowing 36 bytes for
the match condition in the witness when instead it could just be
covered by the signature, and people should probably prefer to do
output binding grafts whenever its reasonably possible.

@_date: 2018-06-21 21:39:20
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 174 thoughts 
On Thu, Jun 21, 2018 at 7:56 PM, Peter D. Gray via bitcoin-dev
When you implement proposals that have little to no public discussion
about them you take the risk that your work needs to be changed when
other people do actually begin reviewing the work.  It is incredibly
demoralizing as a designer and a reviewer to have proposals that were
put out for discussion show up implemented in things with these vested
interests then insisting that they not be refined further.  I think
kind of handling is toxic to performing development in public.
Although it's silly enough that it won't happen, I think our industry
would be better off if there was a social norm that anytime someone
insists an unfinished proposal shouldn't be changed because they
already implemented it that the spec should _always_ be changed, in
order to discourage further instances of that conduct.

@_date: 2018-06-26 05:20:02
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving 
I don't see any problem with doing that... Although an additional
countermeasure we're likely to take against attacks on partial
deployment is that we'd likely make the wallet's use of stem
forwarding be a configuration option which is initially hidden and set
to off.  In a subsistent release after dandelion propagation is widely
deployed we'd unhide the option and default it to on.   This way users
don't begin using it until the deployment is relatively dense.
I believe this approach is a is sufficient such that it could always
select out-peers that were dandelion capable without harm,  but at the
same time I also don't see a reason that we can't do both.
(in fact, for privacy reasons we might want to three-stage the
deployment, with the use of dandelion by wallets having a setting of
off, sometimes, or always so that attackers can't so easily correlate
the use of dandelion with upgrades.)

@_date: 2018-05-17 16:36:37
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I think this is convincing for the txids themselves.
What about also making input prevouts filter based on the scriptpubkey
being _spent_?  Layering wise in the processing it's a bit ugly, but
if you validated the block you have the data needed.
This would eliminate the multiple data type mixing entirely.

@_date: 2018-05-17 16:56:39
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] UHS: Full-node security without maintaining a 
My initial thoughts are it's not _completely_ obvious to me that a 5%
ongoing bandwidth increase is actually a win to get something like a
40% reduction in the size of a pruned node (and less than a 1%
reduction in an archive node) primarily because I've not seen size of
a pruned node cited as a usage limiting factor basically anywhere. I
would assume it is a win but wouldn't be shocked to see a careful
analysis that concluded it wasn't.
But perhaps more interestingly, I think the overhead is not really 5%,
but it's 5% measured in the context of the phenomenally inefficient tx
mechanisms (  ).
Napkin math on the size of a txn alone tells me it's more like a 25%
increase if you just consider size of tx vs size of
tx+scriptpubkeys,amounts.  If I'm not missing something there, I think
that would get in into a very clear not-win range.
On the positive side is that it doesn't change the blockchain
datastructure, so it's something implementations could do without
marrying the network to it forever.

@_date: 2018-05-17 18:34:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I think lite clients cross checking is something which very likely
will never be implemented by anyone, and probably not stay working
(due to under-usage) if it is implemented.  This thought is driven by
three things  (1) the bandwidth overhead of performing the check, (2)
thinking about the network-interacting-state-machine complexity of it,
and by the multitude of sanity checks that lite clients already don't
implement (e.g. when a lite client noticed a split tip it could ask
peers for the respective blocks and check at least the stateless
checks, but none has ever done that), and...
(3) that kind of checking would be moot if the filters were committed
and validated... and the commitment would be both much simpler to
check for lite clients and provide much stronger protection against
malicious peers.
My expectation is that eventually one of these filter-map designs
would become committed-- not after we already had it deployed and had
worked out the design to the n-th generation (just as your proposed
revisions are doing to the initial proposal), but eventually.
Also, even without this change clients can still do that "are multiple
peers telling me the same thing or different things" kind of checking,
which I expect is the strongest testing we'd actually see them
implement absent a commitment.

@_date: 2018-05-17 18:34:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
I think lite clients cross checking is something which very likely
will never be implemented by anyone, and probably not stay working
(due to under-usage) if it is implemented.  This thought is driven by
three things  (1) the bandwidth overhead of performing the check, (2)
thinking about the network-interacting-state-machine complexity of it,
and by the multitude of sanity checks that lite clients already don't
implement (e.g. when a lite client noticed a split tip it could ask
peers for the respective blocks and check at least the stateless
checks, but none has ever done that), and...
(3) that kind of checking would be moot if the filters were committed
and validated... and the commitment would be both much simpler to
check for lite clients and provide much stronger protection against
malicious peers.
My expectation is that eventually one of these filter-map designs
would become committed-- not after we already had it deployed and had
worked out the design to the n-th generation (just as your proposed
revisions are doing to the initial proposal), but eventually.
Also, even without this change clients can still do that "are multiple
peers telling me the same thing or different things" kind of checking,
which I expect is the strongest testing we'd actually see them
implement absent a commitment.

@_date: 2018-05-17 20:45:33
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
Any non-commited form is inherently insecure.  A nearby network
attacker (or eclipse attacker) or whatnot can moot whatever kind of
comparisons you make, and non-comparison based validation doesn't seem
like it would be useful without mooting all the bandwidth improvements
unless I'm missing something.
It isn't a question of 'some lite clients' -- I am aware of no
implementation of these kinds of measures in any cryptocurrency ever.
The same kind of comparison to the block could have been done with
BIP37 filtering, but no one has implemented that. (similarly, the
whitepaper suggests doing that for all network rules when a
disagreement has been seen, though that isn't practical for all
network rules it could be done for many of them-- but again no
implementation or AFAIK any interest in implementing that)
Sure, but at what cost?   And "additional" while nice doesn't
necessarily translate into a meaningful increase in delivered security
for any particular application.
I think we might be speaking too generally here.
What I'm suggesting would still allow a lite client to verify that
multiple parties are offering the same map for a given block (by
asking them for the map hash). It would still allow a future
commitment so that lite client could verify that the hashpower they're
hearing from agrees that the map they got is the correct corresponding
map for the block. It would still allow downloading a block and
verifying that all the outpoints in the block were included.  So still
a lot better than BIP37.
What it would not permit is for a lite client to download a whole
block and completely verify the filter (they could only tell if the
filter at least told them about all the outputs in the block, but if
extra bits were set or inputs were omitted, they couldn't tell).
But in exchange the filters for a given FP rate would be probably
about half the current size (actual measurements would be needed
because the figure depends on much scriptpubkey reuse there is, it
probably could be anywhere between 1/3 and 2/3rd).  In some
applications it would likely have better anonymity properties as well,
because a client that always filters for both an output and and input
as distinct items (and then leaks matches by fetching blocks) is more
I think this trade-off is at leat worth considering because if you
always verify by downloading you wash out the bandwidth gains, strong
verification will eventually need a commitment in any case.  A client
can still partially verify, and can still multi-party comparison
verify.  ... and a big reduction in filter bandwidth
Monitoring inputs by scriptPubkey vs input-txid also has a massive
advantage for parallel filtering:  You can usually known your pubkeys
well in advance, but if you have to change what you're watching block
 N+1 for based on the txids that paid you in N you can't filter them
in parallel.
I think Peter missed Matt's point that you can monitor for a specific
transaction's confirmation by monitoring for any of the outpoints that
transaction contains. Because the txid commits to the outpoints there
shouldn't be any case where the txid is knowable but (an) outpoint is
not.  Removal of the txid and monitoring for any one of the outputs
should be a strict reduction in the false positive rate for a given
filter size (the filter will contain strictly fewer elements and the
client will match for the same (or usually, fewer) number).
I _think_ dropping txids as matt suggests is an obvious win that costs
nothing.  Replacing inputs with scripts as I suggested has some

@_date: 2018-05-23 17:28:29
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
Any chance you could add a graph of input-scripts  (instead of input outpoints)?
On Wed, May 23, 2018 at 7:38 AM, Jim Posen via bitcoin-dev

@_date: 2018-05-23 23:45:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
I am having a bit of difficulty understanding your example.
If graftroot were possible it would mean that the funds were paid to a
public key.  That holder(s) of the corresponding private key could
sign without constraint, and so the accoutability you're expecting
wouldn't exist there regardless of graftroot.
I think maybe your example is only making the case that it should be
possible to send funds constrained by a script without a public key
ever existing at all.  If so, I agree-- but that wasn't the question
here as I understood it.

@_date: 2018-05-24 02:08:07
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Should Graftroot be optional? 
My understanding of the question is this:
Are there any useful applications which would be impeded if a signing
party who could authorize an arbitrary transaction spending a coin had
the option to instead sign a delegation to a new script?
The reason this question is interesting to ask is because the obvious
answer is "no":  since the signer(s) could have signed an arbitrary
transaction instead, being able to delegate is strictly less powerful.
Moreover, absent graftroot they could always "delegate" non-atomically
by spending the coin with the output being the delegated script that
they would have graftrooted instead.
Sometimes obvious answers have non-obvious counter examples, e.g.
Andrews points related to blindsigning are worth keeping in mind.

@_date: 2018-05-25 18:42:41
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Minimizing the redundancy in Golomb Coded Sets 
I did a rough analysis using Pieter's approximations on what
parameters minimizes the total communications for a lite wallet
scanning the chain and fetching a witnessless block whenever they get
a filter hit. For a wallet with 1000 keys and blocks of 1MB if the
number of entries in the is at least 5096 then M=784931 results in a
lower total data rate rate (FP blocks + filters) than M=1569861.
M=392465 (the optimal value for the rice parameter 18) is
communications is better if at least 10192 entries are set, and
M=196233 (optimal FP for rice 17) is better if at least 20384 entries
are set.
The prior filter set proposal is setting roughly 13300 entries per
full block,  and I guestimate that the in+out scripts only ones are
setting about 7500 entries (if that actual number was in any of the
recent posts I missed it, I'm guessing based on jimpo's sizes graph).
The breakpoints are obviously different if the client is monitoring
for, say, 10,000 keys instead of 1000 but I think it generally makes
more sense to optimize for lower key counts since bigger users are
more likely to tolerate the additional bandwidth usage.
So I think that assuming that all-scripts inputs and outputs (but no
txids) are used and that my guess of 7500 bits set for that
configuration is roughly right, then M=1569861 and rice parameter 19
should be used.
The actual optimal FP rate for total data transferred won't be one
that gets the optimal rice coding efficiency, but since different
clients will be monitoring for different numbers of keys, it probably
makes sense to pick a parameter with optimal compression rather than
optimal-data-transfer-for-a-specific-key-count-- at least then we're
spending the least amount of filter bits per false positive rate,
whatever that rate is... if we can't be optimal at least we can be
efficient. :)

@_date: 2018-05-25 21:13:55
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Minimizing the redundancy in Golomb Coded Sets 
That should have been M=784931 B=19  ... paste error.

@_date: 2018-05-28 19:24:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
Is there an application that requires watching for output scripts that
doesn't also require watching for input scrips (or, less efficiently,
input outpoints)?
Any of the wallet application that I'm aware of need to see coins
being spent as well as created, else they may try to spend already
spent coins. If we're not aware of any applications that wouldnt use
both, there isn't much reason to separate them and if input scripts
are used instead of input outpoints there is additional savings from
combining them (due to the same scripts being spent as were created in
the block-- due to reuse and chaining).
I still am of the belief, based on Matt's argument, that there is no
use for txid what-so-ever (instead just watch for an outpoint).

@_date: 2018-05-29 03:24:45
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP 158 Flexibility and Filter Size 
In this configuration there is little need to access historical blocks
though, since you're assuming that you'll never recover from a backup.

@_date: 2018-05-30 14:08:08
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] New serialization/encoding format for key material 
Can you explain exactly what you mean there? I can think of to
plausible meanings (that two valid keys could differ by only a single
symbol, which wouldn't be true due to the checksum and could be made
even stronger if we thought that would be useful or I think you could
also be complaining that the same "key material" could be encoded two
ways which I think is both harmless and unavoidable for anything
Personally, I think it's a mistake to believe that any key format can
really make private keying material strongly compatible between
wallets. At best you can hope for a mostly compatible kind of recovery
But the lookahead amount may be pretty integral to the design of the
software, so signaling it may not mean the other side can obey the
signal... but that wouldn't make the signal completely useless.

@_date: 2018-10-24 16:12:39
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Transaction Input/Output Sorting 
On Wed, Oct 24, 2018 at 3:52 PM Chris Belcher via bitcoin-dev
A two input randomly ordered transaction has a 50% chance of
'following' bip-69.  So 60% sound like a small minority.

@_date: 2018-09-05 15:35:14
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
On Wed, Sep 5, 2018 at 1:49 PM Erik Aronesty via bitcoin-dev
This appears to be a repost of the broken scheme you posted about on
Bitcointalk, but then failed to respond to the response.
I think you might be falling into the trap of ignoring feedback you
don't like and and accepting that which sounds like "yea yea,
something like that".
Something "like that" does work: and is expressly and explicitly
anticipated by the BIP but to be both secure and functional requires
proper delineation (E.g. musig) _and_ interaction. What you're
proposing is continually vague.  My best efforts at making sense of
what you've written indicate that either it's non-interactive and
not-actually functional at all,  OR it's interactive and just a less
secure subset (no proper delinearization to prevent rogue key attacks)
of what we already propose.
When Poelstra suggests a CAS implementation he means something like
this Sage notebook:   This
provides for a method of communicating in both directions which is
completely precise.

@_date: 2018-09-06 15:16:34
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] A BIP proposal for transactions that are 
Functionality such as this does not currently exist not because no one
thought of it before, but because it has been proposed many times
before and determined to be harmful.  The existing design of CLTV/CSV
were carefully constructed to make it impossible for a transaction to
go from valid to invalid based on the time. The most naive
construction-- e.g. push the current time/height on the stack-- would
have that property and was specifically avoided.
When a spend goes from valid to invalid it means that a reorganization
will destroy coins even completely absent any dishonest actions of the
coins prior owner in the coins recent casual history. Effectively a
coin with any kind of non-monotone validity event in its recent
history functions like a recently generated coin: a coin that reorgs
destroy. Bitcoin addresses the issue for recently generated coins by
not permitting their use for 100 blocks.  I've yet to see an argument
for a use case for non-monotone validity that still sounds useful once
the negative effects are addressed (e.g. by subjecting coins that have
gone through them to a maturity limitation).

@_date: 2018-09-07 02:31:15
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Overhauled BIP151 
That is the argument. We know for that state level parties are storing
unimaginable amounts of data for future decryption, that threat isn't
Why not?
Currently, Tor provides _no confidentiality at all_ in that threat
model.  Part of why I think this enhancement is interesting is because
without it BIP151 doesn't actually add anything for those p2p
connections running over Tor, but with it -- it at least adds some
long term confidentiality hedge.
Why do you say this?
People already make that claim with respect to public key hashing.  I
don't think "we shouldn't improve security because someone will
mistake an improvement for perfection" is an an especially interesting
Here is where I turn the argument around on you:   This requires
writing a non-trivial amount of moderately complex new cryptographic
code (which is not the case for PQ schemes-- that merely requires
dropping in pre-existing code) and yet I am not aware of any attack
model that this which would any improvement in actually delivered
security:  Bitcoin traffic is _trivially_ identifiable by its traffic
(Blockstream  previously wrote the SW forward transform for asset
generation, but this requires the inverse too, as well as glue code.
It also isn't clear to me if it's possible to make this construction
constant time, which would be okay for BIP151 purposes but if we
wanted to have a generic uniform encoder in libsecp256k1 I think we'd
prefer it be constant time? maybe?)
The scheme in the BIP thus far achieves the property that there are no
fixed bytes for brain-dead byte matching DPI traffic filtering or
anti-virus to match on (or accidentally false positive on).  AV false
positives are already an existing problem with the current protocol
and any fixed bytes in the communication are at risk for false
positives or targeted filtering.   And achieving that property
requires basically nothing: a test for the first byte of a generated
serialized pubkey and a negate on the private key if it was wrong.
No, it doesn't-- due to traffic analysis.  Including, for example, the
pattern that 64-bytes must be sent in each direction, before further
data continues, bursts of traffic coinciding with blocks newly found
on the network, etc.
I don't believe that indistinguishable keys are actually useful
outside of the context of things like stegnographic embedding-- cases
where protocol 'metadata' doesn't tell you that a key is there
I suppose if the code already existed to do it I might as well go
"okay, sure why not", it's not going to harm anything (the added
computation time to generate the uniform encoding would probably be no
more than a 10% slowdown).  I wouldn't argue against it on the basis
that someone might believe it resulted in anti-censorship properties
that it doesn't have ... even though it's clearly the case... because
I categorically reject that form of argument. :)
I think your view on the two distinctive proposals is askew: PQ
agreement has a clear benefit under a plausible threat model and is
quite easy to implement... while uniform encoding is somewhat harder
to implement (though admittedly not very hard) but doesn't appear to
provide a concrete benefit under any threat model that I'm currently
aware of...
I also prefer the contributory key model, but Pieter proved me on IRC
last week that the attack form that I was concerned about was not
Do you actually have an attack in mind that you can spell out here?  I
don't see a harm in changing that, but given that I'd already twice
talked myself out of proposing the same thing, I'd like to understand
if I'm missing something. :)
It's AAD data in the mac, unless I misunderstand the protocol.
That would be pretty harmless, since the rekeying operation costs
similar to one message decryption.
The protocol requires rekeying at least after a given amount of data
is transmitted. Peers that violate that can be disconnected. But it
could be unfortunately infrequent.
I agree that is a good point.
Personally I'd prefer that we used a ciphersuite that effectively
"rekeyed" every message-- along the lines of the constructions
described    Unfortunately I
was unable to find _any_ well analyized  authenticated encryption mode
that has the fast erasure property.   It's too bad because it would be
trivial to adhoc one (e.g. use the extra 32 bytes from the poly1305
chacha run to update the keys for the next message).
It doesn't much matter, except for fingerprinting reasons.
Here we have a very clear motiviation.  On devices without hardware
AES/clmul constant time AES-GCM is _terribly slow_ compared to
ChaCha20/Poly1305. Performance on the slowest devices is where the
the ones where the ciphersuite choice likely matters at all (because
it could actually make a meaningful difference in their system's
ability to keep up), and the slowest devices that I'm aware of users
commonly using are also devices without good AES-GCM performance.
On fast desktop hardware the performance of AES-GCM and
ChaCha20/Poly1305 is also fairly close.
So when it matters, chacha20/poly1305 is higher performance by a wide
margin.  (Too bad, because otherwise I'd much rather use AES-GCM)
This can be done at the message level. E.g. new TX messages that round
tx sizes up to the next multiple. I don't think useful low overhead
padding is otherwise possible.
Writing things in stone is a great way to never finish a protocol.
Right now we know we have new upcoming proposals for messages where
the overhead matters, e.g. we need a replacement addr message ASAP,
and there is ongoing work for transaction relay that would also
benefit from low overhead.
The norm in Bitcoin is to ignore messages you don't know how to parse
anyways,  so there is no complexity that arises from "may negotiate"
itself-- only from actually making use of that possibility in the
future, so the merits of any particular usage could be decided when
something wants to actually use it.  The purpose of pointing out "may
negotiate" is, I think, primarily to avoid a debate about who would
assign numbers from this limited space in the future-- and the answer
just is that they're implementation defined (e.g. by the BIPs using
Right, encryption kills external analysers in any case. It's also easy
to just logprintf traffic (this is open source software after all),
which doesn't have a decoding problem.
I think it says 'received there' mostly because it's implicitly
telling you that you can hang up on someone who violates it. I agree
it would be more consistent to express it sending side there.

@_date: 2018-09-11 17:00:25
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
To this moment there remains no response at your post.
I'm not sure how I am supposted to have figured out that you wrote a
somewhat different repost of it elsewhere...
- An M-1 rogue-key attack would require the attacker would to either
You keep asserting this. It isn't true. Asserting it more does not make it
any more true.  I already explained how to attack this style of signature
(e.g. in the BCT thread).
Set aside your 'interpolation' for a moment, and imagine that you construct
a 2 of 2 signature by just adding the keys.  Your tell me your key, P1  and
then I tell you that my key P2 which I derived by computing -P1  + xG.   We
now compute P = P1 + P2 = P1 + -P1 + xG = xG ... and now in spite adding P1
with an unknown discrete log, I know the discrete log of P with respect to
G and I did not need to violate the standard DL security assumption to
achieve that.
With the 'interpolation' in effect the same attack applies but its
execution is somewhat more complex: instead of adding the negation of P1  I
must add a number of multiplicities of P1 (like P1*2, P1*3, P1*4...)
selected so that their interpolation coefficients add up to -1. Finding a
suitable subset requires solving a randomized modular subset sum problem
and Wagner's algorithm provides a computationally tractable solution to it.
The potential of rogue keys applies to both the keys themselves and to the
nonces. There are several ways to prevent these attacks, the musig paper
describes a delinearization technique which doesn't require additional
interaction or communication.
I haven't tested whether the R,s version is susceptible though.
There is a perfect bijection between the two encodings which is easily
computable, so they're the same thing from an abstract security perspective.

@_date: 2018-09-11 17:27:09
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
There is no "non- edistributiable multisig" proposed for Bitcoin
anywhere that I am aware of.

@_date: 2018-09-11 17:51:01
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Schnorr signatures BIP 
M of M is a particular threshold.   If you want M of M (there are
plenty of cases where M of M _must_ be used) then you get the
consequences of M of M, which presumably you want.
This has nothing to do with musig.  If you want a threshold other than
M of M then you use a threshold other than M of M.
No one is under the impression that M of M is somehow a replacement
for other thresholds.  We've spent more time talking about M of M in
some writeups in the past because it's exactly the case you need for
signature aggregation in Bitcoin and because it's a simpler case to
Yes, that is one possibility which is described in the musig paper,
but it requires users communicate an extra signature per key.  So, for
example, if used with aggregate signature it would completely
eliminate the communications efficiency gains from aggregation, making
aggregation worse than pointless.  It also has somewhat worse failure
properties than delinearization, because a signer that fails to
validate other's share signatures behaves behaves exactly the same as
a correct one, on honest inputs.  That approach has its uses but I
think that in any case where delinearization can be used it's a better

@_date: 2018-09-22 04:59:53
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] [bitcoin-core-dev] Bitcoin Core update notice 
Have been backported, not merely can be.
For instructions to be effective they need to be concise.  Presenting
people with a complex decision tree is not a way to maximize wellfare.
The few parties that would be better off on some other version already
know that they have some reason to not run the latest stable, and can
do more research to find out their other options.   The announcement
posted on the bitcoin core site, I think is adequately clear but if
you see an opportunity to improve it, please make suggestions.
It's decent advice, not misinformation.  You can run the fixed earlier
versions but they have other issues, I wouldn't recommend anyone run
older versions generally.
Reasoning about risk is complicated. For example, when people were
talking about only the crash component of the issue there were some
people stating "I don't care if I go down, an unlikely delay in
processing payments would not be a problem."  But, in fact, a network
exploitable crash is pretty dangerous: an attacker can carve up the
network into partitions that will produce long valid forks and reorg
against each other, enabling double-spends.   The best one sentence
advice available is to upgrade to the latest version. You'd probably
have to get up to two page explanations discussing trade-offs before
it makes sense to talk about running a fixed 0.14 or what not.
Theymos' language is stronger than I would have chosen, but I think
it's language that errors on the side of protecting people from harm.

@_date: 2018-09-22 20:49:04
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Fwd: [bitcoin-core-dev] On the initial notice of 
OKAY.  The only tweet I was shown was this one:
It doesn't many any mention to him not reporting it and I encountered
it in the context of another person citing it to claim it had been
Yes, in fact I referred to the that specifically in my message as well
as including his entire message in my post.
I'm unclear what you're now stating. Are you stating that awemany knew
that it could
cause inflation but indicated otherwise to us or are you stating that
he did not know and
in the abundance of caution he sent the report as fast as possible
before making that
I'm just asking because I'm confused by your response; I don't think
it's particularly important one way or another.

@_date: 2018-09-25 16:09:17
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Trivia on the history of compact fraud proofs and 
It's generally not /too/ important where ideas come from, even in our
open source non-patent encumbering world the only compensation people
get for sharing a good idea is the credit they receive. Most of the
time people are still happy to see their ideas further developed, even
if credit isn't sufficiently given.
But I'm particularly disappointed when attribution gets withheld in
the furtherance of political attaks. In some cases people have adopted
public positions that e.g. Bitcoin developers don't care about
scalability and then show that, by comparison, they care by publishing
work explaining/elaborating the scaling work of Bitcoin devs, but to
maintain consistency with their claims go through an extended effort
to avoid attributing them.
In two cases so far, I've painstakingly walked through an idea with a
political opponent in the Bitcoin space in private, only to have them
turn around and present the ideas I argued into their heads as novel
inventions without a shred of credit to me or the Bitcoin development
One of them was the case of Peter R and the subchains paper, which I
previously forwarded to the list the correspondence between myself and
him where I argued the concept of preconsensus as part of his disproof
of the orphaning-controls-capacity claim.
The other is on compact fraud proofs with Justus Ranvier (again, a BU
person). I promptly complained directly to Justus when I saw him doing
it. I'm now forwarding to the list for posterity, because after almost
two years and several pings, I was never even given a response.
This came up to my attention today because V. Buterin published a
paper on lite client security (  )
that was apparently unaware of proposals from our community on sampled
anti-withholding[1]. ... and this paper cites Justus' writeup as both
the only example of fraud proofs previously, and evidence that
sampling coded data was not previously considered.
[1] e.g. starting at "The improvement we have is this". Error coded
anti-withholding been discussed many times-- and I've been pretty
bummed that I've been unable to excite people much about the idea,
hopefully that will change with the eth hype machine behind it--, but
this particular citation is while not the earliest or clearest
description, perfect for this case since the context is that it's a
complaint that the same author was failing to cite our communities
past efforts on fraud proofs, and as a result they weren't aware of
the state of the art like anti-withholding.
---------- Forwarded message ---------
I spent _hours_ explaining how this technology would work to you on
reddit in private message, walking you through arguments on it.
Pointing out some of the details.
I also originally introduced the idea of compact fraud proofs to the
community (though the general idea was that of Bitcoin's creator,
without the compact-- just the unworkable kind) and was the first
person to enumerate the missing components for it.
Yet, the idea here is attribute solely to you, leaving me erased from history.
This isn't right.  It is especially offensive because the same parties
affiliated with BU use this plagiarism as a proof point that they are
scaling innovators while I am not, -- the height of absurdity when
they do it with ideas I invented and introduced to them.
Mike Hearn didn't have the integrity to credit Matt for the invention
of thinblocks; instead he was happy to have other people misrepresent
the history, I think you are a better person than him and hope you
will say something.

@_date: 2019-02-06 18:17:11
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Interrogating a BIP157 server, 
You keep repeating this smear. Please stop.
If you would actually bother reading the threads where this was
discussed previously you will see that there was significant interest
from bitcoin developers to eventually commit an output filter, and a
significant investment of effort in improving the proposal to that
end.  It is really disheartening to see you continue to repeat your
negative assumptions about other people's wishes when you haven't even
invested the time required to read their words.

@_date: 2019-03-06 03:02:51
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP proposal - addrv2 message 
Is 32 bytes long enough for I2P?  It seems like there are two formats,
is there a reason we might want to use the longer one?
Probably the spec should define the limit per address type (e.g.
sending a 32 byte IPv4 makes no sense).   And either a maximum for ANY
type (so that 1000*largest size is reasonable), or a maximum size for
the message (e.g. regardless of the included size, an add message
should never be over, say 100k).
I think clients should be discouraged from gossiping stuff they cannot
test but not forbidden from doing so. Separately, they should be
strongly discouraged from gossiping types they don't understand at
all. We don't really want to see people doing file xfer over invalid
addr types. :)

@_date: 2019-03-06 23:35:49
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP - Symbol for satoshi 
If Satoshi wanted the currency units named after him, he would simply
have done it. I think this behaviour seems creepy and is harmful to
It absolutely does not. Lightning uses units of 10 picobitcoin (1e-11
btc), which is significantly smaller.
Please don't.

@_date: 2019-03-08 00:40:00
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] BIP174 / PSBT extensions 
I think it's perfectly fine for someone to have a propritary extension
to PSBT that isn't going to work right unless used only between their
own stuff or need a translator to talk to ordinary PSBT stuff.
For that purpose, having some kind of versioning field that you can
use to indicate what weird PSBT dialect you're speaking might be
helpful, if only to allow for more reasonable error messages.

@_date: 2019-03-08 00:52:56
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Removal of reject network messages from Bitcoin 
On Thu, Mar 7, 2019 at 11:46 PM Andreas Schildbach via bitcoin-dev
That is already required because even in the presence of perfectly
honest and cooperative hosts reject messages at most can only tell you
about first-hop behaviour. It won't even tell you if the transaction
was ever even attempted to be sent to a next hop.  So alternative
handling must be provided and must be reliable for the software to
work at all regardless of reject messages.
Rejection on low fee (over the static minimum feerate) only happens at
the point where the nodes mempool is full, which is already at a point
where you might be waiting weeks for confirmation.
Rejection causes were also not stable or reliable because the validity
criteria cannot generally be tested independently. For example, if a
transaction is queued due to missing a parent it isn't rejected
because missing the parent is often a temporary issue, but its feerate
cannot be measured without the parent. Later, when the parent is
obtained, the transaction can then be rejected due to feerate-- but no
reject is sent then.
Output already spend is often completely indistinguishable from a
missing parent and can't get rejects generated for it generally.
Similarly, the error state detected for things like invalid signatures
are often not very useful. The software knows that script execution
returned false, but in the general case _why_ it returned false is not
clear, and a straightforward high performance validation
implementation doesn't necessarily yield a good way of figuring out
and propagating up that information.  (I think invalid signatures end
up returning a stack-nonempty state from validation currently, as an
example of that).

@_date: 2019-03-12 22:14:10
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] Removal of reject network messages from Bitcoin 
On Tue, Mar 12, 2019 at 7:45 PM Andreas Schildbach via bitcoin-dev
I'd like to better understand this, but it would be easier to just
read the code than ask a bunch of questions. I tried looking for the
handling of reject messages in Android  Bitcoin Wallet and BitcoinJ
and didn't really find and handling other than logging exceptions.
Would you mind giving me a couple pointers to where in the code
they're handled?

@_date: 2019-03-13 00:54:23
@_author: Gregory Maxwell 
@_subject: [bitcoin-dev] OP_CODESEPARATOR Re: BIP Proposal: The Great 
On Wed, Mar 13, 2019 at 12:42 AM Jacob Eliosoff via bitcoin-dev
It makes them infeasible to abuse without miner assistance... which
doesn't fix them, but in practice greatly reduces the risk they create
and allows efforts improving the system to be allocated to other more
pressing issues.
Don't underestimate the value of taking a principled position that
*strongly* avoids confiscating user funds.  Among many other benefits
being cautious about this avoids creating a situation where people are
demanding human intervention to restore improperly lost funds and the
associated loss of effort that would come from the effort wasted
debating that.
It's true that most other cryptocurrencies proceed without any such
caution or care-- e.g. bcash recently confiscating all funds
accidentally sent to segwit using Bitcoin addresses because of their
reckless address aliasing as a result of promoting the standardness
rule that made those txn non-standard before segwit without
considering the implications--, but they're not the standard we should
hold Bitcoin to...
All things in balance: Codeseperator and its related costs are not an
especially severe problem. The arguments on both side of this point
have enough merit to be worth discussing, at least.
