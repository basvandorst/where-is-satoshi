
@_date: 2007-08-09 12:28:10
@_author: Josh McFarlane 
@_subject: Proposal of a new hidden wiki 
The problem is setting up a system that is easy to replicate, but at
the same time immune to attacks.
If we attempted to implement an automated distributed service
function, where you could have multiple sites under the same key,
you'd have to have some method to prevent an attacker from simply
launching a massive amount of sites under this key to destroy it. 1000
fake sites would cause probably cause the load balancer to refer to
the fake sites the majority of the time, effectively take it off line.
Perhaps the best way to do a distributed service like this would be to
allow it to have a list of sites that it allows to substitute for
So, if three people wanted to host the Wiki, they would first have to
set up a method to keep all 3 wikis current.
Then, all three could launch their wikis. After the service was
operational, they could update their service definition with 'links'
to the other 2 services. Anytime someone requested one of the
services, it would randomly choose between any of the listed.
This would require the least amount of change but allow a basic
multiple-backend service to go up without allowing it to be
compromised by an attacker.
One could argue that you could also put the replication into Tor, but
I think that may be too complex to integrate directly.
Any ideas / comments?

@_date: 2007-08-09 13:20:54
@_author: Josh McFarlane 
@_subject: Proposal of a new hidden wiki 
However, this still relies on detection of a compromised server. Say
an adversary silently compromises a server, and does not make it known
that they now have direct access to the private key. They can now
launch additional servers that connect into the distributed service
without any barriers. This is detected, and the private key is
changed. The compromised server admin, not realizing that his server
has been compromised, updates his key also. Then, the adversary has
the new key.
The method I proposed limits this affect, as any new servers need to
be indexed by the relevant wiki owners. If a server is compromised but
no data is tampered with, it doesn't matter since the compromised
server can only affect it's own service listings. Any wiki de-synching
could quickly get it removed from the listings on the other services
hosting the wiki.
I think the biggest problem here is ensuring that it's not a password
security issue. Relying on the secrecy of the private key means any
compromise will have drastic affects on the service performance as a
whole. It might be better to try to come up with a solution that does
not rely on a shared secret private key.
The difficulty of updating with hosts with my solution could be easily
remedied with a tool that pulled from all the other hosts on your
current list and informed you of any new links. This way you could
easily review any suspicious links, and barring any detectable
compromise, add them in batches say once a week.
I think the distribution platform could also use some more discussion,
but I don't think this is going to be a Tor implementation issue, but
rather just finding the most effective method of live synchronized
changes. How do you plan on making a network raid array via the Tor

@_date: 2007-08-09 13:59:31
@_author: Josh McFarlane 
@_subject: Proposal of a new hidden wiki 
The problem with a SAN is that it is a central storage site. Give a
server write access and if they get compromised the entire Wiki would
be gone.
What if we set up some sort of communication system that 'informed'
the other wiki sites on the access list to changes made? Then, if a
site is compromised, all the transactions posted to it simply post to
the other wiki's and can be rolled back, and no one has direct control
over the other wiki mirrors. If a wiki owner thinks a site has been
compromised, they can simply remove the site from their list of
approved wiki-mirrors.
YaCy works differently than what we want. They distribute their
results across multiple machines, and do not mirror the data. What we
are aiming to achieve is a service that appears to be one wiki but is
indeed interconnected across many machines, each with it's own copy
but changes are reflected across the entire set. This way, if one
machine dies it does not affect service at all. If we do the YaCy
method, if one machine is compromised, whatever data hashed to that
server is lost forever.
I really think any solution that we want to implement needs to have a
mirror-based setup with each individual host (or groups of hosts) able
to provide the entire wiki on it's own.

@_date: 2007-08-12 23:36:57
@_author: Josh McFarlane 
@_subject: Directory issues 
I recently updated my Tor server to 0.1.2.16, however, I think
somethings wrong with the server now.
My server keeps updating the directory, but never enough to generate a circuit.
Aug 12 22:12:54.412 [notice] I learned some more directory
information, but not enough to build a circuit.
(2-3 days worth of logs of this)
Also, I don't appear to be updating at all on the Network Status pages:
Anyone have any idea what might be going on?

@_date: 2007-08-13 10:44:28
@_author: Josh McFarlane 
@_subject: Directory issues 
Well, that solved my circuit issue, but I'm still not sure whether or
not it's properly pushing to the directory servers. Looking on
 still lists my server as .14, and has not
restarted my uptime even though I had to bring the service down and
restart. Anyone have any idea why?

@_date: 2007-07-23 18:35:13
@_author: Josh McFarlane 
@_subject: Blocking child pornography exits 
You're injecting morality into something that does not have it, and
badly at that
By your arguments, cameras should not exist. They allow someone to
take a possibly offensive picture of someone else and then do whatever
they want with it. The overwhelming evil of someone using a camera
means that since camera makers don't design cameras in a way that
stronger discourages or prevents people from using them for evil, we
should not have cameras at all.
By your logic, we should throw away all of the technology that we
currently have. Most technologies are not designed to prevent evil use
of itself. There are laws and other preventions in place to prevent
this evil. The technology itself is not designed for evil. Content
filtering has no place in Tor code.

@_date: 2007-09-11 13:25:43
@_author: Josh McFarlane 
@_subject: Filtering traffic from your node - for exit points 
You do realize that if we enable filtering as you propose it, the
dangers of running an exit point grow exponentially. You no longer
have any deniability to government officials. Then, you start blocking
any kind of traffic that the government deems unacceptable, and we're
back to lack of free speech.
If you want a system of free communication, then you can't block based
on the data at all. Once you do that, the slippery slope collapses.
Actually, I believe that allowing content filtering would cause a lot
of us that currently donate bandwidth to the project to up and leave.
I believe it'd do a heck of a lot more harm than good.
