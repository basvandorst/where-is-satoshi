
@_date: 2013-12-16 18:18:16
@_author: Gerardus Hendricks 
@_subject: [tor-talk] TOR Research Project 
You need to start reading here: All collected and tidied up for you.
Skim through the abstracts of at least the 'boxed' papers.
"End-to-end traffic confirmation" as it is presented at the developer
page, is only concerned with comparing traffic signatures between two
targets. It doesn't concern itself with actually finding Alice and Bob in
the first place. You could compare the two streams on (the size of) the
content, but also on timing information.
Timing information of a traffic stream is very revealing, and is for a
large part outside the control of Tor: a human can't tweet while sleeping
and your computer needs to be on to be able to connect to the internet.
More importantly, the way the modern web works requires constant
interaction between client and server. That means that Tor traffic must be
relatively low-latency (order of seconds) to be even functional.
There exist high-latency mix networks, designed primarily for sending
email because the medium of 'letters' is tolerant of latencies of hours.
See the Mixminion paper of 2003 for that, but be sure to skip entire
Mixminion's goals isn't really to try to defend against the "end-to-end
traffic confirmation" attack as described on the research pages. It tries
to protect itself against a classic global passive adversary (first
paragraph of section 8.1). For brevity, we could call such an attacker the
Global passive attackers simply listen to all communications. It is often
assumed that the specific protocol is encrypted and that the attacker
cannot decrypt the streams. It can however see who sent how much data when
to whom. "Who" and "whom" mostly refer to network addresses, but
ultimately concern some bureaucratic entity (an organization or a human)
or something in physical space (a machine or a body).
It is not feasible to listen to all communications, let alone correlate
the traffic signatures. The question then becomes how much and which
traffic signatures an attacker would need to see to be able to find out...
what? There are several adversary models. A recent paper describes how
listening to a single AS could deanonymize users within a certain
timeframe having certain communication habits with a certain chance. If I
remember correctly, the paper simply assumes the "end-to-end traffic
confirmation attack" on timing information you are talking about:
(I made a separate post for this)
As an aside, I'm really interesed in how we could modify or build an
adapter to the web so it is more tolerant of high-latency interaction.
Seeing recent events it seems prudent to start thinking of ways in which
common applications could (for a small part) function in a high-latency
environment. We need to have a SlowTor.
It could for example work really well for many read-only uses of
applications, like reading web pages or other structured data (tweets,
video, maps, comments). A high-latency proxy to the open internet would
require nodes to cache content. The main problem is denial of service: how
to prevent nodes from forging the data itself? We assume that for the most
part, publishers won't be bothered to cryptographically vouch for the
authenticity of the data (like Freenet requires). A best-effort service
combined with BadNode flags would already be a lot.
On the writing side of things, I find it really hard to imagine how
high-latency connections would work with a modern web. Note "web", and not
"internet". I would be thinking of browser plugins written in Javascript,
that communicate with a backend that connects to the mix network. What
functionality must such a backend provide to the application?
An aside of an aside: if a node would provide a complete (non PFS) TLS
conversation between him and a webserver, could a 'verifyer' that is
agrees on the certificate belonging to that Common Name, verify that the
data contained in the conversation was indeed sent by the server in
possession of the private key belonging to that certificate?

@_date: 2013-12-16 18:23:24
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Aside of Tor Research Project: SlowTor 
As an aside, I'm really interesed in how we could modify or build an
adapter to the web so it is more tolerant of high-latency interaction.
Seeing recent events it seems prudent to start thinking of ways in which
common applications could (for a small part) function in a high-latency
environment. We need to have a SlowTor.
It could for example work really well for many read-only uses of
applications, like reading web pages or other structured data (tweets,
video, maps, comments). A high-latency proxy to the open internet would
require nodes to cache content. The main problem is denial of service: how
to prevent nodes from forging the data itself? We assume that for the most
part, publishers won't be bothered to cryptographically vouch for the
authenticity of the data (like Freenet requires). A best-effort service
combined with BadNode flags would already be a lot.
On the writing side of things, I find it really hard to imagine how
high-latency connections would work with a modern web. Note "web", and not
"internet". I would be thinking of browser plugins written in Javascript,
that communicate with a backend that connects to the mix network. What
functionality must such a backend provide to the (JS) application?
An aside of an aside: if a node would provide a complete (non PFS) TLS
conversation between him and a webserver, could a 'verifyer' that agrees
on the certificate belonging to that Common Name, verify that the data
contained in the conversation was indeed sent by the server in possession
of the private key belonging to that certificate?

@_date: 2013-12-16 19:51:29
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Aside of Tor Research Project: SlowTor 
The verifier is allowed to know the certificate, which means a public key
that is tied to a Common Name, possibly signed by an authority. The
verifier must know the client's keys so it can decrypt the conversation,
but not the server's private keys. I think the 'node' (client) can always
forge his own side of the story unless you modify TLS, but this shouldn't
Maybe an example works better:
The user wants to look up the frontpage of Wikipedia. The user's SlowTor
client has a certificate store with trusted certificates for certain
Common Names, such as Wikipedia. The client magics around and from a
SlowTor node receives a TLS handshake and an encrypted HTTP response
purportedly from Wikipedia, as well as the key to the response (which is
not Wikipedia's private key).
Can the client now verify that what the node was honest? Honest meaning
that the decrypted HTTP response really came from Wikipedia. I won't even
set the requirement that the response is the frontpage. Might as well be
the article about the NSA or a Wikipedia 404, but using the certificate
store it must be verifiable that it came from Wikipedia.
How does Perfect Forward Secrecy factor into this? How does the 'implicit
signing' work here?

@_date: 2014-04-05 01:56:30
@_author: Gerardus Hendricks 
@_subject: [tor-talk] canvas image data 
I haven't seen reports on the fingerprinting implications of HTML5 video and audio tags. I would like to read them. Here is a paper on browser fingerprinting using the canvas element:
Demo here:

@_date: 2014-04-07 01:45:48
@_author: Gerardus Hendricks 
@_subject: [tor-talk] torbutton icon disappeared 
Go to View -> Toolbars -> Customize and drag it back onto the toolbar. If the button isn't in the list, try the restore button.
The icon sometimes disappears on me as well, but only when installing an extension that adds its own icon to the toolbar.

@_date: 2014-04-07 01:47:28
@_author: Gerardus Hendricks 
@_subject: [tor-talk] torbutton icon disappeared 
Yes, right. You already tried that...
I only saw the first two lines of your post. Scratch my reply.

@_date: 2014-04-14 15:46:58
@_author: Gerardus Hendricks 
@_subject: [tor-talk] browser fingerprinting 
The Tor Browser Bundle does not disable Javascript by default. You can easily disable it by clicking the NoScript button at the left side of the address bar, and clicking 'Disallow all' (or something like that)
Javascript doesn't reveal your "local IP address and files".

@_date: 2014-04-22 15:43:26
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Old version welcome page does not warn about being 
> the version is ancient and should not be used. Not even a hint,
 > that a newer version might exist.
Then probably that ancient version is buggy when it checks if it's out of date.
The old welcome screen at check.torproject.org can't actually detect an old version. The TBB decides if it's out of date, and signals the site with the uptodate=0 query parameter, which in turn presents a warning.

@_date: 2014-02-10 00:09:04
@_author: Gerardus Hendricks 
@_subject: [tor-talk] IMAPS login errors 
That's not enough information to judge.
Assuming the attacker doesn't have a valid certificate, Thunderbird would give you a certificate warning upon connection, not after sending any password.
It could as well be the legitimate IMAP server, plainly failing to authenticate you.

@_date: 2014-02-28 17:56:50
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Can I set multiple socks5 proxies within torrc for 
No, not possible out of the box. It's quite hard to define what 'load-balance' would actually entail here. It would probably be something along the lines of what Multipath TCP does.
You could use the Tor API to periodically reset the value of the socks proxy. That's probably a different kind of load-balancing than you had in mind.

@_date: 2014-02-28 18:13:48
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Using HTTPS Everywhere to redirect to .onion 
Maybe an option to avoid that would be to do something along the lines of HSTS. A Tor-Transport-Security header, that would specify the hidden service that corresponds to the clearnet website being reached, only when reaching the clearnet website over authenticated TLS.
After receiving such a header, the TBB would refuse to load the clearnet website, and instead reach the .onion site for the specified max-age. The .onion site would (have the authority to) update the max-age too.
If would change browser behavior based on past user behavior, which allows for (some limited?) fingerprinting attacks.
Also, like with HSTS, you are still trusting the TLS PKI for the first connection if you don't preload the list. Though, without this you would need to trust the TLS PKI anyway, so there is not much to lose.

@_date: 2014-02-28 18:26:47
@_author: Gerardus Hendricks 
@_subject: [tor-talk] about circuit management 
For 1), read  and the question below that about path 'refresh'.
Your point in 2) that AOL would be able to detect you're using an anonymizer is moot, as they can simply check against the public exit-node database that you're using Tor.
Also, read about IsolateDest* here:  . All those options are disabled by default.
3) No, it was introduced quite recently into mainline Tor if I recall correctly; some months ago.
4) No, there is no maximum lifetime. Not only for hidden services; all TCP sessions persist until they are closed or broken.

@_date: 2014-01-05 02:09:22
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Hammond, Tor 
There exists specialized software to persist your IRC sessions:
Alternatively, just SSH into a remote (and possibly anonymously setup) server and connect to IRC there. Your presence on IRC then only reflects the availability of the server, not the availability of you and/or your Bonus points if you use Tor twice: once to SSH into the server, and once to connect to IRC. It won't matter much for end-to-end timing correlation attacks though.
Tor doesn't act differently. Tor only sees a TCP connection. It's just that HTTP transactions usually don't persist long, and IRC transactions do. You cannot change exit nodes while persisting a connection. Thus your HTTP transactions hop exit nodes on a 10-minute interval, and your IRC connections do not (until you reconnect).

@_date: 2014-01-07 12:48:43
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Risk of selectively enabling JavaScript 
Let us try to define what "fingerprinting Tor use" means exactly. It clearly does not mean "detect if you are using Tor". It probably has more to do with detecting that a certain single TorBrowser installation, including all its settings and plugins, is communicating with a certain server. An adversary could detect this at the side of the exit node (possibly even in the case of encrypted traffic), but more importantly it can be detected at the side of the server.
When does the fingerprinting attack matter? Does it only apply when a user is using the same TorBrowser installation for identities or behaviors that the user wishes to keep separate? It is already recommended to restart the TorBrowser to disconnect behaviors. Wouldn't it be also recommendable to use different TorBrowser installation for different behaviors, or is this going too far?
Please do question this. Don't fall into the false dichotomy between safety and usability. I believe there is a bug (feature request) in the tracker about adding a 'security slider' to Tor, that would allow users to make NoScript a lot stricter. Such a slider would make Javascript-avoiding users share roughly the same browser settings with a larger fraction of the Tor user base. Does anyone have a bugid?
There are bound to be edge case on which fingerprinting attacks can be launched. Please think about which slider implementation would result in lesser TorBrowser-installation specific settings.

@_date: 2014-01-07 18:33:32
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Risk of selectively enabling JavaScript 
Point by point.
Note that (potential) privilege escalation bugs are found way more in the Javascript component of Firefox. The Javascript engine is a complicated and heavily optimized beast and (Javascript-accessed) browser APIs have seen much more active development.
It is very reasonable to assume that more security problems are found there, and it might be reasonable to use a whitelist to mitigate those It's not "outright nonsense". It's supported by fact. Disabling Javascript will protect you against Javascript 0days in TBB (or non-0days deployed by the FBI against non-updated users). You may argue that it's not a good or realistic defense, but not that it doesn't do TBB doesn't disable Javascript by default. The premise of your, argument falls apart.
Filtering at a exit-node level is ridiculous for multiple reasons. You don't want to fix these issues on a stream level, and there are no advantages compared with client-side filtering. NoScript is rightfully in the TBB.
Also, claiming that any amount of filtering will "kill any fingerprinting, injection and tracking attempts" is naive at best. I can think of dozens of attacks, starting with a malicious exit-node.
Location-privacy and privacy between different (pseudonymous) identities have different attacks. We're talking about the latter here. Furthermore, end-to-end traffic confirmation attacks (if that is what you mean with traffic analysis) are not in Tor's adversary model. Tor is very vulnerable to them.

@_date: 2014-01-08 03:13:43
@_author: Gerardus Hendricks 
@_subject: [tor-talk] !!! Important please read. !!! 
Thank you for linking that resource. It explains the issue really well. I don't think the issue they are describing matches your doom scenario Yes, the CA system sucks horribly. It is best to assume that many of the world's intelligence agencies have a spare sub-CA of their national certificate authority. It goes too far to say that "all SSL streams are compromised". But there exist people that can decide them to be, indeed.
You cannot passively decrypt a stream with a CAs private keys. That key is only used for signing (fake) certificates. You would need the server's private keys, or even the temporarily shared secret in the case of EDH.
An active attack would be necessary to listen in on a stream. Active attacks are resource-limited and at least in case of the NSA, risky. You don't want to show everyone your fake certificate, so your attack must be targeted and used sparingly.

@_date: 2014-01-15 11:36:45
@_author: Gerardus Hendricks 
@_subject: [tor-talk] startpage and ixquick 
No, there is no security risk. There is very small anonymity problem, in that I guess that you browsing behavior then is likely to be a little different than the average Tor user: fingerprinting. I don't think it should concern you.
Question for you: what is the cranky behavior you're experiencing?
I sometimes notice with Startpage that for certain keywords it doesn't have a lot of results. I don't remember which keywords I used, and I can't reproduce it now.
Startpage uses Google by proxy, I believe Ixquick has its own crawler.

@_date: 2014-01-15 12:33:15
@_author: Gerardus Hendricks 
@_subject: [tor-talk] key generation on first boot with low entropy 
> How does tor generate its private key? Does it use /dev/random? Is there
 > an issue with bootstrapping a new tor node straight from the first
 > install, when entropy is potentially low?
Hi. I'm not entirely sure of the answer. I'll make a guess.
Reading the source of (the tarballed) Tor 0.2.4.20.
RSA router key and identity is created in router.c
router.c line 767 in init_keys()
line 770 --> prkey = init_key_from_file(keydir, 1, LOG_ERR)
line 393 --> prkey = crypto_pk_new()
crypto.c line 382 in crypto_pk_new()
line 386 --> rsa = RSA_new()
RSA_new() is from OpenSSL [1]
(curve25519 keys might be handled differently, I haven't looked)
I think you can find a lot of information on how OpenSSL handles its entropy. I'm not sure, but I think OpenSSL uses the non-blocking You might want to delay Tor startup / key generation a bit and not do it immediately after boot. If it is possible in your setup, you can simply write entropic bits to /dev/(u)random, like so:
echo "Jf3Gy7K5gvD2dcF" > /dev/urandom
Of course that string should be longer, and should be different in every instance you use it in (and of course it should be secret for any attacker)
Thanks for linking haveged. I hadn't hear about it before. I do have some doubts about its utility. As someone else has put it a year ago in a relevant thread (read it) [3]:
"as a non-mathematician, this sounds a bit like "we make a lot [of entropy] out of little". Maybe somebody could shed some light on this.."
[1] [2] [3]

@_date: 2014-01-20 17:59:05
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Open source firewall. 
Who is your attacker?
There are two kinds of anonymity in this world: anonymity that will stop your kid sister from reading your tweets, and anonymity that will stop major governments from finding your body. Microsoft is about the former.
More seriously: the (rule based or heuristic) firewalls you are describing are a last defense measure. Firewalls are born out of the assumption that running applications might be malicious or broken. You should Have A Little Faith, and trust Tails.
Realize that firewalls make sense under certain attacker models. The 'family' Windows PC has to endure some strain, and (ie. unknown application blocking) firewalls could add some safety.
You could argue that the iOS and the Android API permissions are the new 'firewall': the sandbox. It controls rough aspects of the application's behavior, and can present these aspects to the (casual) user in an understandable UI. The user unfortunately faces an 'all or nothing' dilemma (on grounds which are uncertain for the casual user).
What is the binary logarithm of sixteen to the power of eight?
How long does it take to generate a fresh RSA keypair?
Are the last eight bytes of keyids evenly distributed?
Thank you.

@_date: 2014-01-21 01:25:13
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Security issue 
Could you please explain why the same-origin policy of Firefox doesn't prevent this?

@_date: 2014-01-22 12:07:15
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Open source firewall. 
That might indeed be "snobbish", although I was trying to illustrative. Was is wrong though?

@_date: 2014-05-15 22:21:29
@_author: Gerardus Hendricks 
@_subject: [tor-talk] Firefox, Adobe, and DRM 
Hi David,
I can sympathize with the position that Mozilla has taken concerning W3C EME. I'm left with a related question though:
Suppose that the (necessarily closed-source) DRM component is completely sandboxed and separated from the rest of the code, so that its only inputs are the encrypted stream and some unique key/token, and its only output is a buffer of decrypted frames.
(this illustration by Mozilla paints that picture nicely:  )
Then what prevents the open-source Firefox application, that needs to 'render' that framebuffer into a nice window with shiny buttons, from cleanly reading and saving that buffer of decrypted frames to file?
If the answer is "there is no protection", then it seems insane that 'the bad, evil industry' would allow such sandboxing of a DRM component.
This might already have been discussed elsewhere, but I haven't been able to find that discussion. Any pointers would be welcome.
