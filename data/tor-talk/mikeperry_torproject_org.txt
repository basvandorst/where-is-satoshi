
@_date: 2011-12-05 14:07:27
@_author: Mike Perry 
@_subject: [tor-talk] [liberationtech] Not another Haystack right? 
Thus spake nathan at freitas.net (nathan at freitas.net):
I am a heavy Tor user myself, using Tor Browser almost exclusively for
my web browsing. One major thing I have noticed is that I no longer
browse the web like a normal person does. Most people I observe tend
to browse depth-first, clicking on links of interest in a single tab,
and making heavy use of the back/forward buttons to navigate their
browsing experience.
I however, browse breadth-first. When investigating a topic, I open a
bunch of links in parallel in new tabs, and then just begin reading
the first one to load. I find that parallelizing this latency makes it
manageable, and I don't find myself spending as much total time
waiting on page-load as depth-first Tor users do.
Does this mean we should consider altering Tor Browser's default
behaviors? Perhaps by default, clicking on a link should open it in a
new tab, instead of replacing the current tab, to encourage
parallelism? Or is this too extreme and likely to confuse normal
humans without helping them any?
Link prefetching is a related idea, but it can be fragile and come
with privacy and linkability risk, once you start prefetching across
multiple arbitrary domains automatically without proper identifier and
cache(!) isolation.
Of course, we could also just continue to focus on optimizations to
improve latency in the network itself, instead.

@_date: 2011-11-29 12:09:40
@_author: Mike Perry 
@_subject: [tor-talk] copying old profile to new Aurora not working 
You're still doing it wrong, man.
As I said the last time you shot yourself in the foot doing this
copying the wrong way
  I would recommend just overwriting your old TBB dir with the new data.
  Sometimes we change prefs+create new ones, which could cause bugs for
  you when you copy old-over-new.
  If there are a lot of prefs you need to change, you (or we) might also
  be doing it wrong. Might I ask which ones you need to keep?
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):

@_date: 2012-08-10 12:01:14
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake Maxim Kammerer (mk at dee.su):
Tor is a relatively new and underdeveloped technology, used exclusively
by early adopters, technophiles, weirdos, and people who really really
need it. We don't advertise. We're not in stores. This is because we're
pretty much still in the prototype stage.
As with most new and underdeveloped technologies, we're going to have
our demographics dominated by those early adopters and weirdos rather
than our target demographics.
For example, in terms of number of users, I'd wager a top current
demographic is "Paranoid Schizophrenic". If it's not  it's gotta be
top 5. The more general category "Antisocial tendencies" is probably
another top 5. "People who secretly view fully legal porn" is almost
certainly up there, perhaps vying with the schizophrenics for the The early Internet (even as late as the mid-90s) was also dominated by
these same classes of people. Eventually it became usable by the
normals and the demographics shifted quite dramatically.
We all work on Tor every day so that it sucks a little less each day, so
that those demographics *can* shift. However, right now, it's only the
extremes and certain niche elements of society who will suffer through
using it: Dissidents, journalists, law enforcement, militaries, and the
antisocial weirdos/crazies.
The Tor users page is in my mind a reflection of what our demographics
will look like as we improve our technology enough to be useful for
everyone who wants Internet privacy. We leave out the antisocial
creeps/weirdos/crazies because they are not our target userbase, and
their relative dominance right now is merely a reflection of our
relatively early development status.
No offense to the weirdos, though. You guys are my people :).
Even today, this statement is not accurate wrt exit traffic. The handful
of papers you and others have linked show that even with our current
userbase, illegal and questionable traffic takes up a small percentage
of the Tor exit traffic, unless you count all forms of pornography as
In fact, the paper you linked even has an "Illegal/Questionable"
category, and guess what, it's  at 0.15% of the traffic:
I don't know what you're looking for, but perhaps your own desire for
everyone to use Tor for "illegal and questionable" stuff is biasing what
*you* find?

@_date: 2012-08-10 14:46:20
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake Juan Garofalo (juan.g71 at gmail.com):
The O(10M) "Do Not Track" Firefox users. Similar numbers of Chrome users
use Incognito mode (actually, probably more).
In general, my bet is that approximately 10% of the population has a
strong desire for personal communications privacy pretty much all the
I'm also betting that way more would like real privacy at least
sometimes, even if they don't want it all the time.
I work for me, man. I use Tor every day. In particular, I work on Tor
Browser because every time I am forced to use a browser that does not
defend against third party tracking, I cringe.
That's a shame. I kid because I love.
(What did I say about "Antisocial tendencies" being one of the main
demographics of Tor users? :)

@_date: 2012-08-10 15:14:43
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake Maximum Camera (mk at dee.su):
Right. People who want to read a book on the topic can just read the
mailinglist archives, I guess?
Or maybe just read our FAQ?
.onion is another thing that is tragically failing to reach its
potential because no one tries to make it useful for normal stuff. I
rather intensely dislike the way it is being used now, but I also know
that good use cases exist, and amazing ones are possible.
Anonymized communication endpoints have the potential to revolutionize
how people communicate. The ability to transmit a message to a hidden
endpoint inside an overlay network allows for chat, email and social
network sharing mechanisms that do not disclose your social network
activity to observers or to infrastructure maintainers. This is an
incredibly awesome and powerful tool. I worry deeply we'll lose it
before it has a chance to develop away from just being used for
Also, for a point of clarity, Silk Road doesn't exist because of Tor. It
exists because of bitcoin. Not that I have any problems with the idea of
private crypto-currency, but it's worth clarifying.
They're not make-believe, they are all real users. It seems for some
reason, you just want us to *also* add "Lunatics", "Paranoids",
"Hackers" and "Criminals" to that page, and we think that material is
better off in the FAQ instead.
Go advertise that stuff for your own tools, Mr. Maximum Camera. We think
it's better suited for the FAQ rather than the main use case page.

@_date: 2012-08-10 20:19:42
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake Gregory Maxwell (gmaxwell at gmail.com):
Yeah, unlike exit nodes, this is definitely not the case. Your node can
be chosen as either the guard, the introduction or rendezvous point and
you will see traffic correlation for page views at a much greater
frequency than expected, if you're not careful about it.
Yeah, a related question is "How much 'Illegal/Questionable' traffic
through exits actually *is* law enfocement?" It's not all of it, of
course. Might not even be most of it. Unless they have automated
Still, it is a little surprising they can't trace bitcoin yet, though.
Maybe they can. I think my bet is also on Silk Road not surviving in the
long run for that reason... It's very interesting to watch, for sure.
It's like we're getting an extra season of The Wire, except in a much
weirder world that couldn't possibly exist except in some Sci Fi novel.

@_date: 2012-08-11 01:08:55
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake grarpamp (grarpamp at gmail.com):
Word. In terms of chill: in the past, harassment like that has always
created more nodes, not less. So we've got that going for us, which is
Based on that, if this were to happen, I think it would be quietly at
the data center level before the operator level. I'm pretty sure we'd
hear about it if someone in the relay operator community itself was
"leaned on", by anyone. At least I hope a lawyer would hear about it. We
know enough of them. But, this is why we have 3-hop distributed trust properties as opposed
to allowing one hop exits. This is also why it drives me nuts that path
selection can be easily manipulated if node keys are quietly stolen, or
worse, stolen through coercion. I don't want to be on the other end of
that rubber hose.
Fortunately, one major network symptom of that type of attack is
excessive circuit failure on the client side. Unfortunately, if the
network is super unreliable to begin with, there will be some noise on
that signal. (Also, as you know from your path bias warn bug, it turns
out there's a damn lot of crazy codepaths in circuit construction. Who
knew? I didn't write the original code. I just tried to help modify it
to make it slightly safer).

@_date: 2012-08-11 10:54:04
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake grarpamp (grarpamp at gmail.com):
Yeah, thats the thing.. Individual buyers and perhaps even some of the
small-time sellers might be able to take enough precautions (if they
know how) that will significantly reduce their visibility.
But from the paper, it sounds like the BTC flow to Silk Road itself is
quite large and might be measurable or at least can be approximated from
the website itself... Also, if it ever exits the BTC network and starts
moving into the old-skool financial system at that quantity, there's (at
least currently) not very many points it can do that without being seen.
The problem is that even with mixes and batching, bitcoin provides a
Global Passive Adversary for free, which can be used to map and measure
total BTC flow through the network to various sinks (eigenvectors +
eigenflow). Based on the established dogma that still rules the Tor
threat model, "BTC cannot win!!!1" for this reason. At least, not when
you're a substantial and atypical chunk of the BTC flow versus norm.
Like I said, it will be very interesting to watch. It's almost like some
aliens came down from space and double-dog-dared the ballsiest,
craziest, most aggro humans on the planet to try to solve timing
correlation attacks and then called them all pussies, threw the bitcoin
source code at their feet, and then flew off. You know, because they
needed that shit to interact with our violent monkey society at a safe
enough distance and everybody else on this planet had given up. The bad
Sci Fi just writes itself. ;)
Hrmm.. Yeah, this is probably also likely. I wonder if they started
sporting the bling yet. Unless they're *also* cackling madly from their
space ship, wondering why the monkeys didn't learn the futility of this
whole game a century ago with alcohol...

@_date: 2012-08-11 14:11:52
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake antispam06 at sent.at (antispam06 at sent.at):
Yeah, I know. I saw that. Pretty amazing, especially given that it came
out right around when Face book's user's numbers declined by 1%.
Someone saw their portfolio about take a pretty big hit I guess.
I bet I can also guess why the article was written towards making kids
get accounts, too. I think Facebook is losing cool among the Young's.
The good news is that parents telling teenagers they *have* to do
something is the /fastest/ way to make it uncool. Let's hope it starts
an epidemic. I hate being the only weirdo without a Facebook account ;)
Yeah, but it's not like wordpress blogs are the bastion of great
Internet content, either. The thing that makes published Internet
content useful is the search index, and that thing *could* just
memory-hole all the scum, and maybe even the stuff that links to the
scum, too.
But I'm also not even talking about self-publishing. These things are
arbitrary communications endpoints. What if you could call them and use
voice? Ok, that's a bit far off. But chat and email could work today, if
someone built the software to support it and people used it.
It would be costly and our current network and protocols might need
several upgrades first, but we could build out a whole platform on this
technology. Steve Jobs built the most valuable company in the world
around a computer that has adoption rates smaller than the 10% of people
who probably *really* want private personal communications at least
sometimes. The secret sauce is in how the integration works between
devices and technologies, and how much is sucks to use versus
centralized and non-private solutions.
It wouldn't be easy or quick to get there and existing players may be
sorely tempted to manipulate the legal system to keep new decentralized
technologies out. That would be depressing for a while, but ultimately I
think the space aliens would probably think of something to handle that
scenario. They appear to be pretty smart and strangely determined, for
some reason...

@_date: 2012-08-12 01:52:08
@_author: Mike Perry 
@_subject: [tor-talk] Tor as ecommerce platform 
Thus spake Gregory Maxwell (gmaxwell at gmail.com):
Yeah, I was just saying that Bitcoin *might* not be very safe, given the
built-in transaction surveillance properties. It might be possible to
make it safe, if you're part of the 1% in terms of your ability to use
the software.
But also, don't forget that both classification/correlation attacks in
specific and academic research in general are of course expected to be
bullshit by default until publicly reproduced externally (which never
happens, because why publish your data or even your source code?).
I would *hope* that most real scientists already know that latter part.
But that is only the tip of the iceberg... If you want to know what *really* drove me over the line into full
madness when  didn't give up, this was the core idea:
During the  attack, I kept waiting for a "dump" of my personal
files to show up with "evidence" of Freedom Hosting content inside.
Here's why: In a world where intelligence agencies and maybe even LEO
organizations stockpile software exploits, do we really have chain of
custody over evidence? Can these lunatics who purchase exploits really
expect their exploit dealers not to get owned by organized crime, or
*other* intelligence agencies?
After my  attack, I now wonder how long until any one of us
gets framed up for Silk Road. I also wonder who will be the lucky winner
this time. I really hope I get a pass this time. I need a vacation from
this shit ;).
I mean, Silk Road is an organization with a budget large enough to
"anonymously" purchase Chrome 0days at the rate of at least *two per
month*...  And they're like 1% of the almost *ONE TRILLION* annual
market for illegal drugs:
They also seem to have little else to spend their "cash" on that isn't
obviously traceable by old-skool finance... At least, not until they
achieve Real Ultimate Power.
In other words: they could frame up anybody they want. After my comments
earlier about the potential traceability of their bitcoin flows, I
actually had a panic attack today that I might soon be unwittingly
running some of their infrastructure.
Exciting times we live in, for sure.
Everything about how we do computer security is totally fucked. I mean,
totally. We're in a whole lot of trouble, and if COMSEC doesn't start
winning out over SIGINT again (did I just type that? must be the greys)
there's really no hope of justice for any of us. Except maybe the
super-rich. Let the good times roll, eh?
Here's an idea: perhaps instead of spending quite so many hundreds of
billions on datacenters in Utah, perhaps we should be spending a couple
of bucks here and there to pay to keep exploits out of the hands of the
lunatics, and make sure the bugs actually get fixed, for *everyone*?
Also, you know what, fuck the drug war too. It's going to consume us all
like a cancer. That shit is so *over*.
P.S. "Be seeing you!" ;)

@_date: 2012-08-24 15:12:42
@_author: Mike Perry 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Thus spake Ted Smith (tedks at riseup.net):
The Raccoon has made a believer out of me, but there are some limits to
both of his/her proofs.. The full proofs can still be found here:
The actual numbers from the examples of the first proof are affected by
the resolution of the data retention. The core concept of the proof
seems to hold no matter what (that full dragnet n^2 correlation is hard,
and the amount of similar co-incident traffic - aka the base rate - is
what makes it hard), but if the adversary has full observation of *all*
traffic data, they *might* be able to do better than 99.9% true positive
rate. It's not clear that low-resolution connection-level data retention
or even sampled netflow data can provide anywhere near that true
positive rate, though.
A full adversary may also get to combine repeat observations (assuming
it is possible to identify them as from the same user), but the post
mentions that.
Incidentally, my guess is that's probably one of the reasons for the
huge boondoggle^W datacenter in Utah. They probably realized that to
reliably track large botnet activity, they really needed to log all data
forever. Well, keep sitting on the unpublished 0day software
vulnerabilities, guys. That should totally help you solve both those
problems, once and for all. Oh wait. ;)
Anyways, the key thing I think the first proof tells us is that even
sloppy defenses against correlation attacks are likely to work against
dragnet surveillance/data retention, especially if you have a lot of
co-incident traffic to blend in with and if the data retention
resolution is low.
I think this alone can justify experimentation with traffic padding
to/from Guard nodes, where bandwidth is relatively cheap and plentiful.
It especially justifies minimal amounts of Guard node padding to defend
against the single-ended version of the end-to-end correlation attack,
which is also known as the "website traffic fingerprinting attack". The
single ended version is even *more* vulnerable to the properties of
background traffic than the double-ended version, and has far fewer
reliably recognizable traffic features to extract from data streams as
well. See this blog post and its links for more details:
 It's my personal opinion that we should also experiment with Guard
padding against the website traffic fingerprinting attack, and see
how far that gets us against e2e correlation while we're at it.
Unfortunately, current academic religious dogma tends to hold that
correlation is unbeatable no matter what. This publication and research
bias already has hindered and will likely continue to hinder research
into viable defenses :(.
The second proof wrt tagging attacks scared the crap out of me. However,
the "c/n" compromise result at the end hinges crucially on nodes that
fail circuits being able to attract additional traffic to make up for
it. The bandwidth authorities might do this to a certain extent
currently, and will certainly do it if operated in "PID feedback mode".
However it's still not clear that the 3 guard node round-robin circuit
selection properties of Tor wouldn't end up also hampering the attack
against specific clients (unless the Guard nodes' keys were stolen and
the attack is locally targeted).
Either way, it's caused me to drive Nick nuts by pushing hard to include
at least *some* kind of simple defense for circuit failure attacks on
the client-side. How much of that actually survives in 0.2.3.x in a
functional form remains to be seen :/.
P.S. Incidentally, you used to be able to get the full copy of the first
proof in the old seul archives at
 but since seul
is currently down with unknown hardware and disk issues,
might be the last full public copy other than your repost. I've added
the Raccoon on Cc so s/he can hopefully do a full repost if the seul
archives end up being destroyed forever.

@_date: 2012-08-25 18:11:46
@_author: Mike Perry 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Thus spake Maxim Kammerer (mk at dee.su):
Well, the argument over correlation accuracy comes down to observation
resolution, feature extraction ability, and academic lab conditions
versus reality. For an example, let's assume that the adversary cannot
see inside of Guard TLS connections. With this assumption: if at any
point there's concurrent Guard TLS activity from a single client (either
other circuit activity, directory fetch activity, or circuit
pre-building activity), then some or all of your fine-grained timing and
size information features go out the window.
To see the effects of this currently, consider: Is it *really* the case
that only one connection in *a billion* experiences incidental
concurrent activity that interferes with or obliterates high-resolution
feature extraction? I think the actual rate of random (or deliberate)
concurrent activity is much higher than that, especially for heavily
used tor clients, and even more so if they are serving as bridges or
relays. But, against high-resolution adversaries, the really interesting
question is: How little real cover traffic is actually needed to obscure
timing and size information to the point where the remaining features
are insufficient for high rates of correlation success? And over how
many observations can such activity be expected to survive for a given
base rate of similar activity?
I suspect that for relatively short-lived bursts of traffic like web
site views and random webapp AJAX activity, we can actually do pretty
well with very little effort and overhead. Especially against the
one-ended version of the correlation attack: the website fingerprinting
attack, but probably against both.
But for long-lived or otherwise atypical connections, you're absolutely
right. There's just a whole lot of information encoded there.. Almost
any level of observation will likely be able to correlate such flows
eventually, and it's also hard to imagine generalized padding techniques
that could blend these flows with web traffic.
Unfortunately, because academia has mostly concluded that this work is
uninteresting and that all forms of this problem are generally
"impossible", we have no solid answers to these types of questions wrt
what can be done in practice. Perhaps it is merely because defense
work is less sexy than attack work when it comes to getting
publications. I don't know for sure. I haven't yet figured out exactly
why CS academia is broken. There's a whole lot of symptoms, though...
But anyway, failing real research, there's always the botnets, the drug
war, and the aliens to guide us... Can I get three cheers for Big Data?
After all, I'm sure we can trust Them to tell us how the science shakes
out in the end, amirite? ;).

@_date: 2012-08-28 19:06:32
@_author: Mike Perry 
@_subject: [tor-talk] [Tails-dev] Please review Tails stream isolation 
Thus spake Robert Ransom (rransom.8774 at gmail.com):
Here's the ticket for implementing Tor Browser's use of the circuit
isolation feature: Summary: we plan on using some variation of the "url bar isolation"
property from
 to guide
our circuit reuse implementation. So long as the url bar stays the same,
we'll use the same circuit for sure. This shouldn't be *too* tricky to
do using mozIThirdPartyUtil.
I'm still debating if we should *also* try to track user
click-nagivation, and use the same circuit so long as the user is
clicking on links (as opposed to entering a fresh new value in the URL
bar). This could be modeled by tracking the referer, or the last url bar
domain to be entered. This will be trickier to implement, but will
reduce client circuit creation.
Either route will require a patch to Firefox, since it is not possible
to set SOCKS usernames+passwords from a .xpi right now.
Roger also wants to turn this into a research project of some kind to
determine the optimal circuit isolation mechanism network-wide, but that
seems like a waste of time to me, since what I'm proposing doesn't
strike me as very resource-intensive in the common case. I'm open to
suggestions on how to make it less painful, though.

@_date: 2012-12-09 00:38:11
@_author: Mike Perry 
@_subject: [tor-talk] Botnets through Tor 
Thus spake andrew at torproject.is (andrew at torproject.is):
Indeed. A little over a year ago, I came across this article:
It describes a 4.5 million host botnet that uses a side channel inside
the Bittorrent DHT (Kademlia) as a communication channel for C&C.
Incidentally, we've pondered using similar side channels ourselves
either for bridge discovery or even as a full-fledged pluggable
transport. It's low bandwidth, but there's a *lot* of noise to hide in.
There have also been academic analysis of similar (but much smaller)
DHT-based botnets as early as 2007/2008:
In comparison to using covert channels in a pre-existing public DHT,
using vanilla tor + a hidden service is awfully centralized for C&C
communications, and with a comparatively not very large userbase to hide
Tor does have the advantage of being much easier to deploy, though. If I
had to guess, this means they probably don't have much of a crimeware
dev budget as compared to much larger, more sophisticated operations.
This would also be consistent with it just being a one-man operation run
by the IAmA poster on reddit.

@_date: 2012-12-19 02:49:12
@_author: Mike Perry 
@_subject: [tor-talk] Roger's status report, Nov 2012 
Thus spake Roger Dingledine (arma at mit.edu):
Does this theory have a better name?
Did he describe the phenomenon in any more detail?

@_date: 2012-02-02 10:00:03
@_author: Mike Perry 
@_subject: [tor-talk] How does the new browser know where to find old 
Thus spake M Robinson (mr.m.robinson at gmail.com):
Good question. This doesn't happen to me. What OS?

@_date: 2012-02-08 10:16:00
@_author: Mike Perry 
@_subject: [tor-talk] tor-blocking sites 
Thus spake Roger Dingledine (arma at mit.edu):
One of my goals is to provide an alternative to captchas and IP bans in
the form of computational proof of work:
If anyone has any experience with distributed computing projects that
could meet our requirements, I'd love to hear about it.

@_date: 2012-02-08 19:44:57
@_author: Mike Perry 
@_subject: [tor-talk] tor-blocking sites 
Thus spake Moritz Bartl (moritz at torservers.net):
If you read the ticket, the design sketch does not require constant CPU
burning. You would only use the CPU until you built up a sufficient pile
of tokens, and you would only do that intermittently.
Maybe. But for sites like Google, Yelp, scroogle, it is often just
anonymous read-only access that causes problems.
Historically, Google and Yelp have been hard to work with over this
point. I think the core problem for them is that the minority of
scrapers consist of the majority of Tor requests, at least until the
captchas/bans kick in and the scrapers go away until they expire..

@_date: 2012-02-08 19:59:08
@_author: Mike Perry 
@_subject: [tor-talk] tor-blocking sites 
Thus spake Andrew Lewman (andrew at torproject.org):
I admit I haven't read all of the various iterations of the Nymble
literature, but every one I've looked at so far seems to start with
"Assume you have some expensive, scare resource. Let's say IP
address"... Even if they blind it properly with some clever distributed trust scheme
that requires multiple colluding parties to divulge the entire Tor
userbase IP list, it seems to me that IPv4 addresses aren't really
scarce when you're talking about one-time use only to obtain a Nym that
can be used for a while.
Therefore, my current thinking in
 is that if we can
authenticate computation as the scarce resource, why do we even need a
full Nymble server? At best it *might* ease implementation for account
banning, but it probably would just add another point of failure and
useless complexity.
Am I wrong?

@_date: 2012-02-09 15:10:05
@_author: Mike Perry 
@_subject: [tor-talk] tor-blocking sites 
Thus spake Maxim Kammerer (mk at dee.su):
As far as I know, no one has ever tried it. Some academics once pointed
out that proof-of-work would not work for email, but that was primarily
because email is often one-to-many. They did not consider one-to-one
activity (like web page access) in their analysis. Perhaps everyone
simply read their work and just assumed proof-of-work could never work
for anything?
The proposed system has two knobs that site admins can use: computation
quantity, and computation freshness. As scraping abuse increases, admins
would be free to set the "price" higher as needed, and require more
recent, fresh computation as needed. When abuse is low, the requirements
can be turned down.
I created these two knobs because what we have seen over the years is
that scraping abuse over Tor is not constant. Every few months, some
jerk decides "Hey, I know, I'll scrape $SITEX and resell the data and
make MEEELIONS", until the bans or captchas go up and they shut down.
Then, all is quiet until the bans expire and the next jerk gets the idea
a few months later. At least, this is the pattern that the Scroogle
admin sees. I assume the situation is similar with Google directly, but
they are very tight lipped.
Captchas currently cost anywhere from $0.01 to $0.001 to solve. Yes,
that's 1/10 of 1 US cent each:
If they are working at all now, they work only because they marginally
raise the cost of bulk scraping enough to slow scraping crawls and
reduce the server load back to acceptable levels.
I think tunable proof-of-work could easily beat this very low bar, with
much less hassle for users.

@_date: 2012-01-13 14:10:34
@_author: Mike Perry 
@_subject: [tor-talk] Vidalia+Tor separtely from TBB`s Firefox 
Thus spake Greg Kalitnikoff (kalitnikoff at privatdemail.net):
Yes, this should be fine. You will also get New Identity back if you
set the TOR_CONTROL_PASSWD and TOR_CONTROL_PORT environment variables
properly. I should probably document these two somewhere, as you're
not the first to ask about this sort of setup...
There should be no need to use filters to address 3rd party linkability
with a proper implementation of the requirements in
To my knowledge, the only remaining 3rd party direct linkability risk
is through HTTP Keepalive (Section 3.5.6), but this linkability is
limited to a 20 second window.
There is a risk of first party linkability through redirects (Section
3.5.7). This one will be harder to solve, but it is more noticable
There are also fingerprinting risks involving time that need to be
addressed (3.6.6 and 3.6.7), but the verdict is not in as to exactly
how much info these provide in practice. Regardless, we should also
have some level of mitigation in place for Tor Browser 2.3.x.
It is my opinion that these remaining threats do not justify the need
for filters, and that we should focus on eliminating these few
remaining issues rather than trying to design a filter mechanism that
isn't full of fail.

@_date: 2012-01-16 18:59:11
@_author: Mike Perry 
@_subject: [tor-talk] Vidalia+Tor separtely from TBB`s Firefox 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
These beacons can record your visit on one site, but they are not
able to store identifiers in the browser that would allow them to
determine it is the same you when you visit another site.
This removes the linkability property that gives 3rd parties
omniscience over browsing activity.
Yes, I believe that at best, blocking referrers only prevents
accidental info leakage. There are too many direct channels between
page elements to expect to stop all transmission. In fact, if you look
at Google+ +1 buttons, they already encode the referer url directly in
the GET request, probably because of the lack of referrers in
http->https scheme transmissions.
See  and the rest of
that thread for more detailed discussion.
I guess you mean they could be malware? Again, we should persue a
general approach of least priviledge and hardening. You face the same
risk from exit nodes, after all.
I'd love to see Seatbelt, AppArmor, and SELinux policies created for
TBB, though.

@_date: 2012-01-19 23:27:54
@_author: Mike Perry 
@_subject: [tor-talk] A secure browsing model? 
Thus spake grarpamp (grarpamp at gmail.com):
Are you trolling me? This is exactly what I've designed Tor Browser
for, and I've stated and reiterated this fact on this list,
tor-dev, and the blog quite a few times now.
Is that too technical? How can I improve the design document so that
it is more clear that it is exactly what you're looking for?

@_date: 2012-01-20 00:42:09
@_author: Mike Perry 
@_subject: [tor-talk] A secure browsing model? 
My mistake. I'd love to hear non-browser based solutions for privately
browsing the web. Sounds like an interesting topic!
Thus spake Gozu-san (gozu at xerobank.net):

@_date: 2012-01-20 20:49:36
@_author: Mike Perry 
@_subject: [tor-talk] A secure browsing model? 
Thus spake Andrew Lewman (andrew at torproject.org):
Yeah, I was thinking that we may want to make a human version of the
design document for use on the main website. It should be a short
description of the url bar origin isolation idea in plain english, and
introduce the "New Identity" concept, perhaps with some images.
In fact, I think the new Firefox 4.0+ URL bar hostname shaddowing
already suggests subdomain-based isolation. They did it for SSL
awareness and related phishing issues, but it helps suggest our
privacy properties too. I think the most surprising thing to laypeople will be "Hey wait, you
mean Google *could* somehow know what I'm doing on twiter in my normal
browser?" The answer, of course, is that Google can and does (at least
at some level). In fact, I'm not aware of too many big web players
that don't have this ability in all existing browsers other than TBB.
We need to make this fact quite clear, I think.
You're right, on a more technical level we need to tighten some
definitions. Unfortunately, the underlying implementation for each
identifier storage is not always uniform between FQDNs versus
subdomains. But, this could just mean we take the loosest definition.
Ie, in most cases mail.google.com can track you on
encrypted.google.com, but mail.google.com can't track you on

@_date: 2012-01-20 21:54:00
@_author: Mike Perry 
@_subject: [tor-talk] A secure browsing model? 
Thus spake Gozu-san (gozu at xerobank.net):
We are very concerned with it, but only up to a point. We have to
assume (for our own sanity) that the underlying OS and concurrent
applications are not malicious.
However, even non-malicious IPC can still cause problems. See for
example That bug was a new proxy bypass vuln (the first one in literally
years) that happened on Ubuntu Unity, causing a regression in
previously tested drag and drop features that were initially evaluated
as safe.
We really need an automated testing infrastructure to catch stuff like
that, where the platform changes out from under us. I believe we'd
find the need for automated testing with just about any approach as
technologies change out from under us. It's either that, or learn to
accept a higher failure rate over time in the field. That's just basic
engineering :/.
We however have zero automated testing in TBB, and instead depend
entirely on the community, and anonymous reporters like the one who
filed that ticket. Is that enough to lower overall risk? Well, we're
just about the only ones in the world operating even at the level that
we do currently, so who knows. Maybe it is good enough, for now. Can't
beat the price :).

@_date: 2012-01-25 11:34:24
@_author: Mike Perry 
@_subject: [tor-talk] Fwd: ANONdroid 
Thus spake Nathan Freitas (nathan at freitas.net):
I also work with Georg Koppen of JonDos on Tor Browser. He's given a lot
of feedback to the design document, has pointed out bugs, and has helped
with a couple fixes to issues we both shared (he works on the JonDos
browser). I think Tor Browser is quite a bit better off thanks to his
input and assistance.
JonDos has something of a bad reputation in the Tor community because of
the "backdoor" Karsten and Robert mentioned. I think that against
adversaries like organized crime and oppressive governments, this
property can be a dangerous one for both users and node operators.
However, while I would not feel as comfortable operating nodes of such a
service as compared to relatively risk-free Tor bridge, entry and middle
nodes, the "three hop collusion" requirement is the extent of that
backdoor, as far as I am aware. They otherwise take their privacy work
very seriously.

@_date: 2012-01-25 17:07:59
@_author: Mike Perry 
@_subject: [tor-talk] Fwd: ANONdroid 
Thus spake Maxim Kammerer (mk at dee.su):
Please see other replies, but the "backdoor" in question is:
Tor does not have this backtrace capability. I have not inspected the
underlying implementation, but if it is retroactive, it seems vulnerable
to rubber hose cryptanalysis (and possibly even if not):
This property also prevents JonDos entry and middle nodes from being
"hassle free", as is the case with Tor.

@_date: 2012-07-27 15:46:30
@_author: Mike Perry 
@_subject: [tor-talk] circuit_send_next_onion_skin(): Bug: Unexpectedly 
Thus spake grarpamp (grarpamp at gmail.com):
Ticket and assign it to me. Cc nickm (if you can).
We do need a bit more info in the ticket, though. See below.
Hundreds of these on a normal client? Are you doing anything atypical?
Running a hidden service? Running a relay? Running a network scanner?
Using a custom Tor controller?
You get both this message and the above message for the same guard? What ranges of values of M, m, and n have you seen?

@_date: 2012-03-07 10:59:02
@_author: Mike Perry 
@_subject: [tor-talk] Awareness for identity correlation through circuit 
Thus spake Nick Mathewson (nickm at alum.mit.edu):
The plan for TBB is to use the "Request Origin" as the SOCKS password to
isolate web activity by urlbar domain/navigation session. The "Request
Origin" roughly translates to the referer domain.
We'll probably also use "mozilla" or "TBB" as the SOCKS username, to
address Robert's concerns in

@_date: 2012-03-07 12:27:53
@_author: Mike Perry 
@_subject: [tor-talk] Tor and HTTPS graphic 
Thus spake Paul Syverson (syverson at itd.nrl.navy.mil):
Thanks to Mark Klein, we know that the NSA wiretaps in the US are
passive in nature, not active. But who knows what they do to overseas
links and specific high-value targets...
I have to agree with the Raccoon here. I actually don't think Murdoch's
work demonstrated that sampling adversaries can adequately correlate
web-sized traffic.
It seems pretty clear to me that the typical sampling rate of 1/2048 did
not become effective until you were around O(100MB) in transfer. He
wrote that 1/500 became effective at around O(1MB) in transfer, but that
is still a bit above most web page sizes.
There is also the question of an extremely low concurrent flow count
compared to reality today. He used only 500 flows/hour to correlate,
where as at any given *second* O(10k) TCP connections are opened through
every gbit Tor node in operation today. He also used an artificial prior
distribution on connection sizes. Both of these properties alter the
event rate and thus the overall accuracy in the experimental results as
compared to reality.
I think we can agree that large video uploaders stick out like sore
thumbs (due to relative lack of upload traffic frequency), but I don't
think The Man can correlate millions of simultaneous web page views and
expect to have certainty over who is viewing what at all times. At some
point, you simply run out of differentiating bits to extract from size
and timing information to properly segment the userbase.
And as far as I know, no one has really considered the full impact of
userbase size on correlation in the research community (aside from the

@_date: 2012-03-07 13:17:40
@_author: Mike Perry 
@_subject: [tor-talk] Tor and HTTPS graphic 
Thus spake Seth David Schoen (schoen at eff.org):
This is a really awesome graphic and instructional tool. I just wanted
to point out a couple things that may or may not actually matter:
1. Technically, the NSA, site.com, site.com's ISP, and the subpoena
trifecta all see that you're using Tor to connect to site.com. We don't
try to hide this fact, but new users are often surprised and frustrated
by it. Perhaps "Tor" should be added to their infoboxes?
2. In a slightly more detailed version of the graphic that we might want
to create for training purposes, we could also add a "Bridge" button and
an "Obfsproxy" button. The "Bridge" button would change "Tor" to "Tor?"
at the Hacker and user's ISP and maybe the NSA. The "Obfsproxy" button
would remove "Tor" from those points (due to protocol obfuscation).
3. I agree with the Raccoon that the NSA's data sharing link is most
accurately described as "Uncertain". Maybe the fact that there's two of
them with separate info already conveys that? Hard to say.

@_date: 2012-03-07 13:54:18
@_author: Mike Perry 
@_subject: [tor-talk] Tor and HTTPS graphic 
Thus spake Mike Perry (mikeperry at torproject.org):
You know, in hindsight, I don't want to sound like I'm hating on Steven
or his work. His work was quite clear along all of the dimensions I am
talking about, and was excellent research.
He in fact did even compare 500 flows/hour to 50 flows/hour and found
that the success rate did drastically improve, implicitly acknowledging
and measuring the relationship between event rate and accuracy.
I just think that web traffic on the Tor network today is *waaaaaay*
outside the bounds of where you can take his attack and say with any
certainty it would work, both in terms of traffic quantity (much smaller
than his success range) and flows per hour (much larger than his success
And I think the same applies to general correlation, especially in the
face of things like Tor-obfuscated-as-http. Your event rate at the first
NSA guy in the graphic goes waaaay up then, too. Of course, there will
likely have to be a long arms race with the censors before that actually

@_date: 2012-03-12 11:57:02
@_author: Mike Perry 
@_subject: [tor-talk] Tor and HTTPS graphic 
Thus spake coderman (coderman at gmail.com):
Your ideas intrigue me and I wish to subscribe to your newsletter.
Can you describe in a bit more detail (perhaps in a new thread) how
stitching together a Frankenstein's creation from this collection of
protocols would work, and how it would be deployed?
And what about the edge vulnerability to these same tagging and/or
timing attacks? Data's gotta get into this mess somehow, and come
out again, right?
Do free reference implementations exist for all of these protocols?

@_date: 2012-03-21 12:26:32
@_author: Mike Perry 
@_subject: [tor-talk] Setting up Tor on Ubuntu 
Thus spake Simon Brereton (simon.brereton at buongiorno.com):
You can configure TBB to allow you to store history and cookies, it's
just painful and spread across like 3 different options in Torbutton.
They are under subtabs in the the Security Settings tab:
History->"Block history writes during Tor"
Cookies->"Use the Cookie Protections Dialog to Choose"
Shutdown->"Do not clear my cookies at shutdown"
Yes, of course this is insane and sucks. It's a relic of Torbutton's
incremental development and the need for testers and security
researchers to debug features individually.
 is the trac ticket
to create something more reasonable from all of those prefs. Patches
welcome :).
Yes. There are two ways to do this. The easiest is to just start the
fucker, let Tor bootstrap, let Tor Browser launch, and then tell Vidalia
to "Stop Tor". The Tor Browser should still remain open.
After that, you can go into the Torbutton Preferences and tell the TBB
Firefox to use an alternate Tor SOCKS port (9050 is the system tor
default on Ubuntu). You can also click the "Transparent Torification"
radiobutton if you are using transparent firewall rules to torify all of
your traffic.
The more involved method is to edit the start-tor-browser shell
Not really. In fact, traffic analysis is made easier if you only run tor
when you are actually using it.
Does this warning make sense:
Point (c) is about exit point safety. If we can do anything to improve
it, let us know.
Privoxy and polipo are no longer maintained. We've stopped using them in
favor of pure SOCKS4A+SOCKS5.
Obfsproxy is experimental still, but has some very attractive features.
In particular, it can make traffic analysis even harder by preventing
your ISP from easily telling you're even using Tor. It is meant for
deployment in censored locations with a high degree of risk and/or
conflict. But I bet people concerned with privacy will be interested in
it too. It's a bit early for end users to just jump in and start using
it, though.

@_date: 2012-05-03 13:21:04
@_author: Mike Perry 
@_subject: [tor-talk] Firefox security bug (proxy-bypass) in current TBBs 
Thus spake unknown (unknown at pgpru.com):
Repasting my comment at
 here:
For people who use layered defenses: Please add iptables rules/AppArmor
rules/whatever rules that LOG violations so we can learn about them. We
are desperately in need of testers and auditors so this never happens in
production again. See also  and consider signing up to test builds
in your hardened, auditing setups.

@_date: 2012-05-03 14:26:38
@_author: Mike Perry 
@_subject: [tor-talk] testing TBBs 
Thus spake tagnaq (tagnaq at gmail.com):
Yeah, there is no signup process yet. There's not even a testing process.
Technically, Erinn was supposed to manage a pool of build testers, but she is
also overwhelmed with other deliverables, and this testing process is not an
official funded deliverable.
However, regardless of what funders think, the process is absolutely
critical, as recent events now clearly illustrate.
Now that Sebastian is also officially working on build stuff, my hope is
that between the two of them, we can come up with some process that
works. At this point, any process is better than no process.
Once we have a process and some interested testers, I will jot down a
list of existing test sites that we can use as a stopgap while we figure
out  and
You did the right thing. Adding yourself to Cc on
 will keep you
appraised of the process as it materializes.
I'll leave that to Sebastian and/or Erinn to decide.

@_date: 2012-05-13 14:26:59
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser disabling Javascript anonymity set 
Thus spake proper at secure-mail.biz (proper at secure-mail.biz):
Actually, the FAQ makes two assumptions:
1. That nearly all of the information available to Javascript is also
available to CSS and HTTP even when JS is disabled. This includes fonts,
desktop resolution, browser widget resolution, caching-based
identifiers, and probably a few more things, too.
2. The additional amount of information JS provides beyond these things
is not substantial, if properly mitigated.
That said, disabling JS all-or-nothing is probably not a very big hit
right now with our current userbase. Enough people probably do it that
you still have an anonymity set. But, as the demographics of our
userbase change as it (hopefully) becomes easier to use Tor, this will
be less and less true.
However, disabling a whole random collection of random junk specific to
you is a *huge* hit, if anyone bothers to look.  You cannot expect to be
able to use the same web service under two different accounts if you use
3rd party domain JS filtering features of NoScript, for example. They
will have a wealth of fingerprinting information based simply on the
scripts you choose to download and run, if they care to look.
You want likely want this query instead:
Concerns about Javascript are rooted in two avenues:
1. Fingerprinting concerns.
2. Zero-day exploits against Firefox.
The reason we feel that leaving Javascript enabled trumps these concerns
1. We want enough people to actually use Tor Browser such that it
becomes less interesting that you're a Tor user. We have plenty of
academic research and mathematical proofs that tell us quite clearly
that the more people use Tor, the better the privacy, anonymity, and
traffic analysis resistance properties will become.
In fact, my personal goal is to grab the entire "Do Not Track" userbase
from Mozilla. That userbase is probably well in excess of 12.5 million
I do *not* believe we can capture that userbase if we ship a
JS-disabled-by-default browser.
2. Exploitable vulnerabilities can be anywhere in the browser, not just
in the JS interpreter. We disable and/or click-to-play the known major
vectors, but the best solutions here are providing bug bounties (Mozilla
does this; we should too, if we had any money) and sandboxing systems
(Seatbelt, AppArmor, SELinux).
Hope this clarifies some things for you.

@_date: 2012-05-13 16:46:30
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser disabling Javascript anonymity set 
Thus spake Maxim Kammerer (mk at dee.su):
Well, I don't buy the "we force pseudonymity" argument. I don't believe
that there are that many bits in the CSS and JS fingerprinting vectors
that remain to segment a sizeable userbase size.
If by some chance so few people actually use Tor Browser today that
there *are* enough bits right now, I'd rather focus on growing the
userbase size, instead.
I believe you're pretty much going to be left with a fancy browser UI
dressing up something functionally equivalent to lynx (or worse) here,
But if the community wants to step up and implement it for us, I could
see adding a "Text mode browsing" radio button to the 4 options we plan
to transform the Torbutton interface into:
Maybe we can launch a lynx window for you if you click that (if it's
Otherwise, "patches welcome".
Laugh, probably. Academia has a penchant for misrepresenting statistical
results by using small sample sizes and artificially contrived
experiments that make their papers look sexy. Natural consequence of
"pubish or perish" coupled with limited peer review, closed-source
implementations, and unreproducible results. But more seriously, Javascript is effectively a VM which is now fully
under our control. From a technical perspective, there's not much there
we can't alter with full control of the browser. We might make mistakes
and/or miss things, but that is the nature of software engineering. We
need better processes in place to deal with that. (We're actively
working on that part. Stay tuned).
However, from a programmer resource perspective, I'm a tad overbooked
(putting it mildly). But that doesn't exactly make me want to jump up
and spend my limited time supporting a text-only mode to browse the web,
I'd rather fix the real problems.

@_date: 2012-05-14 11:56:51
@_author: Mike Perry 
@_subject: [tor-talk] Evercookies / supercookies tracking & No Script 
The short answer is "Yes, we've looked into it. New Identity removes
The long answer is
 and
The footnote is "Please help us test this shit in new releases. We just
had a race condition on the cache that allowed cache cookies to persist
for up to a minute after clicking New Identity (though they did go away
after that)."
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):

@_date: 2012-05-14 11:59:16
@_author: Mike Perry 
@_subject: [tor-talk] any issue with TBB extensions auto updating? 
We're not aware of any fingerprinting or anonymity issues, so long as
you keep the same set of addons in TBB and do not install extras.
The updates are authenticated over SSL.
However, we want to ween ourselves off of our dependence upon the CA
model for authenticating these updates:
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):

@_date: 2012-05-14 13:52:58
@_author: Mike Perry 
@_subject: [tor-talk] Evercookies / supercookies tracking & No Script 
Let's continue speculating instead of reading any documentation.
That's totally a productive use of everyone's time.
Thus spake Matthew Kaufman (mkfmncom at gmail.com):

@_date: 2012-05-14 14:00:52
@_author: Mike Perry 
@_subject: [tor-talk] Evercookies / supercookies tracking & No Script 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
Oh, sorry, you did ask how. Well:
For the other stuff, I refer you to about a half dozen blog posts on the
topic, several tor-talk posts, and probably a few dozen mailinglist
threads, may of which you yourself participated in.

@_date: 2012-05-14 14:04:38
@_author: Mike Perry 
@_subject: [tor-talk] Evercookies / supercookies tracking & No Script 
Thus spake Mike Perry (mikeperry at torproject.org):
Here's a better link direct to the comment above the implementation,
which describes it.

@_date: 2012-05-14 14:15:06
@_author: Mike Perry 
@_subject: [tor-talk] Evercookies / supercookies tracking & No Script 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
We want to keep it short and sweet, though. Normal people don't care
about enumerating evercookie locations, only mentats do.
Mentats are encouraged to read the design doc, suggest improvements, and
review the source code.
Word. The design does come from a thorough understanding of all the
places browsers can store state about your browsing experience, in what
cases it gets transmitted and/or side channeled, and how
to deal with it.
The design is documented so others with this understanding can verify
we've done our jobs, though perhaps we could make the "New Identity"
section itself more legible, somehow.
Keeping TBB relatively simple (only three addons, no plugins) makes this
a whole lot easier than for vanilla Firefox. That's one of the reasons
why New Identity is disabled there.
To be fair, the EverCookie problem does grow exponentially more
complicated once you add in third party plugins and addons, AV software,
etc. That's why we try to keep TBB simple, and keep other addons and
plugins out of it.

@_date: 2012-05-14 16:57:33
@_author: Mike Perry 
@_subject: [tor-talk] Towards a Torbutton for Thunderbird (torbutton-birdy) 
Thus spake Sukhbir Singh (sukhbir.in at gmail.com):
Yikes! Don't disable Thunderbird autoupdate like this without a damn
good reason, analysis, *and* alternatives. Otherwise, you're condemning the user to get owned as their client
gradually gets more and more exploitable from the date they install

@_date: 2012-05-15 19:28:06
@_author: Mike Perry 
@_subject: [tor-talk] JScript on/off 
Do it to it, Dr. Prosif. That's why the mentat button is there.
Thus spake Hans van Zuilen (hans.vanzuilen at gmail.com):

@_date: 2012-05-17 11:52:36
@_author: Mike Perry 
@_subject: [tor-talk] Volunteer QA: The Price of Freedom is Eternal Vigilance 
We have a dream. We believe it is possible to produce free, secure
privacy software that is regularly used by many millions of ordinary
people all over the world. They will use it to inform themselves,
explore new and controversial ideas, communicate with one another, and
safely share things about their lives. They will do so with confidence
that so long as the software rests upon a secure foundation, it will
not betray them; in fact it *cannot* betray them, by design.
Tor Browser Bundle can be that software. Sadly, we know that it is not
yet that software. We're aware that many aspects of the Bundle are
either imperfect, incomplete, or absent entirely. We intend to work as
hard as we can to improve this situation. However, we also know that even the keystone of true security is not yet
properly in place. We know that we must properly deploy this keystone,
or we risk the collapse of everything we have built so far.
That keystone is the community that reviews our designs. It is the
community that audits our source code. It is the community that tests
the binaries produced from that source code. It is the community that
will verify that the binaries that we distribute are produced from that
source code and nothing else.
It is time to organize our community into place to serve as that
keystone. We cannot have true security without it.
Our plan is to start small, with manual testing and manual analysis of
each build. We will use that to incrementally work towards full
automation available to be run on any of the arbitrary platform
configurations available to the community. Runa Sandvik will begin
coordinating these releases.
We expect the process to be bumpy at first. To start, Runa will simply
give interested people a url to a release candidate with a grab bag of
urls[1] along with some basic tests to perform within some time limit
before the build is to be released. These urls will initially come from
arbitrary pages around the web, but hopefully we'll eventually distill
them into our own collection of minimal test cases[2] for which testing
is fully automated. Until that point, test pages will need to come with
a description of expected behavior and results.  We're hoping that the
community will also seek out new and useful test urls and write up
result descriptions for them.
We will soon be switching to the 10.x-ESR Firefox branch for our stable
TBBs, while concurrently maintaining an alpha series based on Rapid
Release. To minimize the incidence of surprise issues in the stable TBB
when ESR undergoes major upgrades, we will need people devoted to testing
both branches on multiple platforms. We will also need people running
auditing systems that verify the TBB they test is well behaved[3].
To participate, please inform Runa via email (runa at torproject.org) which
TBB branch or branches you intend to test on which platform (Operating
System and CPU). Bonus points if you have a unique configuration such as
AntiVirus software, and/or are able to analyze TBB in an auditing
sandbox framework such as Seatbelt, AppArmor, SELinux, a firewall that
will log proxy bypass attempts, or simply with Wireshark or any other
network analyzer. Extra bonus points if you document your setup for
others to use[4].
Independent from the group Runa will coordinate, we also need people
analyzing our builds[5], to ensure against tampering at the build
machines themselves. We need to use the differences uncovered by this
analysis to work towards the ability to produce the same binary on
multiple, clean instances of build platforms which can be brought up
from scratch anywhere around the world[6].
To help us get started, we will also need people who simply create
independent builds for others to compare against our official builds. I
suspect that many reversers and hobbyists interested in learning
reversing may find devoting the system resources and time to build their
own TBB binaries a prohibitive barrier.
Basically, everyone should try to help eliminate the need for duplicate
work done by others at each step of both the QA and build inspection
processes. Similarly, we should be constantly looking to refine our
testing and analysis processes to eliminate manual labor.
Thank you all for you willingness to help. Let's work together to build
the world we want to live in.
1. 2. 3. 4. 5. 6.

@_date: 2012-05-17 15:10:12
@_author: Mike Perry 
@_subject: [tor-talk] Volunteer QA: The Price of Freedom is Eternal 
Thus spake tagnaq (tagnaq at gmail.com):
We briefly had a mailinglist, and then it got deleted due to concerns
over confusion and the wrong people subscribing to it.
Anyway, we've since decided that we'd prefer to have a specific
committed group of testers and also do some tracking on what platforms
are actually being tested regularly. Runa will manage everything via
direct email, especially to start.

@_date: 2012-05-17 21:03:27
@_author: Mike Perry 
@_subject: [tor-talk] Technical Documentation for the TBB Update 
Thus spake tagnaq (tagnaq at gmail.com):
FYI: I've added a reminder to myself to update the TBB design doc to describe
the update check.

@_date: 2012-05-19 13:14:12
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
Yes, that is correct. We consider the ability to link user activity
across different url bar domains a violation of our design requirements
( and
any ability to do so is a major bug.
Unfortunately, there are a couple such bugs we're already currently
aware of:
We'll fix them, eventually. Help is always appreciated, though.
Yes, you are absolutely right. Normal web browsers do not consider the
ability to link your accounts and activity across multiple url domains
to be a problem. Actually, most of the major browsers see it as a
totally awesome feature....
As a result, we have all sorts of stupid crazy conflict between policy
people arguing for bullshit like "Do Not Track"; crazy lawsuits against
Facebook and other companies who are simply using the tracking
technology provided to them by browser makers; and weird filter addons
like Request Policy and Ghostery to try to filter "bad actors" (who can
simply reappear under new domains on a moment's notice anyway).
Almost no one wants to solve the real technical problem, it seems.
Sick sad world.

@_date: 2012-05-19 13:16:21
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Thus spake Mike Perry (mikeperry at torproject.org):
Oh, I should also mention
as the laundry list of linkability mechanisms we've already at least
partially fixed.

@_date: 2012-05-19 15:00:19
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
If you prevent the associated identifier transmission and fingerprinting
issues, "web beacons" do not link your activity on one url to another.
If we prevent identifier transmission and fingerpritning, web beacons
will see both visits, but they do not know it is the same user on both
The reason we don't care that they can still see both visits is because
the urls you visit can and do simply sell their logs to third parties
If a site tries to deploy web beacons, you should assume they are also
selling your data to whoever is buying, regardless of what the browser
actually does.
Not exactly. In Tor Browser, cache is isolated by url bar domain,
meaning that the cached copy of a web beacon that was loaded under one
url bar is actually *not* used when the same web beacon is loaded under
a different url bar.
Though in interest of full disclosure, you'll notice that one of the
"tbb-linkability" tagged bugs is an issue with this cache isolation
specifically for images:
Tracking scripts are correctly isolated in the cache, however (which is
more important, as many tracking scripts *do* embed unique identifiers
to get cached and used when the user clears cookies).
Two reasons come to mind:
1. Ghostery subscriptions and user blocks can be fingerprintable
2. We'd rather devote the effort to fixing the root technical issues
rather than figuring out how to audit and safely configure Ghostery.
Never wish for regulation, especially when problems can be solved by
technology instead:
The EFF and the policy crowd may disagree. But frankly, they're wrong.
Well, that's also not the only issue with Flash. Flash has tons of
fingerprinting and proxybypass issues hidden in its binary blob. We
really need a full sandboxing technology to make it safe to uniformly
I think Steve Jobs was right on this one. Flash needs to be replaced
with open technologies.

@_date: 2012-05-19 15:51:56
@_author: Mike Perry 
@_subject: [tor-talk] Adobe Flash 
Thus spake krishna e bera (keb at cyblings.on.ca):
Gnash is not a fully open replacement; it's a reimplementation of a
proprietary technology. It will always be subject to keeping up with the
whims of Adobe (who has a vested interested in ensuring that its own
proprietary media server works best with its own proprietary client
software). This means on at least some level, Adobe has an interest in
actively sabotaging projects like Gnash.
Moreover, the Gnash effort has slowed tremendously in recent years.
Developer-wise, the project hangs on by a thread. Can it even play major
video sites (like Youtube and Hulu) reliably these days?
Admittedly, we're in a similar position with Mozilla (which is one of
the reasons I prefer them over Google for a fork base). However, because
Firefox is fully open source, our job is much easier. I may have a hard
time keeping up with bugs, but I pretty much *can* do it by myself at
the moment, so long as we're able to accept a significant lag time on
rather serious issues being fixed, and able to accept that almost
nothing other than violations of our privacy and security requirements
will *ever* get fixed.
Sadly, this will lead to a shitty user experience with the browser, due
to most of that being "normal" level bugs, but at least it will be a
phenomenally better experience than one with JS, cache, fonts, and image
rendering disabled.

@_date: 2012-05-19 19:17:04
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
Yep. Any userbases is worth money, though. Even one that prefers privacy
still might like to buy a few things here and there.
For example, I sure wish I could buy a fucking book without being
tracked these days. There are no major brick and mortar bookstores left
in my city, and I live in a pretty big city.
So if Mozilla wants to surrender 5%+ of their userbase to us, hey man,
we'll take it :)
I'm confused. What vectors do you belief remain that we have not covered
a few dozen times in this thread and others?
You smell a lot like a timewasting troll... I'm trying to help you
understand what we're doing because I think it's important for everyone
to understand. But you sure aren't making it easy ;)

@_date: 2012-05-19 19:42:42
@_author: Mike Perry 
@_subject: [tor-talk] Adobe Flash 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
No, they did it to shape web standards and to set an example for web
client engineering, because no one else was taking that shit seriously.
Google's intentions with Chrome are directed by self-interest, but they
are not rooted in subterfuge.
Chrome has done a great deal to show everyone else in the browser space
how they were doing it wrong. However, I agree that it would be dangerous to assume that Google's
position wrt Chrome will always remain what it is now.

@_date: 2012-05-19 21:07:43
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
This is correct. Any violations of this property are major bugs for us.
Yes, it would require a custom sandbox of our design. Current sandboxing
tech (Seatbealt, AppArmor, Seccomp, SELinux) may actually need some
additional hacking before they are sufficient for our needs for

@_date: 2012-05-19 21:48:34
@_author: Mike Perry 
@_subject: [tor-talk] google analytics says it can track across separate 
Everything takes time. You get upset because the software and the
network you get to use for free is not perfect and you wonder why.
I get upset because you make me explain to you over and over again how
things work and why we've made the choices we've made with respect to
development priorities.
I have to do this instead of actually fixing the problems you complain
about, you know.. Actually, I don't have to do it. I don't even know why
I'm doing it... So don't push your luck, or no one will bother to answer
you at all.
How do we get to the point where I can stop answering the same questions
from you over and over again? I've tried to document everything I've done. Maybe that documentation is
still over your head, but you're not really telling me how or where it
is over your head.
It seems we might need a more effective translator between us before we
can expect to come to an understanding.
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):

@_date: 2012-05-25 19:17:15
@_author: Mike Perry 
@_subject: [tor-talk] Torbutton-birdy version 0.0.2 
Thus spake Jacob Appelbaum (jacob at appelbaum.net):
I say we just call it TorBirdy. Easier to type, and has a better ring to it
I think.
Hopefully, Torbutton will soon be entirely forgotten in favor of Tor
Browser, anyways :).
I already mentioned this in the ticket, but might as well say it here
too, in case people don't bother to click the link: Attachments can also
cause proxy bypass when external apps are launched to open them, esp for
doc and pdf attachments.
It would be great if someone could test trying to open those
attachments, especially after setting the prefs I mention in:
If the prefs don't cause a warning of some kind first, you might need to
adapt that component I linked to in comment 11...

@_date: 2012-05-27 15:32:24
@_author: Mike Perry 
@_subject: [tor-talk] Torbutton-birdy version 0.0.2 
Thus spake tagnaq (tagnaq at gmail.com):
This is an awesome doc.
Is this sourced from latex? Is it possible to output an html version
somehow, too?
I find the pdf format heavy and unnerving from a security perspective..
Hrmm. Actually, if we can avoid revealing this anonymity set explicitly
to mailing lists and recipients, I think that might be a worthy goal. The primary reason we don't bother with it on Tor Browser is because tor
exits are meant to be discoverable (with a useful secondary reason being
to take Mozilla to school). Since Tor IPs are often absent from mailing
list headers if the SMTP server(s) are not run by a total jerk, can we
figure out a way to look more common?
What's wrong with using the Thunderbird default locale string for the
quotation here? If you're posting on a mailing list where discussion
occurs in only one human language, shouldn't you be using that same
localization for mail client? For multilingual users, can we solve that
problem a different way, perhaps by a localization dropdown menu or
I agree this is a tricky issue.. I could see this choice a few different
ways. I just want to make sure we don't unnecessarily explicitly expose
the user agent to a mailing list unless we really have to. Doing so can lead to targeted attacks...

@_date: 2012-11-06 14:44:37
@_author: Mike Perry 
@_subject: [tor-talk] ads slow Tor Browser dramatically 
Thus spake Joe Btfsplk (joebtfsplk at gmx.com):
I am deeply opposed to shipping an always-on universal adblocker with
the default TBB. I think it would be political suicide in terms of
accomplishing our goals with acceptance of Tor users by sites, lobbying
for private browsing origin changes, and convincing the world that
privacy by design is possible without resorting to filtering schemes
and/or DNT-style begging.
Further, adblocker filter choices are fingerprintable.
*However*, I recognize that many sites use advertising networks that are
obnoxious, deceptive, and possibly even dangerous wrt vectors for
malware (though safebrowsing filters are supposed to exist for this
last reason and we do use those).
So, I *might* support an adblocker that allows the user to block ads on
specific first party domains that they choose. The incentives this would
create for sites/ad networks to be less obnoxious is particularly
appealing, but that probably will only take effect if the major browser
vendors also provided this feature.
However, the fingerprinting issues with this approach would require
resetting these choices upon "New Identity" by default, unless the user
also opted to protect their choices...
Unfortunately, this is probably a lot of work to implement and get the
UI right, and so it's not high on the priorities list compared to other
things. It's also one of the few areas where I think partial solutions
will do us way more harm than good.

@_date: 2012-11-07 00:37:57
@_author: Mike Perry 
@_subject: [tor-talk] ads slow Tor Browser dramatically 
Thus spake Julian Yon (julian at yon.org.uk):
I considered writing an in-depth reply questioning the distinction
between these "Global Adversary" third parties and the first parties
that would willingly collaborate with such third parties, but I think in
the interest of brevity, I'll just start with two simple questions:
Can you explain how regular expressions will hinder such a "true Global
Adversary" in any way?
If not, can you suggest an alternate, non-regex ablocker design that
would withstand such a threat?

@_date: 2012-11-08 17:39:46
@_author: Mike Perry 
@_subject: [tor-talk] Reduced latency transport for TOR 
Thus spake Gregory Maxwell (gmaxwell at gmail.com):
FWIW, I spent the better part of today reviewing our UDP plans[1,2] and
pondering this design, but I think I've decided this is not terribly
useful to us right now, primarily because it would seem to require the
very same circuit crypto changes as we'd need for a full client-to-exit
datagram transport (to support reordering at the circuit crypto level).
1. 2.

@_date: 2012-11-09 18:05:22
@_author: Mike Perry 
@_subject: [tor-talk] Orbot blocks internet connection when off wifi 
Thus spake Pete K (asdpete at gmail.com):
Is your mobile 3G/4G or does it often end up as 1x/EDGE?
If it is significantly slower than your wifi (especially in terms of
ping times/latency), this could be bug

@_date: 2012-11-28 19:52:59
@_author: Mike Perry 
@_subject: [tor-talk] upgrading procedure for TBB 
Thus spake The Doctor (drwho at virtadpt.net):
For what it's worth, I do this too. I expect our updater will eventually
use this mechanism underneath, too.
The most annoying result is that you lose your custom pref settings, due
to  I find this
annoying because I actually like having local TBB browsing history.
On the plus side, any custom Firefox extensions you have in your TBB do
stay active after updating this way (though the usual caveat about
choosing your extensions carefully to avoid leaks and fingerprinting

@_date: 2012-09-12 17:40:09
@_author: Mike Perry 
@_subject: [tor-talk] Distribution of Linux static tor binary? 
Thus spake adrelanos (adrelanos at riseup.net):
My security concern would be around the absence of ASLR. It's my
understanding that static binaries have less/no ASLR on most Linux
For this reason, I think a dynamic binary+$LD_LIBRARY_PATH+shared libs
is the best option for third party bundlers..

@_date: 2012-09-24 18:15:42
@_author: Mike Perry 
@_subject: [tor-talk] About path selection (or node selection) strategy in 
Thus spake esolve esolve (esolvepolito at gmail.com):

@_date: 2012-09-26 11:48:14
@_author: Mike Perry 
@_subject: [tor-talk] Tor and P2P 
Thus spake Nathan Freitas (nathan at freitas.net):
This is a great point, and I wish I could reply to it and Robert's
comments about DoSing the hsdirs in the same mail.
It would seem that "simple" solutions might end up destroying the Tor
network. Based on Robert's comments, it sounds like the properties we
need are:
1. Persistent hidserv connections. Reconnecting for each message via an
HTTP POST is right out. Way too many circuits+onionskins to scale.
2. Avoid the situation where a single user is creating multiple hidden
services for all their crazy P2P apps.
For 1: It would seem to me that a system that ships a local torified
XMPP server would satisfy this. XMPP is fully decentralized, and
maintains persistent connections between servers. Each user would run
their own server over .onion.
For 2: The resource identifiers of XMPP mean we can connect multiple
XMPP clients to a single local XMPP server, and have them provide
multiple (admittedly linkable) P2P services over XMPP 'streams' without
spinning up additional hidden services for each client app.
XMPP has some obvious downsides... We'd need to audit the whole beast to
make sure the federation+decentralization properties can't be
manipulated to connect to things over non-tor.
It also appears to have the property that social networks where
everybody wants presence notifications for everybody else end up
requiring O(n^2) persistent hidserv connections between the n XMPP
servers... Not sure how serious this is, or if there are any workable
decentralized alternatives.
However, unlike torchat, the XMPP protocol itself is well documented,
widely used, and seems to be designed for a superset of the things we
want. I was able to spend just 10 minutes reviewing the XMPP specs to
fact-check before composing this email:
I was unable to determine if torchat even has property 1 in that time...

@_date: 2012-09-26 17:52:41
@_author: Mike Perry 
@_subject: [tor-talk] Tor and P2P 
Thus spake adrelanos (adrelanos at riseup.net):
Yeah. Due to my distributed systems background, I read "P2P" in this
thread as "peer to peer", not "filesharing". I assume that was the case
for most of the other Tor people commenting in this thread.
I would love it for Tor to support ways for people to communicate
without revealing either their social network or message content to the
network or infrastructure. I literally cannot stand the fact that there
are no ways to communicate right now without handing your social graph
to someone who wants to datamine it or sell it, or both.
I do think it is important to devote thought even to something as simple
as P2P chat, or we could end up destroying the network as soon as it
becomes popular, as Nathan and Robert said. Hidden services in Tor are
quite expensive: ~3-4X more expensive than exit circuits in terms of CPU
usage during connection setup, and 2X as expensive in terms of bandwidth
consumption during usage.
Because of this, I would be annoyed if people wrote bittorrent clients
that used hidden services the way I2P does. I would prefer it if people
focused such efforts on networks other than Tor, because I like Tor to
remain useful for things other than simply filesharing.
I recognize we don't have a whole lot of options to prevent such abuse,
but I am not opposed to stopgaps such as throttling loud clients and QoS
mechanisms to impact the popularity of resource-intensive filesharing
I suspect most of the other Tor folks in favor of "Tor and P2P" are in
the same boat.

@_date: 2013-04-07 16:31:54
@_author: Mike Perry 
@_subject: [tor-talk] NSA supercomputer 
Thus spake cmeclax (cmeclax-sazri at ixazon.dynip.com):
Not necessarily. Because identity keys also authenticate the TLS links between nodes, if
you can break/steal relay identity keys, you can perform a number of
attacks at various points between a client and its guard nodes, or with
a device placed upstream of the relay in question. With the secret half
of the identity key, for the most part you get to behave as if you were
actually operating that relay.
These attacks can be quite powerful (ie full deanonymization of all
traffic, even without the use of timing attacks), and can be targeted at
interesting users.
I think identity keys should be either ephemeral or offline for this
reason. See also:
If the NSA can also break or steal identity keys, this attack is still
quite feasible :/.
Unfortunately, based on their own statements about the safety of prime
factorization, either the NSA already *can* break 1024bit RSA identity
keys, or they expect to be capable of this very soon. (My bet is that
bathtubs full of DNA will work out before quantum computing does ;).
Either that, or they are running some kind of massive psy-op to get the
world to switch to smaller ECC keys which they *can* break. Based on
their past behavior in terms of their suggestions wrt crypto, it is
probably safest to assume they are acting in good faith about their
statements about prime factorization.
However, it would be interesting to have some benchmarks for high-bit
ECC implementations. It seems to me they should still be faster than
modular exponentiation at the same bitwidth, no?
Sadly though, it's also not as easy to adjust key sizes for ECC-based
operations as it is for prime integer based groups. The problem is we
need to find "safe" ECC curves for each bitwidth..  This seems more
blackmagic to me than anything else in crypto, save for perhaps S-box
No. Nor are there direct ways to tell which RPs get used with which
hidden services. There are some statistical attacks, but they are error
prone (high amounts of false positives, or low accuracy).
Correct. In the current Tor network, the adversary would simply go after
the identity key. It is way more valuable in terms of the attacks it
enables, and way more long-lived by default.
As far as I can tell, also having the onion key doesn't get you much
beyond what the identity key enables, especially if you're an "external"
adversary (such as we would presume the NSA and other intelligence
agencies to be).

@_date: 2013-04-07 17:20:37
@_author: Mike Perry 
@_subject: [tor-talk] NSA supercomputer 
Thus spake unknown (unknown at pgpru.com):
I think this attack is actually easier to defend against than identity
key theft right now. It would seem to me that what you really want to do
with this attack is feed fake consensuses to certain clients to capture
their traffic.
People who suspect they may be such targets can record hashes of their
consensuses using a Tor Controller, and later verify that hash history
against our archives ( or
ideally against other people also running such a controller and
mirroring their consensus hash history results:
I think an initial prototype of something like this shouldn't be too
hard to hack up with a well-documented controller library such as Stem:
I would be happy to help mentor someone to do this for GSoC, etc.

@_date: 2013-04-12 20:27:52
@_author: Mike Perry 
@_subject: [tor-talk] Bridge Communities? 
Cool story bro.
We're worried about these things too, I guess.
I mean, if killing us all is really the best way to stop Tor, then I
would submit to you that Tor is unstoppable. After all, network
engineers are basically throwaway commodities to the mexican mafia:
I mean, if they kill their *own* codeslaves^Wemployees, what exactly do
you think murdering us will accomplish?
In the meantime, we have nothing to fear except fear itself. Oh, and
0day. Don't forget to ph34r the 0day. Turns out that shit costs way less
than high-profile assassination contracts (fortunately or unfortunately,
depending on your perspective :/).
P.S. If you're annoyed by this flippant response, it was given because
your rant is basically a long series of FAQs. There are ways to fix your
concerns but they require development effort, and in fact many of them
(including custom pluggable transports and private bridge distribution)
are already supported. For the others: Patches welcome.
P.P.S. I'll leave the point-by-point discussion to the other NSA thread
participants ;)
Thus spake Alex M (Coyo) (coyo at darkdna.net):

@_date: 2013-04-12 22:13:33
@_author: Mike Perry 
@_subject: [tor-talk] Bridge Communities? 
Thus spake Alex M (Coyo) (coyo at darkdna.net):
If you have a specific list of design flaws that aren't couched in long
rants, we can perhaps help instruct you on how you might solve them in
your redesign with Mr Disney, or at least point you toward some tickets
you two should read and follow during that process.
Otherwise, thanks for your concern/veiled threats/trolling.

@_date: 2013-04-12 23:29:04
@_author: Mike Perry 
@_subject: [tor-talk] Bridge Communities? 
Thus spake Alex M (Coyo) (coyo at darkdna.net):
Ooh. A flame war. I love these. *Boop* I just took your nose over
It's called solidarity. I won't stand idly by while you suggest that Tor
developers and relay volunteers could be murdered or threatened to
sabotage "our" project. As if such tactics would even work without
someone instantly running to EFF/ACLU or proposing a design change...
Perhaps I'm just annoyed you didn't include my name among the death
threats in your first rant.
Now you know better, I hope.
Dude, the source code is BSD/MIT licensed. Sell binaries with your own
secret sauce to others if you wish. "We" don't care. Just don't tell
people you're giving them "Tor".
P.S. Cite your specific design concerns or this is my last reply to you
on this list. (I totally promise.. Flame wars are baaaad... Mmmkay?)

@_date: 2013-04-13 00:15:27
@_author: Mike Perry 
@_subject: [tor-talk] Bridge Communities? 
Thus spake Seth David Schoen (schoen at eff.org):
No no dude don't do that! Now "they" know why "they" should kill me!
Aww fuck it.
Well, if anyone asks why I died, the official answer is now that it was
totally the fault of doubleclick.net (or their current majority
shareholder ;).
P.S. Thanks, Seth. ;)

@_date: 2013-04-13 17:31:58
@_author: Mike Perry 
@_subject: [tor-talk] Bridge Communities? 
Thus spake grarpamp (grarpamp at gmail.com):
While I appreciate people standing up for me, there's not really much
need to defend me to a drama queen who can't be bothered to RTFM before
suggesting features, and moreover who thinks that suggesting specific
people will be murdered is the right way to contribute to a FOSS project
or ensure the prioritization of their desired features.
I mean, I had more than a few lullz patiently toying with this idiocy
waiting for the doxx to drop (so to speak), that's for sure ;).

@_date: 2013-08-27 14:48:57
@_author: Mike Perry 
@_subject: [tor-talk] So what about Pirate Browser? 
Andrew Lewman:
Yes. I also attempted to contact them at least twice, in addition to
your attempts Andrew. I think Runa also tried to contact them a couple
I even reactivated my official sf.net account and informed them I had
been added to the project to make it appear as if I endorsed it without
any particular involvement or knowledge.
I was able to remove my account from the project, and thankfully I have
not been re-added, but sf.net did not react to my abuse/trademark
violation complaint in any way.

@_date: 2013-08-30 00:14:29
@_author: Mike Perry 
@_subject: [tor-talk] Many more Tor users in the past week? 
Collin Anderson:
Dude I love math and whatnot. Maybe science too, but this isn't exactly
a controlled experiment. So whatnot it is, then! (Also plus one to you
Collin, for being awesome).
Can someone with more free time than me try to investigate one or more
of the following ideas.. You know, in the interest of whatnot?
We seem to have three competing hypotheses, sorted in order of
decreasing prior probabilities:
1. Botnet (Totally *not* run from Israel, we swear)
2. Pirate Browser (?s & PLUR guys, but get with the program: Src+Gitian FTW)
3. Censorship/sudden unrest (sudden+globally coordinated? Seems unlikely)
Breaking down the math and whatnot for each case:
1. If this was a botnet:
  A) The change in user counts for each country should be proportional
     to the installation base of some infectable software population
     for that country. Can we start with the easy ones with lots of
     public data on them, such as Windows, Flash, or Java?
  B) Weird that we saw no new countries. Why? Is this just a canary
     test to see if we'd fall over? Can we check for correlations
     between our change in userbase per-country and the current number
     of Internet users per-country too, to see if that matches?
  C) People on IRC have suggested there is correlation to work hours.
     Does this actually apply to countries with atypical work weeks
     and holidays? (Is some popular corporate/work-group software the
     target here?)
2. If this was Pirate Browser:   A) The change in user counts should be proportional to their existing
     userbase
  B) There may be some additional weighting with their censorship areas
  C) Do our numbers should correlate with their download counts for PBB?
  Would any of the Pirate Bay crew care to leak the us your numbers for
  that? Pastebins are fine. We won't ask too many questions and whatnot.
3. If this is widespread local censorship/unrest:
  A) No level of censorship/social unrest happens across 91 countries at
     once. Justify your existence, meme.
P.S. To the bot-herders who are totally not Israeli: Our network can't
scale as well as anything you can infect this fast. It will fall over,
and when it does, the whole Internet will be looking for you. Maybe you
should chillax a bit and consider running your own mix network. Good
luck! ;)

@_date: 2013-08-30 01:49:51
@_author: Mike Perry 
@_subject: [tor-talk] Many more Tor users in the past week? 
Georg Koppen:
Nice. So it would appear** we're solidly in botnet territory then? Good
thing I didn't bother to proofread the Pirate Browser section of my
email for typos then.
** The key assumption here is that the PB folks did not disable addon
updates or pings. Since I can't reproduce their bundles, there's no way
in hell I'll be the one to confirm this. Any braver souls want to

@_date: 2013-08-30 02:16:06
@_author: Mike Perry 
@_subject: [tor-talk] Tor and Financial Transparency 
Juan Garofalo:
Seriously? How long have you been on this list?
We discuss methods of subverting the NSA/GPA/key theft adversary every
time it comes up here. It turns out this is actually a hard problem. It
also turns out we're not doing *that* bad.
Unless of course you believe that all the leaks through Tor were
sanctioned and/or orchestrated by the military industrial complex. In
which case, can I just say "Scoreboard, plz?" I think I can still say
that here. We'll see... Avenge my death? ;)
Anyway, I would ask that you attempt at least one of the following: 1. Respect our efforts on this front. We're doing our best with what we have
2. Provide citations to support your conspiracy theories of Tor's subversion
3. Find better meds
4. Go fuck off
Pick one. Or more. Whatever.

@_date: 2013-12-23 01:14:31
@_author: Mike Perry 
@_subject: [tor-talk] TBB 3.5 - lot of Unable to connect errors 
Does the problem go away if you set 'OptimisticData 0' in your torrc by
any chance?
Bobnomnom claims that there is some kind of issue in how we retry
streams on circuits (which I think was changed by OptimisticData), and
this is getting aggrevated further by a reduction in the Linux kernel
TCP timeout on the exit side.
David Bala?ic:

@_date: 2013-02-01 12:27:57
@_author: Mike Perry 
@_subject: [tor-talk] TorBrowser does not work with mediafire.com 
Thus spake Karsten N. (kn at awxcnx.de):
The TorBrowser 2.4.x-alpha series should support DOMstorage safely (no
disk access, and isolated to first party domain):
Be aware that it is an alpha, and more auditing work needs to be done
on Firefox 17 in general, though.

@_date: 2013-01-13 01:49:33
@_author: Mike Perry 
@_subject: [tor-talk] Gmail and Bitcoin? [OT] 
Thus spake Maxim Kammerer (mk at dee.su):
For gmail and search, Google's account value comes from account history
and current activity context (which they utilize to serve you ads), not
meatspace authentication**. In terms of "authenticating" your account for this purpose, it doesn't
matter to them what sort of abuse reduction mechanism they deploy, so
long as it reduces any activity on their network that costs them more
money to service than they can extract through advertising. That's why
they don't bother requiring SMS/Captchas for everyone, and instead
prefer to keep the account creation process as frictionless as possible
for the overwhelming majority of users.
I do believe that any number of proof-of-work/scarce-resource mechanisms
could be deployed behind a Nymble/blind signature layer (ie Nymble +
captchas/SMS/payment/whatever) to provide both better privacy *and*
better abuse reduction properties, but I don't think it is really
worthwhile to go into detail about that in this thread. There has been
plenty of other discussion already about these ideas. The blocking
factor always seems to boil down to "That sounds like it might work. But
who is going to build it to find out?"
For the record, though, I have never suggested paying for accounts with
bitcoin by itself. I strongly believe we should regard bitcoin as no
different from the traditional banking system in terms of transaction
privacy for the *average* user. For this reason, I believe it is
inappropriate for the Tor Project to advocate use of bitcoin without
*also* providing some kind of additional privacy preserving layer on top
of it, just as we would have to do if we were to endorse people using
credit cards to pay for/authenticate Google accounts (even though
"anonymous" prepaid credit cards are available in some areas).
This last paragraph is the real reason I replied here. Please try to
avoid putting such statements in my mouth unless you have clear
memory/citation for them..
** Google+ with its "real name" policy is of course a different beast.
That team seems to have some misguided fantasy of turning G+ into an
"Identity provider" for the Internet. Good luck with that, guys... I
for one love a good lolocaust ;).

@_date: 2013-01-13 15:00:23
@_author: Mike Perry 
@_subject: [tor-talk] Gmail and Bitcoin? [OT] 
Thus spake grarpamp (grarpamp at gmail.com):
The core problem is a deeper one. My point is that *any* procedure that
requires the user to do something other than the default usage scenario
for the system/resource we're relying on must be considered
In the case of Bitcoin: in the default configuration and usage scenario,
mining Bitcoins risks revealing your IP, and purchasing them at an
exchange means relying on traditional financial system privacy *and*
exchange server security. Neither of these use cases meet the level of
privacy Tor itself seeks to provide, and telling people to manually jump
through a bunch of technical hoops/special configuration steps to
"solve" these shortcomings is no solution at all. At least, not at
Google-scale. Too many people will do it wrong. You don't build
trustworthy privacy software by telling everybody "Too confusing? Too
bad! It's survival of the fittest around here! Also, maybe you're just
too dumb to deserve privacy!"
Therefore, if we are to rely on a scarce resource that is not private by
default, we must also provide a suitably private (and
transparent/invisible!) layer (such as Nymble/blind sigs) to provide
real privacy to the *default* usage scenarios for acquiring the scarce
In other words, we must think outside the Bitcoin here.
In the case of Nymble tokens/blind sigs purchased with Bitcoin, users
would obtain Bitcoins using the default mechanisms, and then use those
Bitcoins to purchase blinded authorization tokens provided by the Nymble
mix. The privacy would come from the Nymble system's mix properties
(which are transparent to the user), not the particular way the user
managed to configure+use their Bitcoin software.
Then, such a system could take as input any number of scarce resources
using the same authorization mix: Bitcoin, SMS, two dozen solved
Captchas, a real IP address, or any micropayment scheme.
How to make the payment-based schemes refundable is another fun problem,
especially if we use only a single mix layer for all of them. However, I
expect in the overwhelming majority of cases, people won't want or expect
their money back, but would instead prefer a review process that simply
got their account reinstated.
Thanks for clarifying this for me. Sorry for the misunderstanding.

@_date: 2013-01-20 15:47:45
@_author: Mike Perry 
@_subject: [tor-talk] Tor relay on small and cheap devices 
Thus spake Mirko Vogt (mirko at openwrt.org):
Disclaimer: I haven't read either the source or the research paper
behind haveged.
However, based on
 it sounds
like the underlying assumption is that *any* externally driven interrupt
that happens on your system will alter the internal state of the CPU and
caches such that a measurement codepath will actually end up taking a
variable number of clock cycles, which they then use as their source of
If they're right (critically: their assumption also applies as well to
MIPS or ARM CPUs as it does to x86, SPARC, and DEC CPUs. Their docs
don't mention interrupt uncertainty stats for ARM or MIPS), then
anything that causes an external interrupt on your machine should
effectively provide entropy to haveged.
But, even with this, it would seem that if you're booting your SoC
without any enabled devices from readonly boot media, I would still
expect there is a high likelihood that the internal CPU state will still
be identical through the boot process, unless you have thermal sensors
or a battery monitor or something.
On the other hand, if their assumption holds equally well for these
CPUs, I would also guess that the interrupts even from passively
scanning WiFi should provide enough entropy even without driver support,
though. From the table in that that url, it looks like the least amount
of entropy they could extract per interrupt was 8Kbit (Itanium I)?
Certainly is an interesting idea.

@_date: 2013-01-20 16:28:13
@_author: Mike Perry 
@_subject: [tor-talk] Tor relay on small and cheap devices 
Thus spake Okhin (okhin at okhin.fr):
I hope this reply isn't too discouraging, but I'm somewhat of a
pessimist/hater when it comes to "Let's put a Tor relay on that tiny
computer!" projects. Sure, I guess everybody loves the endorphin rush
from bragging that their phone/wristwatch/fridge is a Tor relay, but
with the current limits on Tor network size, it's unlikely that the
network could support enough of these tiny relays to actually make any
substantial capacity difference, and they may actually harm overall
performance rather than help it.
As it stands, directory overhead will probably become unmanageable if we
get much more than 10k tor relays in total right now, and perhaps more
importantly queuing theory and experimental results indicate that adding
more slow relays will actually hurt performance more than help[1,2].
I think a super-portable device that can either do transparent Tor
proxying and/or run a hidden service for you are probably both better
use cases for tiny hardware than a full fledged relay, especially if
your goal is to actually have these things see real use.
[1].  section 6.
[2].

@_date: 2013-01-30 15:49:54
@_author: Mike Perry 
@_subject: [tor-talk] Directory Server Decentralization 
Thus spake Raynardine (raynardine at tormail.org):
Authority key theft is a serious concern, since an adversary who is able
to maintain control of a majority of the consensus signing keys can
generate fake consensuses, possibly feeding them only to specific users
under targeted attack.
Right now, we try to mitigate this by yearly rotating each authority's
consensus signing key. Most/all authority operators keep the key that
authenticates these rotated consensus keys offline.
Longer term, I'm interested in having some form (or better: many forms)
of multipath consensus validation:
Note that it's possible for an enterprising individual to start working
on some form of  without really modifying any existing tor C code.
For example, a couple python scripts and a public signed git
repository would be enough to be able to provide multipath consensus
validation using the existing consensus data today. One could even use a
Tor Controller library such as Stem, txtorcon, or pyTorCtl to provide a
client-side tool that automatically stores consensus history for later
out-of-band comparison against the public git repo, or even for live
multipath validation.
Might be a fun intro project for someone interested in getting involved
in Tor, actually, and it would potentially be a helpful validation tool
for people currently under extremely censored/filtered/targeted attack

@_date: 2013-01-30 16:35:15
@_author: Mike Perry 
@_subject: [tor-talk] Directory Server Decentralization 
Thus spake unknown (unknown at pgpru.com):
Yeah, an unsigned git repo of hashes + timestamps for each consensus is
an easy way to get a well-tested implementation of such a data structure
without doing any additional coding. Plus it comes with easy mirroring
capabilities, since git is a very common tool these days.
I suggested adding signatures to the git repo as a way of authenticating
the "newspaper" in that wikipedia page. But yes, technically signatures
are not required if you have some other way of authenticating/verifying
the head hash of the git repo is the real, current head that everyone
The reason you want this hash chain + timestamp history property is for
efficient retroactive verification of a large number of consensus
hashes. Say for example, to discover if you were ever given a
compromised network view upon returning from a visit to a censored
location, or similar circumstance.

@_date: 2013-01-31 09:24:17
@_author: Mike Perry 
@_subject: [tor-talk] Directory Server Decentralization 
Thus spake Raynardine (raynardine at tormail.org):
Can you explain why multipath consensus verification would not address
the issues you're concerned about?
We're also not fully convinced that either TorSK or PIR-TOR solutions
are perfect (or even across-the-board improvements). That's why we
haven't deployed them.
In addition to having their own debatably risky security properties,
each approach will introduce their own new engineering problems,
especially on the load balancing, metrics, and performance end.
Signed consensus documents that everyone can globally verify are the
best way we know of to "take this matter seriously".

@_date: 2013-01-31 18:04:16
@_author: Mike Perry 
@_subject: [tor-talk] tor crashing: [err] circuit_build_times_get_xm(): 
0 failed; aborting
Thus spake Quan (quan at tormail.org):
Bleh. I'm guessing your circuit build times data is somehow
degenerate/corrupted. Perhaps you ran out of disk space and the state
file got truncated?
If you can reproduce the issue reliably, can you perhaps send us the
results of something like 'grep "Build" /var/lib/tor/state'? Private
email is fine.
Don't ever publish the entire state file though: it is dangerous to your
hidden services' anonymity.

@_date: 2013-07-02 12:33:10
@_author: Mike Perry 
@_subject: [tor-talk] Theft of Tor relay private keys? 
thomas.hluchnik at netcologne.de:
If their intercepts are passive, merely stealing relays' private
identity key won't accomplish much because Tor uses Forward Secrecy for
both the relay TLS links and for circuit setup.
However, if their intercepts are active (as in they can arbitrarily
manipulate traffic in-flight), then stealing either Guard node keys or
directory authority keys allows complete route capture and traffic
discovery of targeted clients.
So you are right to worry about this, in my opinion. I am also very
I want to make some changes to Tor to make such key theft easier to
detect, less damaging, and harder to make use of:
However, I want to do a lot of other things in 0.2.5.x though too, and
there's this whole browser thing that I'm technically supposed to be
paying attention to too, so I might not get to those. :/
That first one ( is probably a good volunteer/student project for
someone who likes Python, though. It would be easy to make a prototype
with Stem, txtorcon, or even TorCtl.
I am in favor of regular identity key rotation for relays, and I want to
work towards supporting that better by default:
I think keeping your keys on a ramdisk or encrypted filesystem with a
memory-only random key (so if you experience unexplained reboots, etc
they go away) is a good idea.
Also, since Tor reads/creates its identity key at startup and doesn't
need the file afterwords, you can even 'shred'/'wipe' it after that, so
the adversary can't easily pull them off the FS while the system is
Better still, you can load a Kernel module to disable gdb debugger
support so the adversary has to actually dig through/manipulate raw
memory to get the key (which will be error prone and is more likely to
lead to crashes/panics/reboots): I started describing these and related ideas here:
But I got distracted by more pressing issues before I could finish the
scripts.. Also, many of those encrypted+authenticated Tor container
things probably don't make much sense without Secure Boot to
authenticate the boot process up until you can start up Tor. :/
Sort of. Weekly identity key rotation is too frequent to recommend for
people to do for a few reasons. First, it takes the bandwidth measurement servers a couple days to ramp
up your capacity of your new identity key, so you will spend a lot of
time below your max throughput. Second, you would also likely never get
the Guard flag. Third, there are also load balancing issues with Guard
nodes where as soon as you get the Guard flag, it will take 1-2 months
before clients switch to your new Guard, so you will also likely spend
that time at less than your full capacity.
No and no, but your hidden server might have brief downtimes/descriptor
publish times that correlate with your key rotation. Not sure how severe
that is in practice.

@_date: 2013-07-02 23:01:15
@_author: Mike Perry 
@_subject: [tor-talk] Theft of Tor relay private keys? 
Andreas Krey:
What do you mean by 'that'?
Guard node key theft allows a form of route capture where your upstream
gets to direct your traffic to the exit nodes of their choice, using
either cryptographic tagging/bitstomping or a timing-based version,
*without* actually running the Guard nodes you use.
In 0.2.4.x, I did a lot of work on the "path bias" detectors so that
your Tor client could at least alert you in these situations.  However,
I think we need a combination of  (which I mentioned in my first
reply) and  to make
these attacks significantly less feasible in the first place.
Now, the NSA (or any other upstream) could force you to only use Guards
they control to perform the attack without key theft. For that, we need
However, I think that even in that case, a proper implementation of
 should still prevent them from being able to directly control your
circuits easily, especially in combination with the path bias detectors.

@_date: 2013-07-06 14:06:21
@_author: Mike Perry 
@_subject: [tor-talk] Should I warn against Tor? 
Jens Lechtenboerger:
It's also important to understand the limitations of these attacks. If
the data they record is low resolution (such as Murdoch's IX sampled
results), the accuracy will be poor.
Murdoch didn't achieve any success at all until several megabytes were
transmitted in a single connection, and even after that, the accuracy
was heavily impacted by the prevalence of similar traffic elsewhere in
the network (due to a phenomenon called the 'base rate fallacy').
As more people use Tor, the better this property gets. In fact, a
Raccoon (when you run an anonymity network, you get all sorts of
interesting characters) proved that the accuracy of dragnet correlation
attacks falls proportional to 1/U^2, where U is the number of concurrent
active users. This creature also pointed out the same property is
visible in Murdoch's own graphs:
I think this property suggests that with better usability and some
lightweight defenses, Tor can actually do quite well, especially for
relatively small, short transmissions like website loads.
I am worried about the level and duration of timing resolution that
datacenters as large as the NSA one in Utah could provide (assuming that
all that storage is for traffic, and not for stuff like mapping ECC
curves onto Z_p). Even so, I still think protocol-level active attacks
(such as RPW's hidden service Guard discovery attack, and the Raccoon's
bitstomping/tagging attack) are far more likely to be how intelligence
agencies and others will attack Tor:

@_date: 2013-07-14 11:28:25
@_author: Mike Perry 
@_subject: [tor-talk] Would Conflux have a positive effect against website 
Sebastian G. :
This was my estimation, too. Against passive adversaries, it should do
quite well, especially since they should have no information (or at best
incomplete information) about the Conflux path load balancing ratio for
each circuit in the Conflux path, and which bridges are participating in
which Conflux paths.
Against active adversaries, it seems like they may be able to mess with
the Conflux adaptive path load balancing to shift enough traffic onto a
single guard/bridge to get better website traffic fingerprinting
results, or at least to get traffic to flow in a way that lets them
infer or manipulate the ratios.
Still, I think I am in favor of Conflux overall. There are some minor
concerns (such as the need to prevent 2-Conflux from turning into
N-Conflux and allowing individual clients to soak up a lot of network
capacity), but I haven't heard anything that indicates it shouldn't be
There will be some potential issues to adapt with what happens in
circuit failure conditions, how to handle adaptive timeouts, and how to
adapt our other path selection improvements and defenses to Conflux, but
I don't think there will be any extreme blockers, there either. We just
need to be careful that the proper and equivalent behaviors are applied
to conflux circuits, too (which may take some reasoning and engineering
time, though).
The core problem is that bridges are unmeasured and not load balanced.
We currently have not implemented a way to check if a potential bridge
relay is fast enough for the "Fast" relay cutoff, for example, let alone
making sure users are allocated to them in proportion to bandwidth
(which is a much harder problem).

@_date: 2013-07-16 11:59:56
@_author: Mike Perry 
@_subject: [tor-talk] Would Conflux have a positive effect against website 
Roger Dingledine:
I doubt very much that size is enough, actually.  All the studies so far
that claim success with low-resolution features like size classify only
somewhere between 20 and 100 pages. I think we're seeing clear evidence
of publication bias in favor of attack papers, and people gaming the
peer review system by dressing up their attack papers to look more
impressive than they are, at least on this topic.
After all, how many web pages are there these days? A few trillion? It's
quite clear that there will be tremendous overlap in the cell counts on
anything even remotely realistic in scale.
I still agree we should defend against both size and timing, because I
believe that defenses that succeed against small-scale/rigged website
traffic fingerprinting attack papers will also make end-to-end
correlation more difficult.

@_date: 2013-06-14 19:39:31
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
The new TBB 3.0 series is almost ready for its first alpha release!
Here are the major highlights of the 3.0 series:
 1. Usability, usability, usability! We've attempted to solve several
    major usability issues in this series, including:
    A. No more Vidalia. The Tor process management is handled by the Tor
       Launcher extension. If you want the Vidalia features, you can
       point an existing Vidalia binary at control port 9151 after Tor
       Browser has launched, and it should still work.
    B. The browser now uses a local about:tor homepage instead of
       check.torproject.org. A local verification against the control port
       is still performed, to ensure Tor is working, and a link to
       check.torproject.org is provided from the about:Tor homepage
       for manual verification as well.
    C. For Windows users: an NSIS-based extractor now guides you through
       the TBB extraction and ensures the extracted bundle ends up on your
       Desktop, or in a known location chosen by you. Hopefully this
       will mean no more losing track of the extracted bundle files!
 2. The bundles are all under the 25M gmail attachment size limit, so
    direct email and gettor attachments are once again possible.
 3. We now use Gitian to build the bundles. The idea behind Gitian is to
    allow independent people to take our source code and produce exactly
    identical binaries on their own. We're not quite at the point where
    you always get a matching build, but the remaining differences are
    minor, and within a couple more releases we should have it fully
    reproducible. For now, we are posting all of the builds for
    comparison, and you can of course build and compare your own:
    Please try these out, test them, and give us feedback! The plan is to
post them on the blog by Monday, unless something goes horribly wrong.

@_date: 2013-06-14 20:50:08
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Moritz Bartl:
Sounds good. I will see if I can merge this for the next release.
Can anyone else reproduce this? I am not able to. I've been using
various versions of the en_US bundles for about a month now just fine,
including the "official" ones I linked to in the parent post.
Are you sure you're not overwriting an old copy with pref damage or
Ok, also when you're satisfied, can you tag a release and sign that tag?

@_date: 2013-06-15 19:09:28
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Matt Pagan:
This crash does not happen to me on a fresh x64 Win7 VM. It is possibly
an AV/other software conflict.
I got an off-list reply from someone claiming that this was due to a
system Tor being installed, and that the conflict happens even if the
system Tor is not running.
I am not sure how they arrived at this conclusion/diagnosis, so we
should try to reproduce it.
For everyone who is experiencing these crashes: Do you have a system tor
installed? If so, if you uninstall it and reboot, does TBB still crash?

@_date: 2013-06-15 19:22:48
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Micah Lee:
Yeah, all of these filename changes are due to various aspects of Gitian
that make the bundle names different. The current naming scheme should
remain the same for the rest of the 3.0 series, though.
As a heads up: somewhere later in the alpha series we will be redoing
the directory organization inside the bundles as well.
There will always be a chicken/egg problem here. We need to update the
recommended versions file before the bundles get uploaded so that the
first people who download them don't immediately get told they are out
of date.

@_date: 2013-06-15 21:42:53
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Andreas Krey:
I mean the bridge bundles, relay bundles or the vidalia+tor bundles that
get installed into your program files directory as opposed to just your
desktop. These tor binaries get registered as a Windows system service,
which could have a number of different effects.
If people are experiencing these crashes without having installed a Tor
Relay or similar bundle, then the next step is to make a fresh VM and
start adding your favorite security and other always-on service software
to the VM until it breaks.
Once again, there are quite a few people for which these bundles work
just fine on vanilla x64+Win7, so there has to be some third party
software component causing these crash/exit issues.

@_date: 2013-06-15 23:05:12
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Matt Pagan:
Ok, we have a new hypothesis.
One of the crash scenarios seems to be if you install to non-Desktop
(such as C:\Program Files (x86)\) as Administratror, and then try to run
it as non-Administrator.
Are any of you who experience crashes installing to the Desktop (the
I confirmed that I am able to run TBB from the default Desktop location
as both Admin and non-Admin Win7 users.

@_date: 2013-06-16 15:18:47
@_author: Mike Perry 
@_subject: [tor-talk] Tor 0.2.4.13-alpha is out 
Roger Dingledine:
As a heads up, a bug was introduced in this release that allows
malicious websites to discover a client's Guard nodes in a very short
amount of time (on the order an hour), if those Guard nodes upgrade to
this release.
Unfortunately, the bug was introduced by fixing another issue that
allows Guard nodes to be selectively DoSed with an OOM condition, so
Guard node (and Guard+Exit node) operators are kind of in a jam.
I think the best course of action is to suggest that nodes with the
Guard flag *not* upgrade to this release, unless they are experiencing
unexplained OOMing?
If we can't find a solution that rigorously fixes both issues, I think
that future releases should have the OOM DoS fix off by default but
available through a torrc option.
See also:

@_date: 2013-06-17 00:33:07
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Andrew Lewman:
I would like to blog about these bundles tomorrow morning. Where can
they go for that announcement?
I don't appear to have access to archive, nor do I have the
infrastructure to set up and seed magnet links atm.
Should I just serve them out of people, and should Tor Browser Launcher
use my people homedir instead of www?
In the meantime, I've synced the mirrors to create this url, which
should be up shortly:

@_date: 2013-06-17 16:40:23
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Alright, so now we have at least 4 independent reports of a crash in
d2d1.dll, which is Microsoft's 2D acceleration library..
The question now is: What do you guys all have in common that others do
Do you mind sharing your video cards and/or video driver versions?
Cat S:

@_date: 2013-06-17 20:13:07
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle 3.0alpha1 test builds 
Andreas Krey:
Ok, people on liberation-tech are reporting that they can run the bundle
if they right-click on it and go to "Properties->Compatibilty->Run in
Windows XP Compatibility mode."
If 'Compatibility Mode' works, While in compatibility mode, can you try
setting one or both of these to 'true' in about:config:
  gfx.direct2d.disabled
  layers.acceleration.disabled
Then try without XP compatibility mode, and see if one or both allow you
to run without crashes?
It also looks like Mozilla hit a similar bug some years ago, and
maintains a blacklist of bad D2D drivers to disable D2D for:
I wonder if we somehow didn't pick up their D2D driver blacklist in this

@_date: 2013-06-29 19:32:09
@_author: Mike Perry 
@_subject: [tor-talk] Binary patch downloads (for updating TBB)? 
David Bala?ic:
We hope to support the Firefox updater in TBB soon. After some Tor
Launcher cleanup, this is Pearl Crescent's next task.
The Firefox updater uses Mozilla MAR format, and updates contain only
the binary deltas (patches) between two release versions.
Until then, you're still basically stuck removing your previous TBB and
downloading a new one to replace it, though..

@_date: 2013-06-29 20:32:26
@_author: Mike Perry 
@_subject: [tor-talk] Binary patch downloads (for updating TBB)? 
Nick Mathewson:
The initial design doc is at:
Here's a smattering of illustrative urls:
Also note that Firefox does support cert-level pinning specifically for
its update servers, so in addition to MAR signatures, the system also
has a trust path through the compiled-in https cert to the updates.xml
hash value for the update.
I have not yet thought hard about how to integrate it with deterministic
builds, multiple builder signtures, etc..
In terms of Firefox update vs Thandy, my estimation is that supporting
Firefox Update will be considerably less engineering effort and future
maintenance, but that we should still work towards deploying both in
case either updater experiences unexpected issues.
If we find any terribly bad security properties in the process of
understanding, adapting, and deploying Firefox update, we can consider
either patching it or making it optional.
For example, I am sure that it is not hardened against freeze attacks,
infinite-download attacks, and other TUF/Thandy threat model issues.
Some of these are no worse than our current status quo with our existing
in-browser update notification. For more severe issues, we can probably
convince Mozilla to fix them.

@_date: 2013-03-10 18:17:03
@_author: Mike Perry 
@_subject: [tor-talk] Fwd: Starting out: Project Coordinator 
Thus spake adrelanos (adrelanos at riseup.net):
If you haven't noticed, there are quite a few slightly more serious
issues I need to deal with at the moment.
Also, if you check git blame, you'll also see that I have never touched
start-tor-browser, nor have the browser hackers.
If you recall, it was because of this, and because your patches were
specific to the Linux version, that I suggested that you maintain your
own fork for now, because no one maintains that script.
We're also looking to replace start-tor-browser with a direct launch
of Firefox on all platforms, which will hopefully solve a lot of
problems, not just yours.
Thank you for your patience!

@_date: 2013-03-14 17:52:07
@_author: Mike Perry 
@_subject: [tor-talk] How to obfuscate the Tor Browser activity from the 
You got me. I'll bite. This is probably my favorite FAQ to answer, in
part because I have a dissenting opinion from the established dogma. :)
Right now, I think the best answer we have is "Also run an obfsproxy
bridge", but that's not a great answer for many reasons (in addition to
being tricky to do with TBB's Tor instance).
The big problem with e2e correlation is the multiple visit problem. A
bridge might improve things if there is enough traffic, but over a long
period of time the adversary can build up confidence in their
correlations, especially if the bridge is only in infrequent use.
So we still have no great answers at the moment. But as I said, I'm an
optimist about the future.
In my opinion, the first problem we need to solve is the Website Traffic
Fingerprinting problem. If we're clever, lightweight solutions to that
problem will also serve to reduce e2e correlation accuracy. Dual-use
examples include running an HTTP to SPDY conversion proxy at every Exit
(so we can make use of more efficient request re-combination,
pipelining, and randomization than most servers currently support), and
light-weight adaptive padding to the Guard node. See these two links for
further thoughts in that direction:
With those defenses in place, I am hopeful that if we can also scale Tor
to tens of millions of concurrent users, those defenses will also
significantly increase the number of observations an adversary has to
make to succeed at e2e correlation. But we probably need lots more users
(to increase the rate of similarly-sized concurrent activity to within
our ability to blend it with padding) for it to actually work with
acceptable amounts of padding.
Thus spake avarageanonymous at hushmail.com (avarageanonymous at hushmail.com):

@_date: 2013-03-15 00:43:55
@_author: Mike Perry 
@_subject: [tor-talk] What would Tor v1.0 look like? 
Thus spake Nick Mathewson (nickm at alum.mit.edu):
Hrm.. I don't really like the "drop the 0" idea. I really think it was a
bad idea for TBB. I don't feel like TBB is 2.x software, but I wasn't
paying attention when that decision was made (I was still in delusion
about being able to maintain a secure, usable browser addon to pay much
attention to TBB at that point) :/.
Recently however, I actually was thinking it might be a good idea to
revision TBB lower as soon as it stops being a "bundle" and just becomes
"Tor Browser" (ie it loses Vidalia and gains an updater). I think at
that point it would be much closer to what a 1.0 software release should
Of course, TBB is already sort of trapped by the current versioning,
which is why I think it's important to caution tor-core against making
the same mistake.
Obviously not my decision, but if we want to start bumping major
tor-core version numbers, these are the milestones I would use:
1.0 - Anonymity: Cryptographic protection against tagging attacks
2.0 - Performance: Datagram transport/other performance improvements
3.0 - Scalability: The network can support tens of millions of users
If 1.0 is instead "Damnit, it's been 10 years!" (which seems OK to me;
there are certainly worse milestones), then I think that list just moves
back a version number, but dropping the 0 still doesn't strike me as the
right move.
We do have quite a bit more work to do before this stuff is actually
usable by most of the population, and I think that is what a 3.x version
number represents to most people (or at least it used to).
After all, we are attempting to do what is more or less perceived as
impossible by many, and I think being somewhat conservative about
version numbers is still the right decision...
Your eternal optimist,

@_date: 2013-03-28 12:55:01
@_author: Mike Perry 
@_subject: [tor-talk] Mozilla Persona and Tor 
Thus spake NoName (antispam06 at sent.at):
I actually really like the privacy properties Persona *could* provide in
theory. In theory, it can solve most (or maybe even all) of the problems
we have with third party identity providers today.
There seem to be some wrinkles in practice, though.
I'm glad that it is seeing use, but I'm not sure exactly how, since
navigator.id (where its browser-side API is supposed to live) doesn't
exist in any browsers I have installed, including Firefox 19.
Here's the best documentation I've found that gives you an idea of what
happens when a site deploys Persona:
This page is also interesting:
 From my perspective the most important properties of Persona are:
1. In theory, the identity provider does not discover the sites that you
visit. It merely issues a signed statement that your browser stores to
later present to websites. If this property holds, it's quite awesome.
2. Sites that you visit do not get to inspect which identity statements
you have installed. The user is prompted to send the site either zero or
one of their potentially many signed identity statements. This is also
3. From my read of the spec, there also seems to be no technical reason
why the identity provider can't be kept offline -- except for the fact
that they recommend user certificate expiration times on the order of 24
hours for some reason (which introduces potentially serious privacy
problems. See below).
The first two properties make me think that if the user is allowing disk
history records, there should be no remaining content-facing privacy
reasons that prevent these certs from being stored and persisting
forever (or until the user deletes them). That is also pretty awesome.
However, the following general privacy issues seem to exist with the
specific deployment and implementation that the spec seems to suggest:
0. Can sites determine if I have at least one identity statement? Do the
APIs behave differently in this case? Perhaps this is why I don't have
navigator.id, for example? If so, that's a fingerprinting bit. From the
point of view of the website, the APIs should behave identically
regardless of how many certificates I have, even if I have zero. (Note
this doesn't mean that what the user actually sees has to be the same in
that case).
1. Why do users have to get their certs re-signed by the Identity
Provider every 24 hours? This means the Identity Provider gets to know
which of their users are using the web on a given day, and from which IP
addresses. Worse, the specs seem to suggest this re-signing should
happen transparently in the background without user input. That means by
installing an identity cert, you are actually handing over your daily IP
address history to your Identity Provider, regardless of how frequently
you really want to interact with them. Even worse, if the Identity
Provider sets lower cert expiration times, it seems that they would get
even higher resolution IP address data, perhaps down to the minute!
2. What authenticates an Identity Provider to a website? If their IdP
certs have to be signed by the CA model, that kinda sucks, but at least
authentication could still be done offline. However, if their certs have
to be fetched by the authenticating site over CA-authenticated HTTPS,
that *really* sucks (because it damages privacy property  - identity
providers would know if at least one of their users visited a site). It
would be nice to have both options.
3. The combination of issues 1 and 2 might actually completely destroy
the main privacy benefit of Persona. Ie: the identity provider may still
be able to infer which sites each of their users is visiting, especially
over a number of days of repeated activity, and especially for smaller
identity providers (or malicious providers that set very low cert
expiration times for both user certs and their own IdP certs).
I guess I probably should bring these issues up with someone at Mozilla
before it's too late..

@_date: 2013-03-28 17:08:42
@_author: Mike Perry 
@_subject: [tor-talk] Mozilla Persona and Tor 
Thus spake Guido Witmond (guido at witmond.nl):
Where exactly does the spec specify that identity providers MUST
allow users to receive email at the identity address?
The FAQ appears to state the opposite:
As far as I can tell, the email address convention was created for
usability reasons, not as a protocol requirement.
Persona allows for identity providers with different policies to exist.
It is very federated. It is perhaps even more federated than OpenID in
that websites will implement a generalized protocol that supports *any*
identity provider, as opposed requiring them to implement support for
a specific list of identity providers.
Nowhere in the spec do I see anything that would prevent Tor from
creating an Identity Provider that gives you a new identity every time
you clicked New Identity (except for the fact that we would learn where
and when Tor Identities were later used, and we really don't want to
have that data in our logs, hence my previous concerns).
The downside is that many websites can and probably will eventually
restrict the number of Persona providers they actually accept, and they
may choose to draw this line arbitrarily at ones that support email.
However, many websites already restrict account creation to this line,
and some even draw the line at specific email providers.
So we would be no worse off than now, and if Persona could be made
acceptable to us, we would also have the ability to propose alternate
solutions within the model (such as Nymble and/or proof-of-work:
 If those
solutions end up working better than email verification for limiting
abuse, they will gain acceptance among Persona-supporting websites.
As far as I can tell, Persona is a missing piece in a puzzle we have
been trying to solve for a long time. It's not the whole puzzle and it's
not *exactly* the right shape of puzzle piece right now, but it looks
like we can certainly hammer it in there if we have to ;).

@_date: 2013-05-02 00:36:44
@_author: Mike Perry 
@_subject: [tor-talk] HTML5 video and Tor anonymity. 
Thus spake Tom Ritter (tom at ritter.vg):
There's a fourth reason: Flash can enumerate a separate and more
detailed set of facts about your computer than Javascript can, and we
have many indications that this set of facts is much larger (and thus is
more identifying).
The major one that the EFF found was that not only does flash export a
full list of fonts installed on your computer, it also provides this
list in a machine-dependent order. There are probably quite a few other
surprises like that, too.
Depending on the nature of the sandbox/VM, it may or may not be possible
to address those fingerprinting issues...
This shouldn't discourage anyone from working on a minimalistic flash
sandbox though. Any solution would be better than none, especially since
we already allow people to go into the TBB settings and mash the thing
on if they really want..
I believe we even have an upstream deliverable for a flash sandbox.. Not
my area of personal expertise or interest, though. I'm with Steve Jobs
on this one: kill that fucker until it is dead.

@_date: 2013-05-03 20:26:21
@_author: Mike Perry 
@_subject: [tor-talk] torslap! 
Thus spake Tom Ritter (tom at ritter.vg):
What about Memory Based Puzzles?  is one
example. A Google search for Memory Based Puzzled will turn up a ton
Are such attackers likely to also use Tor?
This sounds decent, too.
For the record, I'm not trying to change your mind wrt memory puzzles.
I think Tor should deploy multiple Mozilla Persona[1] servers that issue
Persona identities, each with their own criteria. By leveraging Persona, we would provide sites with an out-of-the-box
ability to choose the rate limiting system they want to accept. For
example, a provider might decide that bitcoin.persona.torproject.org
provides sufficiently expensive "identities" to block abuse, but
discover that multi-captcha.persona.torproject.org and
busywork.persona.torproject.org do not.
We could also create a blinding step to unlink your bitcoin address from
the Persona identity we give you, but then we probably need to introduce
a delay to ensure sufficient mixing. But such a delay will probably only
serve to allow us to more effectively rate limit abuse.
1.

@_date: 2013-05-09 11:48:28
@_author: Mike Perry 
@_subject: [tor-talk] Finger printing 
You know, all you people who keep asking the same questions over and
over again back-to-back in new threads for days on end could try
Googling first.. It might be just a tad quicker.
(which is result  for "tor browser fingerprinting" on startpage.com's
Google results).
tl;dr: We prevent read access to the HTML5 Canvas (which doubles as the
WebGL rendering surface, among other things) to prevent video card,
font, and other rendering differences from being extracted, hashed, and
fingerprinted. If you go to certain obnoxious websites (such as
 you can see this defense in action.
We also run WebGL in "minimal mode" which disables disable video card
and driver-specific extensions, so that this information is not
available to JS.
Still, WebGL is still a huge beast with an unknown and previously
unexposed vulnrability surface, which is why we still leave it
click-to-play via NoScript.
Thus spake Andrew F (andrewfriedman101 at gmail.com):

@_date: 2013-05-17 15:33:08
@_author: Mike Perry 
@_subject: [tor-talk] Rewriting a text to avoid stylometry 
Thus spake NoName (antispam06 at sent.at):
The above research is a great starting point (and comes with some open
source tools you can try out, albeit they are a bit slow), but this is a
very hard problem because language provides many, many ways for style
variations to differentiate people. Audio is of course even worse.
On the converse, while stylometry attacks are scary in theory, in
practice they tend to fall apart when thrown against "suspect lists"
even as large as tor-talk (I believe the current state-of-the-art is
O(100) suspects). This is a reflection of the difficulty in identifying
population-bisecting features that actually work in the general sense
without introducing false positives due to the natural tendency for
people to share and imitate elements of writing style. At least when it
comes to written text, the adversary really needs to start with a short
list of suspects using prior knowledge.
In both cases, what we really need are solid metrics to rank the
contribution of features to classification accuracy, so we can choose
the language features to obfuscate first:
Ad-hoc techniques as simple as making a conscious effort to "sound" like
someone else have also been shown to be effective without requiring much
practice, but it can also be difficult to break certain key stylistic

@_date: 2013-05-28 22:38:38
@_author: Mike Perry 
@_subject: [tor-talk] Skype banned tor-nodes? 
Cross-posting to tor-relays to give everyone the heads up - I just added
33033 to the ReducedExitPolicy page for Skype:
Non-relay related discussion should remove tor-relays from the Cc.
James Brown:
Not sure if that's actually the problem, but if the only way you can
get to Skype is to use a Bittorrent-supporting exit, it certainly seems
like a possibility.
Thanks for the heads up James!

@_date: 2013-11-14 21:21:05
@_author: Mike Perry 
@_subject: [tor-talk] Firefox DNS leak? 
There is a known DNS leak in WebSockets. Does it happen if you set
'network.websocket.enabled' to false in about:config?
Another possibility is WebRTC, which can leak more than just DNS.
 will use WebRTC mechanisms to probe your IP for
testing purposes.
I believe you can disable WebRTC by setting media.peerconnection.enabled
to false, but there may be other prefs involved too.

@_date: 2013-09-30 19:33:30
@_author: Mike Perry 
@_subject: [tor-talk] [monkeysphere] [Fwd:  Why the Web of Trust Sucks] 
Daniel Kahn Gillmor:
Likewise, I am not on monkeysphere. Monkeysphere users: Please keep me
on Cc.
People on-list: Forgive the extensive quoting. Leaving as much as I
could in-place to avoid removing dkg's comments from the record.
Ok, let's forget "trust". Let's use "key authentication" instead from
now on.
My objections are limited to the fully decentralized "Everyone is a
certifier" model, when used in *any* form.
For purposes of simplifying our discussion, let's pretend for the
moment I am only concerned about authenticating the mapping of keys to
email addresses. This is based on the assumption that most users are
interested in authenticating the keys for people they wish to
communicate with, but that they have never met in person.
It is possible to have global auditability without a social graph, and
without everyone being a certifier. Notary-based certification systems are
quite capable of this. The append-only log utilized by Certificate
Transparency is the hot new example of such a system, but there are
other ways of accomplishing the same basic idea.
If we can't automate the key authentication system, it will fail due to
user error in the overwhelming majority of cases.
My point was that hypothetical Bruce was publishing his authentication
signature in Roger's key, specifically so that people could use his
signature to verify Roger's key, and other keys that Roger certifies.
This transitive model is where the system breaks down. And yet without a
replacement or auxiliary mechanism, it does not scale beyond people who
have already met in person.
My interpretation of you here that "If the user lowers the trust level
of keys they import and/or tweaks other settings, then the adversary
would have to compromise more than just one key before enough paths
could be generated to meet the user's desired level of authentication."
This is not very comforting. It would still seem to be the case that the
adversary gets their choice of compromising the weakest keys out of a
pool of thousands (or hundreds of millions, if the system were capable
of supporting Internet-scale key authentication).
I am not arguing in favor of the CA model. Everyone knows that it sucks
too, I hope.
Here, I'm arguing against any certification system that requires a
social and/or meatspace meetup graph.
Ok, I will grant you that we can store the data. In fact, if it is
possible, there are way more interesting things we can do with that
storage than merely record the meatspace interactions of the users of
the system (which I'll get to later).
However, as the system scales though, the percentage of the keyspace the
adversary has to compromise necessarily goes down, unless the software
increases the number of signature paths required for authentication in
proportion to the number of total users.
In other words, as the system scales, the adversary still only has to
compromise enough keys to make a few signature paths, where as the
number of keys available to compromise has been growing considerably.
I am interested in alternate OpenPGP certification mechanisms. I admit
I'm not familiar with them, and I'm glad Isis chimed in with several
esoteric options. It is possible that the protocols to support what I
want already exist.
Here's another related option that may already be possible:
5. A "Certifying Keyserver" verifies that someone has control of both a
key and an email address. Such a key server could verify this fact by
requiring the user to respond with a signed email to an emailed nonce
encrypted to their submitted key.
This keyserver would have its own keys, and could publish its list of
authentication statements in an append-only, auditable, mirrored log
(Certificate Transparency style).
More than one certifier and audit log could exist, to provide redundancy
against compromise of the certifier itself.
It depends on the mirroring mechanisms available, actually. If the log
is widely mirrored, the user needs only to find a way to reach an active
Basically, for the current signature-based authentication mechanism to
work, the user needs to be sure that when they download a key and a set
of signature paths, they are seeing all of the available paths, rather
than a subset chosen/created by the adversary.

@_date: 2013-10-02 22:08:36
@_author: Mike Perry 
@_subject: [tor-talk] Silk Road taken down by FBI 
This extended quote was very well said.
At the risk of escalating the war on privacy, I feel I must say this:
Aside from the alleged attempted contract killing by the Silk Road admin
(which is of course despicable), I had always felt that if Silk Road and
anonymity could take the violence and personal risk out of the drug war,
it was a net positive.
Really, the drug war is beyond absurd, and even the violence that
apparently did happen on Silk Road can almost certainly be attributed to
the social effects of prohibition and related pressures. :/

@_date: 2013-09-13 11:25:08
@_author: Mike Perry 
@_subject: [tor-talk] Tor browser can be fingerprinted 
Georg Koppen:
An important detail that hasn't been mentioned explicitly in this thread
is that the inner render window size is the *only* information we report
(unlike the other major browsers, who also report desktop size, taskbar
size, and window and toolbar decoration size information).
The way this is accomplished is that TBB reports the inner render window
size as your entire desktop, and it reports the outer window size as the
same size as this render window (effectively this means that the
window decorations and toolbars are all 0-sized).
So this means that if you resize your browser, you also get a totally
different display fingerprint. However, if you resize it to something
weird, and continue to use that weird size for a while, all of that
activity is highly linkable to advertisers until you resize again.
Also it is worth mentioning that disabling Javascript does not protect
against this type of fingerprinting, as it is available to CSS too:
To learn about more fingerprinting issues in TBB that could use some

@_date: 2013-09-13 14:06:45
@_author: Mike Perry 
@_subject: [tor-talk] Tor browser can be fingerprinted 
Your Tor Browser should *not* be maximizing itself during New Identity.
It should be setting its content window to a 200x100 multiple.
I've never seen one maximize for that step. Does that always happen for
you? Sounds like a bug caused by something about your setup. Does it
happen with a fresh bundle in a new directory? Do you mind sharing your
monitor resolution?
Maybe. It depends on if you resizing the window is actually as "random"
as you think it is. If you keep doing that, and you're one of the few
people who does, you might stand out over time?  On the other hand, it
seems like a tricky algorithm for an advertiser-class adversary to
write, and for little economic gain since it is rare behavior.
However, if your adversary includes people with access to raw
advertising logs, that may be a different matter. My guess is
capital-t-They wouldn't bother with that vector though. Too expensive
for too little information.
So on balance, I think it's probably a decent thing to do for that odd
website account you don't want linked to anything else?

@_date: 2013-09-27 14:18:17
@_author: Mike Perry 
@_subject: [tor-talk] New GPG key for Mike Perry 
Hi everyone, I've finally made a new GPG key (after a scant 7 years!). This new key will be used to sign email from me going forward, and will
be used to sign software releases until such time as I get around to
creating a second set of keys on a hardware token for that purpose.
While I dislike the Web of Trust for a number of reasons*, my plan is to
cross-certify these two sets of new keys, and also sign both with my old
key. Hence I will not immediately be issuing a revocation for my old
The new key is attached, and is available on the keyservers (with a
signature from my old key) at:
Here's the fingerprint and current subkey information for reference:
pub   8192R/29846B3C683686CC 2013-09-11
      Key fingerprint = C963 C21D 6356 4E2B 10BB  335B 2984 6B3C 6836 86CC
uid                          Mike Perry (Regular use key) sub   4096R/717F1F130E3A92E4 2013-09-11 [expires: 2014-09-11]
sub   4096R/A3BD8153BC40FFA0 2013-09-11 [expires: 2014-09-11]
This message should also be signed by my previous key, which was used
extensively to sign my email and my source code releases prior to today.
* Ensuing flamewars about the Web of Trust should reply only to tor-talk.

@_date: 2013-09-28 23:43:27
@_author: Mike Perry 
@_subject: [tor-talk] Why the Web of Trust Sucks 
Joel R. Voss:
The web of trust has three main problems:
1. It leaks information.
This includes social graph metadata, time and place of interaction, and
in some cases government/slave-name identity. This issue has been
discussed at length, of course.
2. It has many single points of failure.
Because by default GPG uses shortest-weighted paths to establish trust
in a key, and moreover because nothing authenticates the *entire* Web of
Trust graph, each and every member of the "Strong Set" essentially
functions as a CA, especially for keys only weakly connected to the
Strong Set. If you compromise just one of those keys, you get to use
that key to certify arbitrary keys for any name you like.
To understand how and why this is a problem, let's walk through a
typical Web of Trust workflow. Let's say we have a GPG user named Edward who wants to send an encrypted
email about the extreme level of corruption at his workplace to a
journalist that he has never met. Let's call that journalist Glenn. For
the sake of argument, let's say that both individuals are active
participants in the Web of Trust.
Edward also knows that the network systems administrators at his
workplace are very sophisticated, and intercept all encrypted
communications for purposes of active MITM attacks to obtain the
communications content. So Edward decides to download Glenn's key from
subkeys.pgp.net, and requests that his gpg client provide him with a
trust value for Glenn's key.
Now, the network systems administrators at Edward's workplace have
anticipated this. They have a compromised HTTPS CA cert, as well as have
compromised a couple of highly trusted keys from the Web of Trust. Let's
call one of these GPG keys Roger.
When Edward goes to download a key to use for Glenn, the network systems
administrator gives him a new fake key that they generate on the spot.
The network systems administrator also attaches a fully trusted
signature using Roger's compromised key. They also block the actual key
for Glenn from reaching Edward.
Edward's GPG client has trust in a couple keys. It turns out that one of
his trusted keys, Bruce, has full trust in Roger's key (the compromised
Edward's GPG client then computes a fully trusted path from Bruce to
Roger to the fake Glenn, and Edward then sends an encrypted email to
fake Glenn that is then subsequently read by the network systems
Game over for Edward :/.
This scenario is possible against arbitrary keys using any of the high
degree keys in the Strong Set. They effectively function as single point
of failure CAs for the Web of Trust, which destroy its utility as an
independent key authentication mechanism.
3. It doesn't scale very well to the global population.
The amount of storage to maintain the Web of Trust for the whole world
would be immense. For the level of authentication it provides, it just
doesn't make sense to have this much storage involved.
So what should we do instead?
Well, I think it is important to take a step back and think about what
the Web of Trust is trying to accomplish. Aside from being a global
popularity contest and some kind of weird quasi-religious hacker ritual,
it is an authentication mechanism for the keys that you retrieve.
It turns out there are lots of ways to authenticate keys using multipath
authentication that do not suffer from the Web of Trust's downsides**.
Here's a few examples:
1. Every time GPG downloads a new key, re-download it several times via
multiple Tor circuits to ensure you always get the same key.
2. Every time I verify a signature from a key sent to an email address
that is not mine (like a mailinglist), my mail client adds a tiny amount
of trust to that key (since each new public email+signature downloaded
represents an observation of the key via a potentially distinct network
path that should also be observed by multiple people, including the
3. Every time I am about to encrypt mail to a key, check the key servers
for that email address, download the key, and make sure it is still the
same (SSH/TOFU-style).
4. When downloading a key, GPG could verify that the same email to key
mapping exists on multiple key servers, with each key server
authenticated by an independent TLS key that is stored in the GPG source
code or packaging itself. (Perspectives/notary-style cryptographic
multipath authentication).
** The Web of Trust is technically capable of multipath authentication
by itself, but only if you are aware of all of the multiple paths that
*should* exist. Unfortunately, nothing authenticates the whole Web of
Trust in its entirety, so it is impossible to use it to reliably verify
that multiple paths to a key do actually exist and are valid.

@_date: 2014-04-02 18:35:35
@_author: Mike Perry 
@_subject: [tor-talk] How safe is smartphones today? 
anonymous coward:
Not very, or not at all, depending on your threat model.
Very easy. At least, for most devices. We can do better, though. Read
If someone gets access to your Google account, they can silently install
any app they want behind your back:
This has been the case for years. I still don't know why Google hasn't
added at least some kind of on-device confirmation for apps that get
installed from a web login unrelated to the device.
You may like:
It's still not perfect (nothing is), and it's certainly nowhere near
user-friendly yet, but I happen to think it's a step in the right
I dunno man. I've heard Bruce Schneier once decrypted a box of
AlphaBits, so perhaps he knows something we don't.
Not nothing. You either need a device without a baseband, or with a
hardware isolated baseband. See that blog post.
Nah. It's just going to be kinda tricky. I'm cautiously optimistic,

@_date: 2014-04-03 19:44:08
@_author: Mike Perry 
@_subject: [tor-talk] How safe is smartphones today? 
Yes, I mention this and that link several times in that post.
Thanks for the direct link to their wiki though. I updated one of the
links to the backdoor to that URL.

@_date: 2014-04-03 19:52:14
@_author: Mike Perry 
@_subject: [tor-talk] How safe is smartphones today? 
David Rajchenbach-Teller:
I looked into this and made contact with the FFOS team about potential
collaboration, but it was not a priority for them. We would effectively
be responsible for doing all of this work ourselves.
This would actually be a lot of work for us to do, too. There are
several architectural changes needed to Firefox OS in order for us to be
able to do the things I did with Android in this post:
In particular, the following is a sampling of my more major concerns:
1. Apps share a lot more state and linkable identifiers due to running
in the same parent Gecko process (and sharing much of the HTTP stack).
2. This also means that apps are way less protected from one another
than on Android (where everything runs as both a separate process *and*
a separate user ID).
3. There are no per-app proxy settings, and individual apps can not be
blocked from accessing the network.
4. The system-wide proxy settings still allow for a number of things to
leak outside of Tor.
5. It is my understanding that apps can source remote JS libraries over
HTTP if they wish, and nothing prevents this. This effectively means
that what you think is your app may not be your app at all.

@_date: 2014-04-05 11:07:08
@_author: Mike Perry 
@_subject: [tor-talk] How safe is smartphones today? 
anonymous coward:
If you are concerned with protecting the social graph of who you are
communicating with, there is *maybe* exactly one communication system
that exists today that can protect this information from a dedicated
adversary with resources on the order of a drug cartel. The system I'm referring to is a prototype written by a Google engineer
in their spare time: It comes with this disclaimer:
"Dear God, please don't use Pond for anything real yet. I've hammered out
nearly 20K lines of code that have never been reviewed. Unless you're
looking to experiment you should go use something that actually works."
Of course, even if Pond itself is secure (and it very well may be -
despite the disclaimer, Adam Langley is actually a very capable
cryptographic engineer), if you use it on a normal, non-hardened
computer, your social graph can still be obtained by compromising that
It is my opinion that there is little substantial difference between a
computer you get off the shelf today, and a WiFi-only mobile device you
get off the shelf today. Both have to be hardened in ways that are just
as involved as the blog post I wrote about hardening Android.
This is sadly currently out of the reach of most humans today, if they
are dealing with an adversary with resources significantly beyond their
There is no "perfect", there is only "bad", and "better". Because of
this, every situation needs detailed analysis to understand the nature
of the information you are trying to protect, and the resources and
capabilities of the adversary you are trying to protect it from.
Unfortunately, the current state of security is that it sucks. In fact
dangerously so - to the point where I am not optimistic about our
ability to have functional computing devices at all in about 5-10 years
time unless drastic changes are made:

@_date: 2014-08-25 16:31:25
@_author: Mike Perry 
@_subject: [tor-talk] BBC: NSA and GCHQ agents 'leak Tor bugs', 
For the record, in the original interview transcript[1] Andrew states
that "it's a hunch" that these orgs are leaking us bugs, not known fact.
I kind of wish Andrew didn't fan the flames of conspiracy on this point,
though it probably is causing some intelligence bureaucrats to be
scratching their heads in confusion right now, which I guess is a good
thing? On the other hand, if this was happening, making a press release
about it probably is one of the best ways to get it to stop. Which I
also find to be a confusing move by Andrew, if this is what he really
Regardless, in my opinion, while it's fun to speculate that our
favorite bug reporter (bobnomnom/skruffy) is actually an intelligence
service, and that the other "cypherpunk" bug reports we get are also
leaks from this service, I think what is more likely is that we're just
witnessing the "With enough eyes, all bugs are shallow"[2] phenomenon of
Open Source development, coupled with a userbase that is probably at
least a couple sigmas above the norm in terms of technical proficiency.
This is naturally leading to all sorts of interesting bugs being found
by the wider community at a regular frequency.
I also suspect that once bobnomnom/skruffy's bug reporting and
linguistic signature (broken English with a Slavic accent so thick you
can hear it over ASCII) became legendary, many other random people began
to mimic it to report their own bugs, if nothing else to avoid
stylometry attacks.
I've repeatedly seen multiple cypherpunks users with very similar broken
English writing styles argue with each other on the bugtracker. Very
strange, but more supportive of the "random mimicker" scenario than of
multiple NSA/GCHQ agents arguing openly on our bugtracker.
We have gotten some patches from anonymous contributors, but we review
them very closely, and they usually end up going through a few revisions
before we merge them. We obviously subject all contributed patches to
careful review like this, regardless of if they are named, pseudonymous,
anonymous, or "bobnomnymous".
1. 2.

@_date: 2014-07-25 18:36:01
@_author: Mike Perry 
@_subject: [tor-talk] Android app: Torrific 
FWIW, in the shell scripts in my howto[1], I do this UID detection in
shell with dumpsys. Here's an example script:
The userinit problem I solved in a Cyanogenmod-specific way (I think).
Cyanogenmod has a special init script location in
of more standard Android init-scripts, because the AFWall+ startup
script hack does not work on my devices. That's the main reason I
created this userinit hack:
That LinPhone example script above also has another neat feature that I
wish were available by default in a firewall app such as this. It allows
only the UDP activity of LinPhone to bypass the Tor proxy. This means I
can make TLS+SIP+ZRTP calls where the call setup and signaling goes over
Tor, but encrypted voice and video data goes directly peer-to-peer over
I recognize the UI for supporting this in the general case is a bit
tricky to create without a lot of clutter, and it's questionable if you
want to expose this ability for all apps (because for non peer-to-peer
apps it can mean deanonymization to a central server). However, for this
specific case it is very handy, at least until Tor is performant enough
to support live, unbuffered voice+video data.
1.

@_date: 2014-03-28 12:43:12
@_author: Mike Perry 
@_subject: [tor-talk] Linux kernel transproxy packet leak (w/ repro case + 
Hello all,
I've discovered that the Linux kernel appears to have a leak in how it
applies transproxy rules to the TCP CLOSE_WAIT shutdown condition under
certain circumstances. This applies to both the kernels in use by common
Android devices (Cyanogenmod 10.x and 11-M4), as well as the Linux
kernel in Ubuntu 13.04 (3.8.0-35-generic).
The bug can be triggered either by a remote server closing a connection,
or by restarting the local tor client.
Basically, the bug happens when a transproxy connection shuts down
completely before a client application properly closes the socket. This
seems to cause the kernel to lose track of the fact that the client
application connection was being transproxied, and when the client
application finally does close its socket (or exits), the Linux kernel
generates a FIN ACK that completely bypasses any transproxy rules you
have installed. It sends this packet first as the UID of the app in
question, and if that fails, it resends it as a blank UID (the kernel
Here's how to reproduce it and see for yourself:
First run the attached iptables script, and launch a tor daemon with the
attached torrc (edit the iptables script's TOR_UID=`id -u debian-tor`
and NETWORK_USER_ID=1000 vars if your setup is different).
Then, fire up tcpdump, like so:
 sudo tcpdump -n -i wlan0 host 74.125.28.104 and tcp port 80
Replace '-i wlan0' with your network interface. If you use '-i any', you
will also see transproxied packets (which are not normally leaked).
Then, as your transproxied user, paste this python snippet into a python
 import socket
 s = socket.create_connection(("74.125.28.104", 80))
(That IP handles After the connection is made, you should see something like the
following in 'netstat -natp':
 tcp  0 0  127.0.0.1:9040      192.168.1.23:42235 ESTABLISHED 1121/tor         tcp  0 0  192.168.1.23:42235  74.125.28.104:80   ESTABLISHED 977/python      At this point, either wait a couple minutes for Google to close that
connection on you, or shut down your Tor daemon. In either case, you
should see the first connection transition to TIME_WAIT, or FIN_WAIT2,
or similar, lose track of its PID+UID, and then finally disappear
entirely, but the python program will remain in CLOSE_WAIT indefinitely.
Once the first connection is fully gone (this takes 60s from TIME_WAIT
state with default TCP settings), issue this in your python shell:
 s.close()
At this point, you will see a FIN ACK or RST ACK packet appear in your
tcpdump window. That packet has leaked past the iptables firewall rules,
and past the transproxy rules. It went straight to Google.
I have noticed several Android apps (including Firefox, F-Droid, and
many Google apps and Android services) that allow their sockets to sit
in CLOSE_WAIT upon remote close while transproxied, and they all leak
packets in this case, which happens frequently in normal usage. I am not
sure if this is just a common programming error, an issue with how the
Android networking APIs are designed, something specifically exacerbated
by the transproxy, or some combination of these.
For a workaround, I was able to prevent this issue with the addition
of the following rules:
 iptables -I OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,FIN ACK,FIN -j DROP
 iptables -I OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,RST ACK,RST -j DROP
None of the transproxy documentation I could find mentions this issue,
nor suggests any additional safety rules. This means every transproxied
Tor user is unwittingly leaking packets, at least some of the time.
Sorry to be the bearer of bad news.
Please send workaround discussion to tor-talk, and kernel/TCP state
machine discussion to tor-dev. I Cc'd both like a jerk, because I figure
each group might have different sets of commentary, and both groups
should be aware of this issue. Don't be a jerk like me, please. Use your
best judgment to Cc one list or the other.

@_date: 2014-03-28 14:34:58
@_author: Mike Perry 
@_subject: [tor-talk] Linux kernel transproxy packet leak (w/ repro case + 
Velope on IRC suggested a better workaround. It turns out these
connections actually end up in state INVALID when the transproxy side
dies. I tested this with my repro case and confirmed that the --ctstate
rule is working by itself.
Additional iptables rules inline below. Preserving full original
text for historical record.
Mike Perry:
Here's a set of rules to try both --ctstate and --state invalid, as well
as log which ones get hit, for testing purposes. Note the use of -A in
this case, for readability wrt ordering. These rules should come before
any other rule in the OUTPUT chain section of the firewall script you
 -A OUTPUT -m conntrack --ctstate INVALID -j LOG --log-prefix "Transproxy ctstate leak blocked: " --log-uid
iptables -A OUTPUT -m conntrack --ctstate INVALID -j DROP
iptables -A OUTPUT -m state --state INVALID -j LOG --log-prefix "Transproxy state leak blocked: " --log-uid
iptables -A OUTPUT -m state --state INVALID -j DROP
iptables -A OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,FIN ACK,FIN -j LOG --log-prefix "Transproxy leak blocked: " --log-uid
iptables -A OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,RST ACK,RST -j LOG --log-prefix "Transproxy leak blocked: " --log-uid
iptables -A OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,FIN ACK,FIN -j DROP
iptables -A OUTPUT ! -o lo ! -d 127.0.0.1 ! -s 127.0.0.1 -p tcp -m tcp --tcp-flags ACK,RST ACK,RST -j DROP
It's likely only the first pair is needed, and you may want to comment
out the --ctstate LOG line as I did to limit noise for successfully
handled --ctstate INVALID DROP blocks.
I did test this with the above repro method, and --ctstate INVALID did
appear sufficient by itself, but reports of any --ctstate DROP rule
bypass happening will be tremendously useful (which will result in the
later LOG lines being hit, and sending output to 'dmesg').

@_date: 2014-05-19 03:13:14
@_author: Mike Perry 
@_subject: [tor-talk] Firefox, Adobe, and DRM 
paul at crable.us:
I hope that it goes without saying that any changes that Mozilla makes
to allow or include additional third party closed-source/binary
components will be rejected by us, due to the inability to audit these
components for Tor safety, privacy, or general security. There is a long
history of such components completely ignoring the Tor threat model in
their design and implementation, even if by some miracle they end up
being securely sandboxed for normal usage. It would be foolish of us to
assume that this DRM mechanism would be any different.
Moreover, simply removing the DRM will be trivial, and it will be high
on our list of tasks for any rebase effort onto the Firefox release to
support it. I'm not too worried about the technical details of that.
What does worry me is that based on Mozilla's blog post on the topic, it
seems at best their implementation will still provide websites with a
per-device unique identifier:
Due to the ubiquity of deployment of this scheme, it is likely that this
identifier will soon be abused by all sorts of entities, likely starting
with banking and government sectors, and quickly moving on to the
advertising industry (why not play a short device-linked DRM video with
your banner ad? You get a persistent, device-specific tracking
identifier as part of the deal!). I think it is also quite likely that
many arbitrary sites will actually deny access to users who do not
provide them with such a device-id, if only due to ease of increased
revenue generation from a fully identified userbase.
I hope that when this happens, we will begin to see FOSS
re-implementations of this identifier mechanism, if not the CDM itself.
Hopefully we won't be fighting this aspect of the battle by ourselves.
It will be a way more costly battle to fight than simply removing the
It seems that neither Mozilla nor Google have fully thought through the
social effects of giving a unique device-id to arbitrary websites. Or
worse, they simply do not care.
That is indeed deeply troubling.

@_date: 2014-11-03 15:05:22
@_author: Mike Perry 
@_subject: [tor-talk] Krypton Anonymous: A Chromium Tor Browser 
Cyrus Katrak:
We've been going for URL bar domain isolation in Tor Browser to avoid
divergence with how users expect the browser to behave:
Even still, per-tab isolation is a common request, so it's easy to
assume that this is what most people really want. But I think if you
think through how it will work in practice, it becomes fairly clear it's
actually a very bad property for usability.
The easiest way to see how per-tab isolation will cause confusion is to
imagine the twitter use case. In a normal twitter user flow, the user
logs in to twitter, opens some lists and conversations (often in new
tabs), perhaps opens tweetdeck in a new tab, follows links from people
in their feed, and sends and receives twitter conversation links from
their friends over DM, chat, IRC, and email. If each these actions happens in a new, isolated tab, the user will be
forced to log in repeatedly to twitter, and worse, forget which tabs
they logged in to twitter on, especially once they start following links
(both on and off site) from people's feeds.
Is Tor Browser-style url bar domain isolation also possible to achieve
with simple configuration, or did you just go per-tab because the
Chromium plumbing was already set up to make per-tab isolation easy?
I see a cookie policy file that appears to block third party cookies,
but I don't see the per-tab isolation mechanism in the source.
Are these also simple prefs?
Do you interact with the Tor Control port at all here? Or do you just
re-write the torrc? Where is your tor handling located in the code?
It looks like you've seen the Tor Browser design doc and the important
Chrome Bugs links, but I'd like to point these sections out again as
they have recently been updated:
In particular, that fingerprinting section was just updated this past
I also have an OpenWRT configuration I can give you to monitor for proxy
leaks on an upstream router, but you need to be able to configure Tor
Bridges to make use of it.

@_date: 2014-11-03 19:05:45
@_author: Mike Perry 
@_subject: [tor-talk] Krypton Anonymous: A Chromium Tor Browser 
Mike Perry:
Actually, I should point out that I'd love to hear hard data on this, in
case I'm wrong.
I did some testing, and most interactions on twitter seem to in fact be
designed to keep you in the same tab while following links on the site,
but open all off-site links in independent tabs. At least in TBB on the
I bet sites like Facebook, Google, and Twitter have data on how many of
their users end up using multiple tabs/windows vs staying in the same
tab until logout. It could be that I'm wrong and that
multitab/multiwindow users are not the norm for these services.

@_date: 2014-10-03 15:27:06
@_author: Mike Perry 
@_subject: [tor-talk] orWall 1.0.0 released! 
Hey CJ, just wanted to let you know that I've tried OrWall and it's a
huge improvement! Way better user experience on just about every front!
I also have not detected any leaks on my upstream router, either.
When I get a chance, I will update the original blog post to recommend
OrWall instead of my crazy Droidwall hack scripts.
The one thing is that I find the long-press options for "Connectype
type" confusing:  - "Force connection" to what? I assume through Tor's transproxy because
    of the REDIRECT text, but this will not be clear to users who are
    unfamiliar with iptables.
    How about: "Redirect all network activity"
 - What does "native capacity"/"fenced path" mean? Does that mean only
   access to the local SOCKS/HTTP proxy ports in Tor's case?
   How about: "Only allow local proxy port access"
These are complicated ideas to convey, though. I'm not sure my
suggestions are the best ones either.
I also suggest soliciting input about the DNS issue we discussed where
DNS queries are done by root on Android 4.3+ unless the
'ANDROID_DNS_MODE=local' environment variable is set. Perhaps someone
will come up with a clever hack to set this env var in a persistent way
that we haven't thought of, or find some way to write a shim on the DNS
resolution filesystem socket to enforce what we want.
You could list this on a known issues or FAQ page, or in your bugtracker
I guess. Making root/UID 0 handle DNS is also a security risk, and I'm
very surprised the Android team thought this was a good idea. :/
Also looking forward to the "Logs" window doing something :)

@_date: 2014-10-04 15:27:12
@_author: Mike Perry 
@_subject: [tor-talk] orWall 1.0.0 released! 
Yeah, sounds messy. Though from the droidwall days, I thought that LOG +
dmesg was the common denominator, but I've been running Cyanogenmod for
a long time...
Hrmm. Let's hope that AFWall is being careful with this. I get nervous when I hear that root apps are going to start exposing
APIs and Intents to configure stuff at the request of other apps.
This is especially risky when we're talking about stuff like iptables
commands that are destined for shell/direct execution. There's just too
many ways to mess that up and open up potentially remote exploitable
root holes (which even webpages can sometimes exploit in the case of

@_date: 2014-10-16 02:48:26
@_author: Mike Perry 
@_subject: [tor-talk] orWall 1.0.0 released! 
Mike Perry:
I just noticed another issue this DNS-as-root snafu causes: The "Enable
Browser" option seems to leave the UID 0 DNS redirect rule in place,
which causes DNS lookups to fail if Tor is unreachable, which in turn
makes most captive portals unusable (since Tor can't be used to do the
DNS resolution for them).
I guess for now the only option is to remove the DNS redirect rule for
the duration that the "Enable Browser" option is active? Sucky, but
better than not being able to use captive portals..

@_date: 2014-10-23 16:29:21
@_author: Mike Perry 
@_subject: [tor-talk] 
=?utf-8?q?=28Alex_Biryukov_/_Ivan_Pustogarov__story=29?=
?yvind Saether:
I skimmed this paper this morning, and the crux of the attack is the
interplay of the Bitcoin DoS protection mechanisms and the limited
supply of Tor Exit IPs.
Basically, you cause most Bitcoin peers to end up deciding to ban all
Tor Exit IPs except your exits, and then you are able to observe all
Tor+Bitcoin users, and maybe even feed them divergent versions of the
blockchain (assuming you can muster enough proof of work to hit the
difficulty), or easier still: hide certain unconfirmed transactions. The
amount of capacity you have basically governs how quickly you can expect
clients to converge on your exit (after failing with all the other
The paper also points out that some Bitcoin clients were hoping to use
Tor to obtain multiple network perspectives on unconfirmed transactions,
to provide additional confidence that you can accept an unconfirmed
transaction before it hits the blockchain. Obviously, if you are able to
control exits used for this, you can fool such clients into accepting
double-spends. Personally, I think Bitcoin clients are still much better off
double-checking transactions via Tor than trusting only the local wifi
network, especially for accepting quick, unconfirmed transactions. But
it is useful to know that a naive "dude, just shove it through Tor,
man!" solution to this problem is not the best one.
The countermeasures section at the end is pretty good, though. In
addition to either tweaking or disabling the IP-based rate limiting for
Tor nodes, they also recommend encrypting bitcoin peer protocol traffic
(hard, but should probably be done for lots of reasons), or making use
of Bitcoin peers who also have Tor hidden service addresses available
(easy, and the paper provides a list of these that were found to exist
already in the wild).
One can also imagine that such bitcoin clients could also use a Tor
control port library to enforce that they actually are able to use a
certain number of independent exit families without failure, too. This
was not suggested, but it is possible.
It struck me as a notable work with respect to Tor because it is yet
another (surprising) area where having some kind of anonymous credential
system for proof of sacrifice/scarcity could benefit not only Tor users,
but also the rest of the Internet as well. It is also interesting
because right now, the naive proposal people often make for such systems
is "dude, just use Bitcoin, man!", but clearly we now have a catch-22
here (in addition to the privacy issues with Bitcoin).

@_date: 2014-10-29 17:52:51
@_author: Mike Perry 
@_subject: [tor-talk] 
=?utf-8?q?=28Alex_Biryukov_/_Ivan_Pustogarov_story=29?=
Thomas White:
You may be interested in:
It lists not only the official bitcoin ports, but also the Electrum soft
wallet HTTPS and TLS ports, and several other common Internet services.
It's been a TODO of mine to make a torrc config option
"ReducedExitPolicy 1" (or maybe "ExitPolicy reduced") that captures that
list in the source code. This should not be a difficult patch, and would
be a good intro for anyone looking to write their first Tor patch.

@_date: 2014-10-29 21:06:39
@_author: Mike Perry 
@_subject: [tor-talk] 
=?utf-8?q?=28Alex_Biryukov_/_Ivan_Pustogarov_story=29?=
Mike Perry:
Btw, I filed  for

@_date: 2014-09-02 21:21:44
@_author: Mike Perry 
@_subject: [tor-talk] Dropping Tor Browser support for Mac OSX 10.6? 
Mac OS 10.6 has been end-of-life since either September 2013 (last
security update) or February 2014, depending on how you count:
Our policy with dropping support in the past has been to discontinue
support for a platform when the vendor stops providing security
Moreover dropping 10.6 support would mean that we could ship 64bit-only
binaries for Mac, which would save us build time and the trouble of
assembling double-sized "fat" 32+64bit binaries, just for these already
insecure users. Moreover, these 64bit binaries could be hardened with
10.7+ hardening options.
Unfortunately, roughly 19% of Mac users were still on OSX 10.6 as of
March 2014, which is quite a lot of people. Technically these people
should be able to use Tails on their Macs, since Tails has officially
supported UEFI and Mac hardware since Tails v1.1. These ancient Mac
users would be much better off in Tails, too.
Are there any 10.6 32bit Mac users on this list? Is Tails a workable
option for you?

@_date: 2014-09-03 16:00:17
@_author: Mike Perry 
@_subject: [tor-talk] TBB 3.6.5 not on download site? 
Unfortunately this sort of confusion is likely to happen with some
probability due to the narrow window between our release day and the TWN
posting day.
Tuesday (in PDT time) is the official Firefox release day, and we aim
for that as well for Tor Browser. Wednesday is the TWN release day
(usually CEST time), which means that there are only a few hours from
when we're supposed to have Tor Browser released and when TWN goes live.
Unfortunately, any number of minor snags can cause us to get delayed by
these few hours and miss our target window enough to cause problems for
TWN. In this case, we were waiting for a disk space issue to be resolved
on the website mirrors, and for Erinn to sign the bundles.
The blog post is now up, though.
In the future, if you'd like to avoid this possibility, probably the
safest thing to do is simply report on the Tor Browser release the week

@_date: 2014-09-15 14:16:06
@_author: Mike Perry 
@_subject: [tor-talk] more sites requiring captchas from Cloudfare (using 
?yvind Saether:
I too find this situation unacceptable, since it seems to have been
unilaterally decided by CloudFlare and not by their customers who are
paying them. It has also proven to be buggy: I've gotten infinite
captcha loops, no captchas, and broken no-JS support (even though
ReCaptcha does support no-JS operation). I've also experienced repeated
captchas even if I'm logged into a given site, and the captcha prompting
has also caused me to lose web application state, form submissions, and
authentication status on more than one occasion.
I think the next step here is to try to gather a list of cloudflare
customers we suspect to be Tor friendly, and have them politely request
that their Tor users not be discriminated in this way, and failing that,
publicly leave Cloudflare for a competing ISP. I think pushback
from actual CloudFlare customers will carry far more weight here than
pushback from the Tor Project or the EFF. It also makes zero sense for
CloudFlare to serve Tor users captchas at all if their customers are the
ones paying the hosting bills and are happy to serve Tor users. For my part, I've noticed that nearly all of the Bitcoin web
infrastructure is hosted on Cloudflare. Surely some of those people
might be willing to speak up for us. Has anyone else noticed Cloudflare captchas on sites that they would
otherwise expect to be run by Tor-friendly entities?

@_date: 2014-09-17 18:06:14
@_author: Mike Perry 
@_subject: [tor-talk] more sites requiring captchas from Cloudfare (using 
Thank you grarpamp, this is *exaclty* the type of case we need -- where
a CloudFlare customer that should probably be Tor friendly was impacted,
and best of all, lost money as a result. I could not ask for a better
Except, I will ask for more of them :).
Was anyone else blocked from any other sites that should probably care
about Tor users not being able to reach them reliably? I am writing all
of these down. Here's my list so far:
All of bitcoin's web infrastructure

@_date: 2015-04-22 13:49:42
@_author: Mike Perry 
@_subject: [tor-talk] Quantum Insert detection for everyone 
I'm being a jerk and cross-posting to tor-relays, because I want to make
sure that relay operators are aware of the differences in the Snort vs
HoneyBadger approach.
Chris Dagdigian:
I too look forward to David's writeup!
For what it's worth, I think HoneyBadger is likely to be safer for
exits, more comprehensive, more accurate, less noisy, and more high
performance than a Snort-based solution.
HoneyBadger is focused only on this particular attack and is written in
golang, whereas Snort has tons of rules for everything and is written in
C. This means that HoneyBadger will have a much smaller vulnerability
surface and should be much harder to directly exploit than Snort. Since
we're talking about detecting and capturing attacks from well funded
state/world-class adversaries here (wow, what a world), vulnerability
surface minimization and general memory safety are top priority.
Snort is also vulnerable to tailored attacks designed to flood its logs
and/or avoid detection. Snort is particularly susceptible to missing
stateful attacks designed to subvert its stateless rule-based approach to
detection. Several types of TCP injection attacks that rely on TCP
reassembly will likely fall into this category (type 4 in:
HoneyBadger also appears to have better logging options than the Snort
rules. David has been in contact with malware researchers who were quite
insistent that to properly analyze 0day, a single evilpacket is very
likely to be insufficient -- context is essential, especially if the
attacker wants to obfuscate the attack or otherwise avoid exploit
Hence the need to provide optional full-take and rolling logging options
that make it easier to extract the full TCP stream of a tampered
connection, as well as related concurrent traffic (such as a stream from
a related HTTP redirect to an ephemeral URL). I've been talking with
David about ways to place these logs on a ramdisk or an ephemerally
encrypted partition, so that when detailed logs are needed, they can be
handled as safely as possible.

@_date: 2015-04-28 18:56:41
@_author: Mike Perry 
@_subject: [tor-talk] [TBB] Tor Project's amendments to Firefox 
Janis Haldemeyer:
Browser preference changes:
Addon preference changes:
We've made many more signficant changes than just about:config settings,
though. We have over 60 patches to Firefox in Tor Browser:
If you haven't seen the design doc, it explains what we did and why:
I am currently in the process of updating that for 4.5-stable this week.

@_date: 2015-08-10 19:07:36
@_author: Mike Perry 
@_subject: [tor-talk] pdf with tor 
tor-admin at torland.me:
FYI: The PDF.js exploit in the wild does not affect TBB 4.5 users. It
exploited a specific property of Firefox 38 that did not apply to
Firefox 31[1]. Unfortunately, this does mean our 5.0a3/5.0a4 alpha users
are vulnerable, since they are based on Firefox 38. The "High" Security
Slider setting will block the exploit even for those users, since
Javascript is required for it to function. We don't recommend disabling pdf.js long-term via pref, since every
other PDF reader in existence can deanonymize you by loading embedded
remote resources outside of your Tor proxy settings (in addition to
likely being vulnerable to far more serious security issues).
5.0 and 5.5a1 will be out on Tuesday, August 11th (ie: in about 12 hours
or so). 4.5 users will be upgraded to 5.0 (based on Firefox 38-esr, but
with the fix included). 5.0a3 and 5.0a4 users will be upgraded to 5.5a1
(also based on Firefox 38-esr, but with the fix included).
1.  is the
statement from Mozilla for FF31 not being vulnerable. They have made a
similar statement on the ESR mailinglist (but that does not have open

@_date: 2015-08-13 14:22:44
@_author: Mike Perry 
@_subject: [tor-talk] Can't download my Facebook archive via Tor Browser 
Alec Muffett:
Alec, please also let me know if this ends up being due to either some
feature we've patched/disabled in Tor Browser, or due to a recent
regression in Tor Browser 5.

@_date: 2015-08-28 16:05:02
@_author: Mike Perry 
@_subject: [tor-talk] 1PassWord Firefox extension 
Graham & Heather Harrison:
Kudos to the 1Password team. This is a surprisingly informative and
thoughtful response they gave you. I will clarify a couple points below.
There are actually some things they can do, though. Feel free to forward
my comments to the 1Password people.
Also, note that I have not tested 1Password or reviewed its code. Only
the nature of their response led me to believe they were capable of
taking Tor Browser's threat model into consideration, and were unlikely
to deliberately violate it. This might not rule out accidental
violations, though.
The specific cost is that you then allow websites to also attempt to
connect to these ports on your computer, which can be used to
fingerprint you. See for example
 and the specific item "Open
TCP Port and Local Network Fingerprinting" in the fingerprinting section
of the Tor Browser Design documentation:
(sorry, you will have to search for that "Open TCP Port and Local
Network Fingerprinting" text, we don't have a direct anchor to it).
I have observed banks attempting to fingerprint users upon login with
various forms of this local port scanning technique, which prompted us
to remove 127.0.0.1 from the destinations allowed to bypass the proxy.
Actually, since 1Password is an addon, they can technically exempt their
specific requests from the proxy settings and allow only themselves to
connect directly to their specific port on 127.0.0.1, without allowing
websites to do the same. One way to do this is to create a low-level
XPCOM socket instead of a websocket. Another way is to use
nsIProtocolProxyService to disable proxy settings for this specific
request. Both of these approaches are possible from addon Javascript.
The XPCOM socket will bypass proxy settings straight-up, but it is
somewhat complicated to construct and very hairy to use. It may also
be removed in the transition away from XPCOM. I think it may be best
avoided in favor of nsIProtocolProxyService.
However, one downside of nsIProtocolProxyService is that it is hard to
tell the origin of the request, to ensure that it came from 1Password
and not some website trying to fingerprinting you to determine if you
have 1Password installed.
In the past, in HTTPS-Everywhere, we have used the hack of creating
special anchor-tag nonces appended to requests when we needed to do this
(ex: for the SSL observatory, to ensure those requests go over Tor if it
is present, but not configured).  Recently, however, we just landed a
new API that gives you access to the nsIChannel of the request, allowing
you to inspect it more thoroughly to ensure that it is yours.
Here's code examples of this, in case the 1Password team is curious.
The old way:
The new way:
Unfortunately, Mozilla has not yet updated the nsIProtocolProxyService
documentation (at
to describe the new channel-based filter.
Here's where we landed the patch for the new API, though:

@_date: 2015-08-28 19:01:51
@_author: Mike Perry 
@_subject: [tor-talk] Privacy Badger 
sg.info at email-postfach.info:
The filters in use by Privacy Badger are fingerprintable - it is
possible for sites to determine that you have it installed.
In general, Tor Browser is opposed to adblockers, censorship lists, and
related filters, since they are trivial for a dedicated adversary to
bypass, and also distract from our mission of protecting from
fingerprinting and tracking threats through altering the browser to
provide real privacy by design. See also point 5 under:
At the end of the day though, it is up to the user to decide if they
want to incur the fingerprinting hit of installing such filters. This
unfortunately has its own problems, since there are so many of these
types of filtering addons (and even different blacklist subscription
feeds for those addons) that they probably ultimately end up fragmenting
the userbase quite a bit in total.
Still, I also don't think that there's any reason to believe that even
if we shipped Tor Browser with "the one true block everything adblocker"
that userbase fragmentation wouldn't happen anyway. Many people would
still install one or more additional filters for various reasons, if
nothing else than because of personal preferences. Others may end up
disabling filters (or subsets of them) because they break random stuff
(if they could even figure that out).
The only way to win the blacklist game is not to play it, I'm afraid.

@_date: 2015-08-28 20:05:17
@_author: Mike Perry 
@_subject: [tor-talk] Privacy Badger 
Garrett Robinson:
Yikes! I didn't know this. This is especially bad, especially if Privacy
Badger has custom storage mechanisms for this that aren't cleared
regularly (which you touch on below). It may also result in browsing
history leaking to disk, which wouldn't normally happen in the default
Tor Browser.
I'm now actually curious if these types of filter addons aren't already
being exploited for these and related weaknesses/shortcomings.
If any academics are interested in a good publication (Gunes Acar - are
you listening? :), it would be *very* interesting to run a crawl to see
if any websites have begun to behave adversarially against addons like
Adblock, Privacy Badger, Ghostery, etc. After all, it is easy for sites
to determine if an adblocker has blocked an ad load, and then proceed to
load an ad from an alternate URL, domain name, or even another ad
network that may not be covered by the default filters.
This may not be likely for less popular addons such as Privacy Badger
yet, but for extremely popular adblockers such as AdBlock, I bet this
behavior is already happening somewhere in the wild right now.
Because of the current privilege level and capabilities of extensions
(basically the same as the browser C++ code), we cannot ensure that
extensions are well behaved in this way.
However, when the user clicks "New Identity" in the Torbutton menu, we
do emit "browser:purge-session-history", "last-pb-context-exited", and
clear all cookies (which emits another event that triggers various bits
of the browser to clear state, according to spec). We also manually
clear a bunch of related state in the browser:
Firefox extensions are encouraged to obey these events to clear their
stored state, but this is only by convention[1]. Many do not do this.
In fact, a while ago we found issues with NoScript where it was storing
state in prefs that got written to disk, and were not reset by these
As for session restarts, that is enforced only by preventing the browser
from storing disk records on a per-mechanism basis:
Again, because of their full privilege level and capabilities, addons
can store their own state however they wish. Custom/ad-hoc storage
mechanisms (such as using special preference names as value storage, or
writing raw data to files on disk) will not be covered by our changes.
I'm now curious about how Privacy Badger stores its state...
1.

@_date: 2015-08-29 01:28:54
@_author: Mike Perry 
@_subject: [tor-talk] Privacy Badger 
????? ???????:
I "eat my own dog food" as the saying goes. I almost exclusively use Tor
Browser. I do not use any additional addons other than the default
(which includes NoScript). I do not use an adblocker.
I tend to use the Medium-High Security Slider level most of the time
(which among other things blocks Javascript for all non-https pages) so
I occasionally need to tell NoScript to allow scripts on http sites.
Thankfully, more and more sites appear to be either moving to https, or
ensuring that they work without Javascript. I use the default Tor
Browser NoScript settings.
There was a time when I used to do some things over non-Tor (like
watching Hulu), but since the loss of a reliable and regularly updated
flash player on Linux, I quit doing that. Since I managed to break that
habit, I'm unlikely to start doing it again, even if the DRM EME shit
ends up being supported by Hulu/Netflix/whatever.
I also don't think the current EME implementations are specified well
enough to be sure that the closed-source components are properly
sandboxed against insecurities and/or malicious operation. Mozilla's
implementation of EME came close, but until the sandbox itself can be
built reproducibly, it is really hard to say what is in the binaries
that Mozilla is giving us (especially when a new one arrives every
couple weeks). So for now at least, there appear to be only two choices:
live free, or die! ;)

@_date: 2015-02-09 14:28:48
@_author: Mike Perry 
@_subject: [tor-talk] Please help evaluate WebRTC for Tor Browser safety 
There seems to be a lot of interest in WebRTC Tor safety lately on this
list. The simple  PoC does not work
against Tor Browser for two reasons:
1. We don't compile in WebRTC at all.
2. We set the pref 'media.peerconnection.enabled' to false.
We would like to change property  so that it is easier to support
QRCode-encoded bridge entry and bridge sharing in Tor Launcher
( In my testing,
and according to Mozilla security engineers, it should be safe for us to
compile WebRTC in and set media.peerconnection.enabled to false, but
there may be other vectors to this code that we've all missed to date.
Hence, this is a request to interested parties to try harder to bypass
Tor in a stock Firefox using WebRTC and associated protocols (RTSP,
SCTP) with media.peerconnection.enabled set to false. Again, the
existing PoC fails in this case for me, but we need more in-depth tests.
For more info, see:
 and

@_date: 2015-02-19 14:34:28
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle with Chromium 
Seth David Schoen:
You might also like:
In particular, this paragraph is relevant to the recent Superfish MITM
(see "The worst offender on this front is the use of the Microsoft Windows
CryptoAPI for certificate validation, without any alternative. This bug
means that certificate revocation checking and intermediate certificate
retrieval happen outside of the browser's proxy settings, and is subject
to alteration by the OEM and/or the enterprise administrator. Worse,
beyond the Tor proxy issues, the use of this OS certificate validation
API means that the OEM and enterprise also have a simple entry point for
installing their own root certificates to enable transparent HTTPS
man-in-the-middle, with full browser validation and no user consent or
In fact, I tried to argue with Ryan Sleevi and Adam Langley about the
dangers of using CryptoAPI in this way, but I got crickets in response.
I believe that supporting such MITMs is a deliberate policy from Google
corporate that they cannot change. Adam went so far as to tell me that I
should just fork Chromium, because they would not even consider merging
an alternate browser-only cert store, even as a user option.
However, since our ultimate goal with any browser fork is to re-merge
with upstream so we don't have to maintain invasive patches like this, a
corporate-level blocker on basic security patches is a non-starter for
any project involving Chrome.
P.S. How I miss the days when the outlandish doomsday scenarios that I
imagined were still merely hypothetical. It seems every week a new
nightmare comes true. (Man, I sure hope I'm wrong about the likelihood
of wide-scale software build system attacks. I kind of like having

@_date: 2015-02-28 20:57:11
@_author: Mike Perry 
@_subject: [tor-talk] TBB update experience 
Fixed for the 4.5 series already in 4.5a4:
Andreas Krey:

@_date: 2015-03-10 15:17:53
@_author: Mike Perry 
@_subject: [tor-talk] Tor Browser Bundle with Chromium 
Mike Perry:
"The security researchers also claimed they had created a modified
version of Apple?s proprietary software development tool, Xcode, which
could sneak surveillance backdoors into any apps or programs created
using the tool. Xcode, which is distributed by Apple to hundreds of
thousands of developers, is used to create apps that are sold through
Apple?s App Store.
The modified version of Xcode, the researchers claimed, could enable
spies to steal passwords and grab messages on infected devices.
Researchers also claimed the modified Xcode could ?force all iOS
applications to send embedded data to a listening post.? It remains
unclear how intelligence agencies would get developers to use the
poisoned version of Xcode."

@_date: 2015-09-14 20:27:34
@_author: Mike Perry 
@_subject: [tor-talk] tor + twitter issues 
Colin Mahns:
If you have Bitcoin, you can buy some SMS numbers for getting out of
this lockout without giving up your real phone number:
That service also has free numbers available on their front page:
 but those are likely to already be
flagged/overused. Also, you might risk getting your account
extra-special-locked by trying to use a free one that Twitter knows
Yeah, maybe. However, once a company starts down this path, it tends to
become entrenched beyond all rational thought. I am not optimistic that
Twitter will be any different.
Welcome to the Nymwars 2.0!

@_date: 2015-09-19 16:43:58
@_author: Mike Perry 
@_subject: [tor-talk] How can I speed up my Tor connection? 
Restarting TBB is equivalent to the Tor onion menu option "New
Identity". On a per-site basis, it is also equivalent to "New Tor
Circuit for this site".
The main performance pain point for Tor these days is that some Guard
nodes (the entry into the Tor network) are overloaded. Unfortunately, because we both reduced the number of guard nodes from 3
to 1 and also simultaneously increased the Guard lifetime from 1 to 3
months, this problem has gotten much much worse. This means that some
people who install TBB chose a slow guard node, and are stuck with it
for months.
The only way to fix it is to remove your tor state file (in
./Browser/TorBrowser/Data/Tor/state on Linux), or reinstall a fresh TBB.
If you pick a fast guard, the experience overall will be much better.
You can also try to specify a Tor bridge from
 even if you are not a censored user. In
many cases, these bridges will be faster than our overloaded guard
nodes, since almost no one is able to successfully configure and use
bridges these days (unfortunately, censorship circumvention maintenance
and usability is not a priority for our current sponsors, but this does
have some benefits for those precious few who can figure it out).

@_date: 2016-04-20 14:47:37
@_author: Mike Perry 
@_subject: [tor-talk] Traffic shaping attack 
Oskar Wendel:
I think you may be right, or at least prudent. Perhaps for the purposes
of motivating us towards rigorous analysis, let's assume the worst case
here, and see what more we can uncover.
Ok. So then if this was an attack, then we are actually looking at a
combination of multiple attacks. First, since this attack was highly
targeted, this means that before the traffic shaping happened, some
other attack allowed the general location of the hidden service or its
Guard node to be discovered, and then that uplink was targeted for
traffic shaping.
If the traffic volume estimates are correct, this first attack could
have been the enormous traffic volume that an intelligence agency
observed and illegally reported for purposes of parallel construction
(aka: lying to courts, juries, judges, and the public). However, it
could also have been Guard discovery (See
Things like OnionBalance (
custom padding hacks, and custom Tor path selection implementations
*may* mitigate *some* forms of this first stage, while we work towards
general solutions like Proposals 247 and 254. Sloppy deployment of these
things could also just make sites more vulnerable, however.
Based on what you have reported, the second stage traffic shaping could
be due to some kind of in-line traffic shaping device, though it could
also still be due to targeted denial of service attacks against specific
nodes in the network.
Remaining open questions for the secondary traffic shaping attack
1. Did the operator try running test hidden services or test torified
downloads from the same locations using the same Guard nodes, but
different Tor clients/machines?
2. How about from the same exact Tor client, but for non-HS activity
(to make use of the same TLS connections to Guards, and test if the
Guard itself is using extra information to do fine-grained targeting)?
3. How about the same Guard nodes from different locations? OnionBalance?
4. Was the hidden service itself subject to heavy internal (ie CPU) or
external network load at the time of shaping events?
Thank you for not disclosing the name of the hidden service, despite
requests otherwise (note: I have not viewed any of the links in this
It is always best practice to avoid disclosing specific identifying
information, jurisdiction, or activities about any site that is under
attack (or having issues in general). That information doesn't matter to
me, and knowing that information can create risk for the Tor Project
and/or other parties on this mailinglist, depending on the jurisdiction
I only care about the incident on a technical basis as a case study in
security/stability of the Tor network, and am willing to diagnose the
issue to that end, so long as the conversation remains useful and
Well, no. One could still shape traffic by DoSing the Guard nodes or ISP
uplinks in question in various ways (CPU or otherwise). Depending on the
severity and targeted nature of the DoS, its effect may still be
detectable at other observation points, regardless of the scheduler in
use on any of these systems.
However, like Roger said, these attacks will still have false positives,
and will need to be highly targeted with lots of extra information to
filter out exactly when/where to observe, and even then, likely run over
a long period of time.

@_date: 2016-08-22 17:19:57
@_author: Mike Perry 
@_subject: [tor-talk] sadly have to shut down my tor relay after less then 
Sarah Alawami:
Was this a DMCA takedown related to bittorrent traffic? When I ran an
exit, I had a lot of luck with this policy:
Basically restricting yourself to core internet services reduces the
chances that bittorrent clients choose a port from the policy. With a
1Gbit exit and that policy, I went from 60 DMCA notices a day down to 0
over the life of the exit (about 3 years).
Unless something new is happening? Did the complaint(s) give specifics
about the location and type of infringing content that was accessed?
More services are always better. I've been thinking about making that
policy into a torrc option, so it would be useful to know if the
situation has changed.

@_date: 2016-03-20 15:14:27
@_author: Mike Perry 
@_subject: [tor-talk] Traffic shaping attack 
Oskar Wendel:
I'm still with Roger on being careful about assuming its an attack (and
not a bug, or other emergent behavior) before conducting more tests. At
least, that is what proper engineering and science demands before we can
respond, anyway.
For example, I wonder if users see such interrupts on all of their Tor
traffic at that time, or just hidden service traffic? Or just hidden
service traffic to specific services?
I am wondering the same thing about the hidden service side. Is it
seeing interrupts of all traffic, or just some?
If this is an attack, this information could help inform us as to if
we're looking at an attack targeting all users, certain guard nodes, or
just specific hidden services. With this information, we will also be
able to better consider defenses, if it is an attack.
Even if it is not an attack, it would still be useful to know, because
we may be looking at some other kind of bug or bad emergent property in
I think Roger is talking about our windowing mechanism (Sections 7.3 and
7.4 of tor-spec.txt:
 He
could also be talking about interactions with TCP and the multiplexing
of many users' traffic over a single connection, as grarpamp alluded to.
It could also be due to the fact that Tor is effectively
single-threaded. If something on the user's guard node, intermediate
node, or hidden service is taking large amounts of CPU time, this will
prevent traffic from flowing while that operation is happening. See:
 (though that
ticket could use some help with clarity).
That single-threaded issue may be exploited by an attack, or it could
just be happening naturally. Again, more analysis needed :/.

@_date: 2016-03-22 17:08:35
@_author: Mike Perry 
@_subject: [tor-talk] Extend auto-IP-switching-time in TorBrowser (and 
Ben Stover:
We had a long discussion about this in
 Ultimately, a fix
was merged to Tor, but it did not cause Tor to update its circuit
discard timeout (the "dirtyness" timeout) upon stream detach.
I have also noticed worse behavior since Tor Browser switched from the patch I
wrote in
to the version in Tor today.
I also agree we should be more aggressive about keeping circuits in use.
I think we should go back to updating this timeout when streams are
closed, otherwise we risk the situation where HTTP KeepAlive keeps an
idle stream open for several minutes, and then when that stream closes,
it is more likely that a new stream will go on a separate circuit
because the timeout expired while the stream was open but idle.
This situation will only get worse if/when we enable HTTP 2. I have noticed websites switching language on me mid-use as a result.
This is extremely bad for usability, and makes no sense for privacy on
the web, since web sessions contain plenty of identifying information
when they resume, regardless of what circuit they use. In my
not-so-humble opinion here, the Tor network usage should match Tor
Browser's identifier protections exactly. Any mismatch is just bad
usability for no privacy gain...

@_date: 2020-01-08 19:38:39
@_author: Mike Perry 
@_subject: [tor-talk] Strange Vanguards behavior? And some related 
Without more details this is hard to say. Was this a first-start of the
Tor client, or was it offline for a long time?
These connections might be directory mirror fetches unrelated to
vanguards. If Tor's consensus is stale or non-existent, it will
bootstrap from these mirrors instead of dirauths.
After this phase, a steady-state vanguards Tor client should use only
two Tor network connections. If this is not the case, please file a
ticket on github at This is unclear. You can see some details at an attempt at this here:
I think it won't be as helpful as other rate limiting solutions that
have already been merged to Tor:
But that fix may not drastically improve things yet either. More
complete HS DoS fixes are still in the works, and require significant
Tor protocol upgrades.
This will be a long project. The vanguards addon has many
sub-components, some of which still require more research and analysis
wrt false positives and reliability effects, and some may be
deprecated/altered by future changes such as conflux (multipath Tor
circuits). Overall timeline could be multiple years. This is why we put
the effort into getting the addon itself well-tested, included in
Debian, etc.
Of all the defenses, the Proposal  multi-layer guards sub-component
is closest to being ready for inclusion in Tor itself in terms of being
well-understood, but even this piece by itself is a large engineering
effort that currently has no funding to complete.
