
@_date: 2006-08-12 21:46:57
@_author: Seth David Schoen 
@_subject: Can governments block tor? 
Different people have different threat models.  Tor fits well into the
threat model of people in places where privacy-enhancing technologies
are legal to use.  It currently isn't as much help to people who could
get in trouble just for using these technologies.
It doesn't seem that anyone has solved the problem of how to make a
publicly-deployable privacy technology whose users can't be identified
as privacy-enhancing technology users by an eavesdropping adversary*.
So we might say that there are potentially a lot of people whose threat
models haven't been addressed well by the state of the art.
* Whether this is true depends on what counts.  If people are allowed to
  use SSL or VPNs, are allowed to send what could be cover traffic over
  them, are allowed to use them for a long time starting fairly abruptly,
  and have access to means of publicity about proxy locations and access
  methods that the adversary might not hear about, they might have a
  chance.  I think that's a lot of conditions.

@_date: 2006-12-01 14:55:52
@_author: Seth David Schoen 
@_subject: How can I trust all my Tor nodes in path 
Whoops, thanks for the clarification!  That makes more sense.

@_date: 2006-12-01 13:15:39
@_author: Seth David Schoen 
@_subject: How can I trust all my Tor nodes in path 
Hmmm, if someone owns (not just eavesdrops on) all three nodes, they can
connect the sessions in a more reliable way than just a timing attack.
One approach would be to record TCP port pairs, which temporarily identify
a connection on one end with a connection on the other end.  For example,
my local machine knows that I'm currently using TCP port 43514 to make a
connection to the SSH service on the server; the server also knows that
the client connecting to it is using TCP port 43514.  Thus, both ends know
that client:43514 <----> server:22 (at this particular moment) refers to
the same TCP session.
Tor nodes could log this information, and, if they did, it would not be
a speculative matter to link circuits across servers.  You would have
the existence of the TCP connections
client:a <---> tornode1:9001
tornode1:b <---> tornode2:9001
tornode2:c <---> tornode3:9001
tornode3:d <---> host:e
where a, b, c, and d are randomly chosen TCP ports and e is the TCP
port used by host for contacting a service (such as 443 for HTTPS).
If all of the Tor nodes were paying attention, then
tornode1 knows that its connections involving client:a and tornode1:b are
part of the same circuit
tornode2 knows that its connections involving tornode1:b and tornode2:c are
part of the same circuit
tornode3 knows that its connections involving tornode2:c and host:e are
part of the same circuit
Knowing all of these facts, these nodes could deduce that client:a and
host:e are actually communicating with one another.  This is not a
"timing attack" and does not rely on observing any packets actually
transmitted across the fully-established circuit.
Malicious nodes that log this kind of information could also collaborate
after the fact to correlate it, without recording large quantities of
timing information.  They just need TCP port pairs and accurate times
when TCP connections were established.
Summary: 3 malicious nodes, whether owned by the same entity or not, can
work together to identify, in a straightforward and reliable way, the
endpoints of a Tor circuit while the circuit is active or afterward,
without having to do any timing attacks.
To learn more about the relevance of TCP port numbers as connection
identifiers, see RFC 793 or try running netstat (or netstat -p, if
your implementation supports it) on the machines on both sides of a
connection.  Observe that, with the output of netstat -p on both
ends, one can see which processes on one machine are talking to which
processes on the other machine.

@_date: 2006-12-01 13:23:34
@_author: Seth David Schoen 
@_subject: How can I trust all my Tor nodes in path 
P.S. Even if it weren't possible to use TCP ports to link connections,
malicious nodes controlled by the same party could modify the Tor
protocol to add tracking features, and then all implement the same
tracking features.  For example, malicious nodes (which could all
know about each other by means of a malicious nodes table) could
implement a revised Tor protocol which adds a connection origin packet
(showing the originating IP address) during Tor connection setup.
Since the nodes are malicious, they will speak the same modified
protocol amongst themselves but not reveal this fact to the end user.
Some people have suggested that this is a good application for
trusted computing; proxies could prove that they're running the
real, official proxy software on top of real hardware.  Then timing
attacks are still possible, but actually logging data directly could
be prevented.  The problem with this seems to be that intentionally
doing timing attacks directly against a proxy you operate, from within
the same network, is probably pretty effective!  This approach might
be more relevant to lower-latency anonymity services such as e-mail

@_date: 2006-06-12 18:10:03
@_author: Seth David Schoen 
@_subject: Tor,security and web-usability 
Yes, or you could run a standardized live CD (something which there are
efforts to produce).  The live CD would tend to conceal your native OS
and browser version because all live CD users would have the same OS
and browser.
However, the privacy risk to your real IP address still exists with a
live CD.  Emulation might do better there, because the emulator could
provide an emulated private IP address and conceivably hide everything
unique about your computer from the programs running in the emulator.
Emulation and sandboxing for privacy are a good project; they potentially
need to work in two directions:
(1) Confining the browser and applets to prevent them from discovering
    local unique or private information (like non-anonymized cookies,
    files on disk, host OS version, processor serial number, MAC address,
    IP address, etc., etc.).  [If they could learn this information, they
    might communicate it in-band over an anonymized Tor circuit.]
(2) Confining the browser and applets to prevent them from communicating
    otherwise than through Tor (to prevent them from directly generating
    any network packets).  [These packets could be observed and correlated
    with the anonymized browsing activity, and they would reveal, at least,
    the user's true, non-anonymized IP address.]
That word is "Sisyphean".  In gdict:
 From The Collaborative International Dictionary of English v.0.48 [gcide]:
  Sisyphean \Sis`y*phe"an\, a.
     Relating to Sisyphus; incessantly recurring; as, Sisyphean
     labors.
     [1913 Webster]
 From WordNet (r) 2.0 [wn]:
  Sisyphean
       adj 1: of or relating to Sisyphus
       2: both extremely effortful and futile

@_date: 2006-06-12 18:10:03
@_author: Seth David Schoen 
@_subject: Tor,security and web-usability 
Yes, or you could run a standardized live CD (something which there are
efforts to produce).  The live CD would tend to conceal your native OS
and browser version because all live CD users would have the same OS
and browser.
However, the privacy risk to your real IP address still exists with a
live CD.  Emulation might do better there, because the emulator could
provide an emulated private IP address and conceivably hide everything
unique about your computer from the programs running in the emulator.
Emulation and sandboxing for privacy are a good project; they potentially
need to work in two directions:
(1) Confining the browser and applets to prevent them from discovering
    local unique or private information (like non-anonymized cookies,
    files on disk, host OS version, processor serial number, MAC address,
    IP address, etc., etc.).  [If they could learn this information, they
    might communicate it in-band over an anonymized Tor circuit.]
(2) Confining the browser and applets to prevent them from communicating
    otherwise than through Tor (to prevent them from directly generating
    any network packets).  [These packets could be observed and correlated
    with the anonymized browsing activity, and they would reveal, at least,
    the user's true, non-anonymized IP address.]
That word is "Sisyphean".  In gdict:
 From The Collaborative International Dictionary of English v.0.48 [gcide]:
  Sisyphean \Sis`y*phe"an\, a.
     Relating to Sisyphus; incessantly recurring; as, Sisyphean
     labors.
     [1913 Webster]
 From WordNet (r) 2.0 [wn]:
  Sisyphean
       adj 1: of or relating to Sisyphus
       2: both extremely effortful and futile

@_date: 2006-05-19 11:51:54
@_author: Seth David Schoen 
@_subject: Threats to anonymity set at and above the application layer; HTTP headers 
It's pretty well understood that anonymity can be lost at higher protocol
layers even when it's well protected at lower layers.
One eye-opening paper on this point is "Can Pseudonymity Really Guarantee
Privacy?" by Rao and Rohatgi (in the Freehaven Anonymity Bibliography):
This is a philosophically interesting problem; it prompts the question "if
pseudonymity can't guarantee privacy, what _can_?".  (Rao and Rohatgi
remind us that the authors of the Federalist Papers used pseudonyms and
were still identified solely from the evidence of their writing.)
There is also the scary field of timing attacks on users' typing:
(The Tygar paper is not really relevant for network surveillance, but
it shows the scariness of statistical methods for figuring out what
users are doing based on seemingly irrelevant information.)
In a sense, there are many privacy-threatening features in and above the
application layer (some of them depending on the nature and latency of a
* timing of access (what time zone are you in, when do you usually do
something?) -- for communications with non-randomized latency < 1 day
* typing patterns (cf. Cliff Stoll's _Cuckoo's Egg_ and the Song et al. paper)
* typing speed
* language comprehension and selection
* language proficiency
* idiosyncratic language use
* idiosyncratic language errors (cf. Rao and Rohatgi)
* cookies and their equivalents (cf. Martin Pool's "meantime", a cookie
equivalent using client-side information that was intended for a
totally different purpose -- cache control)
* unique browser or other application headers or behavior (distinguishing
MSIE from Firefox from Opera? not just based on User-agent but based on
request patterns, e.g. for inline images, and different interpretations
of HTTP standards and perhaps CSS and JavaScript standards)
* different user-agent versions (including leaked information about the
* different privoxy versions and configurations
I'm not sure what to do to mitigate these things.  The Rao paper alone
strongly suggests that providing privacy up to the application layer
will not always make communications unlinkable (and then there is the
problem of insulating what pseudonymous personae are supposed to know
about or not know about, and the likelihood of correlations between
things they mention).
These problems are alluded to in on the Tor web site:
   Tor can't solve all anonymity problems. It focuses only on protecting
   the transport of data. You need to use protocol-specific support
   software if you don't want the sites you visit to see your identifying
   information. For example, you can use web proxies such as Privoxy
   while web browsing to block cookies and withhold information about
   your browser type.
   Also, to protect your anonymity, be smart. Don't provide your name
   or other revealing information in web forms. Be aware that, like
   all anonymizing networks that are fast enough for web browsing, Tor
   does not provide protection against end-to-end timing attacks: If
   your attacker can watch the traffic coming out of your computer,
   and also the traffic arriving at your chosen destination, he can
   use statistical analysis to discover that they are part of the same
   circuit.
However, the recommendation to use Privoxy, by itself, is far from
solving the problem of correlations between user and user sessions.
I think a low-hanging target is the uniqueness of HTTP headers sent by
particular users of HTTP and HTTPS over Tor.  Accept-Language, User-Agent,
and a few browser-specific features are likely to reveal locale and OS
and browser version -- sometimes relatively uniquely, as when someone
uses a Linux distribution that ships with a highly specific build of
Firefox -- and this combination may serve to make people linkable or
distinguishable in particular contexts.  Privoxy does _not_, depending on
its configuration, necessarily remove or rewrite all of the potentially
relevant HTTP protocol headers.  Worse, different Privoxy configurations
may actually introduce _new_ headers or behaviors that further serve to
differentiate users from one another.
One example is that some Privoxy configurations insert headers specifically
identifying the user as a Privoxy user and taunting the server operator;
but if some users do this and other users don't, the anonymity set is
chopped up into lots of little bitty anonymity sets.  For instance:
+add-header{X-User-Tracking: sucks}
User tracking does suck, but adding an optional header saying so has the
obvious effect of splitting the anonymity set in some circumstances into
people who send the X-User-Tracking: sucks header and people who don't.
Any variation in practice here is potentially bad for the size of the
anonymity set.
A remedy for this would be to try to create a standardized Privoxy
configuration and set of browser headers, and then try to convince as
many Tor users as possible to use that particular configuration.  (One
way to do this is to try to convince everyone who makes a Tor+Privoxy
distribution or product to use the agreed-upon default configuration.)
The goal is not to prevent people from controlling their own Privoxy
configurations or doing more things to protect their privacy; rather,
it is to try to reduce the variety in headers and behaviors seen by
web servers contacted by Tor users on different platforms.

@_date: 2006-05-21 19:39:52
@_author: Seth David Schoen 
@_subject: Threats to anonymity set at and above the application layer; HTTP headers 
That is the kind of idea that I have in mind.  If we assume that all
web sites can tell which connections are from Tor users (for example,
by consulting a blacklist of Tor exit node IP addresses), then Tor
users can't increase the size of the anonymity set by using different
user-agent (etc.) from other Tor users.
I suppose I'm assuming that Tor users would like to increase the size
of the anonymity set in order to get better anonymity.  It's true that
there are cases in which they might not like to do this, for example
if they simply want to hide their physical location without hiding
their identity (e.g. in cases of domestic abuse).  In that sense, there
may be more than one definition of anonymity set.
It is also true that Tor users who are afraid of being accused of being
responsible for other Tor users' activities might want to do things that
they believe would let them make the case that they were _not_ a
particular oher user.  Do we need to discuss how common these different
motives are among different Tor users?

@_date: 2006-11-20 16:16:46
@_author: Seth David Schoen 
@_subject: A different solution for anonymous browsing ! 
By itself, that doesn't completely solve the problem.  It's easy to log
information other than user agent, which might be sufficient to
distinguish browsers and other information.

@_date: 2007-04-23 08:06:03
@_author: Seth David Schoen 
@_subject: Open DNS 
"Atrocitas" is feminine, but the adjective form "ater" is masculine.
To agree with "atrocitas", assuming "ater" is meant to describe it,
you need a feminine adjective "atra" (compare "post equitem sedet
atra Cura" from Horace III, 1).

@_date: 2007-02-12 14:38:01
@_author: Seth David Schoen 
@_subject: PHP coder needs Tor details 
"SIGNALS" standardly refers to the Unix signals mechanism; see the man
page for signal(2).  The command-line program to send a signal to a process
is called kill; see the man page for kill(1):
       kill - send a signal to a process
By using the kill program on the command line, you can generate the signals
described in the SIGNALS section of the Tor man pages.
It's probably also possible to generate signals from within a PHP script
using an appropriate library function.  It looks like the function to
use would be posix_kill().
The effects of posix_kill will be equivalent to the effects of using
kill from the command line.

@_date: 2007-02-12 17:36:45
@_author: Seth David Schoen 
@_subject: PHP coder needs Tor details 
There is a Unix feature called setuid which allows a program to run with
the privileges of a user other than the user who started it.  On a Unix
system where setuid is supported (and you are allowed to use it), a
program which is mode 4xxx (in octal notation) or u+s (in text notation)
will get the privileges of the _program file's owner_ when run, rather
than the privileges of the user who started the program.  You might be
able to use this in a variety of ways in your application.  Please keep
in mind that the improper use of the setuid facility is a major source
of security holes, so you should think through what you're doing (and
the consequences of allowing other users on the machine, if any, to
run programs with those privileges!).
Here is a simple example showing the difference between a regular
executable program and a program with the setuid bit set:
schoen at sescenties:~$ cp /usr/bin/id regular_id
schoen at sescenties:~$ cp /usr/bin/id setuid_id
schoen at sescenties:~$ sudo chown root:root setuid_id
schoen at sescenties:~$ sudo chmod u+s setuid_id
schoen at sescenties:~$ ./regular_id uid=1000(schoen) gid=1000(schoen) grupos=4(adm),20(dialout),24(cdrom),25(floppy),29(audio),30(dip),44(video),46(plugdev),104(lpadmin),105(scanner),106(admin),1000(schoen)
schoen at sescenties:~$ ./setuid_id uid=1000(schoen) gid=1000(schoen) euid=0(root) grupos=4(adm),20(dialout),24(cdrom),25(floppy),29(audio),30(dip),44(video),46(plugdev),104(lpadmin),105(scanner),106(admin),1000(schoen)
(Here the word "grupos" means "groups"; I'm using pt_BR as my locale.)
euid is the "effective user ID" of the process.
One more note: some Unix versions forbid a script (such as a shell script)
to run setuid.  Some scripting languages have workarounds for this where
the scripting language itself has a setuid root version (for example,
sperl for Perl) that will handle the privileges for the scripts.  If you
try to make a script setuid, don't be surprised if this process doesn't
work on your system (for security reasons based on the concerns of the
kernel developer, the distributor, or the system administrator).
The normal paradigm of the command line is that when you start a command,
that command will keep running in your terminal until it's done, and
only then will you get another shell prompt.
You should probably look at documentation for your shell (if you don't
have good documentation, I can recommend the use of the bash shell and
the O'Reilly and Associates book _Learning the bash Shell_), in particular
the features called "job control", which are aimed at helping you do
multitasking using the command line.
The first job control feature to learn about is the ampersand character.
If you put an ampersand at the end of a command line, the job will run
"in the background" and the shell won't wait for it to exit before giving
you another prompt.

@_date: 2007-01-27 17:21:07
@_author: Seth David Schoen 
@_subject: more letters from the feds 
The current directory scheme does allow (in fact, requires) policies
to be specified in terms of IP addresses and TCP port numbers.  So
a "web browsing only" exit node is possible.  A "Google only" exit
node is possible if you knew the IP address of every Google server,
which is a fairly tricky proposition.
A "GET-only" exit node can't be specified with the current directory
system, which isn't capable of expressing any information about what
an node will do with connections to a particular TCP port other than
allow or deny them.  You could make an "HTTP GET only" exit node, but
you wouldn't have a way to tell clients that your node enforced that
policy, and users would probably get mad (and stop using your exit
node entirely) when some of their transactions failed mysteriously.
The fine-grainedness of exit policy languages is a difficult strategic
question akin to the problem of the fine-grainedness of DRM policy
languages.  It's possible that making an exit policy language more
specific would lead some existing exit node operators to forbid more
things -- things that they would actually like to forbid but currently
don't have a technical means of forbidding without getting effectively
kicked out of the Tor network.  On the other hand, it's possible that
making an exit policy language more specific would lead some existing
node operators to allow new things -- things that they wanted to allow
but didn't have a technical means of specifying that they wanted to
allow without also allowing other things that they didn't want to
allow.  It's also possible that some people who current don't run
exit nodes would start allowing extremely limited exit nodes that
they wouldn't have been willing to operate any other way.
The technical overhead of moving beyond ports to a more specific kind
of exit policy seems to me quite high, not because of the need to
develop a language to express it, but because of the need to find a
way of communicating it between the Tor client and client applications
(to prevent applications from making requests that exit nodes they're
using will block, or, conversely, to allow the Tor client to choose
exit nodes that will not forbid any of the things that an application
intends to do, or might possibly do).  I'm not aware of any existing
protocol that allows this information to be conveyed or any applications
that support this kind of feature right now.  To take a concrete
example, how would Firefox tell Tor "I need to be able to HTTP POST"
or how would an old version of lynx tell Tor "I only support HTTP/1.0"?
How would ssh tell Tor that it was ssh?
See section 2.1 of
for the (extremely simple) status quo.

@_date: 2007-01-28 01:15:22
@_author: Seth David Schoen 
@_subject: more letters from the feds 
Though not really, because the set of servers listening on TCP port 80
is neither a subset nor a superset of the set of web servers.

@_date: 2008-08-20 14:54:28
@_author: Seth David Schoen 
@_subject: xB Mail: Anonymous Email Client 
In case some people weren't around a year and a half ago, I invite
them to look at the thread archived at
in which objections to calling such software "open source" are
discussed.  (I don't think it's necessary to repeat that thread.)

@_date: 2008-12-05 01:22:07
@_author: Seth David Schoen 
@_subject: No data retention in germany for donated services 
I'm not a lawyer in Germany or any jurisdiction and I don't have any
knowledge or opinion of the convincingness or legal well-foundedness
of this article.  I encourage anyone who might want to rely on it to
seek the expert opinion of a German lawyer.  But I do read German, so
I've translated Karsten's note and (most of) the text of the article
below for the benefit of anyone interested in this material who doesn't
read German.
NO DATA RETENTION FOR FREE-OF-CHARGE SERVICES
  Original German text of this article "Keine Vorratsdatenspeicherung f?r
  unentgeltliche Dienste" is available at
     Copyright 2008 Patrick Breyer; licensed under Creative Commons BY-2.0
  (Germany) license.
    Translation by Seth Schoen.  This text version omits hyperlinks to the
  German text of laws, treaties, and court decisions which appear in-line
  in the original German version.
Beginning on January 1, 2009 at the latest, those offering certain
publicly-accessible telecommunications services must store their users'
traffic data (? 113a TKG). This applies to providers of land line
telephone services, mobile telephone services, Internet telephone
services, e-mail, Internet access, and anonymizing services. However,
it has thus far remained unnoticed that the obligation to store this
data applies only to compensated or commercial services. Free-of-charge
services do not have to store data. Indeed, data retention is forbidden
to them under penalty of a fine.
* Legal situation
The obligation to retain data in Germany arises from ? 113a TKG.
This rule is only applicable to "telecommunications services"
[Telekommunikationsdienste]. According to ? 3 no. 24 TKG,
"telecommunications services" are only "services normally provided
for remuneration" [in der Regel gegen Entgelt erbrachte Dienste].
For clarification of this distinguishing criterion, we find in the
explanatory statement of the corresponding bill: "This definition is
in accordance with art. 2 letter c sec. 1 RRL".
This refers to the EU Directive 2002/21/EC on on a common regulatory
framework for electronic communications networks and services.
This Directive defines as an "electronic communications service"
[quotation from official English version] only "a service normally
provided for remuneration" [quotation from official English version;
German 'gew?hnlich gegen Entgelt erbrachte Dienste']. The Commission
had originally even wanted to include only "a service provided for
remuneration" [gegen Entgelt erbrachte Dienste]. This was in accordance
with the then-effective German telecommunications law of 1996,
which was largely applicable only to "the commercial provision of
telecommunication" [das gewerbliche Angebot von Telekommunikation].
The European Parliament asked, however, for a broadening of the
framework directive to all services that are rendered "on a commercial
basis" [quotation from official English version; German 'auf
kommerzieller Basis']. For explanation it argued that "electronic
communications services may be offered on an unremunerated, yet
commercial basis" [quotation from official English version]. The
Council finally decided on the current formulation, according to which
all "services normally provided for remuneration" [gew?hnlich gegen
Entgelt erbrachte[n] Dienste] are included. The Council did not offer
an explanation for this formulation.
However, it is clear that as a compromise the definition of service
provision fom the EC Treaty was adopted in full (EC Treaty article
50). The EC Treaty defines service provision in article 50 as follows:
"Services shall be considered to be 'services' within the meaning of
this Treaty where they are normally provided for remuneration, in so
far as they are not governed by the provisions relating to freedom of
movement for goods, capital and persons. 'Services' shall in particular
include: (a) activities of an industrial character; (b) activities of
a commercial character; (c) activities of craftsmen; (d) activities
of the professions." [quotation from official English version] The
distinguishing criterion "normally provided for remuneration" [in der
Regel gegen Entgelt erbracht] was thus adopted word-for-word in the
telecommunications Framework Directive, which is also evident from the
other translations of the Directive. This imitation of the EC Treaty was
meaningful because the Framework Directive is founded upon the basis of
the European Single Market powers of the EC. The EC may not regulate
the provision of services other than those that are the subject of the
Single Market.
The definition of electronic communications services in the Framework
Directive applies to data retention as well according to article 2,
paragraph 1 of Directive 2006/24/EC. The data retention directive
applies according to its article 3 only to "electronic communications
services" [elektronische Kommunikationsdienste] in this sense. The
Directive on data retention could not include other services because
in any case it is founded on the basis of the Single Market powers (EC
Treaty article 95) and may thereby only regulate the Single Market.
The scope of ? 113a TKG is, in the end, consequently identical to
the scope of EC Treaty article 50. The German legislature did not
merely intend to take the definition of "telecommunications service"
[Telekommunikationsdienst] in ? 3 TKG from EC law (see explanation).
In this regard it also intended to implement data retention itself only
in accordance with European legal requirements. This is clear from the
title of the implementing legislation, but also from the rationale
for the law (pages 30 and 69). The Bundestag wanted to require only
those services (with the exception of anonymizing services) to retain
data which it had to require to do so according to the EC Directive on
data retention. The German Constitutional Court has ruled with regard
to European arrest warrants that European legal requirements must be
implemented as narrowly and compatibly with basic rights as possible.
In the application of implementing laws, a construction compatible
with basic rights should also be taken. A constitutionally compatible
construction of ? 113a TKG requires that data retention ? to avoid
breaching German basic rights and the rule of proportionality ? not be
expanded beyond what is necessary as a matter of European law.
* Precedent on the defining criterion "normally [provided] for
remuneration" [in der Regel gegen Entgelt]
Many decisions of the German court have been issued on the question of
when a service is "normally provided for remuneration" [in der Regel
gegen Entgelt erbracht]. In the leading decision "Humbel" from 1988,
the Court decided: "According to article 60, para. 1 of the EEC Treaty,
only 'services that are normally provided for remuneration' [quotation
from official English version; German 'Leistungen, die in der Regel
gegen Entgelt erbracht werden'] fall under the chapter on services. Even
if the term 'remuneration' [Entgelt] has not been expressly defined in
articles 59 and following of the EEC Treaty, its meaning can be inferred
from article 60, para. 2 of the EEC Treaty, according to which, in
particular, industrial, commercial, craft and professional activities
count as services. The essential feature of remuneration [Entgelt]
is thus that it shows the service provided in return for the service
concerned, in which the service provided in return is normally agreed
between the service provider and the recipient of the service."
In subsequent decisions, the court has clarified that such a service in
return can also be deemed to occur if it is paid for by a person other
than the recipient. Thus private television broadcasting is regarded as
a service provided for remuneration [gegen Entgelt erbrachte Leistung],
because it is paid for through advertising. Indeed, one can regard
private television broadcasting as a commercial service [entgeltliche
Leistung] for the advertising purchasers, which serves to draw viewers
for the commercials. The court has also considered the services of
hospitals as furnished for remuneration [gegen Entgelt erbracht], since
the hospitals are financed by health insurance companies ? although in
the form of standard flat rates.
Now it is important that the court takes particular services into
account. The criterion "normally" [in der Regel] thus does not go so
far as to make a single category dispositive of all services. Rather,
the court determined with regard to universities, for instance, that
the freedom of services had no application to public, tax-supported
universities, but that it did apply to private colleges. It is thus
critical whether the particular provider offers his service "normally
for remuneration" [in der Regel gegen Entgelt] or not. The service must
be assigned according to EC Treaty article 2 to "economic activities"
[quotation from original English version; German 'Wirtschaftsleben'].
In the case of public schools, the court established that their public
funding still did not establish a remunerated service [entgeltliche
Leistung]. The public financing did not constitute a service directly
provided in return for a service rendered. Even a mandatory levied
tuition fee does not constitute a remunerated service [entgeltliche
Leistung], so long as the institution is funded substantially by
public means. That a service (necessarily) must be funded in one
way or another, then, still does not make it a remunerated service
[entgeltliche Leistung]. The funding must rather be able to be regarded
as provided directly in return for the service [gerade als Gegenleistung
f?r den Dienst].
* Application to data retention obligations
For telecommunication services and the obligation to retain data the
following thus apply:
Services that are financed essentially by something provided in exchange
[Gegenleistungen] by the user are in any event normally provided for
remuneration [in der Regel gegen Entgelt erbracht]. Such services must
thus retain data.
Those services that are financed essentially by accepting advertising
? such as banner ads ? and that are run for profit are also normally
provided for remuneration [in der Regel gegen Entgelt erbracht]. Thus,
for example, the commercial free e-mail services must store data, even
if their users don't have to pay a subscription fee [Entgelt].
On the other hand, services that are provided for nothing substantial
in exchange [keine wesentliche Gegenleistung], either by their users
or by their parties, are normally provided for no remuneration [in der
Regel unentgeltlich erbracht]. For instance, a private party may provide
a free e-mail service, an open wireless network providing Internet
access, or a Tor server for no compensation [unentgeltlich] and financed
with his own means, and then no telecommunications service normally
provided for remuneration [in der Regel gegen Entgelt erbrachter
Telekommunikationsdienst] exists and the data retention obligation
according to ? 113a TKG does not apply.
Government services are also normally not provided for compensation [in
der Regel unentgeltlich erbracht]. Many local authorities, for example,
provide free Internet access or e-mail accounts. These essential
tax-financed services are exempt from data retention. This is even true
if a service charge is levied on the user, but the charge only defrays
a small part of the costs. This situation should not be considered as
different from the imposition of tuition fees or charges, concerning
which the European court has already ruled.
Correspondingly, even a private noncommercial service [unentgeltlicher
Dienst] does not always lose its noncommercial character
[unentgeltlichen Charakter] by collecting a service charge or showing
commercial advertising, as long as accepting these makes up only a
trivial share of the cost of the service. Whoever wants to be confident
in being exempt from the obligation to retain data should, however,
forego such sources of funding entirely.
Services that are offered by noncommercial providers (for example, by
individuals or organizations) without a profit motive, but that recoup
their costs essentially by payments from users or advertising customers,
will be regarded as "normally provided for remuneration" [in der Regel
gegen Entgelt erbracht]. After all, the requirement "for remuneration"
[gegen Entgelt] does not require any profit motive. Accordingly, the
European court has regarded private schools or hospitals as commercial
providers [entgeltliche Anbieter], even though they have no profit
motive. Thus even noncommercial services fall under the data retention
requirement if they are provided for remuneration [gegen Entgelt
The treatment of services whose costs are actually essentially borne by
private individual means, but which are offered as "additional" services
by commercial providers, is unclear. For example, some firms offer, in
addition to their paid services, a free public webmail service. The
question is whether the self-promotion, that is to say the publicity
for a commercial offering of the same firm, should be seen as a form
of compensation for the ostensibly free service [ein Entgelt f?r
den an sich kostenlosen Dienst]. EC Treaty article 50 particularly
includes industrial services, and a commercial firm always has a profit
movie. In this connection, the court has decided with regard to tobacco
advertising that commercial advertising falls within the scope of the
Single Market.
Uncompensated services [unentgeltliche Dienste] of a commercial firm
are thus to be viewed as "normally provided for remuneration" [in
der Regel gegen Entgelt erbracht] if they serve as advertising for
compensated products [entgeltliche Angebote] of the firm. With regard
to commercial firms, a certain appearance argues that their services
in the end promote their own profit motive. Nonetheless an individual
service of a commercial firm need not serve as advertising for the
firm's own compensated products [entgeltlichen Angebote]. For instance a
noncommercial offering [unentgeltliche Angbot] may be clearly separate
from the commercial operation [gewerblichen T?tigkeit] of its provider,
in that it for example is delivered through a separate portal without
any self-promotional materials; then a product normally provided without
remuneration [in der Regel ein unentgeltliches Angebot] will exist,
which does not fall under data retention requirements. If, on the other
hand, the uncompensated service is embedded in the commercial appearance
of the firm, normally a publicity interest and thereby a compensated
product will be presumed.
In summary, we should note that those services that are essentially
funded by private means and that also do not serve as advertising for
paid services are exempted from the obligation to retain data. For
instance, when an individual person offers an e-mail service, a public
wireless LAN with Internet access, or a Tor server without being paid
for it, and he essentially funds from his own resources and not by
receipts from its users or advertising customers, the data retention
obligation according to ? 113a TKG does not apply.
* Prohibition on data retention by non-commercial services
Uncompensated services are not only excepted from the obligation
to retain data. Their providers may also not "voluntarily" retain
data. This results from ? 96 para. 2 TKG, according to which traffic
data must be erased immediately after the end of the connection, if
they are not "necessary for the purposes established through [...]
legal requirements" [f?r die durch [...] gesetzliche Vorschriften
begr?ndeten Zwecke erforderlich]. This obligation to erase data
applies to all businesslike providers of telecommunications services
[gesch?ftsm??igen Anbieter von Telekommunikationsdiensten]. According
to ? 3 TKG, these are all providers of telecommunications, even if
their offering is uncompensated [unentgeltlich].
That "voluntary" data retention, as some Internet service providers
currently practice it, may also not be done on "security grounds"
according to ? 100 TKG has already been explained in more detail
Whoever violates the prohibition on data retention in ? 96 TKG is
acting illegally and can be punished with a fine up to ten thousand
Euro by the Federal Network Agency (? 149 para. 1 no. 17 TKG). Anyone
may file a complaint. However, someone who is required to retain data
and does not do so is also acting illegally. Each provider of telephone
service, e-mail, Internet access, or anonymizing service should thus
make sure that he acts correctly. In case of doubt, he should ask the
Federal Network Agency.
* Non-public services
Data retention applies only to services that are publicly available
(? 113a TKG). This is an independent restriction in addition to the
commercialness discussed above. The data retention obligation thus
applies only if a service is both normally provided for remuneration
and also publicly accessible. If either of these two criteria is not
met, data retention is not applicable and is forbidden.
A service is publicly available if anyone ? and not just specified
groups of users ? can use it. The availability of a service only
to members of an organization doesn't affect the public availability
of that service if anyone can become a member of the organization.
Non-publicly-accessible services, by contrast, are those offered, for
instance, by employers or universities, since these can be accessed only
by a limited group of people. These providers are neither required nor
authorized to retain data.
[Final section ("Ausweichm?glichkeiten f?r entgeltliche Dienste") omitted
from this translation; it describes a procedure for commercial services
to try to obtain a financial indemnity or exemption from the government
while the constitutional challenge to the data retention law is pending,
by writing a letter to the Federal Network Agency.  This letter would
demand that that agency temporarily exempt commercial service providers
from implementing data retention or else promise to reimburse the service
providers for their implementation costs in case the German constitutional
court rules data retention unconstitutional or in case it rules that the
government must pay implementation costs.]

@_date: 2008-02-13 09:51:30
@_author: Seth David Schoen 
@_subject: OSI 1-3 attack on Tor? in it.wikipedia 
The title of that article should probably be either "Tor (anonimit?)" or
"The Onion Router" (not "Routing").  My understanding is that the
principle of operation of Tor, as well as the name of the original
predecessor research project, is "onion routing", whereas the name "Tor"
was chosen as an acronym for "The Onion Router".

@_date: 2008-02-13 10:54:57
@_author: Seth David Schoen 
@_subject: OSI 1-3 attack on Tor? in it.wikipedia 
Your explanation makes sense, and you're certainly in a much better
position than I to know the history of the project's name.  On the
other hand, there are many references that seem to call it "The
Onion Router", including Wikipedia and the Tor wiki:
I think it makes sense to call the _software_ "the onion router" and
any individual node "an onion router" (akin to "the Apache web server"
and "an Apache web server", "the Squid proxy" and "a Squid proxy").

@_date: 2008-11-17 12:05:36
@_author: Seth David Schoen 
@_subject: Tor cleverness? 
There's some more discussion of this kind of thing at
Notably, on Unix-like systems including Linux and Mac OS X, a file can
have multiple names or no names at all and still be the "same file",
because the notion of the identity of a file is so thoroughly separated
from the notion of a filename.

@_date: 2008-11-07 12:46:36
@_author: Seth David Schoen 
@_subject: Data Retention in UK? 
The Independent article says
"Nevertheless, ministers have said they are committed to consulting on
the new Communications Data Bill early in the new year."
That means that the particular law discussed is a proposed law, not an
already-enacted law.

@_date: 2009-12-25 10:17:46
@_author: Seth David Schoen 
@_subject: Cannot Load The Onion 
Perhaps The Onion has something against the onion router.

@_date: 2009-12-26 15:13:34
@_author: Seth David Schoen 
@_subject: TOR and ISP 
The ISP would see the user visiting a number of Tor nodes.  If the user
isn't using bridges, then the ISP will know that the user is using Tor,
but not what the user is doing with Tor.  For example, the ISP won't
know what sites or services the user is using through Tor.

@_date: 2009-12-26 17:21:01
@_author: Seth David Schoen 
@_subject: TOR and ISP 
That's correct.
I've never heard of an ISP in the United States providing this service.
It sounds extremely unusual to me.
ISPs are easily _able_ to record what sites users visit, but in many places
there are legal and cultural norms against the ISPs doing so, or against
the ISPs reminding users that ISPs have this ability. :-)

@_date: 2009-06-08 12:35:52
@_author: Seth David Schoen 
@_subject: Using POSTDESCRIPTOR command 
Maybe you could have a ~/.tor/extra-consensus or ~/.tor/local-consensus
which is a parallel cache of network status from nonpublic sources but
that doesn't get overwritten by the regular public consensus.  There's
still an interesting problem of determining the validity of the
descriptors mentioned there if there's no regular source of update of
that information.

@_date: 2009-10-21 10:49:11
@_author: Seth David Schoen 
@_subject: Kaspersky wants to make Tor illegal 
The last time that I looked at a specific IP traceback proposal, it
was only about preventing IP spoofing (basically like a sort of
passive traceroute system, where the end recipient could find out
which AS numbers an IP flow probably passed through).  The only system
I know of that used IP spoofing in the service of privacy or censorship
resistance was SafeWeb, which I think is totally defunct now.  Does
anyone know of another example?

@_date: 2010-08-19 10:34:43
@_author: Seth David Schoen 
@_subject: Tor Project 2008 Tax Return Now Online 
I think there are a number of techniques that law enforcement and
intelligence agencies have that don't get introduced in court at a
particular time because the agencies don't want people to know about
their capabilities, even at a potential cost of not being able to
get particular convictions.  One analogy to this is the
unsubstantied claim that the British intentionally avoided making
an effective air defense of Coventry during World War 2 in order
to avoid compromising the Ultra program (the ability to read Enigma
(I wish I had an analogy that was actually based on something we
know really happened...)
I think two contemporary examples could be the ability to decrypt
GSM traffic over the air, as described by many researchers, and
the ability to obtain false certificates from CAs in the global
PKI, as suggested in Soghoian and Stamm's paper.
I don't mean to say that any particular agency has these
capabilities, just that it seems plausible that some do.  People
who can do these things might not want to mention it in court
because that might have the effect of changing a lot of people's
One that's actually more alarming to me (because I don't know how
to defend against it) is backdoors in hardware, like those described
I don't think someone who had incorporated a backdoor like that in
some popular device would want to mention it in any public context.

@_date: 2010-08-19 10:34:36
@_author: Seth David Schoen 
@_subject: Tor Project 2008 Tax Return Now Online 
I think there are a number of techniques that law enforcement and
intelligence agencies have that don't get introduced in court at a
particular time because the agencies don't want people to know about
their capabilities, even at a potential cost of not being able to
get particular convictions.  One analogy to this is the
unsubstantied claim that the British intentionally avoided making
an effective air defense of Coventry during World War 2 in order
to avoid compromising the Ultra program (the ability to read Enigma
(I wish I had an analogy that was actually based on something we
know really happened...)
I think two contemporary examples could be the ability to decrypt
GSM traffic over the air, as described by many researchers, and
the ability to obtain false certificates from CAs in the global
PKI, as suggested in Soghoian and Stamm's paper.
I don't mean to say that any particular agency has these
capabilities, just that it seems plausible that some do.  People
who can do these things might not want to mention it in court
because that might have the effect of changing a lot of people's
One that's actually more alarming to me (because I don't know how
to defend against it) is backdoors in hardware, like those described
I don't think someone who had incorporated a backdoor like that in
some popular device would want to mention it in any public context.

@_date: 2010-08-23 11:42:51
@_author: Seth David Schoen 
@_subject: The team of PayPal is a band of pigs and cads! 
I'll put Andrew in touch with a contact at PayPal.  Unfortunately,
their view may be that Tor is frequently used for fraudulent access
to accounts and so they want to count it as a potential indicator
of fraud in their fraud-detection systems.

@_date: 2010-08-16 23:08:27
@_author: Seth David Schoen 
@_subject: TLS NPN (Next Protocol Negotiation) 
Over on the TLS WG mailing list at IETF there is some debate over
the NPN (Next Protocol Negotation) TLS extension, which originated
outside of TLS WG but is now starting to be brought up there for
standardization.  The thread starts at
Much of the debate centers around the idea that NPN will make it
harder for network operators to know what protocols users are using
over TLS and hence to block particular protocols while permitting
others.  One of the proponents (Adam Langley, who has been doing a
lot of other fantastic work on making TLS better and more ubiquitous)
mentioned the idea that Tor is an intended use case for this
behavior, although there hasn't been any other explicit discussion
of this.
"The design, as is, was picked because the use cases considered were
either ambivalent on this point [in effect, whether to reveal which
service the client is interested in contacting earlier in the
protocol] or favoured the privacy side (i.e.  Tor)."
(Apparently the notion is that the protocol negotiation would
happen late enough that the encrypted session is already
established before the client and server decide which particular
service the client wants to talk to, so you could multiplex,
say, a web server, a Jabber server, a Tor server, and an IMAPS
server all over tcp/443 and an eavesdropper wouldn't trivially
be able to determine which one the client was communicating
with -- except if side channels gave it away, of course.)
I'm tempted to reply pointing out that _all_ uses of TLS represent
at least potential support for a threat model in which a network
operator is the adversary whom users are trying to defend against.
So there's not much conceptually new about having TLS reduce network
operators' control over traffic, although some of the people in
the discussion seem to feel there is a qualitative difference
between, say, keyword filtering and protocol filtering.
Has anybody from Tor been working on NPN?

@_date: 2010-01-11 12:43:27
@_author: Seth David Schoen 
@_subject: Trend Micro blocking Tor site? 
A Spanish EFF supporter just sent us a screen shot of what he said was
the Trend Micro Internet Security program blocking
as "Peligroso" ('Dangerous').  It seems that he's using Trend Micro's
antivirus product, not a censorware product.  I can forward the screen
shot to anyone interested.  If this is right, Trend Micro's antivirus
might be blocking the entirety of the Tor site.

@_date: 2010-01-11 13:01:35
@_author: Seth David Schoen 
@_subject: Trend Micro blocking Tor site? 
I'm not sure that I want to start a precedent of people sending
graphical attachments to this list.  I put a copy of the image at
The text translates as "Blocked by Trend Micro / Trend Micro
Internet Security has identified this Web page as undesirable.
Address: ...  Credibility: Dangerous / To visit this blocked
page anyway:" following by directions for how to override the
block in Trend Micro's user interface.

@_date: 2010-01-28 13:56:16
@_author: Seth David Schoen 
@_subject: browser fingerprinting - panopticlick 
Are you unhappy with the phrase "modern versions" in
or do you think that page as a whole isn't prominent enough?

@_date: 2010-07-30 12:32:43
@_author: Seth David Schoen 
@_subject: Practical web-site-specific traffic analyses 
While trying to find more information about the Hansen and Sokol
talk at BlackHat, I found that Hansen had recommended this recent
which describes practical traffic analysis of particular sites
that use HTTPS (just by observing encrypted flows).  They mention
several clever ways to deduce what the user is doing on the site --
for example, inferring what particular illness a user is researching
on a health site, or deducing the contents of a financial chart
from its image file size (!).  The paper is called "Side-Channel
Leaks in Web Applications: a Reality Today, a Challenge Tomorrow".
The researchers suggest that web application developers should use
padding to make different activities on their sites less
distinguishable.  That sounds pretty optimistic to me.  I've heard
other privacy researchers complain that it's extremely hard to get
web developers to do things.
Obviously, the existence of traffic analysis attacks is not new.
I'm wondering about the severity of this problem.
The simplest threat scenario for Tor users would be when an
attacker in a position to observe a particular user's traffic,
but not any exit node traffic, hypothesizes that the user is
likely to visit a particular site and builds up a profile of
what web browsing traffic to that site will look like.  The
attacker could then try to confirm the hypothesis that the
user is using that site and also try to infer some details of
what the user is doing.  This is quite different from traffic
confirmation because the attacker only has to be present at
one end.

@_date: 2010-07-25 13:40:33
@_author: Seth David Schoen 
@_subject: A suggestion to TOR [a proxy server] 
The Tor developers don't think that would achieve the "same
objective" as Tor, because the proxy server would be in a
position to know both where you are coming from and what you
are doing.  Tor aims to prevent anyone from being in this
position, and in any case the Tor Project wouldn't want to be:

@_date: 2010-06-22 01:18:36
@_author: Seth David Schoen 
@_subject: Downloading attachments with Tor - is this secure? 
NoScript provides important protections for Firefox users that HTTPS
Everywhere doesn't, but there are two kinds of gaps where HTTPS
Everywhere provides functionality that NoScript doesn't.  (HTTPS
Everywhere is also partly based on code from NoScript.)
(1) HTTPS Everywhere bundles a specific, and growing, list of sites
that support HTTPS, so you don't (necessarily) have to find those
sites yourself.
(2) NoScript only supports using HTTPS for an entire domain, but we
have several examples where sites support HTTPS only on parts of
the site, or even using a different hostname, requiring a more
specific translation rule and not just a hostname list.
For example, the Google support in the current HTTPS Everywhere
tree is currently _fifteen_ different substitution rules, and we
are already aware of several parts of Google's services that it
still handles slightly incorrectly and that will require additional
rules and exclusions.
Just adding " to your NoScript configuration will
provide quite a bit less correct functionality; to use a simple
example, Google Language Tools currently breaks if you try to access
it in HTTPS.  Google has also suggested that they may move HTTPS
support off of  entirely in order to help schools censor
access to Google services; if they do that, there will be an even more
conspicuous need for HTTPS Everywhere to rewrite URLs beginning with
" to something like "
or "
In fact, we already have a similar case with Wikimedia services,
where the rewrite rules understand things like
--> --> Again, NoScript currently can't do that kind of substitution.  If
you tried to force HTTPS access on, say, en.wikipedia.org, it would
just break because there is no HTTPS support on that host.
On the other hand, some users might definitely prioritize
confidentiality over availability and argue that the browser
should never load an HTTP resource from an HTTPS page (which I
understand was actually a common browser default in the past).
This policy would break some functionality on some pages, like
preventing certain images from displaying.  Some users might
think that's an appropriate trade-off because of the risks of
unencrypted access.  Currently HTTPS Everywhere doesn't include
any rules which deliberately do this when it would break some of
the page's functionality.  It's possible to implement that
behavior on your own machine with either NoScript or HTTPS
Everywhere if you know all of the relevant domain names (for
example, for Wikimedia you might block access to
For Tor users, HTTPS Everywhere particularly provides protection
against exit node operators and their ISPs, while NoScript
particularly provides protection against sites trying to
recognize or track users.  (Of course, NoScript is also helpful
for defending against other kinds of threats.)  In this sense,
the protections provided by the two programs can be complementary.

@_date: 2010-06-17 17:44:55
@_author: Seth David Schoen 
@_subject: SSL only firefox add-on? 
EFF has been working on one called HTTPS Everywhere:
There are some subtle issues around situations where a site
supports HTTPS for some resources but not others.  For example,
you can currently use
for encrypted web search, but only the unencrypted form
for translation services.  As a result, HTTPS Everywhere has a
database of rules with exceptions, so that a rule can apply to
only a portion of a site.
This may not do exactly what you want because you might prefer
to block HTTP URLs entirely, rather than allowing them only if
no HTTPS equivalent exists.  You could probably achieve this in
HTTPS Everywhere by adding a local wildcard rule that matches
every HTTP site and redirects it to an intentionally broken
page, such as a URL within your local host.  The means of setting
up your own local rewrite rules are described at

@_date: 2010-03-26 11:27:16
@_author: Seth David Schoen 
@_subject: How does TOR deal with mac addresses 
In the message you're replying to, "the ISP" means your ISP,
not some other ISP.  It's true that your ISP knows your home
router's MAC address.  Other ISPs don't.

@_date: 2010-05-28 12:47:11
@_author: Seth David Schoen 
@_subject: HTTPS Everywhere Firefox addon 
Three examples of sites that are broken by this are Google,
Facebook, and LibraryThing, simply because they violate the
assumption that the HTTPS and HTTP sites are sufficiently
identical to be used interchangeably.  We think there are
several others out there like this.
In fact there are many potential concerns about sites that
expose only a _portion_ of their resources in HTTPS, or that
provide different things in the HTTPS and HTTP versions.
This basically goes to the question of what "if it works" means.
Peter points out that many virtual hosters don't yet support
SNI, which means that name-based virtual hosts that are
distinct in HTTP won't appear distinct in HTTPS (and users
who access those hosts via HTTPS will get a single default
site in place of several distinct sites).  In that case the
content will be extremely different, and wrong, but the site
will still return an HTTP 200 OK.
The presence of a regular expression-based rewrite rule in
HTTPS Everywhere basically connotes that a human being checked
out the site a bit and believes that the particular resources
covered by the rule are safe to rewrite this way, without
breaking other things.

@_date: 2010-05-28 12:55:19
@_author: Seth David Schoen 
@_subject: HTTPS Everywhere Firefox addon 
There's one piece of additional functionality that was added in
HTTPS Everywhere that can be important for these sites.  Although
NoScript lets you use regular expressions to choose which URLs
within a site get converted to HTTPS, NoScript doesn't let you
rewrite _the URL itself_, which HTTPS Everywhere does (also
using regular expression substitutions).  For example, the
current alpha version of HTTPS Everywhere correctly handles
Wikipedia rewrites like
 -->  -->  --> It's certainly annoying that Wikimedia doesn't let you use
HTTPS directly this way, but given the status quo, HTTPS
Everywhere can address this.

@_date: 2010-10-27 12:37:56
@_author: Seth David Schoen 
@_subject: Hints and Tips for Whistleblowers - their comments on Tor and 
Session resumption can be used to recognize an individual browser
that connects from different IP addresses, or even over Tor.  This
kind of recognition can be perfect because the resumption involves
a session key which is large, random, and could not legitimately
have been known to any other browser. :-(

@_date: 2010-10-27 17:43:30
@_author: Seth David Schoen 
@_subject: Hints and Tips for Whistleblowers - their comments on Tor and 
Sorry, I only wanted to point out that the use of HTTPS in general
does create this tracking mechanism (and that Tor and other
TCP-level proxies won't remove it by themselves).  Your thoroughness
in dealing with details like this is a tremendous argument for
always using Torbutton.

@_date: 2011-02-02 10:24:08
@_author: Seth David Schoen 
@_subject: What are email risks? 
Hi Bjarni,
There is a stylometry item in the anonbib where they do statistical
analysis of features of writing style:
I bet these techniques have gotten more powerful as the field of
machine learning has developed, although I don't know if there are
more recent studies of what this means for anonymity.

@_date: 2011-06-11 23:22:40
@_author: Seth David Schoen 
@_subject: [tor-talk] When to use and not to use tor. 
Your communication with an online banking site usually _would_ be
encrypted with HTTPS, which would encrypt your login password.  For
instance, if you were banking with Bank of America, you would normally
start your login process at
This encryption is complementary to Tor because Tor protects the anonymity
of where you're connecting from, while HTTPS protects the confidentiality
of your communications, including the password.
There's a different problem with using Tor for online banking: some
financial institutions consider it a likely sign of fraud attempts,
since (for most financial institutions) few legitimate customers
currently try to hide their location from the financial institution,
but many people committing fraud do.  If the financial institution
misinterprets your Tor use as a sign of fraud, they might block your
on-line access or restrict it in some way.
Although Tor is introducing an unknown third party, it doesn't in any
way prevent you from also using HTTPS to protect your communications
against that third party.  In fact, all the published Tor documentation
strongly urges Tor users to always use HTTPS for this reason, and the Tor
Project is co-developing HTTPS Everywhere with EFF for this reason, and
has now included it with the Tor Browser Bundle.
But if you're using webmail, you could use HTTPS to connect to the
webmail operator over Tor, thereby protecting your e-mail from the
exit node operator.
This can be complementary to Tor _and_ HTTPS, because e-mail encryption
protects your e-mail contents from your e-mail service provider and the
other person's e-mail service provider.  I think it would be nice to
have a threat-model diagram to show what's meant to protect you against
whom, but let me try to summarize in text:
Suppose you're using Hotmail (Windows Live Mail) and e-mailing with your
friend who's using Gmail.
If you didn't use any security tools, then, among other things,
* other people on your wifi network would see what you're doing and could
  steal your password or read your e-mail;
* your ISP could do the same thing;
* the other ISPs that carry your communications to Hotmail could do the
  same thing;
* Hotmail would record your IP address, so they would know where you are
  connecting from, which could be used to trace your identity or location
  later on;
* Hotmail could read the e-mail that you ask them to deliver;
* the ISPs that carry your communications between Hotmail and Gmail could
  read the e-mail too (depending on whether Hotmail and Gmail are
  successfully using a security technology called ESMTP STARTTLS);
* Gmail could read the e-mail at any time after it's delivered to them;
* depending on how securely your friend accesses Gmail, other people like
  your friend's ISP might be able to read the e-mail as your friend opens it.
Also, people who are doing wiretaps (like tapping fiber optic cables or
microwave links) could read the communications between ISPs, perhaps
with the ISPs' knowledge or perhaps without it.  This goes to show that,
in the absence of security technology, there are plenty of entities that
might be in a position to spy on you in some way.
Different security tools try to address very different parts of this
Primarily, Tor tries to address the "Hotmail would record your IP address"
problem.  It incidentally solves the "other people on your wifi network"
and "your ISP" problems while adding a new, related problem: "the Tor exit
node operator could spy on you and read the e-mail".
Using HTTPS to connect to Hotmail addresses the "other people on your
wifi network", "your ISP", and "the other ISPs" problems, and, if you're
using Tor, it also addreses "the Tor exit node operator could spy on you"
Using GPG addresses the "Hotmail could read the e-mail" and "Gmail could
read the e-mail" problems.  It partially addresses all the problems
related to any ISP reading the e-mail: it prevents any of the ISPs from
understanding the content of the message, but it doesn't conceal the fact
that you're e-mailing a particular person at a particular time.

@_date: 2011-09-02 10:11:01
@_author: Seth David Schoen 
@_subject: [tor-talk] Dutch CA issues fake *.torproject.org cert (among 
How do you know it was really the Tor Project server?

@_date: 2011-09-08 19:32:10
@_author: Seth David Schoen 
@_subject: [tor-talk] Mac? 
The MAC address usually identifies a particular physical computer
to a local area network.  If someone doesn't want their physical
computer to be recognized by a network, they might want to change
the address.
The most common reasons for this in practice are probably
* Some networks let people use the network for free, but only for
  a limited period of time, or only on one occasion; this is
  enforced using MAC addresses, so changing MAC addresses lets
  people get around the restriction and continue using the
  network.  For example, an airport or university wifi network
  might let a "guest" use the network for 30 minutes without
  paying or registering.
* Some networks might ban someone they consider abusive or
  unwelcome using the MAC address (for example, an open wifi
  network where someone has used it in a way that the operator
  considered abusive or excessive).  In that case, the person
  who was banned might change their MAC address to get around
  the ban.
* ISPs might record or log MAC addresses, which could be used for
  commercial or law enforcement purposes, so someone who doesn't
  want to end up in such logs might use a false or random MAC
  address.  In some places, law enforcement might pressure or
  require the ISPs to keep these logs as a way of trying to catch
  people accused of breaking the law, or as a way of providing
  corroborating evidence after-the-fact when a suspect is caught.
* Although it's not known to happen on a large scale, other people
  on a LAN with you could detect and log your MAC address to
  monitor when your computer is physically present on the LAN
  (perhaps to learn or make a profile of when you're present at
  a certain place that you're known to visit periodically?), so
  changing your MAC address would let you avoid this kind of
  monitoring.
* Some ISPs use a clumsy policy where the subscriber's observed
  MAC address is not allowed to change frequently (sometimes
  because of somewhat obsolete ISP billing systems that used the
  MAC address to identify the subscriber, or sometimes because
  of old ISP policies meant to discourage people from using more
  than one computer with a single account).  In this case,
  people may change the MAC address of one computer (or a wifi
  router) to match the address of a different computer (which
  is called "cloning").  This could also be used by someone
  who has paid for a certain amount of Internet access on a paid
  wifi network (say, in an airport or hotel) let a friend take
  over using the access when the first person is all done.
It's "sent out" to the local router but not out over the Internet,
so web servers, for example, can't observe it.  You have to be on
the same LAN in order to observe it.
Whoever operates the local router can store it (e.g., if you're on a
friend's wifi, the friend could store it; if you're on a commercial
wifi network, the commercial wifi operator could store it; if you're
directly plugged into a cable modem owned by an ISP, the ISP could
program the cable modem to store it; ...).
An exception is that some software could deliberately choose to
transmit the MAC address for its own reasons, like enforcing
anti-copying restrictions or because of a weird choice to use the
MAC address to identify individual computers for some other reason.
There's nothing about how the Internet works that _requires_ any
software to do this, and it's probably not common.
Nope, never.

@_date: 2011-09-09 11:50:39
@_author: Seth David Schoen 
@_subject: [tor-talk] HTTPS Everywhere 
If you're using the current Browser Bundle, you're already using the
HTTPS Everywhere extension.

@_date: 2011-09-14 17:32:23
@_author: Seth David Schoen 
@_subject: [tor-talk] Posting on BBS with the TBB 
When people run into problems like this, it would be great if you could
tell the site operator (so they can fix their broken HTTPS support) or
the HTTPS Everywhere developers (so we can take the rule out of HTTPS
Everywhere).  We have rules for over 1100 sites in the development
version and we don't use most of those sites ourselves, so we're very
reliant on reports from other people to help find these problems.
Sometimes, part of a site works properly in HTTPS and part doesn't, but
perhaps the person who originally created the HTTPS Everywhere rule
didn't discover the problematic parts.  Of course, the very best solution
would be for the sites to fix the compatibility issues because they want
people to be accessing the sites securely!

@_date: 2011-09-24 18:31:32
@_author: Seth David Schoen 
@_subject: [tor-talk] using TOR without any browser 
You're not going to be able to anonymize ping and traceroute over Tor,
because they use ICMP, and Tor only transports TCP (plus DNS requests).
See generally

@_date: 2012-07-18 11:19:13
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor and the National lottery in the UK 
You could talk to Neustar about it.  Presumably less than a thousand
people in the world are in your situation (running middle nodes on
the same IP address that they use for their own personal
communications), so it may be too unusual for Neustar to have bothered
to think about yet.

@_date: 2012-06-27 22:28:20
@_author: Seth David Schoen 
@_subject: [tor-talk] possible to identify tor user via hardware DRM? 
I find this message misleading in various ways.  The basic thing that
I've been telling people is that there are few situations in which
either PSN or TPM uniqueness makes things qualitatively worse.
There are lots of hardware unique IDs.  On Linux, try "sudo lshw" and
be surprised at all the things that have unique serial numbers.  There
are also things that are unique about your machine that are not
hardware serial numbers, like filesystem serial numbers and observed
combinations of software configurations.
These can be bad for privacy because software can tell which computer
it's running on.  If the software has an adversarial relationship with
you, it can then use that information in a way that you don't like.
We would be better off in some regards if operating systems let us
hide local uniqueness from software so that the software couldn't tell
what machine it was running on, or set fake values for these unique
Some proprietary software including Microsoft Windows already makes
a sophisticated profile of the local machine, including many kinds of
observations, to tie a copy of the software (or an "activation") to a
particular device (!).
The only substantive difference with the TPM uniqueness is that the TPM
uniqueness lets you prove (like a smartcard) to a remote system that
you're running some software on the same machine as before.  Even if
the OS did let you set fake values when software tried to examine the
system it was running on, the remote system could see that the
TPM-related values were fake.  That's useful for some applications,
including but not limited to DRM-like ones.
I've argued that this is bad in some ways, but at least you can still
turn off the TPM.  Then your system can't attempt to offer that kind
of proof.  As far as I know, turning off the TPM is pretty robust:
it really is turned off.
All of these things are anonymity problems in particular when some
software on your computer is actively _trying_ to tell someone else
what machine you're running on, either because it's programmed to do
so or because someone has broken into your computer and installed
spyware and is trying to use it to monitor you.  If you're not in
that situation, there is nothing especially magical about having
unique hardware IDs in your machine, because everyone's machine has
some uniqueness, and (for the most part) that uniqueness isn't part
of standard network protocols like TCP/IP and doesn't automatically
leak out to anyone and everyone you communicate with over the
Internet.  (There is a possible exception about clock skew, which you
can read about in Steven Murdoch's paper from 2006.)
Similarly, having a GPS receiver in your phone does not mean that
everyone you send an SMS to or everyone you call will learn your
exact physical location.  However, it does mean that if there's
spyware on your phone, that spyware is able to use the GPS to learn
your location and leak it.  If you're worried about spyware threats
on your phone, which can be quite a realistic concern, the GPS
itself isn't necessarily the unique core of the threat, because
there are also lots of other things in the phone that can be read to
help physically locate you (like wifi base station MAC addresses,
taking photographs of your surroundings with the phone's camera,
recording the identities and signal strengths of the GSM base
stations your phone sees...).  So a more fundamental question might
be whether your phone operating system is able to either prevent
you from getting malware or prevent the malware from accessing the
sensors on your phone.
In the case of a desktop PC, the hardware uniqueness is _there to
be read by software_, and if it's in a TPM it _may be able to give
the software remotely verifiable cryptographic proof that the
software is really running on the machine containing that particular
TPM_.  In neither case does the hardware uniqueness directly
broadcast itself to other machines, and in neither case does the
hardware uniqueness prevent the operating system from preventing
other software from reading it.
If you do have some kind of software running on your machine that's
trying to track you or trying to help other people track you,
hardware uniqueness is one thing that the software might look at.
But if you're a Tor user, a more basic thing for the software to
try to do is make network connections to leak your real IP address
in order to associate your Tor-based network activity with your
non-Tor-based network activity.  That might be even easier because
the tracking software could just try to make a direct network
If you're not using Tor, at least not at a particular moment, but
are still concerned about tracking, there's another problem, which
is that all existing browsers _already_ reveal a great deal of
software-based uniqueness to any interested web site, usually enough
to make your browser unique.  See
This is important because it doesn't require there to be any
malicious software on your computer, just a traditional web browser.
One of the defenses people have talked about against hardware
fingerprinting is running inside a virtual machine.  Normally,
software inside the virtual machine, even if it's malicious,
doesn't learn much about the physical machine that hosts the VM.
If you always use Tor inside a VM, then even if there's a bug
that lets someone take over your computer (or if they trick you
into installing spyware), the malicious software won't be able
to read much real uniqueness from the host hardware, unless
there's also a bug in the VM software.
Running in a VM isn't exactly a defense against software
fingerprinting (like browser fingerprinting) if you use the VM
for various non-Tor activities that you don't want to be linked
to one another, because the software configuration inside the VM
might be, or become, sufficiently different from others that it
can be recognized.  There's probably more research to be done
about the conditions under which VMs can be uniquely identified
both "from the inside" by malware, and remotely by remote
software fingerprinting, absent VM bugs that give unintended
access to the host.

@_date: 2012-03-05 20:55:36
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor and HTTPS graphic 
Eva Galperin and I worked on this graphic (drawn by Hugh D'Andrade)
that tries to show the difference between the threats Tor addresses
and the threats HTTPS addresses.
The complete interactive version is at
You can click to enable or disable the use of Tor and HTTPS and
see what information different parties can see about your
communications in each case.  (It even includes a global passive
adversary from the NSA in a position to do a traffic correlation
This is a very different kind of educational material than Joe Hall's
cool hands-on anonymity simulation concept, but I hope it will be
useful to some people trying to teach and learn about Tor.

@_date: 2012-03-06 16:20:54
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor and HTTPS graphic 
I was concerned that the graphic should not make people think that
_no one_ can ever associate them with their browsing when they use
Tor.  I've been taught to think of the GPA threat (and other traffic
correlation threats) as real, so I thought people should have some
indication of those threats.

@_date: 2012-03-24 21:31:00
@_author: Seth David Schoen 
@_subject: [tor-talk] AllowDotExit 
The trouble is that the site generates links that contain an explicit domain
name and the browser simply follows those links.
You could abuse HTTPS Everywhere to do what you want, because it's capable of
doing arbitrary URL rewriting.  HTTPS Everywhere is also preinstalled in the
Tor Browser Bundle.  If you make an HTTPS Everywhere rule to do your rewrite
and then put it inside your
directory, it should only affect the Tor Browser.
The content of the rule would look something like
This rewrites any URLs under   or  to instead point through
  The browser will still show the unmodified
links if you mouse over them, but it should use the modified links if you
actually follow a link.
I don't know whether the Tor Browser also edits the Host and the Referer
headers in this case in a way that removes the .exit from the hostname to
avoid confusing the site.

@_date: 2012-03-25 18:54:36
@_author: Seth David Schoen 
@_subject: [tor-talk] How can the video play in TBB without plagins? 
This is something that you can do without creating an account.

@_date: 2012-03-26 10:10:41
@_author: Seth David Schoen 
@_subject: [tor-talk] How can the video play in TBB without plagins? 
TBB deletes cookies when you quit it, so it's hard for users to forget
to delete them.  It's true that Google can use cookies to track a
particular Tor user within a TBB session, including from one Google
site to another.  However, if you quit TBB and run it again another
time, there shouldn't be any information that enables Google to
recognize you from before.
TBB took some measures to respond to the Panopticlick research, so it's
probably not easy for Google or other sites to recognize your browser
by non-cookie means either.

@_date: 2012-03-29 16:54:53
@_author: Seth David Schoen 
@_subject: [tor-talk] Choosing a name for a .onon 
The basic answer is "try a lot of possibilities on a computer until the one
you want comes up"!
According to
the .onion name is a base32-encoded version of the first 80 bits of the
SHA1 of the hidden service public key.  base32 is 5 bits per character,
so the prefix "silkroad" in the .onion address above is 5?8=40 bits
(indeed, exactly half of the 80 bits in the entire .onion name).  Choosing
the first 40 bits of a hash generally requires trying an average of 2??
possibilities; my laptop does about 3-4 million SHA1 operations per second
(per CPU core) so it would take me 3-4 days (per CPU core) of computation
to try that many possibilities on my laptop.  Since hash value searches
are completely parallelizable, you can make this arbitrarily faster by
adding more computers to the search.
Of course this requires being able to change something trivial about the
public key when generating the .onion address.  (Generating a new public
key from scratch would be very time-consuming; making a 1024-bit keypair
from scratch on my machine takes around 0.1 second, depending on how lucky
the key-generation process gets, which is considerably longer than the
0.0000003 seconds that trying a SHA1 operation requires.)  I'm not sure
exactly what you would change because I'm not sure what data is (or can
be) included in a hidden service public key, but there's probably something
you can change without actually needing to make a new keypair.
There's a nice description of the possibility of creating a public key
with a chosen set of bits at the beginning or end at
although note that the Tor hidden service identifiers are 80 bits, while
PGP short key IDs are only 32 bits, so it's 2?? times as hard to fake a
hidden service as it is to make a colliding PGP short key ID.  (Full PGP
fingerprints are 160 bits.)
I first got interested in this when I used to develop a live-CD called
LNX-BBC (for "bootable business card").  At that time we were using MD5
checksums on our web site, and I wrote a program to try lots of
possibilities so that all of our MD5 values started with "bbcbbc" in
hexadecimal (on average only 2?? MD5 attempts).
For more on the base32 encoding, see

@_date: 2012-05-16 12:26:05
@_author: Seth David Schoen 
@_subject: [tor-talk] Encryption on Http 
There's been a lot of talk of this, but we should get someone from a
project that does empirical tests about Internet censorship to say
whether there are countries where this actually happens on a regular
I've heard that in several countries the technical parameters of the
Internet censorship are changed (a lot) during particular periods of
crisis or tension.  Sometimes that includes just turning off the
entire network in a region.

@_date: 2012-11-07 10:20:23
@_author: Seth David Schoen 
@_subject: [tor-talk] Can Tor users be tracked? 
ISPs can tell which users are using Tor (unless the users use
particular technologies that prevent this).  They can't usually
tell easily what the users are doing with Tor.  If an ISP or
government forbids the use of Tor, there is a risk that Tor
users could be recognized as Tor users and punished, although
I don't know of any particular case where this has happened.
The reason that ISPs can identify Tor users in the basic Tor
usage situation is that Tor users must connect to Tor entry
nodes, and a list of all the public entry nodes' IP addresses
is easily available from the directory servers.  The same way
that your Tor client chooses relays to connect to would allow
someone else (like an ISP) to learn that those IP addresses
are operating as Tor relays.
This is one reason that the Tor project is interested in making
sure that Tor is interesting and useful for many different
kinds of users in many different situations, so that lots of
people will use Tor for lots of different purposes.  If this
continues to happen, an ISP that notices that a user is using
Tor won't be able to know for sure _why_ the user wanted to use
However, the Tor project is also working on ways to let people
connect to Tor without making it obvious to an ISP or government
that the service being used is Tor.  This is mostly being done
for blocking-resistance purposes, because determining which
network connections are related to Tor is a necessary step for
blocking Tor (and makes blocking Tor quite easy).
There is a risk when using them that a web site (or someone who
can take over a web site) can figure out a Tor user's real IP
address.  Normally this is not supposed to be possible, but
using ActiveX or Flash creates opportunities for sites to do so.
I don't think Javascript running inside the browser is supposed to be
able to access MAC address or Windows user name.  (This isn't a
limitation of the Javascript language itself, but just a detail of
the security sandbox policies that are supposed to be applied by
browsers to restrict what web pages can do.)  The time zone is
normally available, although the Tor Browser in particular might
apply extra restrictions to prevent sites from accessing information
like this.
They might be able to block Tor completely, but they should not be
able to block particular pages or sites while allowing others.  If
you're able to use Tor at all, you should be able to access every
web site via Tor.
I'd like to recommend again a resource that I worked on with some
of my colleagues:
The original goal of this page is to help people understand why
using both Tor and HTTPS is important (each one protects you
against some things that the other doesn't).  In particular, the
graphic shows what information eavesdroppers at different
locations within the network could see.  The diagram might also
help with some questions that have to do with the structure of
the Internet or the design of Tor.
One example is that our diagram shows that in all the cases we
examine, the user's ISP recognizes that the user is using Tor,
and the web site that the user connects to also recognizes that
the user is using Tor.  The particular value of Tor in this
case is that the ISP doesn't know where the user is going, and
the site doesn't know where the user is coming from.  Instead
of learning specific locations, both of these parties basically
end up learning only "this person is using Tor"!
Our diagram doesn't really address client-side tracking threats,
like some of the threats you mentioned above in which a web site
attempts to get your browser to turn over identifying information.
In this diagram, we basically assume that your browser cannot
be made to disclose information about your identity or location.
A big concern for the Tor developers is that sometimes real
browsers might be made to do this.
Most of the work that has been done to address this kind of
threat is described in the very detailed document by Mike Perry
where he talks about particular details about ways that Firefox
might reveal identifying or unique information about users or
their locations, and ways that TorButton (now the Tor Browser)
prevents those information flows.  Since that document is a
year and a half old, there might be several other information
flows that Mike has managed to squash since then. :-)

@_date: 2012-10-04 09:31:24
@_author: Seth David Schoen 
@_subject: [tor-talk] install adobe flash player on TBB 
The TBB developers' concern about proxy bypass is that sites _can_ make
Flash bypass the proxy (if they decide to), not that it always bypasses
the proxy.  Whether the proxy bypass occurs depends on the particular
Flash applet, so while the YouTube applet (for example) is apparently
safe, other applets are not safe.  This is a problem because people might
actively try to identify Tor users, for instance by trying to get them
to visit pages that embed Flash applets that do cause a proxy bypass.
(Exit node operators could try to do this too: they could modify the
HTML code returned by a web site to add an iframe that loads a Flash
applet to cause a proxy bypass.)
So, it's not that Tor users always lose their anonymity when they use
Flash, but that Tor users could lose their anonymity when they use

@_date: 2012-10-17 10:05:53
@_author: Seth David Schoen 
@_subject: [tor-talk] Will Hidden Service Be Removed? 
I think it's akin to port knocking:
If you're worried about people trying to break into your server (by
guessing passwords or exploiting vulnerabilities), it might be
useful to have access to the server available only via a hidden
service so that some attackers can't even find it to attempt their

@_date: 2012-09-25 11:00:43
@_author: Seth David Schoen 
@_subject: [tor-talk] Could not bind to 0.0.0.0:443: Permission denied 
Yes, if you want to listen on a port below 1024.  Ports below 1024 are
considered "privileged ports" -- an old convention for distinguishing
between services run by the system administrator of a machine and
services run by other "unprivileged" users.  This convention could be
useful for security purposes if you imagine that system administrators
trust each other for some purposes, or that you trust the system
administrator of a certain server but don't trust every user of that
server.  A specific example is NFS:
In the original NFS design, machines are configured to trust each other
and grant each other's requests, but the end users are not necessarily
trustworthy so the machines have to decide whether a particular request
was authorized by the remote servers system administrator.  This did
lead to security problems if you had the ability to send packets on
the network with an arbitrary source IP address, because requests were
not authenticated beyond examination of their source IP address and port
number (the same issue applied to the rsh/rlogin service).
Some people have suggested that the usefulness of the privileged port
convention has decreased a lot over time, but it may still be useful
for some purposes.  (It seems that the use of public-key cryptography is
ultimately better and safer than relying on TCP port numbers as a proof
of identity... but, for example, it can be nice that the sysadmin of a
multiuser system gets to reserve port 80 for the "official" web server
on that system, instead of having a random user come in and set up their
own web server there.)
Operating systems generally still do enforce the rule, so you still can't
bind a privileged port if you aren't root!

@_date: 2012-09-25 23:01:14
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor and P2P 
I wonder if there's a way to extend the protocol to do ephemeral
hidden services (that are only meant to be used once for a single
inbound connection, perhaps, and that can be set up very quickly
with low overhead).  This might be something like the "reply onion"
concept in the original onion routing, where you can create an
object that represents an explicit route to reach a particular Tor
end user (but where the route is opaque to its users, so they don't
know where the connection they establish with it will go).
My limited understanding of onion routing history is that reply
onions were replaced by hidden services, which are meant to be
long-lived and usable by many clients.  I don't know whether reply
onions disappeared solely on efficiency grounds or whether there
are also bad security consequences.
In existing hidden services both sides are building a path through
the Tor network to the rendezvous, so you don't just have one side
choosing the complete path.  I have a vague recollection that there
are bad consequences if you allow one party to choose another party's
complete path through the network -- presumably based on the idea of
making the other party use an entry node secretly controlled by the
hidden service operator (!!!) in order to identify them.

@_date: 2012-09-27 17:32:54
@_author: Seth David Schoen 
@_subject: [tor-talk] flash and tor 
The "completely disregarding proxy settings" risk mentioned on that page
would be comparable to visiting that same web page without using Tor at
all.  Then your IP address would be revealed to everyone who can observe
Internet traffic between you and the web site (including the web site
operator itself).
The "querying your local IP address" risk would mean that your address
would be revealed to the web site operator.

@_date: 2013-04-05 14:57:20
@_author: Seth David Schoen 
@_subject: [tor-talk] NSA supercomputer 
As Gregory pointed out, "flops" is not the right measurement here
because cryptographic operations are not floating-point operations.
Checking a candidate key doesn't involve any floating-point math,
but rather something like a block cipher decryption, which is a
different sort of computation.
The calculations to figure out brute-force speeds are really about
simple multiplication and division.
Just as the distance traveled by a moving object is given by
distance = speed ? elapsed time
the number of decryptions attempted by a brute force search is given
decryptions = speed ? elapsed time
For example, if you have a 128-bit symmetric key, and you want to
talk about a situation in which every possible key value has been
checked, the relationship is
2??? = speed ? elapsed time
or, if you prefer,
340282366920938463463374607431768211456 = speed ? elapsed time
If you want the time, just divide 2??? by the speed.  If you want
the requisite speed to finish in a specified time, just divide 2???
by that time.  You just need to use consistent units, like measuring
speed in trial decryptions per second and measuring elapsed time in
In 1998 EFF built a brute-force cracking machine
which "was capable of testing over 90 billion keys per second",
against the DES system which used 56-bit keys.  To find the time
it would take that machine to be sure of testing every possible
key, just divide 2?? by 90 billion; the answer is given in seconds.
(To convert seconds to days, divide by 86400.)

@_date: 2013-04-05 16:59:13
@_author: Seth David Schoen 
@_subject: [tor-talk] NSA supercomputer 
I don't think that consumer hardware not intended for cryptographic use
is a great basis to estimate cracking speeds of specialized cryptoanalytic
devices.  If you think about bitcoin hash rates, custom hardware is already
clobbering GPUs, not only in overall speed, but also in cost-effectiveness
(hashing speed per dollar).  The same was true when EFF built the DES
cracker back in 1998 -- custom hardware was significantly more cost-effective
for fast DES cracking relative to ordinary desktop CPUs.
In fact (just following Wikipedia's figures, not my own recollections of the
_Cracking DES_ book), the EFF machine was about 90,000 times faster than a
desktop computer at the same time -- but probably only around 100 times the
Recommended key length is a topic that's seen fairly extensive study, often
based on speculating about an adversary's costs.
I think there was a summary paper in the last couple of years trying to
estimate the cost of modern custom hardware to break keys of various sizes,
but I haven't been able to find it again.  The COPACOBANA hardware cracker
is a major milestone using readily-available commercial technology.  I
don't know what their current-generation machines do, but they roughly
matched EFF's cracker for about $10,000 in 2006 and I assume it's quite
a lot better now.  They use Xilinx FPGA chips, which is still not the most
cost-effective option for an organization like NSA which could have its
own microchip fabrication facilities.

@_date: 2013-08-22 18:23:09
@_author: Seth Schoen 
@_subject: [tor-talk] Run the shell script 
The traditional way to run a shell script is to open a terminal, change
to the directory where the shell script is located (with the "cd"
command), and then type the name of the shell script, preceded by "./",
cd /somewhere/someplace

@_date: 2013-08-23 07:37:43
@_author: Seth Schoen 
@_subject: [tor-talk] Run the shell script 
+x for directories has an extremely different meaning from +x for files.
Perhaps you could send along the exact command that you typed and the
exact error message that resulted?

@_date: 2013-12-01 11:49:29
@_author: Seth David Schoen 
@_subject: [tor-talk] TOR peer-to-peer network 
If you want to create your own Tor network, you'll need to run your
own directory server and tell all of the clients and routers to use
and trust it.  The directory servers collect information about all
publicly-available Tor nodes so that clients can get a list and then
choose which nodes the clients want to build circuits through.
In the actual public Tor deployment, there are several directory services
which reach a "consensus" about the list of nodes.  One reason for this
is to make it hard for someone to attack Tor by pressuring directory
server operators to tamper with the list (or by breaking into the
directory computers to change the list).  An attacker who could do
that could remove honest nodes from the list and fill it instead with
nodes that the attacker controlled, and then clients would think those
attacker-controlled nodes were the only ones available.
If you have a copy of the normal Tor client, you can see the information
about the directory servers in "cached-certs" and "cached-consensus"
(with the actual listing of available Tor nodes in "cached-descriptors").
You'll see that the certs provide cryptographic keys which are used to
check the integrity of the directory information (verifying that the
several different directory servers have actually signed off on its
I couldn't find the complete documentation about the directory servers
(or their source code!).  So this might be a good opportunity to make
it clearer somewhere on the Tor site how to deal with this in modern Tor
(maybe at least on the "research" page?).
The specification for the directory servers is at

@_date: 2013-12-16 18:56:17
@_author: Seth David Schoen 
@_subject: [tor-talk] Aside of Tor Research Project: SlowTor 
Maybe content-based networking
can address this model.
I also heard about HTML element attributes to let you refer to objects
by their hashes, although I don't remember which standard this is being
proposed for.  I guess the idea is that instead of (or in addition to)
, you can specify  and you indicate which static
resource to load by its hash value.  In that case, both attributes
could be provided at the same time, and the src attribute can be viewed
as an initial bootstrapping hint but not the only means of resolving
the reference.
Do you mean that the verifier is allowed to know the client's or
server's keys, or only to see the encrypted session as a passive
network adversary would see it?

@_date: 2013-07-11 08:11:00
@_author: Seth David Schoen 
@_subject: [tor-talk] ISP surveillance through Tor? 
There are a few codes that geolocation providers use for "anonymous
proxy" (sometimes "A1").  One possibility is that translating this
into "Afghanistan" is a result of some kind of coding bug.  It
happens that the code "A1" is the alphabetically first geolocation
country code, while the name "Afghanistan" is the alphabetically
first country name in most lists of countries.  Maybe somehow a
code like A1 gets mapped onto "Afghanistan" even though it wasn't
meant to be.

@_date: 2013-07-11 08:43:30
@_author: Seth David Schoen 
@_subject: [tor-talk] ISP surveillance through Tor? 
Bom dia,
We prepared a graphic last year ago to try to help people visualize
which data is concealed by the use of Tor.
This graphic lets you click to turn Tor and HTTPS on and off.  (Here,
"HTTPS" means that your browser is using an HTTPS connection to the
particular web site that you're communicating with.)  The kinds of
data that different entities along the way see or don't see is
There are some surveillance possibilities that the graphic doesn't
directly address, for example that the timing or amount of data
you send might allow one of the eavesdroppers to confirm a hypothesis
or guess about you or what you're accessing.  Instead, the graphic shows
what each entity directly learns from its own eavesdropping or data
requests, not what they might be able to figure out with further

@_date: 2013-07-13 09:17:55
@_author: Seth Schoen 
@_subject: [tor-talk] Will Tor affect Internet Explorer? (newbie question) 
You can have both on your computer, but unless you specifically
tell IE to send information through Tor, you won't be getting any
privacy benefits from Tor when you're using IE.  It doesn't
prevent you from using IE, but it doesn't improve or protect IE
Although IE could be asked to send information through Tor, which
would then provide privacy benefits, people keep talking about the
Tor Browser because
? it's easier (you can just download the Tor Browser and run it,
and all that browsing activity with Tor Browser is automatically
protected by Tor)
? there are some technical problems in other browsers that mean
that the privacy benefits of using them with Tor are not as great
as we would like.  The Tor Browser developers have tried to think
about all of these issues and create a browser that's optimized
to work together with Tor to get the most privacy protection
The discussion of what's meant by "privacy benefits of using
them with Tor" and "most privacy protection possible" is pretty
involved, and mostly has to do with web sites actively trying
to figure out who you are and where you're connecting from, the
things that Tor is trying to conceal.

@_date: 2013-06-04 15:27:02
@_author: Seth David Schoen 
@_subject: [tor-talk] Fwd: Accessing remote host SSH... 
I think this advice is about what you need to do on the server in
order to cause the hidden server to accept the SFTP connections.
This answer is right, but I understood the question differently --
I thought the question was what to do on the client in order to
allow the client to connect to the hidden service.
One way to do this would be to configure your SFTP client software
to access the Internet via a SOCKS proxy.  While Tor is running, it
creates a local SOCKS proxy that other applications can use to access
Internet services -- including both Tor exits and hidden services.
Your SOCKS proxy is likely to exist on localhost, port 9050, so you
can configure other software to use this if it understands SOCKS.
This is alluded to in step two of
(the instructions in this step aren't Windows-specific in any way).
This technique has been discouraged by the Tor developers over time
in favor of using specially modified software (like Tor Browser)
because unmodified software used over Tor might allow leaks that
compromise the user's anonymity.  It can still work, but be aware
that software not specifically designed to work with Tor might leak
data over the ordinary Internet that reveals some information about
the user's location or the destination of the user's communications.
Actually, a lot of this information is also scattered throughout
including the information about how to configure FileZilla to use
a SOCKS proxy to access hidden services. :-)

@_date: 2013-05-23 09:54:23
@_author: Seth David Schoen 
@_subject: [tor-talk] WebGL forbidden in NoScript but Flash is not? 
In the late 1990s people upset with the trend of requiring accounts on
Internet sites started a convention of signing up as user "cypherpunks",
password "cypherpunks".  Sometimes, if this account got deleted, they
would follow up with "cypherpunk/cypherpunk", "cypherpun/cypherpun",
and so on.
This often still works today.

@_date: 2013-05-23 10:32:07
@_author: Seth David Schoen 
@_subject: [tor-talk] The Google Browser, Sand boxing and Tor. 
Free software and open source software are intended to refer to the
_same software_.
Chrome is proprietary (non-open source) software, complete with a
proprietary EULA.  There is also a free and open source software
version called Chromium.

@_date: 2013-11-21 09:33:06
@_author: Seth David Schoen 
@_subject: [tor-talk] The New Threat: Targeted Internet Traffic 
You can't use BGP redirection to impersonate a node because the
individual nodes have unique cryptographic keys that are listed in
the Tor directory consensus.  (We need all other Internet services
to move to having unique cryptographic keys, too, so that people
who can control and redirect networks can't impersonate them!)
You could use BGP redirection to become able to spy on traffic
headed to a guard node or coming out of an exit node that would
otherwise not have passed through networks that you control.
The most relevant consequence of that would probably be increasing
the probability that the attacker can successfully do a traffic
correlation or confirmation attack.

@_date: 2013-10-22 21:48:20
@_author: Seth David Schoen 
@_subject: [tor-talk] Linux Kodachi 
Also enough _Linux_ stuff that source code must be made available.

@_date: 2013-10-28 06:30:03
@_author: Seth David Schoen 
@_subject: [tor-talk] 
=?utf-8?q?First_PRISM-Proof_Tor_Server_in_Public_Cloud?=
Preventing the provider from viewing the virtual CPU's state is the
main goal of their PrivateCore software.  They encrypt the RAM that
contains the VM and they try to ensure that the key used to encrypt
it never leaves the CPU and that the providers don't get to see that
Evidently right now they use a TPM for bootstrapping, so the weak link
is probably the TPM: the provider could try to reboot the host while
attacking the TPM in some way.  If they had a completely fake or cracked
TPM that other people accepted as genuine, they could try to make it
boot the PrivateCore instance itself in a (provider-controlled) VM
pretending to be native hardware.
(The other potential weak link is exploiting the OS running inside the
VM.  Then even if you don't know the crypto keys that encrypt the memory,
you can tell the OS to let you monitor its processes or disk.)
There should be at least a brief discussion of this in the liberationtech

@_date: 2014-04-12 12:04:14
@_author: Seth David Schoen 
@_subject: [tor-talk] Does Tor need to be recompiled *after* the 
The dynamic versus static linking question is about whether the code
from OpenSSL was permanently included into your Tor binary (static
linking) or whether it gets loaded from a separate OpenSSL library
file every time you start Tor (dynamic linking).  In the static
linking case, the vulnerability can't be fixed without getting a
totally new Tor binary, because the vulnerable code is built into the
Tor binary itself.  In the dynamic linking case, the vulnerability
can be fixed by replacing the OpenSSL system libraries, because then
the new, safer ones will be used when Tor is started.
The ldd output above shows dynamic linking, meaning that the system
version of OpenSSL is being used, meaning that having upgraded the
OpenSSL libraries should be enough to make Tor safer against the
recently discovered problems without recompiling Tor.

@_date: 2014-04-17 16:58:22
@_author: Seth David Schoen 
@_subject: [tor-talk] O futuro da internet 
Bom dia,
O idioma da lista tor-talk ? ingl?s e acho que os assinantes, no geral,
n?o aceitar?o discuss?es em outros idiomas aqui.
O nome "Freenet" j? ? usado por outro projeto (desde 2000) cujo
funcionalidade principal tem a ver com o compartilhamento an?nimo
de arquivos.
Por isso, aconselho escolher outro nome para seu projeto para as pessoas
n?o se confundirem.

@_date: 2014-08-28 12:59:16
@_author: Seth David Schoen 
@_subject: [tor-talk] I have a quick question about security of tor with 3 
The link there to the threat model discussion is broken.  A link that
works is
Historically, this is one of the most common questions about Tor.
There's evidence that some people have successfully deanonymized some Tor
users, but I don't know of evidence that this has been done by tracing
each individual hop of the path (tracing the users "through" each relay
in turn) or that there's a case where that would be the easiest way to
deanonymize a user.
I guess it's possible that that would be the easiest way if _all three_
relays are malicious and are working together; the problem with trying to
add more relays as a response to that is that the Tor design has assumed,
seemingly correctly, that having just a malicious entry and exit relay
that are working together is enough to deanonymous a user in practice.
Adding more middle relays can't affect the probability of that situation.

@_date: 2014-12-12 12:57:01
@_author: Seth David Schoen 
@_subject: [tor-talk] CA signed SSL bad for censorship resistance? 
There's definitely a privacy issue about some sites because some
browsers may contact the CA's OCSP responder (mentioning which cert
they've just encountered).
The Tor Browser design document currently says
   We have verified that these settings and patches properly proxy HTTPS,
   OCSP, HTTP, FTP, gopher (now defunct), DNS, SafeBrowsing Queries,
   all JavaScript activity, including HTML5 audio and video objects,
   addon updates, wifi geolocation queries, searchbox queries, XPCOM
   addon HTTPS/HTTP activity, WebSockets, and live bookmark updates. We
   have also verified that IPv6 connections are not attempted, through
   the proxy or otherwise (Tor does not yet support IPv6). We have also
   verified that external protocol helpers, such as smb urls and other
   custom protocol handlers are all blocked.
So, when OCSP queries to the CA happen, they should also be sent over Tor.
Sites can help reduce the incidence of OCSP queries by implementing OCSP

@_date: 2014-12-22 11:02:18
@_author: Seth David Schoen 
@_subject: [tor-talk] Anonbib November papers without papers 
Historically I thought mostly Roger Dingledine and Nick Mathewson; it
looks like there have been a number of other contributions over the
years, though!

@_date: 2014-12-25 12:43:43
@_author: Seth David Schoen 
@_subject: [tor-talk] All I Want For X-mas: TorPhone 
There might already be a tablet out there somewhere that's suitable
for conversion to meet some of these suggestions (since there have been
plenty of them with no GSM interface at all).  One thing to investigate
is whether the wifi MAC address can be changed and how persistent the
changes are.
I'm also wondering if some of the Tor developers could give an update
on the issue about identifying people from their guard node selection
as they roam from one network to another.  Was that a motivation for
the decision to reduce the number of guard nodes, and has that change
happened yet?  Does someone have an estimate of the anonymity set size
if you notice that a mobile Tor user is using a particular guard node?

@_date: 2014-12-26 16:09:48
@_author: Seth David Schoen 
@_subject: [tor-talk] All I Want For X-mas: TorPhone 
Hi Spencer,
The MAC address, at least, is a very important issue if you actually
want users to have location privacy with the device.  One of the most
important ways that governments and companies track physical locations
today is by recognizing individual devices as they connect to networks
(or, with some versions of some technologies, when the devices announce
themselves while searching for networks).  If the device itself has a
recognizable physical address that a network operator or just someone
listening with an antenna can notice, that is a tracking mechanism --
and not a theoretical tracking mechanism but one that's been reduced to
practice by advertisers, hotspot operators, and governments.
Depending on what kind of privacy you're looking for, using Tor in this
scenario might not help much, because other people can still tell where
"you" are (at least a particular device!), and, depending on the scope of
the trackers' view of things, may be able to go on to make a connection
between "your device using Tor today over here" and "your device using
Tor next week over there".  In that case, the users of such devices
don't get the level of blending-into-a-crowd they might expect.
One privacy property you might want as a user of such a device is that
when you get online from a particular network, other people on that
network don't know it's you, but just see that some non-specific user of
the TorPhone is now on the network.  Without solving the MAC address
issue, and possibly some other related issues, you won't get that
property, even if the device is totally great in other ways.
The guard nodes historically may have constituted a similar problem
("oh, it's the Tor user who likes to go through nodes x, y, and z, not
the other Tor user who likes to go through w, x, and y, or the other
other Tor user who likes to go through p, q, and x").
A more general point is that someone who's trying to track you may use
_any_ available observable thing about you, your devices, your behavior,
and so on.  That's why really making users less distinguishable calls
for a lot of careful thinking and a lot of hard work, like in
If you're talking about making a whole device like a phone, a lot of
that process has to be repeated, over and over again, to have a hope of
getting really strong privacy properties.  (Some people trying to make
Tor-centric operating systems like Whonix and Tails have definitely been
thinking about these problems at the operating system level, but they're
currently targeting laptops rather than phones.  And yes, they do worry
about the wifi MAC address!)

@_date: 2014-01-07 18:23:08
@_author: Seth David Schoen 
@_subject: [tor-talk] !!! Important please read. !!! 
Hi Shadowman,
A lot of people have been talking about this lately, and it's true that
the current system is problematic for these reasons.  Fortunately, there
are a number of mechanisms that are trying to address the problems
and reduce vulnerabilities.  Among others, you might be interested in
There are also things to try to create more visibility for end-users who
are willing to scrutinize the certs they're given, like
Many of these things still need some work, so there are a lot of
opportunities for people to contribute to them.  They each have the
effect of making certificate authorities more accountable or less
absolutely trusted in some way, and increase the chance that misissued
certs will be detected, publicized, revoked, and investigated.
Unfortunately, we don't know who runs the exit nodes, and some of them
may be malicious.  If a malicious exit node is running an "exit browser"
for you, it could design the exit browser to do bad stuff to you instead
of good stuff to you.  Instead of removing "rubbish", it could actually
add it, or in any case deliberately fail to protect you from it.  I
don't think the Tor developers would think it was safe to increase the
amount of trust that Tor users have to have in the exit node operators.
If anything, the trend has been to try to reduce it -- which is one of
the reasons that we and the Tor Project have created HTTPS Everywhere:
it reduces the scope of exit node operators' ability to see and tamper
with your browsing sessions.

@_date: 2014-01-08 21:03:16
@_author: Seth David Schoen 
@_subject: [tor-talk] 1) Torproject certificate, 
4) I am working on something similar to Tor
If this were done to all connections, it would be noticed very quickly.
The browser sees the presented certificate and can log it and perform
other analysis.  The default behavior of most browsers is not to warn
the user, provided the cert appears valid.  But there are some users
who are using browsers and clients that have other behaviors and will,
for example,
- compare the cert to the certs seen by other users, or
- automatically log the cert, or
- automatically send a copy of the cert to third parties, or
- notify the user if the cert is different from the previously
  observed cert for this server, or
- notify the user if the cert is different from values this server
  told the client to expect, or
- notify the user if the cert is different from values that the
  client was told to expect by the original software developer
In addition, some people are running bots that check the certs that
appear to be presented when HTTPS sites are accessed over Tor, and
compare these to the certs that appear to be presented when these
sites are access directly.
Collectively, these kinds of mechanisms mean that a wide-scale and
indiscriminate attack using fake certs would probably not stay
undetected for very long.  Hopefully, the probability of detecting
such an attack quickly will go up over time as more users adopt
software that has new mechanisms like this.
That does _not_ mean that these attacks never occur or don't succeed
against some users, just that they probably aren't occurring against
the general public (or the general population of Tor users, at least
not for the most popular HTTPS sites).  But we can always do more to
try to detect attacks.

@_date: 2014-01-12 17:32:34
@_author: Seth David Schoen 
@_subject: [tor-talk] [IDEA] Google App Engine, Tor Hidden Service Viewer 
I think the tor2web service provides an equivalent to your suggestions.

@_date: 2014-01-13 09:53:40
@_author: Seth David Schoen 
@_subject: [tor-talk] [IDEA] Google App Engine, Tor Hidden Service Viewer 
You might have a different experience with a web-to-web proxy and a
web-to-Tor-hidden-services proxy.  The first allows people to reach
things that they already could with their web browser (unless they
have a censored connection), maybe with a little more anonymity from
their local network and less anonymity from you and Google.  The
second allows people to reach things that they would otherwise only
have been able to reach with Tor Browser.
Tor2web also allows linking to and spidering of hidden service sites,
which you might or might not intend to allow.  In that configuration,
it's hard for some people to understand that the proxy isn't actually
hosting the hidden sites.  They don't realize that it's a proxy to
something that exists elsewhere.

@_date: 2014-07-01 23:39:51
@_author: Seth David Schoen 
@_subject: [tor-talk] How to identify owners of .onion services? 
One avenue of attack would be the channel of communication that that
person uses to administer the server.  For example, they might use ssh
over Tor to log in to administer it.  A very powerful adversary, or an
adversary who was already watching a particular user and a particular
server or hosting facility, could try to associate these traffic flows.
Another avenue would be trying to deanonymize the payments.  Bitcoin has
some risks for users' anonymity, including observing the IP address
that relayed a transaction, and trying to trace the payment history of
particular coins backwards to learn where they previously came from.
There's been a fair amount of research interest in trying to find
the physical server that corresponds to a particular hidden service.
There are a lot of ideas for that; some of them involve generating
distinctive traffic to the hidden service and seeing if similar traffic
emerges somewhere on the Internet, or trying to attack or disrupt
different physical-world hosting facilities to see which attacks cause
disruption for the reachability of the hidden service.  (The adversary
can also operate Tor nodes and hope to be chosen as an entry node by
the hidden service.)  In the scenario you asked about, though, the
adversary might possibly already know where the hidden service's server
equipment is physically located and just be unsure where it was being
administrated from.

@_date: 2014-07-02 19:59:16
@_author: Seth David Schoen 
@_subject: [tor-talk] (no subject) 
Are you talking about seeing a markmonitor.com rule in the HTTPS Everywhere
Enable/Disable Rules menu?
If so, this is one of thousands of HTTPS Everywhere rewrite rules that
are included with HTTPS Everywhere, which is included with the Tor
Browser Bundle.  The goal of HTTPS Everywhere and its rewrite rules
is to automatically access as many sites as possible with secure HTTPS
HTTPS Everywhere typically does not make your browser access sites or
services that it would not otherwise have accessed, so it shouldn't help
sites monitor your web browsing if they would otherwise not have been
able to.  There are definitely lots of sites that can monitor some aspects
of your web browsing because the site operator has included content loaded
from those sites in their web page (so your browser automatically retrieves
that content when you visit the page that embedded the content).  For
example, there are ad networks whose ads are embedded in thousands or
millions of different sites, and if you visit any of those sites without
blocking those ads, the ad network operator will get some information
about your visit when your browser loads the embedded content from those
The "monitor" in the name of markmonitor is not a reference to monitoring
users' web browsing.  Instead, it's part of the name of the company
MarkMonitor, a subsidiary of Thomson Reuters, that provides certain
Internet services mostly to very large companies.
Their name is supposed to suggest that they can "monitor" their clients'
trademarks, but not specifically by spying on Internet (or Tor) users'
web browsing.  It seems that one of their original lines of business was
letting companies know about trademark infringement on web sites, so that
MarkMonitor's customers could threaten to sue those web sites' operators.
They subsequently went into other more infrastructural lines of business.
There was an article a few years ago criticizing the large amount of
power that MarkMonitor has, but most of that power seems to have arisen
mainly because it's an infrastructure provider that some very popular
sites decided to sign up with for various purposes (primarily to register
Internet domain names, because MarkMonitor's domain name registration
services make it extremely difficult for somebody else to take over
control of a domain name illicitly).
The markmonitor.com HTTPS Everywhere rule is one of thousands of HTTPS
Everywhere rules, and its goal is solely to make sure that if you're
visiting a web page hosted at (or loading content from) markmonitor.com
itself, that your browser's connection to markmonitor.com's servers will
be a secure HTTPS connection instead of an insecure HTTP connection.  It
is not trying to give any additional information to those servers or to
cause your browser to connect to those servers when it would not
otherwise have done so.
(You can see the rule itself in the atlas link toward the beginning of
my message, and see that its effect is to rewrite some http:// links into
corresponding https:// links, just like other HTTPS Everywhere rules do.)
Having HTTPS Everywhere rules that relate to a site does not necessarily
mean that your browser has ever visited that site or will ever visit
that site.  We've tried to make this clear because many of the rules
do relate to controversial or unpopular sites, or sites that somebody
could disagree with or be unhappy about in some way.  Each rule just
tries to make your connection more secure if and when you as the end
user of HTTPS Everywhere decide to visit a site that loads content from
the servers in question.
You can disable the markmonitor.com HTTPS Everywhere rule from within the
Enable/Disable Rules menu -- but that won't stop your web browser from
loading things from markmonitor.com's servers if and when you visit pages
that refer to content that's hosted on those servers.  It will just stop
HTTPS Eveyrwhere from rewriting that access to take place over HTTPS URLs.

@_date: 2014-07-03 10:47:30
@_author: Seth David Schoen 
@_subject: [tor-talk] according to leaked XKeyScore source NSA marks all 
Does anyone have theories about this part right at the bottom?
   /**
    * Placeholder fingerprint for Tor hidden service addresses.
    * Real fingerpritns will be fired by the plugins
    *   'anonymizer/tor/plugin/onion/*'
    */
   fingerprint('anonymizer/tor/hiddenservice/address') = nil;
   // END_DEFINITION
Does this suggest anything interesting about the ability to determine
either the physical location of a hidden service's service or instances
of people accessing a hidden service?
I also think that it's interesting that there's a category called
"documents/comsec", so the bigger picture is that there's an organized
way to find out about people who are interested in or becoming educated
about COMSEC.  It seems conceivable that documentation that I and other
people here have helped write is a part of other "documents/comsec"

@_date: 2014-07-03 13:29:11
@_author: Seth David Schoen 
@_subject: [tor-talk] Funny, but not amusing browsing 
Can you right-click on the image and Inspect Element?  If so, does it
style="background-image: url(//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);"
or the equivalent for the language version of Wikipedia that you're
Can you press Ctrl+I while visiting that page and look through the Media
list to find that image?  Can you see exactly what URL it was loaded from?

@_date: 2014-07-03 15:16:52
@_author: Seth David Schoen 
@_subject: [tor-talk] High-latency hidden services (was: Re: Secure Hidden 
That's great, but in the context of this thread I would want to imagine
a future-generation version that does a much better job of hiding who
is downloading which pages -- by high-latency mixing, like an
anonymous remailer chain.
The existing Tor network can't directly support this use case very
well, except by acting as a transport.
Right now, people who are using toolks like Pocket or Scrapbook over Tor
_aren't_ really getting the privacy benefits that in principle their
not-needing-to-read-it-right-this-second could be offering.  That is,
a global-enough adversary can sometimes notice that person X has just
downloaded item Y for offline reading.  There's no reason that the
adversary has to be able to do that.

@_date: 2014-07-03 16:58:49
@_author: Seth David Schoen 
@_subject: [tor-talk] BlackHat2014: Deanonymize Tor for $3000 
The description on the Black Hat site refers "a handful of powerful
servers and a couple gigabit links" that are operated for "a couple
of months", which sounds like this involves actually running nodes and
getting the attack targets to build circuits through them.

@_date: 2014-07-14 12:11:56
@_author: Seth David Schoen 
@_subject: [tor-talk] Questions about NSA monitoring of Tor users. 
It will be interesting to see what ISPs do with IPv6 assignment policies
and how much they can be influenced about this.
I was thinking of writing a blog post describing how, depending on
what the ISPs do about this, IPv6 could be drastically better than IPv4
for user privacy, or drastically worse.  After all, an end-user could
get anything from an unrecognizably different IPv6 address _per-TCP
connection_ to a single globally unique IPv6 address _per-device
lifetime_.  The latter was originally seriously proposed as the default
way of assigning IPv6 addresses because it would make some kinds of
roaming easier -- using the device EUI-64 directly in the address.
When talking to journalists about IP geolocation recently, I've been
using the example of Skyhook Wireless.
They're combining observations from multiple sources, including queries
from mobile devices that are connected to wifi networks, to build
associations between locations and IP addresses that may be down to the
building level.  (Google and Microsoft, at least, also have device
positioning services that make similar kinds of observations.)

@_date: 2014-07-24 11:04:40
@_author: Seth David Schoen 
@_subject: [tor-talk] ISP surveillance. 
Oi Marcos,
Normally Tor doesn't try to hide the fact that you are using Tor.  So,
your ISP can see that you're using it, and when.  Tor only tries to hide
the particular details of what you are doing.
Although some Tor connections do "look like simple HTTPS" in some ways,
the connections are always made to the IP addresses of Tor nodes, and
the complete list of those addresses is openly published.  So it's easy
for the ISP to notice that you're using Tor, and some firewalls and
kinds of surveillance equipment can be programmed to detect Tor use if
the person operating them cares about it.
There are other methods to try to hide the fact that you're using Tor,
especially meant for people on networks that block Tor.  The main method
of doing this is called bridges, which you can read more about on the
Tor web site.
Most people who use bridges are on networks where Tor is blocked
completely, so they have a very practical reason to try to hide the fact
that they're using Tor.
One of the benefits of Tails is that it will send all of your
communications over Tor.  So, if you believe that Tor is appropriate to
protect you in a particular situation, you can get that protection
automatically when you are using Tails.  Your ISP will not directly see
what you do, although someone who can see both ends of the connection
can try to use information about the time of the connection to identify
Torsocks and configuring Tor to use a proxy are not very relevant to Tails
users.  Torsocks has to do with getting other applications apart from
the Tor Browser to communicate over Tor (which Tails does
automatically!), while configuring Tor to use a proxy is mostly relevant
if you're behind a firewall which doesn't allow direct Internet
connections.  (Sometimes it's an alternative to bridges, but it may not
be a particularly strong way of hiding your activity from your ISP --
it doesn't add any additional encryption or obfuscation.)

@_date: 2014-07-29 12:54:14
@_author: Seth David Schoen 
@_subject: [tor-talk] Spoofing a browser profile to prevent fingerprinting 
Strictly speaking, the 33 bits figure refers to identifying a _person_,
and comes from Arvind Narayanan, who calculated it by rounding down the
base 2 logarithm of the world's human population.  (If you can ask
33 perfectly independent and identically distributed yes-or-no questions
about a person, the set of answers to those questions will be completely
There are probably fewer Internet-connected browser instances than
living people, so less information might suffice to distinguish them.
If you're using EFF's Panopticlick page, you should be aware of some
limitations about the measurements it gives you.  One is that it doesn't
measure all possible measurable attributes of a browser -- people doing
user tracking may have additional measurement techniques that aren't
included in Panopticlick.  Another is that the "bits" of information
that you get from measuring each attribute don't actually add linearly
(and there's no direct way of adding them without knowing more about
the population statistics and how the attributes interact).  So if you
get an estimate that your Foo browser feature contributes 6 bits of
identifiability and your Bar browser features contributes 5 bits, you
can't necessarily conclude that together they contribute 11 bits.
(Another limitation that Peter Eckersley, the developer of Panopticlick,
pointed out to me is that the sample of fingerprints in Panopticlick's
database isn't very current or very representative of a larger population
of user-agents that are getting used in 2014.)
You're definitely right that Javascript is an important part of many
browser fingerprinting techniques and that browser fingerprinting will
work much less well without it.

@_date: 2014-07-29 14:28:09
@_author: Seth David Schoen 
@_subject: [tor-talk] Spoofing a browser profile to prevent fingerprinting 
It means that if you go to site A today, and site B next week, the site
operators (or the exit node operators, or people spying on the network
links between the exit nodes and the sites) might realize that you're
the same person, even though you took mostly or completely separate paths
through the Tor network and were using Tor on totally different occasions.
There are several ways of looking at why this is a privacy problem.
One is just to say that there's less uncertainty about who you are,
because even if there are lots of site A users and lots of site B users,
there might not be that many people who use both.  Another is that you
might have revealed something about your offline identity to one of the
sites (for example, some people log in to a Twitter account from Tor
just to hide their physical location, but put their real name into their
Twitter profile) but not to the other.  If you told site A who you are,
now there's a possible path for site B to realize who you are, too, if
the sites or people spying on the sites cooperate sufficiently.
In terms of identifying your real-world IP address, it provides more
data points that people can try to feed into their observations.  For
example, if someone is doing pretty course-grained monitoring ("who
was using Tor at all during this hour?") rather than fine-grained
monitoring ("exactly what times were packets sent into the Tor network,
and how many packets, and how big were they?"), having a link between
one time that you used Tor and another time that you used Tor would be
useful for eliminating some candidate users from the course-grained
For instance, suppose that you went to site A at 16:00 one day and to
site B at 20:00 the following day.  If site A and site B (or people
spying on them) can realize that you're actually the same person through
browser fingerprinting methods, then if someone has an approximate
observation that you were using Tor at both of those times, it becomes
much more likely that you are the person in question who was using the
two sites.  Whereas if the observations are taken separately (without
knowing whether the site A user and the site B user are the same person
or not), they could have less confirmatory power.

@_date: 2014-07-29 20:21:43
@_author: Seth David Schoen 
@_subject: [tor-talk] Spoofing a browser profile to prevent fingerprinting 
Yes, ultimately to make the numbers be meaningful in this sense,
they'd need to measure everything that's realistically measureable by
an adversary, and then they would need a current representative sample
of browsers (or of Tor Browser configurations).

@_date: 2014-07-29 20:31:46
@_author: Seth David Schoen 
@_subject: [tor-talk] Spoofing a browser profile to prevent fingerprinting 
Yes!  But other kinds of fingerprinting could drastically reduce the
fine-grainedness of the observations that you need in order to do a
traffic confirmation-style attack.  Instead of sub-second packet timings
or complete circuit flow volumes or whatever, you might be able to say
something like "what approximate times of day on which days was this
person using Tor at all"?
It might be interesting to think about this in terms of a paper like
"Users Get Routed" -- trying to expand understanding of the risk of
attacks, as the authors of that paper say, of "user behavior" when we
include (1) browser fingerprinting risks in relation to user behavior,
and (2) relatively limited adversaries, including some who didn't have
deanonymizing Tor users as a primary goal.
The Harvard bomb threat case, as I understand it, shows a specific
example of deanonymizing a Tor user by an adversary (Harvard's network
administrators) who did retain some data partly in order to reduce network
users' anonymity, but who didn't seem to have had a prior goal of breaking
Tor anonymity in particular.  And the data that they apparently retained
was more course-grained than what would be ideal for traffic confirmation
attacks in general.
I don't mean that the Harvard case involved browser fingerprinting
at all.  I guess I just mean that browser fingerprinting's relevance
to Tor anonymity might include increasing the information available to
limited network adversaries.

@_date: 2014-07-31 13:58:18
@_author: Seth David Schoen 
@_subject: [tor-talk] Why make bad-relays a closed mailing list? 
I think the remedy is ultimately HTTPS everywhere.  Then the problem
is reduced to checking whether particular exits try to tamper with the
reliability or capacity of flows to particular sites, or with the public
keys that those sites present.  (And figuring out whether HTTPS and its
implementations are cryptographically sound.)
The arms race of "we don't really have any idea what constitutes correct
behavior for these vast number of sites that we have no relationship
with, but we want to detect when an adversary tampers with anybody's
interactions with them" seems totally untenable, for exactly the reasons
that you've described.  But detecting whether intermediaries are allowing
correctly-authenticated connections to endpoints is almost tenable,
even without relationships with those endpoints.
(I do think that continuing to work on the untenable secret scanning
methods is great, because attackers should know that they may get caught.
It's a valuable area of "impossible" research.)
Yan has just added an "HTTP nowhere" option to HTTPS Everywhere, which
prevents a browser from making any HTTP connections at all.  Right now
that would probably be quite annoying and confusing to Tor Browser users,
but maybe with some progress on various fronts it could become less so.

@_date: 2014-06-28 16:36:40
@_author: Seth David Schoen 
@_subject: [tor-talk] Bruce Schneier's Guardian Article about N_S_A and 
Tor is preventing the user from being identified by their (true) source
IP address.  In the hypothesis of the article, there's sometimes another
way to identify the user, for example because they've logged into a
(non-TLS) service using a particular username and password, or because
they sent a particular cookie.
The materials that Schneier is reporting on use a very broad notion of a
"selector" -- a way of referring to a particular user or device or
network in order to associate network traffic with them.  One of the
most fundamental selectors on the Internet is someone's source IP
address, which Tor obfuscates.  The Tor Browser also tries not to have
any persistently distinguishable features between one user's traffic
and another's (unlike a normal desktop web browser!), but a user's
particular behavior could still provide ways of identifying them and
distinguishing them from other users.

@_date: 2014-06-29 17:58:31
@_author: Seth David Schoen 
@_subject: [tor-talk] High-latency hidden services (was: Re: Secure Hidden 
I wonder if there's a way to retrofit high-latency hidden services
onto Tor -- much as Pond does, but for applications other than Pond's
messaging application.
For example, maybe there's a way for a hidden service to define an
asynchronous API through which client software can use the service,
and then have some kind of pool of API requests and API replies which
the server can update via asynchronous polling (much as Pond does with
pools of user-to-user messages).
This could conceivably have much better traffic analysis resistance,
maybe even against a global adversary.
Then a question is whether users would want to use a service that takes,
say, several hours to act on or answer their queries (and whether the
amount of padding data required to thwart end-to-end traffic analysis
is acceptable).
One important problem is what counts as "thwarting end-to-end traffic
analysis".  Right now with Pond, the goal is to prevent anyone from
knowing which Pond users communicate with which other Pond users and
when, not necessarily prevent them from knowing who is a Pond user.
(It seems like the Tor traffic generated by a Pond user's client would
be recognizable as coming from Pond.)  On the other hand, the services
that I'm thinking of should prevent a passive adversary from knowing
who is using _which services_, including locating the infrastructure of
those services.  In my analogy, the system should conceal the fact that a
particular person is running Pond on their home network, and also conceal
which servers are providing infrastructure for Pond communications.
Satisfying this property might have implausibly high padding

@_date: 2014-03-18 23:32:19
@_author: Seth David Schoen 
@_subject: [tor-talk] tor and isp 
Tor hides both.
Under some assumptions, Tor can do that.  The ISP will still know that
you are using Tor, and they can see the time and amount of data that you
transmit.  If there's a particular person that they're looking for, they
might be able to guess whether that person was or wasn't you based on
that timing information (for example, the person who sent a certain
e-mail or made a certain forum post at a particular time).
This is especially true if you use Tor over a long period of time and
there are many different "events" that they specifically want to find
the person responsible for, or if you are one of a tiny number of people
who could plausibly have been responsible.
Yes, Tor does that, with the limitations described above.
You might find this diagram helpful.  It doesn't address every
conceivable threat but it shows basically what different entities can
see directly when you use Tor (with or without using an HTTPS connection
to a web site that you're accessing).
MAC address is normally not sent over the Internet beyond the first
router that forwards your communications.  If you are using a wifi
device in a public place, your MAC address can be observed wirelessly
by anyone physically nearby you (including the wifi network operator
or anybody else).  This can be used as a location tracking method.
Tor doesn't directly affect the question of MAC address privacy.  It
might affect whether someone trying to locate you has _other_ data
points suggesting your location, which they could then try to compare
with observations of particular MAC addresses.
If you're using Tor on, say, a wifi network run by a large national
cellular company, and the cellular company knows your device's MAC
address, they will be able to detect your location when that device
associates with a particular wifi base station, and Tor won't prevent
that at all.  The same is true of anybody else who is monitoring MAC
addresses with physical sensors.  In the U.S., for example, some
"location analytics" companies have started to do that for commercial
Some privacy-oriented operating systems that run from CD-ROM, like
Tails, have features to change the MAC address in order to make the
particular device less recognizable, even by the local area network.
The cell phone provider can _ALWAYS_ detect your location if you're
using the cellular network for any purpose, including a voice call,
including if your device is registered for emergency calls only (without
a SIM card installed).  3G and GPS are _not_ necessary for the cell phone
network to detect a device's physical location.  The cell phone has a
unique hardware address (IMEI for GSM networks) and its position can be
detected by the network using triangulation.  This was always true in
principle for every generation of GSM and doesn't rely on newer
technologies.  The ability to locate your device doesn't depend on
position reports from your device, but can be done solely by looking at
the radio signals your device sends when registering on the network.

@_date: 2014-05-21 12:48:23
@_author: Seth David Schoen 
@_subject: [tor-talk] 
=?utf-8?q?ion_of_information_leaks?=
Wow, what an honor!  It's great that people are continuing to find this
so useful.

@_date: 2014-11-12 21:32:02
@_author: Seth David Schoen 
@_subject: [tor-talk] "Hidden Services" vs "Onion services" 
It could be technically consistent to say both "hidden services" and
"onion sites" -- you could say that onion sites are web sites that are
served as hidden services.

@_date: 2014-11-17 11:42:08
@_author: Seth David Schoen 
@_subject: [tor-talk] Hiden service and session integrity 
If you mean end-to-end encrypted to a hidden service, there is a problem
in that most certificate authorities won't issue a certificate for
a .onion hostname today.  That means that the Tor Browser will give a
certificate warning when users navigate to the hidden service via HTTPS,
because the service won't be able to present a certificate that the
browser will accept.  They can still use HTTPS, but they might develop
a risky habit of ignoring or bypassing certificate warnings (which is
riskier when using the Tor Browser to visit an HTTPS site on the public
Internet, since the warning could indicate an attack from the exit node,
a situation which is far less plausible with hidden services).
There was recently a cert issued to Facebook for a .onion name, but
it's not clear when this kind of cert will be easily available to the
general public.

@_date: 2014-10-06 17:06:59
@_author: Seth David Schoen 
@_subject: [tor-talk] isp monitoring tor 
To expand on this theme, there are several traffic attacks that don't
require an adversary to be truly "global".  Creating a popular relay in
the hope that users who are interesting to you will route through it is a
pretty cheap and powerful attack (and one that motivated the creation of
guard nodes).  And there can be timing attacks just based on (sometimes
rather coarse-grained) knowledge of when a particular anonymous user was
active, which might even come from chat or server logs rather than from
monitoring live network traffic, so long as the attacker does have the
ability to monitor the first hop.
I've taken to saying "someone who can observe both ends" most of the time
instead of "the global adversary".  (I think the Tor developers often say
this too; the global adversary is just someone who can _almost always_
observe both ends.)  A kind of challenging wrinkle is that there are
a lot of conceivable ways that someone could "observe" one end of the
connection.  One sometimes underappreciated way is that someone else who
was observing it at the time of the communication, including a party to
the communication or a server operator, could tell the adversary about
it later.

@_date: 2014-10-09 10:40:15
@_author: Seth David Schoen 
@_subject: [tor-talk] (no subject) 
Unfortunately you sent this to a public discussion list for talking
about Tor, which isn't the right address for requesting bridges.
The right place to send that request is .
If you do that and your bridges don't work, you can also try other
resources at
Good luck!

@_date: 2014-10-23 11:47:39
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor in other software 
There was just a new article suggesting that using Tor can be
counterproductive for Bitcoin:
There's an older article suggesting that it's also a problem for
Maybe the lesson of this is that applications starting with "Bit-"
have anonymity risks from using Tor. :-)
More seriously, the Tor Project has traditionally encouraged people
to make various things run over Tor, and there are definitely things
that run over Tor other than web browsing, including TorBirdy, Pond,
and OnionShare (which is sort of web browsing).
I think it would be great if someone who's read both the "Bad Apple" and
the "Bitcoin over Tor" papers could explain if there are any generalizable
lessons about exactly what makes it risky to run a particular service
over Tor.  Maybe that could help future developers make better choices
about how to use Tor.

@_date: 2014-10-27 16:19:08
@_author: Seth David Schoen 
@_subject: [tor-talk] 
=?utf-8?q?=28Alex_Biryukov_/_Ivan_Pustogarov_story=29?=
I don't mean to disparage the contribution of people who are running
Bitcoin hidden service nodes.  I think that's a very useful
I do want to question three things about the benefits of doing so.
First, the security of hidden services among other things relies on the
difficulty of an 80-bit partial hash collision; even without any new
mathematical insight, that isn't regarded by NIST as an adequate hash
length for use past 2010.  (There has been some mathematical insight about
attacking SHA-1, which Tor hidden service names use, although I don't
remember whether any of it is known to be useful for generating partial
preimages.)  Tor hidden service encryption doesn't consistently use crypto
primitives that are as strong as current recommendations, though I think
they matched recommendations when the Tor hidden service protocol was
first invented.  That means that the transport encryption between a hidden
service user and the hidden service operator is not as trustworthy in
some ways as a modern TLS implementation would be.
Second, a passive attacker might be able to distinguish Bitcoin from other
protocols running over Tor by pure traffic analysis methods.  If a new
user were downloading the entire blockchain from scratch, there would
be a very characteristic and predictable amount of data that that user
downloads over Tor (namely, the current size of the entire blockchain --
23394 megabytes as of today).
Not many files are exactly that size, so it's a fairly strong guess that
that's what the user was downloading.  Even submitting new transactions
over hidden services might not be very similar to, say, web browsing,
which is a more typical use of Tor.  The amount of data sent when
submitting transactions is comparatively tiny, while blockchain updates
are comparatively large but aren't necessarily synchronized to occur
immediately after transaction submissions.  So maybe there's a distinctive
statistical signature observable from the way that the Bitcoin client
submits transactions over Tor.  It would at least be worth studying
whether this is so (especially because, if it is, someone who observes
a particular Tor user apparently submitting a transaction could try to
correlate that transaction with new transactions that the hidden services
first appeared to become aware of right around the same time).
Third, to take a simpler version of the attacks proposed in the new
paper, someone who _only_ uses Bitcoin peers that are all run by
TheCthulhu is vulnerable to double-spending attacks, and even more
devious attacks, by TheCthulhu.  (You might say that TheCthulhu is
very trustworthy and would never attack users, but that does at least
undermine the decentralization typically claimed for Bitcoin because
you have to trust a particular hidden service operator, or relatively
small community of hidden service operators, not to attack you by
manipulating your view of the blockchain and transaction history.)
Using Bitcoin over Tor hidden services might be a good choice for most
users today who want their transactions and private key ownership to
be as private as possible, but it's not free of risk, and it's probably
not an appropriate long-term solution to recommend to the general public
without fixes to some of the technologies involved!

@_date: 2014-10-30 09:30:32
@_author: Seth David Schoen 
@_subject: [tor-talk] 
=?utf-8?q?=28Alex_Biryukov_/_Ivan_Pustogarov_story=29?=
I meant this only as a response to the previous poster's remark that
  Hidden services are end-to-end encrypted so the risk of MITM between
  nodes does not exist.
I think the risk of MITM between nodes is extremely small.  But 80-bit
partial preimages are still a disturbingly small safety margin today.
It's kind of comparing apples to kumquats, but the current Bitcoin network
as a whole has a hashrate that (if it were instead testing SHA1 hashes
of RSA-1024 keys) could find such a partial preimage in 25 days, on
While replicating the hashing power of the entire Bitcoin network is
a truly staggering cost (to say nothing of the associated power bill),
that isn't where I'd like to see the order of magnitude comparisons.
Well, here, I was responding to the previous poster's claim that "nobody
listening your wire can have a slight clue that you use bitcoin".
Cool, that sounds like a great area for an enterprising researcher
to investigate.
Do you have a sense of how it would affect payees' concerns about
double-spending attacks if the latency of transaction propagation were
increased?  Is the idea that privacy-enhancing latency additions are
only meant for cases where receivers are already waiting for multiple
That's a clever idea.
Tor's directory decentralization needs a lot of work, but the most
practical sybil attack in Tor is just the original -- making a lot of
compromised nodes and hoping a user will repeatedly pick them.  It
still seems pretty clear that this can work against many randomly
selected Tor users at comparatively low cost (I'd agree that that's
cheaper than attacking randomly-selected Bitcoin-over-Tor users with
Bitcoin-related attacks, for the reasons you mention).  On the other
hand, I don't think we have a clear path for a would-be Tor directory /
consensus subverter to follow.
I wonder if anybody is working on that on the Tor end (or whether it
has any unexpected security consequences).  I guess it would mean
that someone who can compromise even a heavily sandboxed Tor Browser
would become able to use it as a hidden service server (at least while
it was still running) without the knowledge of the browser's user.

@_date: 2014-09-17 17:07:34
@_author: Seth David Schoen 
@_subject: [tor-talk] wake up tor devs 
I don't know of a good basis for estimating what fraction of NSA's
capabilities or lack of capabilities we've learned about.  And even
when someone _working at NSA_ writes that attack X doesn't work or
doesn't exist, they may not know that attack Y achieves some of the
same goals.  For example, there were press reports that there was
some major cryptanalytic breakthrough a few years ago and that it has
far-ranging implications*.  I don't think the details have ever become
public; a best-case-for-cryptographic-privacy scenario might be that it's
"only" an operationalized, albeit expensive, attack against 1024-bit RSA
or DH (one of the possibilities considered in Matthew Green's analysis).
In any case, many people working on surveillance within NSA might not know
what the breakthrough is or how it works, and may still be assiduously
working on attacks that in principle are largely redundant with it.
(Their NSA colleagues may want them to be working on redundant attacks
because many of the existing attacks are described as "fragile" -- so
they want to have parallel ways to achieve some of the same stuff.)
Most of us don't work in highly compartmentalized organizations or
organizations that try to practice a very strict need-to-know rule.
So we might think that if someone in an organization says at some time
that something is easy, or difficult, or cheap, or expensive, that that
reflects the general attitude of all the parts of that organization.
(Like if somebody working at Intel said it was hard to fabricate
semiconductor devices in a particular way, or somebody working at Boeing
said it was hard to take advantage of a particular aerodynamic effect,
or somebody working at EFF said it was hard to sue the government under
a particular legal theory, you might tend to think these things were
basically true, as far as those people's colleagues knew.)
I think that's only approximately or indirectly true of people working
in an organization like NSA or GCHQ.
* Possibly relevant reporting and discussion includes
          (including claims of widespread success at defeating cryptography,
  partly on the basis of sabotaging it but at least partly on the
  basis of "development of advanced mathematical techniques")

@_date: 2015-04-22 21:20:22
@_author: Seth David Schoen 
@_subject: [tor-talk] SIGAINT email service targeted by 70 bad exit nodes 
What's more, you can conceivably detect the bad CAs through your own
scans or tests (if your scans can find widespread BadExits, they could
equally find widespread bad CAs whose certs are fraudulently presented
by those same BadExits).  You could also use HPKP pinning with the
report-uri mechanism to have clients tell you when they encounter fake
keys, although it's not clear that you can get a lot of benefit from
that in the default Tor Browser.
People are _very_ interested in knowing about compromised CAs.  So I
encourage people not to just assume that they're numerous and not bother
to use tools to detect them. :-)

@_date: 2015-04-30 11:17:45
@_author: Seth David Schoen 
@_subject: [tor-talk] What is being detected to alert upon? 
Are they detecting non-public bridge traffic, or only normal entry
Detection and obfuscation is kind of a big topic that's been around for
some years, so there are a lot of possibilities.

@_date: 2015-04-30 11:53:46
@_author: Seth David Schoen 
@_subject: [tor-talk] What is being detected to alert upon? 
It's hard to know what methods they might be using without more data
about their accuracy and which bridges (and transports) they do or don't
Without a pluggable transport to obfuscate the traffic, a connection to
a Tor bridge looks kind of like regular TLS traffic.  However, there are
(or have been) particular anomalies that a network operator might look
for to try to detect Tor use.
Roger and Jacob had a presentation a few years ago about techniques that
were known to have been used to detect Tor traffic up to that point:
A notable example was the use of a particular Diffie-Hellman parameter
in the TLS session negotiation, which at least one government network
operator managed to use to detect Tor.  There may still be other things
in the TLS behavior (or other aspects of the protocol traffic, like
the size and timing of what goes over the connection after TLS session
establishment?) that are distinctive, or distinctive enough if you don't
require perfect accuracy.
Another possibility that's alluded to there is active probing -- with
traditional Tor nodes speaking the plain Tor protocol, you can connect
to a service that your network users connect to, and try speaking the
Tor protocol to it.  If it responds, it's a Tor node. :-)

@_date: 2015-08-07 20:16:16
@_author: Seth David Schoen 
@_subject: [tor-talk] General question regarding tor, ssl and .onion. 
There is an ongoing discussion about how seriously one needs HTTPS with
a .onion address.  There is already end-to-end encryption built into the
Tor hidden service design, so communications with hidden services (even
using an unencrypted application-layer protocol like HTTP) are already
A problem is that the encryption for the current generation of hidden
services is below-par, technically, in comparison to modern HTTPS in
browsers -- it uses less modern cryptographic primitives and shorter
keylengths than would be recommended for HTTPS today.  This will change
eventually with future updates to the hidden service protocol, but right
now there would be incremental cryptographic benefit from connecting to
a hidden service via HTTPS.  But the encryption from HTTPS in this case
serves the same purpose as the hidden service encryption, so you're indeed
"encrypting the encryption" when you use it.
Unfortunately, it's hard to do today because certificate authorities
are reluctant to issue certs for .onion names; the CA/Browser Forum
has allowed them to do so temporarily, but only EV certificates can
be issued, which cost money, take time, and sacrifice anonymity of the
hidden service operator.
The best-known example of a hidden service that managed to navigate the
process successfully is

@_date: 2015-08-08 14:56:55
@_author: Seth David Schoen 
@_subject: [tor-talk] General question regarding tor, ssl and .onion. 
I think it's reasonable to guess that cryptographic attacks would
be extremely expensive, so most prospective attackers today wouldn't
try them.

@_date: 2015-08-08 18:04:19
@_author: Seth David Schoen 
@_subject: [tor-talk] General question regarding tor, ssl and .onion. 
Is there a way to prevent an attacker from simply claiming the same
identifier in Namecoin before the actual hidden service operator does?

@_date: 2015-08-12 11:46:04
@_author: Seth David Schoen 
@_subject: [tor-talk] Why is my message reject at 
tor-announce isn't a discussion list and the public isn't allowed to
send messages to it.  The place where you can have public discussions
is tor-talk -- this list right here.

@_date: 2015-08-19 10:34:52
@_author: Seth David Schoen 
@_subject: [tor-talk] Letsencrypt and Tor Hidden Services 
Hi, I'm working on the Let's Encrypt project.  A difficulty to contend
with is that the certificate industry doesn't want certs to be issued
for domain names in the long term unless the names are official in
some way -- to ensure that they have an unambiguous meaning worldwide.
The theoretical risk is that someone might use a name like .onion in
another way, for example by trying to register it as a DNS TLD through
ICANN.  In that case, users might be confused because they meant to use
a name in one context but it had a different meaning that they didn't
know about in a different context.
Right now, the industry allows .onion certs temporarily, but only EV
certs, not DV certs (the kind that Let's Encrypt is going to issue),
and the approval to issue them under the current compromise is going
to expire.
It's seemed like the efforts at IETF to reserve specific "peer-to-peer
names" would be an important step in making it possible for CAs to issue
certs for these names permanently.  These efforts appeared to get somewhat
bogged down at the last IETF meeting.
(I'm hoping to write something on the EFF site about this issue, which
may have kind of far-reaching consequences.)
Anyway, I would encourage anyone who wants to work on this issue to get
in touch with Christian Grothoff, the lead author of the P2P Names draft,
and ask what the status is and how to help out.
Theoretically the Tor Browser could come up with a different optional
mechanism for ensuring the integrity of TLS connections to hidden services
(based on the idea that virtually everyone who tries to use the hidden
services is using the Tor Browser code).  I don't know whether the Tor
Browser developers currently think this is a worthwhile path.  I can
think of arguments against it -- in particular, the next generation hidden
services design will provide much better cryptographic security than the
current HS mechanism does, so maybe it should just be a higher priority
to get that rolled out, rather than trying to make up new mechanisms to
help people use TLS on hidden services.

@_date: 2015-08-19 10:38:48
@_author: Seth David Schoen 
@_subject: [tor-talk] Letsencrypt and Tor Hidden Services 
If the CA/Browser Forum agreed that it was proper to do this, we could
create a special case for requests that include a .onion name to use
a different (non-DNS) resolution mechanism, recognizing "that DNS is
not the only name resolution protocol on the Internet", as Christian
Grothoff put it.
I can't promise that Let's Encrypt would do this, but I think we would
be interested in the possibility.
In a way, the special-casing is what makes some folks in the CA/Browser
Forum nervous right now: if there's no "official" notion of the meaning
of some names, how can CAs know which names should use which resolution
mechanisms?  (For example, maybe some CAs have heard that they should
treat .onion specially, but others haven't.)  If they're unsure which
mechanisms to use, how can they know that the interpretation they give
to the names will be the same as end-users' interpretation?

@_date: 2015-08-19 12:08:22
@_author: Seth David Schoen 
@_subject: [tor-talk] Letsencrypt and Tor Hidden Services 
For the reasons described elsewhere in this thread, it's definitely
just clearnet for the foreseeable future.

@_date: 2015-08-25 13:19:50
@_author: Seth David Schoen 
@_subject: [tor-talk] I am getting European nodes only? 
The European nodes include some of the fastest nodes in the world, and
the probability of choosing a node in a path is related to how fast the
node is (you're more likely to use nodes that have more capacity than
nodes that have less capacity).
Depending on how you update, you might be using a new set of guard
nodes.  The guard nodes are chosen randomly when you first run Tor (or
a fresh copy that's not using the old configuration).  The guard nodes
affect which exit nodes your Tor client will choose because the guard
node can't also be used as an exit node.  So one possibility is that
if you have several fast European nodes as guard nodes, you'll tend to
choose other nodes as exits (relatively more likely outside of Europe),
while if you have several non-European nodes as guard nodes, you'll
tend to choose other nodes as exits (relatively more likely within
Europe, especially since that's where the fastest exits are).

@_date: 2015-12-15 08:14:27
@_author: Seth David Schoen 
@_subject: [tor-talk] Ordering a .onion EV certificate from Digitcert 
Let's Encrypt doesn't issue EV, so the CA/B Forum needs to agree that
DV certs can be issued for .onion names too (some people have suggested
that they would be called something other than "DV", but be analogous to
DV, based on proof of possession of a cryptographic key from which the
name is derived).

@_date: 2015-12-29 05:05:30
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
I would suggest looking at Tom Ritter's overview presentation about Tor.
It is very detailed.  Hopefully the technical level will be appropriate
for you and the English content will be clear.
He gives a number of discussions of limitations of Tor and possible
attacks.  There are also attacks that try to deanonymize users (finding
the true IP address of a user responsible for a circuit) or hidden
services (finding the true IP address of a server responsible for a
hidden service) under various conditions and circumstances.  This is an
ongoing area of research for academic studies, and also probably for
governments that want to identify Tor users.
Particular research on Tor has been written about on the Tor blog at
and also collected as part of the anonymity bibliography at
Of course only some of the later papers there relate to Tor, because Tor
didn't even exist at the time that the anonymity field first began! :-)
There are a lot of attacks that are effective at least some of the time.
If you look at the original Tor design paper, they assume that someone
who is watching the place where a user enters the network (the first
node in the chain, today called entry guard) and the place where the
user's communications exit the network (the exit node) will be able to
break the user's anonymity by noticing that the amount and timing of data
going in on one side matches the amount and timing of data coming out on
the other side.  This is pretty serious and has been used to deanonymize
people in real life.  Some of the research papers propose ways of trying
to deanonymize users or hidden services under more restrictive
conditions, where the attacker controls or monitors less of the network,
or controls or monitors something other than entry and exit traffic.
One issue about this is understanding what counts as a successful
attack.  I'm still concerned that Tor users may not understand the issue
presented in the original design about how someone watching both sides
can recognize them!
Another kind of attack that hasn't been discussed very much is the
idea of hacking the individual servers that provide the Tor network,
either by exploiting software vulnerabilities in the Tor server itself
or by exploiting vulnerabilities in other software that these servers
run like Linux or OpenSSH.  This sort of attack could be quite serious
if it affected many different Tor nodes at the same time, because the
nodes could be reprogrammed by the attacker to start logging data and to
cooperate to reveal users' activities.  There's no specific publicly-known
vulnerability that can be used to do this right now; an attacker would
need to find or buy knowledge of a new one (although there might be some
portion of Tor nodes that are slow to apply server software updates,
which might still be vulnerable to older software bugs or might have
stayed vulnerable for a longer period of time).
It's important to understand the difference between hidden services and
exit traffic when reading the academic research, because a lot of
research focuses on deanonymizing hidden services, which poses different
challenges from deanonymizing regular users.  Attacks against hidden
services can be quite serious, but they only represent a small fraction
of the overall use of the Tor system.

@_date: 2015-12-31 04:06:31
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
The hidden service users can be identified as users of the individual
services using the same sybil approach: if a user uses a particular
guard node and the hidden service uses a guard node controlled (or
observed) by the same entity, that entity can correlate the traffic
between the two.  I don't know how easy it is to infer right at that
moment that the communication is between a user and a hidden service
rather than between two users intermediated by something else.  However,
the attacker can potentially realize that it's a guard node for some
hidden service because a particular user connects to the guard node
all the time, has a high traffic volume, and for some hidden services,
uploads more than it downloads on average (which is the reverse of the
usual pattern for a Tor Browser user).  (That inference might be even
easier if the hidden service's guard node just notices whether that user
tends to upload a little data followed by downloading a lot of data,
or download a little data followed by uploading a lot of data, since
web browsers usually do the former and web servers usually do the latter.)
The guard node has a conceptually harder task in figuring out _which_
hidden service it's a guard node for.  There has been a lot of research
that touches on this issue and it's clearly not as easy for hidden
services to conceal their identities from their guard nodes as it
should be, especially if the guard nodes actively experiment on the
hidden service.  One example that shows why this is a difficult problem
is that if you control a guard node and you know about the existence of a
particular hidden service, you can connect to the hidden service yourself
and see if that results in any traffic coming out of your guard node.
You can also deliberately shut down clearnet traffic to and from your
guard node for a few seconds at a time at randomly-chosen moments and
see if that results in outages of availability for the hidden services
at the same moments.
I think some of these ideas are developed in published papers and I'm
sorry for not thinking of which papers at the moment.  You can see that
this can make the situation of the hidden service somewhat precarious.
See also
There might be some more hope in the future from high-latency services
(based on examples like Pond), or, based on what some crypto folks have
been telling me, from software obfuscation (!!).

@_date: 2015-12-31 06:06:06
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
As I said in my previous message, I don't think this is the case because
the correlation just requires seeing the two endpoints of the connection,
even without knowing the complete path.  This is even possible with
a hidden service because the server that provides the hidden service
also uses an entry guard of its own, which is the "endpoint" for traffic
correlation purposes when a user is contacting the hidden service, despite
the much longer (and so harder to observe) path within the Tor network.
The lack of security improvement from longer path lengths is described in
That's definitely an improvement, although there's an issue in the long
run that the crypto in HTTPS is getting better faster than the crypto
in Tor's hidden services implementation. :-)

@_date: 2015-12-31 08:46:35
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
============================== START ==============================
It's also a question of practical deployment: it should be improved
eventually with new Tor protocol versions, but I don't believe that it
has been yet (although I'd love for the Tor developers to correct me on
this point).
Yes, Digicert is offering them.
But as you can see from their page, they only offer EV certificates,
which involve verifying the legal identity of an organization.  So the
certificates aren't available for onion sites that are operated by
individuals or that are operated by anonymous people or organizations.
Right now, probably most onion sites wouldn't be able to get a certificate
for their sites because of these restrictions.  (I'm grateful to Digicert
for their work on this -- the restrictions aren't their fault!)

@_date: 2015-02-02 11:06:09
@_author: Seth David Schoen 
@_subject: [tor-talk] VPN/TOR Router 
There have been a number of discussions on this mailing list before
about standalone Tor routers.  The usual consensus is that using a
separate router together with regular Internet applications is risky,
because the applications don't know that they shouldn't behave in
certain ways.  For example, the applications might mention your real IP
address in the course of some protocol, or they might send or allow to
be sent a persistent cookie, which might eventually be sent over both a
Torified and a non-Torified connection.
The Tor Browser has had a ton of work put into it
to try to make sure it works safely with Tor (again, by making all Tor
Browser instances look alike, making sure that they don't allow
long-lived cookies or cookie equivalents, and various other
precautions).  The router running as a separate device can't usefully
apply all of these protections to regular Internet applications "from
the outside", and the applications, again, won't realize that they're
being used in an anonymous way and that they shouldn't send data that
might compromise their user's anonymity.
That's why the Tor Project doesn't currently recommend using Tor with a
web browser other than Tor Browser, and that's something that would
inevitably happen when using one of these standalone routers.

@_date: 2015-02-03 18:25:24
@_author: Seth David Schoen 
@_subject: [tor-talk] "Confidant Mail" 
The Tor Project itself has found that users often don't verify GPG
signatures on binaries (I think Mike Perry quoted some statistics about
how often the Tor Browser binary had been downloaded in comparison to
the .asc signature file -- it was orders of magnitude less often).  That
suggests to me that HTTPS should be used for software distribution
authenticity even when there's a signature available; the importance of
this only diminishes if the signature will be verified automatically
before installation (like in some package managers).  That's usually
not the case for first-time installations of software downloaded from the
(I don't think the Tor Project has studied _why_ the users didn't verify
the signatures -- there are tons of possible reasons.  But it's clear
that most didn't, because the .asc file is so rarely downloaded.)

@_date: 2015-02-03 18:34:29
@_author: Seth David Schoen 
@_subject: [tor-talk] "Confidant Mail" 
You can help mitigate each of these attacks by using HTTPS together with
HPKP to cause browsers to reject attack certs.  Anyway, you shouldn't
only think of one intelligence agency as a threat when distributing
privacy software.  Governments in any country where you may have users
might be interested in introducing malware into the versions downloaded
by some or all users in that country.  If manual signature checking is
rare -- as it probably will be -- then using HTTPS can be an important
step toward addressing that thread.  Maybe the actual attacks against
the integrity of your software distribution won't come from NSA, but
rather from some other government -- and maybe they _won't_ be able to
mount a successful attack against HTTPS certificate verification.

@_date: 2015-02-19 00:10:21
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor Browser Bundle with Chromium 
I think that list is kept relatively up-to-date.
More generally, there are a lot of customizations in Tor Browser to turn
off or alter Firefox features that might identify a user (by making one
Tor user's browser look recognizably different from others) or might
bypass the proxy (causing the browser to send non-Torified traffic over
the Internet).
The Tor Project hasn't received a lot of help from the Chromium developers
on changes that would be important for making these customizations --
but with or without that help, they would be a lot of work in their own
right, just as they were a lot of work on the Firefox side.
You can read about some of the customizations in the Tor Browser design
document at

@_date: 2015-01-05 08:14:55
@_author: Seth David Schoen 
@_subject: [tor-talk] TOR issues 
I think some of your questions are based on misunderstanding the
difference between circuits that exit to public Internet services, and
circuits that terminate at Tor hidden services.  These are two separate
Tor features, and each circuit (that eventually reaches some service)
terminates in one way or the other but not both at the same time.
Exit nodes aren't used for hidden services at all.  Onion URLs are only
used to refer to hidden services, which communicate entirely within the
Tor network and don't exit.  Most uses of Tor use exit nodes to reach
public services on the ordinary Internet, instead of using onion URLs.
The hidden service directory mapping is performed by the hidden service
directory. :-)
Everyone can become an exit node by declaring a non-empty exit policy.
That does allow them to monitor user communications and see where the
users are connecting.  In Tor's design this is not considered bad,
because the _identity_ of those particular users should still be hidden
(although it's potentially bad in some threat models, like when the
same adversary operates, or monitors, both the entry and exit points of
a particular user simultaneously).
Exit nodes (or at least their exit service!) are not used in any way
for contacting hidden services.  Hidden services and hidden service
users communicate entirely within the Tor network.  (Hidden services
themselves build Tor circuits in order to talk to their users.)
Each Tor relay has its own public key which it declares when registering
with the Tor directories.  The Tor directories confirm that they have
the same view of the relays on the network, and the relays' public key,
through the consensus mechanism.
That means that the Tor directories are something like certificate
authorities or PKI for the regular Tor relays.  You have to trust the
consensus of the directories to give you the correct public keys for the
relays you plan to use, so that no relay (or ISP) can perform an
undetected man-in-the-middle attack.
The directory authorities, for some purposes, might be the single point
of failure you're looking for.  Since they have some redundancy, you
might not call them a "single" point of failure, but they could be seen
collectively as a point of failure because they need to be operating and
reachable in order for users to be able to learn how to connect to the
Tor network.

@_date: 2015-07-03 11:40:30
@_author: Seth David Schoen 
@_subject: [tor-talk] pdf with tor 
There are two kinds of risks that lead to the suggestion not to view
documents like PDFs inside your Tor Browser (or even not on the same
machine) -- exploits and IP address leaks.
The first risk is that sometimes there are software bugs in application
and viewer software that would allow someone who knew about the bugs
to take over your computer by constructing an invalid input file that
exploits the bug and then getting you to render the file.  So in that
case, someone could, for example, make an invalid PDF that exploits a
bug in the PDF renderer in your browser, and get you to view it somehow,
and then take over the browser.
The other is that many formats can cause software to make Internet
requests (for example, it's possible to embed image links in a Word
document so that a Word viewer will go and download those images).
Here, the concern is that if the software makes some kind of network
request when displaying the document, whoever is on the other end may
see that request coming directly over the Internet -- not via Tor --
and connect the request with your Tor activity.
So, some cautious Tor users advise copying all downloaded files onto
a different computer that's not connected to the Internet, or at least
inside of a virtual machine with no direct Internet access, and viewing
them there.
I don't know of specific cases in which people have deliberately used
these approaches to identify anonymous Tor users, but it's something
that's been discussed, and there _is_ a high rate of malware and tracking
links hidden inside e-mail attachments.  I liked the anecdote (which
I've seen in a few places) that Tibetan Buddhists who've received a lot
of malware are now practicing a new "non-attachment principle".

@_date: 2015-07-22 16:36:16
@_author: Seth David Schoen 
@_subject: [tor-talk] HORNET onion routing design 
Has anybody looked at the new HORNET system?
It's a new onion routing design that seems to call for participation
by clients, servers, and network-layer routers; in exchange it claims
extremely good performance and scalability results.
I think it also calls for the use of network-layer features that aren't
present in today's Internet, so it might be hard to get a practical
deployment up and running at the moment.

@_date: 2015-07-24 09:48:51
@_author: Seth David Schoen 
@_subject: [tor-talk] HORNET onion routing design 
If the remote peer has to be actively involved in the onion routing
process, couldn't it detect replays rather than having the routers do it?
Or is the replay problem a problem of wasting network resources rather
than fooling the peer into thinking a communication was repeated?

@_date: 2015-07-28 10:09:16
@_author: Seth David Schoen 
@_subject: [tor-talk] tor not running 
Most users don't use GPG to verify their downloads -- probably much
fewer than 1%.  If the download succeeds without interference, it isn't
technically necessary to verify it before using it.  It's a security

@_date: 2015-06-25 09:56:12
@_author: Seth David Schoen 
@_subject: [tor-talk] Is this still valid? 
There are some mistakes in the article -- for example the notion that
Tor "was built for a specific purpose, which was the circumvention of
restrictive firewalls" like the Great Firewall of China.
If you read the original Tor design paper from 2004, censorship
circumvention was actually not an intended application at that time:
("Tor does not try to conceal who is connected to the network.")
That has subsequently changed, the project adopted anticensorship uses
as an additional goal, and nowadays Tor does sometimes try to conceal
who is connected to the network, when they ask it to.  (Sometimes this
succeeds against a particular network operator, and sometimes not.)
But the original design goal was privacy in a particular sense, and
not censorship circumvention.
My colleagues and I made an interactive diagram a few years ago to try
to explain the same concern that this article presents.
One part of it is that if you use Tor without additional crypto protection
to your destination (like HTTPS), a different set of people can eavesdrop
on you than if you didn't use Tor at all.  That's definitely still
true and is always a basic part of Tor's design.  You might think those
people are better or worse as eavesdroppers than the nearby potential
eavesdroppers.  The faraway eavesdroppers might be more organized and
malicious about it, but they also might start out not knowing who you are.
Whereas the nearby eavesdroppers might physically see you, or have issued
you an ID card, or have your credit card.
As we thought when we made that diagram, probably the best solution for
this is more and better HTTPS.  At some point (which may already be in the
past), it might even be a good idea for Tor Browser to refuse to connect
to non-HTTPS sites by default, although that might be a difficult policy
to explain to users who don't understand exactly what HTTPS is and how
it protects them, and just see that Tor Browser stops being able to use
some sites that Internet Explorer can work with.

@_date: 2015-06-25 10:04:31
@_author: Seth David Schoen 
@_subject: [tor-talk] Is this still valid? 
The connection to censorship circumvention is that, on a censored
network, people are normally not allowed to connect to censorship
circumvention services (that the network operator knows about).  So if
you allow the network operator to easily know who is connecting to the
service -- as the 2004 version of Tor always did -- they can block it
immediately (as several governments did when they noticed Tor was
becoming popular in their countries).
Now that Tor also has censorship circumvention as a goal, there are
several methods it can use to try to disguise the fact that a particular
person is connected to the Tor network.

@_date: 2015-03-04 12:02:07
@_author: Seth David Schoen 
@_subject: [tor-talk] New Tor project idea for internet comments 
This is a major change from the existing approach of Tor and the Tor
First, the Tor project has only focused on preventing censorship
by networks and network operators, not by web sites.  The
censorship-resistance approach of Tor has been that your ISP shouldn't
be able to control whom you can communicate with, as opposed to that
web sites shouldn't be able to control who can post there or what they
can post.
Although the Tor Project has been very interested in ways to encourage
sites not to block anonymous users, there's never been an effort to
force the sites to accept anonymous users, or to conceal the fact that
someone is using Tor on the exit side.  In fact, the Tor Project has
specifically rejected the idea of doing that:
("If people want to block us [on the exit side], we believe that they
should be allowed to do so.")
Second, Tor has never tried to "force" people to route other people's
traffic or to hide the fact that this is happening.  Instead, there
are a lot of cautions given to people who are considering operating
exit relays.  In your proposal, all of the users would be acting as
exits and routing (some) traffic to the public Internet.  That would
tend to put unsuspecting users at risk because they'd start to be the
subject of abuse complaints, including on their home Internet connections.
(In some designs, people could also deliberately target specific people
they don't like by posting threats through those people's connections.)
That would also probably make running Tor a lot less appealing to some
users because they wouldn't be given the choice about whether to provide
exits for other people's traffic.
Third, the distinction between "comments" and other kinds of traffic is
one that requires a huge amount of programming to enforce, and that can
probably only be enforced if users aren't using HTTPS to connect to the
sites.  The Tor Project and larger Tor community have been trying very
hard to get HTTPS deployed everywhere specifically so that Tor exit
nodes _won't_ be able to spy on or examine what Tor users are doing.  If
progress continues to be made on that front, the Tor exits will be less
and less in a position to make the distinction that you suggest between
comments and other stuff.
(It might be possible to extend the Tor protocol to have "comment posting"
be a special kind of exit, where the user explicitly entrusts the text of
the comment to the exit node, which then makes its own HTTPS connection to
the site and posts the comment.  But that would be a lot of engineering
work and would entail a new arms race with the web site operators, who
would be able to update the HTML code of their sites frequently to stop
Tor exit nodes from being able to recognize where and how to post the
comments.  So that's a lot of effort for a kind of blocking resistance
that Tor developers don't necessarily support philosophically and that
would be challenging to sustain over time.)
Fourth, there are some other technical problems with having everyone be
a relay.

@_date: 2015-05-11 16:39:10
@_author: Seth David Schoen 
@_subject: [tor-talk] a question about ip addresses 
It's probably the entry guard that Tor chose for you.
This is meant to reduce the chance that you'll choose an entry node and
an exit node secretly controlled or monitored by the same entity.
Some more technical details are in
and probably other places.

@_date: 2016-02-05 12:16:02
@_author: Seth David Schoen 
@_subject: [tor-talk] Not able to download Tor to droid] 
If you're using the default Android browser, try pressing the Menu button
and then looking for "Downloads" in the menu.  This will show a list of
files that have been downloaded using the browser; selecting an APK file
there will cause it to be installed (if you've already set your settings
to allow non-Play Store app installs).

@_date: 2016-02-19 09:12:50
@_author: Seth David Schoen 
@_subject: [tor-talk] PGP and Signed Messages, 
The traditional answer, which amazingly nobody has mentioned in this
thread, is called the PGP web of trust.
In the original conception of PGP, people were supposed to sign other
people's keys, asserting that they had checked that those keys were
genuine and belonged to the people they purported to.
This is used most successfully by the Debian project for authenticating
its developers, all of whom have had to meet other developers in person
and get their keys signed.  Debian people and others still practice
keysigning parties.
This method has scaling problems, transitive-trust problems (it's possible
that some people in your extended social network don't understand the
purpose of verifying keys, or even actively want to subvert the system),
and the problem that it reveals publicly who knows or has met whom.  For
example, after a keysigning party, if the signatures are uploaded to
key servers, there is public cryptographic evidence that all of those
people were together at the same time.
So there is a lot of concern that the web of trust hasn't lived up to
the expectations people had for it at the time of PGP's creation.
People also don't necessarily check it in practice.  Someone made fake
keys for all of the attendees of a particular keysigning party in
2010 (including me); I've gotten unreadable encrypted messages from
over a dozen PGP users as a result, because they believed the fake key
was real or because software auto-downloaded it for them without
checking the signatures.
If you did try to check the signatures but didn't already have some
genuine key as a point of reference, there's also this problem:

@_date: 2016-02-19 11:53:15
@_author: Seth David Schoen 
@_subject: [tor-talk] PGP and Signed Messages, 
This happened once again today, shortly after I wrote this message!
The person who made the mistake was a cryptography expert who has done
research in this area.  So I fear the web of trust isn't holding up
very well under strain, at least in terms of common user practices with
popular PGP clients.

@_date: 2016-02-19 21:58:35
@_author: Seth David Schoen 
@_subject: [tor-talk] Tracking blocker 
The recommendation not to install add-ons is because they will make
your Tor browser more different from others and so potentially more
recognizable to sites you visit -- because they could look at their
logs and say "oh, that's the Tor Browser user who was also using
Disconnect!".  If you didn't use Disconnect, they wouldn't necessarily
have a straightforward way to distinguish you from any other Tor Browser
users who also visited the site, or to speculate about whether a Tor
Browser user who visited site A was also the same Tor Browser user who
visited site B.
The Tor Browser design already provides quite strong tracker protection
compared to a run-of-the-mill desktop web browser because of all of the
ways that it tries not to keep state between sessions, tries not to let
sites find out many things about your computer or browser, and tries not
to let one site see what you've done on another site.
If you can point out a specific way that Disconnect protects your privacy
that Tor Browser currently doesn't, or if the Disconnect developers
can think of one, it might be constructive to bring it up with the Tor
Browser developers, because they might be willing to consider adding it
as a standard feature for all users.

@_date: 2016-02-20 13:28:52
@_author: Seth David Schoen 
@_subject: [tor-talk] Bridges and Exits together 
Your bridge will become less useful for censorship circumvention because
its IP address (as an exit) will get published in the public directory
of Tor nodes and so automatically added to blacklists of Tor-related
addresses.  The censorship-circumvention benefit of bridges, ideally,
comes in because censors don't know that their traffic is related to Tor.

@_date: 2016-02-29 12:21:00
@_author: Seth David Schoen 
@_subject: [tor-talk] Lets Encrypt compared to self-signed certs 
You can use pinning with Let's Encrypt certs too.  The default client
behavior changes the subject key on every renewal, but I can add a
feature to keep the old key if you want to pin at the key level.
We don't know how large the risk of legally-compelled misissuance is,
but we have lots of lawyers who would be excited to fight very hard
against it.  I think that makes us a less attractive target than other
CAs that might not find it as objectionable or have as many lawyers
standing by to challenge it.
Remember that (without CA-level pinning) users are always at risk
from misissuance by any CA that they trust, not just the CA that
you specifically chose to use.  For example, google.com was attacked
(successfully at first) with misissued certs from DigiNotar even though
Google had no relationship with DigiNotar at all.
We also publish all of the certs that we issue in Certificate
Transparency.  You can watch the CT logs for your domain or other certs
that you care about.  If you ever see a cert in CT for your domain
that you didn't request, please make a big deal out of it.  Likewise,
if you ever see a valid cert in the wild from Let's Encrypt that doesn't
appear in the CT logs, please make a very big deal out of it.  At some
point it should become possible to get browsers to require inclusion CT
proofs for certs from Let's Encrypt, though we don't have the tools in
place for this yet.

@_date: 2016-01-01 13:40:12
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
As a guard node (or someone observing a guard node) trying to locate the
operator of a hidden service, you can use the IP address of the inbound
connection and the Tor directory to see if it's another Tor node or not.
I guess the hidden service operator could use a bridge to create more
ambiguity about what's happening; I don't know for sure if a guard node
has a way to distinguish an inbound connection from a bridge from an
inbound connection directly from a client.

@_date: 2016-01-22 16:22:58
@_author: Seth David Schoen 
@_subject: [tor-talk] Hello I have a few question about tor network 
I've mentioned the Jeremy Hammond and Eldo Kim cases, which can be seen
as "good enough" coarse-grained correlation.  I think there are others
if we look for them.

@_date: 2016-01-26 11:18:37
@_author: Seth David Schoen 
@_subject: [tor-talk] onion routing MITM 
The hidden service name isn't chosen directly by the hidden service
operator and you can't just make one up and start using it.  Instead,
it's derived from the hidden service's cryptographic public key.
Tor checks that the public key matches when you're connecting to the
hidden service, so someone can't simply substitute their own service
without knowing the corresponding private key.
In effect, the crypto key is used as a name (or identifier), which
provides an intrinsic cryptographic way to know whether you're talking to
someone who has the right to use that name (or is properly referred to by
it), assuming hidden service operators can keep their private keys secret.
Somewhat confusingly, people do manage to make their hidden services
start with strings of their choice, but they do this by generating
enormous numbers of different keys over and over again until they get
one that they like.  Despite that, it takes an exponentially-increasing
number of attempts for each additional character of the onion name that
you want to control, so even if Facebook can get one that starts with
"facebook" (as they did), we don't tend to think anyone* has the time
or computational resources to be able to choose the entire onion name,
for example to choose one that matches an existing one controlled by
somebody else.  For instance, even if I had generated an onion name
beginning "3g2upl4", it would take me about 32 times as much work to get
one beginning "3g2upl4p", 1024 times as much work to get one beginning
"3g2upl4pq", 32768 times as much work to get one beginning "3g2upl4pq6",
and overall 35184372088832 times as much to get one that exactly matches
DuckDuckGo's onion name.
Yes, or from DuckDuckGo's regular site.
* The Bitcoin network is doing quite a bit more computation, in total,
  than this per year, so it's actually conceivable that someone with a
  very large amount of money to spend on custom hardware could do this.
  So the next generation of Tor hidden services will use a longer
  onion name.

@_date: 2016-01-26 11:30:27
@_author: Seth David Schoen 
@_subject: [tor-talk] onion routing MITM 
In the Zooko's Triangle sense, Tor hidden service names are secure and
decentralized, but not human-meaningful (or human-memorable).
That is to say that Tor hasn't tried to solve the problem you mention
at all.  The answer seems to be that you're supposed to get the names
somewhere else and store them in something other than your human memory.
This is in common with a few other designs that use representations
of crypto keys directly (for example, PGP and Bitcoin) and where
someone could try to trick you into using a key that isn't really the
right one.  In the PGP example, someone has uploaded a fake key with my
name and e-mail address to the keyservers (several years ago), which has
already fooled a number of people because they couldn't or didn't readily
distinguish my real key from the fake key, both of which are just numbers
that someone on the Internet has claimed are relevant to contacting me.
If you have ideas for making this more convenient, I'm sure they would
be welcome.  Aaron Swartz proposed in 2011 that blockchains and related
systems could solve it by letting people publicly announce claims to
(human-memorable) names in an append-only log.
There are some implementations of related ideas, like okTurtles, but
none is extremely widely used yet.
Indeed, Juha Nurmi described earlier today that people are doing exactly
that right now, probably with some success.

@_date: 2016-02-29 16:08:00
@_author: Seth David Schoen 
@_subject: [tor-talk] Lets Encrypt compared to self-signed certs 
We don't have any tools for pinning at all but you can read people's
tips about it on the Let's Encrypt community forum.
The logs obviously don't have metadata about whether certificates are a
result of coercion, but if you are the site owner and you see a
certificate in the log that you didn't ask for, you have evidence that
there's been a problem, while if you are a user and you see a
certificate on the site that isn't in the log, you have evidence that
there's been a different kind of problem.
I'll be happy to take a look at that.

@_date: 2016-03-04 12:52:50
@_author: Seth David Schoen 
@_subject: [tor-talk] .onion name gen 
This would be true if the public key were used directly as the onion name
(which might be possible in certain elliptic curve systems because keys
are so small).
But in this case, the onion name is calculated from a hash of the public
key, and the size of the hash is much smaller than the size of the
underlying pubkey (80 bits vs. 1024 bits).  The pigeonhole principle
requires that many, many different pubkeys must have the same hash --
on average, about 2??? pubkeys would have the same hash.  When you
get a perfect collision from scallion, after doing that 2?? work
(analogous to about 11 days of entire work of the Bitcoin network --
which you can think of as surprisingly much or surprisingly little work),
you're still astronomically unlikely to have the same private key!

@_date: 2016-03-04 16:10:01
@_author: Seth David Schoen 
@_subject: [tor-talk] .onion name gen 
Mirimir's reference at
shows that they are truncated SHA-1 hashes, 80 bits in length, of "the
DER-encoded ASN.1 public key" of "an RSA-1024 keypair".
So you have the space of public keys (indeed, it's considerably less than
1024 bits if you want to actually be able to use it as a keypair) and the
space of 80-bit truncated hashes, and the former is dramatically larger
than the latter.  So over the entire space of keys, collisions are not
just possible but are required and even extremely frequent.  On the other
hand, they're so difficult to find that nobody knows a single example!

@_date: 2016-05-16 10:26:51
@_author: Seth David Schoen 
@_subject: [tor-talk] augmented browsing - "sed inside torbrowser" 
There is a nice existing and non-Tor Browser-specific tool that does
something along these lines:
It may be a bit more elaborate than what you were thinking of but it's
a nice tool that can handle a variety of use cases -- and should be
fully compatible with Tor Browser already.

@_date: 2016-05-30 18:08:40
@_author: Seth David Schoen 
@_subject: [tor-talk] Could Tor be used for health informatics? 
I'm not convinced this is a good tradeoff for this application.  The
crypto in the current version of hidden services is weaker in several
respects than what you would get from an ordinary HTTPS connection.
These users probably don't need (or want?) location anonymity for either
side of the connection and may not appreciate the extra latency and
possible occasional reachability problems associated with the hidden
service connection.

@_date: 2016-05-31 16:02:37
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor-Retro' for OS/2 Warp 4.52 Latest Release (2001) ? 
There's a certain circularity to this: if you use rare OSes because
attackers aren't interested in them and you convince lots of people that
this is a good strategy, attackers may then get more interested in them.
So it's at least not a strategy that can scale very well.

@_date: 2016-10-03 11:34:23
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I appreciate your approach to analyzing what Tor-like tools need to be
able to do, but I wanted to question this a little bit.
Some of us privacy advocates have felt that it's quite bad that
communications technologies generate location and association metadata
in the first place.  I've often said in interviews that it's a flaw in
the cell phone infrastructure that it generates location data about
its users, for example, and that it would be better to have a mobile
communications infrastructure where location anonymity was the default
for everybody all of the time.
In the Netflix case, when you use an account to sign into their service,
just like any other, you're creating evidence of where you were when
you were watching that movie, which is also the basis of other evidence
about who was with you or who knows whom (like if you watched Netflix
from someone else's house, or two people watched Netflix from the same
place, or one person watched Netflix and another person signed into a
different service).
I wouldn't want to concede that it's appropriate that all of that data
gets generated all of the time, even if you can't see any sensitivity to
it at a particular moment.  And if location privacy continues to happen
on a purely opt-in basis, it'll continue to draw more attention to people
who are using tools to protect it, and it'll continue to be hard for
people to anticipate when they're going to turn out to have needed it.
It seems that people often discover later on that they wish they had
taken precautions to protect some data that didn't seem significant at
the moment.

@_date: 2017-08-09 15:53:59
@_author: Seth David Schoen 
@_subject: [tor-talk] Motivations for certificate issues for onion services 
Hi folks,
For a long time, publicly-trusted certificate authorities were not
clearly permitted to issue certificates for .onion names.  However, RFC
7686 and a series of three CA/Browser Forum ballots sponsored by Digicert
have allowed issuance of EV certificates (where the legal identity of
the certificate requester is verified offline before the certificate is
issued).  This has allowed Digicert to issue a number of such certificates
to interested (extremely non-anonymous!) onion service operators.
So far Digicert is the only browser-trusted CA to have taken advantage of
this policy.  Notably, it doesn't apply to certificate authorities that
only issue DV certificates, because nobody at the time found a consensus
about how to validate control over these domain names.  There was also
a long-standard concern about cryptographic strength mismatch in the
sense that the cryptography used by onion services was weaker than the
cryptography that's now used in TLS.  (I think this concern was misplaced,
but I believe it's served as one of the main rationales for distinguishing
EV from DV.)
So, there has been a suggestion that this issue might be revisted with
the next generation onion services because they have stronger
cryptographic primitives.  Apparently these have now been not only
implemented but actually demonstrated:
I'd like to prepare to raise this issue with the CA/Browser forum in
anticipation of a ballot there to have it be possible for DV certificates
to be issued to onion services.  So I wanted to ask two things here:
(1) What's the status of onion services looking like now?  I haven't
seen Roger's DEF CON talk.  (Was it recorded?)
(2) What reasons do people have for wanting certificates that cover
onion names?  I think I know of at least three or four reasons, but I'm
interested in creating a list that's as thorough as possible.

@_date: 2017-08-09 23:24:00
@_author: Seth David Schoen 
@_subject: [tor-talk] Motivations for certificate issues for onion services 
I think Roger's reply to my message addresses reasons why I think this
is a good argument, and I'm in agreement with you.  However, with
next-generation onion services, it should no longer be necessary to have
any form of this argument.

@_date: 2017-08-30 11:28:13
@_author: Seth David Schoen 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
Well, I'm still working on being able to write to the CA/B Forum about
this issue... hopefully we'll find out soon what that community is

@_date: 2017-08-31 09:53:44
@_author: Seth David Schoen 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
============================== START ==============================
Thanks, I guess that's Section 5 there.
Do you think there should perhaps be a new OID with semantics like "for
each identifier that is a subject of this certificate and that contains
'onion' as one DNS label, we performed both clearnet and onion site DV"
and so "you can feel free to access the .onion version of this site
while also believing that it's run by the same organization as the TLD"?
Presumably such an OID could be added by a CA without a new CA/B Forum
ballot because it's just asserting an additional check and not reducing
the CA's verification obligations.

@_date: 2017-01-04 20:37:31
@_author: Seth David Schoen 
@_subject: [tor-talk] Exits: In Crossfire on the Front Lines 
This is a fairly different angle on what Micah originally wrote.
(His article says that, while it's plausible that these attacks were
sponsored by the Russian government, the IP addresses involved don't
tend to prove that because many of them -- being Tor exit nodes --
could have been used by any attacker.)

@_date: 2017-06-08 12:37:27
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor source code 
Yes, it has always been since the beginning of the project.  Currently,
the code is available at
Open source means that anyone is allowed to make their own changes (and
share those with the public if they want), but there is an official
version from the Tor Project which only official Tor maintainers can
change.  The official Tor maintainers receive suggestions from the public,
but they make the final decision about whether or not other people's
changes can become part of the official version of Tor.
For example, if you wanted to change something, you could make your own
modified version without anyone's permission, but it wouldn't be the
official version.  You would need to ask the maintainers to adopt your
changes if you wanted them to become part of the official version.
There is still an interesting question about whether people could somehow
trick the Tor maintainers into including a change that is actually
detrimental, even though it appears to be useful.  In many ways, the Tor
project relies on public scrutiny to confirm that changes that get
included in the official version are useful and don't introduce problems
or security holes.  There is a fairly broad consensus that this is a
useful way to work, yet I don't think that people are confident that all
of the risk has been mitigated, since there are also security research
projects that show that there are ways of intentionally creating bugs
that are subtle and carefully disguised as useful functionality.
So, there is still a need for ongoing research about how to learn to
detect (whether by human knowledge, by coding standards, by using
different languages or libraries, by creating new software tools, or
by something call formal methods where properties of code are proven) if
people are trying to disguise or hide a bug or vulnerability inside of a
useful contribution.
The Tor Project has actually thought about this issue a lot, if you're
very interested in it... there are probably other resources and
presentations that you could look at that further examine the issue.

@_date: 2017-06-08 12:49:13
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor source code 
By the way, there's an interesting new study
that claims that many people believe communications security is "futile"
because of inaccurate mental models of cryptography, and strongly
endorse security through obscurity.
I've been thinking a lot about these results (it's worth reading the
paper) and one way that I've been trying to conceive of it is that the
research showed that many participants thought that the developer of a
security technology must, inherently, always know how to crack or
defeat that technology.  This might be true at a technical level if
encryption always worked like a substitution cipher, where there is no
secret key but knowledge of the details of the cipher is equivalent to
knowing how to crack it, or if public key cryptography didn't exist,
so that many-to-many communications required trusted authorities to
distribute key material.
Participants in that study did not tend to feel that encryption software
ought to be open source because they seemed to believe that the
developer of a security tool inherently, so to speak, knows the code
and can always use that knowledge to break users' security.  In this
model other motivated attackers will gradually also learn the secret
knowledge that they need to break the system, but disclosing technical
details of how it works would be an especially bad idea because it would
greatly speed up the process for the attackers.  (Then security through
obscurity is understood to be the only possible form of security.)
The study suggests that an important challenge for developers of security
systems may be finding a way to communicate how security need not depend
on obscurity, and also need not depend on trusting inventors of security
systems to keep secrets.

@_date: 2017-06-08 16:59:40
@_author: Seth David Schoen 
@_subject: [tor-talk] tor-talk Digest, Vol 77, Issue 9 
It seems to me that one useful possibility is to modify the Tor client so
that it outputs logs of the decisions it makes and the actions it takes,
as well as, maybe, the cryptographic secrets that it uses.  For example,
your modified Tor client could print out how it chose a path, and the
actions that it took to build the path, and the actual encryption keys
that it used in communicating with the nodes along the path.
You could then also use a packet sniffer (or some mechanism for packet
capture if your network is totally virtual) to examine the actual
traffic in your simulated network, and, for example, to decrypt it using
the keys that were logged by the modified client, showing exactly what
information can be seen by someone in possession of each secret key, and
conversely which keys are necessary in order to learn which information.

@_date: 2017-06-18 22:16:41
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor's work 
I'm glad that my answer was helpful to you, but I don't think I have
enough familiarity with the Tor code base to help you with the specific
things that you're looking for.

@_date: 2017-03-03 11:45:47
@_author: Seth David Schoen 
@_subject: [tor-talk] State of bad relays (March 2017) 
(strictly speaking, the assumption that no more than one relay in a
circuit is operated by the same operator)

@_date: 2017-11-02 17:23:33
@_author: Seth David Schoen 
@_subject: [tor-talk] Proposed DV certificate issuance for next-generation 
Coinciding with the Tor blog post today about next-generation onion
services, I sent a proposal to the CA/Browser Forum to amend the rules
to allow issuance of publicly-trusted certificates for use with TLS
services on next-generation onion addresses (with DV validation methods,
in addition to the currently-permitted EV methods -- thereby permitting
individuals as well as anonymous service operators to receive these
Thanks to various people here for discussing the merits of this with me.
We'll see what the Forum's membership thinks of the idea!

@_date: 2017-11-20 09:35:25
@_author: Seth David Schoen 
@_subject: [tor-talk] Privacy Pass from Cloudflare, and the CAPTCHA problem 
If the protocol is sound here in its unlinkability property, the Tor
Browser should not need to erase the store of tokens.  I realize that
this may be a challenge architecturally and conceptually, but in the
design of this protocol, persistence of the tokens shouldn't compromise
Tor's anonymity goals.
(Although it does potentially reduce the anonymity set a bit by
partitioning users into those who have the extension and those who don't
have the extension, as well those who currently have tokens remaining
and those who are currently out of tokens.)

@_date: 2017-10-06 12:12:47
@_author: Seth David Schoen 
@_subject: [tor-talk] noise traffic generator? 
There have been a few projects in this space before, like Helen
Nissenbaum's TrackMeNot, and at least two others that I'm not thinking
of right away.
I agree with your concern that it's currently too easy for an adversary
to use statistics to learn if traffic is human activity or synthesized.
Another problem is that the sites that the traffic generator interacts
with might themselves get suspicious and start responding with CAPTCHAs
or something -- which would then also reduce the plausibility of the
I also wonder if someone has studied higher-order statistics of online
activity, in the sense that engaging in one activity affects your
likelihood of engaging in another activity afterward (or concurrently).
For example, you might receive an e-mail or instant message asking you
to look at something on another site, and you might actually do that.
On the other hand, some sites are more distracting and less conducive
to multitasking than others.  For example, you probably wouldn't be
playing a real-time online game while composing an e-mail... but you
might play a turn-based game.
There are also kind of complicated probability distributions about events
that retain attention.  For instance, if you're doing something that
involves low-latency interactions with other people, it's only plausible
that you're actually doing that if the other people were also available
and interacting with you.  The probability that a given person continues
communicating with you declines over time, and is also related to time
zone and time of day.  But there's also a probability that someone else
starts interacting with you.
Some of these things will probably have to be studied in some depth in
order to have a hope of fooling really sophisticated adversaries with
synthesized online activity.

@_date: 2017-09-01 20:08:30
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor users in US up by nearly 100,000 this month 
Amusingly, CloudFlare would probably be in a position to do so because
they present many Tor users with CAPTCHAs.  While this has annoyed Tor
users quite a bit, if we assume that
* old and new Tor users are about equally likely to attempt the CAPTCHA
* old and new Tor users are about equally likely to pass it
* old and new users visit a similar proportion of CloudFlare-hosted sites
  via Tor exits
* CAPTCHAs are relatively effective at preventing access by bots
* CloudFlare keeps logs that clearly identify total volumes of successful
  CAPTCHA completion from Tor exit nodes
then CloudFlare would have good, meaningful data about trends in human
use of Tor.  They wouldn't know the overall volume of human or bot use
of Tor, but they could tell pretty accurately when human use is up or
down and by what fraction.
One confounding factor would arise if the new users are significantly
more or less likely than old users to use onion services.
I'd be happy to ask CloudFlare if they'd be willing to share this data
(maybe in relative rather than absolute numeric terms, like "the number
of people successfully completing a CAPTCHA per day from a Tor exit
node on September 1, 2017 is x% of what it was on January 1, 2016").

@_date: 2017-09-02 22:58:12
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor users in US up by nearly 100,000 this month 
I'm just figuring that you can get useful relative rather than absolute
metrics if you assume that people's tendency to do this is relatively
stable across time and across user populations.  So you don't know how
many of the non-solvers are bots, but you can say that the solvers are
up 10% this month or something, which perhaps then suggests that non-bot
Tor users are up about 10% this month.
This still wouldn't reveal whether 60% or 95% of the non-solvers are

@_date: 2017-09-27 10:53:12
@_author: Seth David Schoen 
@_subject: [tor-talk] New OONI release: Test sites you care about! 
Hi Maria,
I also posted this question as a comment on the blog post, but I was
wondering if OONI encounters adversarial activity from censors who
try to either locate and shut down OONI nodes, or return different
information to OONI nodes than to other Internet users.  If so, the
OONI Run feature could make it extremely easy for censors to identify
where OONI probe nodes are located, by just setting up a site under
their control and submitting it to OONI Run.  But maybe this isn't a
problem that the project has encountered so far.

@_date: 2017-09-27 11:40:44
@_author: Seth David Schoen 
@_subject: [tor-talk] New OONI release: Test sites you care about! 
Yes, I guess I'm just a bit surprised that there aren't more known
cases of censors actively trying to interfere with OONI in some way --
especially since it's already led to published reports about specific
censorship events and practices in specific countries.

@_date: 2017-09-27 15:11:30
@_author: Seth David Schoen 
@_subject: [tor-talk] How to find trust nodes? 
A challenge is that there are threat models in which a considerable number
of Tor users may be exposed, at least for some of their circuits.
* If a single adversary runs several fast nodes that are popular and whose
  relationship to each other is undisclosed, a pretty high amount of traffic
  may select that adversary's nodes as entry and exit nodes for the same
  circuit.  The guard node design gives a relatively low probability of this
  happening to any individual user with respect to any individual
  adversary in any specific time period, but doesn't guarantee that it
  would be a particularly rare event for Tor users as a whole.
* If adversaries cooperate, they can get benefits equivalent to running many
  nodes even though each one only runs a few.
* If an adversary can monitor network activity and see both entry and exit
  points, for a given circuit, it can perform correlations even though
  it doesn't operate any nodes.  Or, an adversary that can monitor some
  networks can increase its chance of getting visibility of both ends of
  a connection by also operating some nodes, since some users whose entry
  or exit activity the adversary otherwise wouldn't have been able to
  monitor from network surveillance alone may sometimes randomly choose to
  use that adversary's nodes in one of these positions.
* An adversary that can monitor some kind of public or private online
  activity can perform coarse-grained timing correlation attacks between
  its own entry nodes (or parts of the Internet where it can see Tor
  node entry) and the online activity that it can see.  For example, if a
  user regularly uses Tor to participate in some kind of public forum,
  public chat, etc., the adversary could gather data about how entry
  traffic that it can see does or doesn't correlate with that participation.
  Or if an adversary can obtain logs about the use of a particular online
  service, even though those logs aren't available to the general public,
  it can also correlate that statistically with entry data that it has
  available for some other reason.
The "good news" is that a given Tor user is probably not very likely to
be vulnerable to many of these attacks from many adversaries when using
Tor infrequently or for brief periods.  Yet many of these attacks would
work at least some of the time against a pretty considerable amount of
Tor traffic.
I agree with your point that just having more random people run nodes
helps decrease the probability of success of several of these attacks.

@_date: 2018-12-05 21:43:53
@_author: Seth David Schoen 
@_subject: [tor-talk] You Can Now Watch YouTube Videos with Onion Hidden 
Hi bo0od,
Thanks for the links.
This seems to be in a category of "third-party onion proxy for clearnet
service" which is distinct from the situation where a site operator
provides its own official onion service (like Facebook's facebookcorewwwi,
which the company has repeatedly noted it runs itself on its own
Could you explain how this kind of design improves users' privacy or
security compared to using a Tor exit node to access the public version
of YouTube?  In this case the proxy will need to act as one side of
users' TLS sessions with YouTube, so it's in a position to directly
record what (anonymous) people are watching, uploading, or writing --
unlike an ordinary exit node which can at most try to infer these
things from traffic analysis.  Meanwhile, it doesn't prevent YouTube
from gathering that same information about the anonymous users, meaning
that this information about users' activity on YouTube can potentially
tbe gathered by wo entities rather than just one.
The proxy could also block or falsely claim the nonexistence of selected
videos, which a regular exit node couldn't do, and if its operator knew
a vulnerability in some clients' video codecs, it could also serve a
maliciously modified video to attack them -- which YouTube could do, but
a regular exit node couldn't.
Are there tradeoffs that make these risks worth it for some set of
users?  Maybe teaching people more about how onion services work, or
showing YouTube that there's a significant level of demand for an
official onion service?

@_date: 2018-12-05 21:47:03
@_author: Seth David Schoen 
@_subject: [tor-talk] You Can Now Watch YouTube Videos with Onion Hidden 
(or in some other part of Tor Browser, since the proxy can also serve
arbitrary HTTP headers, HTML, CSS, Javascript, JSON, and media files of
various types)

@_date: 2018-02-08 12:18:27
@_author: Seth David Schoen 
@_subject: [tor-talk] catastrophe: ip-api.com sees me 
As the documentation says, there are a couple of different things that
can go awry here.
* Your non-Tor Browser can be vulnerable to a proxy bypass (because
  other browsers don't necessarily consider that a very serious
  problem).  E.g., an attacker can serve you some HTML that uses
  some kind of browser feature that goes directly over the Internet,
  not via Tor.
* Your non-Tor Browser can be vulnerable to various kinds of
  tracking and fingerprinting, because other browsers haven't done as
  much to mitigate that.  E.g., an attacker can use some kind of
  supercookie to recognize you across sessions, or serve some kind
  of Javascript that queries various system properties that produce a
  unique long-term fingerprint that Tor Browser might have prevented.
* Your non-Tor Browser can be inherently distinctive because very
  few people are using any given other configuration.  E.g., you might
  be the only person in the world currently using Tor with a particular
  browser version, OS, language, and browser window size (even if a
  site doesn't use elaborate or complex Javascript to find out about
  your system's properties).
Your particular setup has probably mitigated the first of these
effectively, but maybe not the other two.
Now, there are ways that the Tor Browser may also have failed to fully
mitigate each of these risks.  And there could be other benefits to
using a different browser in terms of adversaries who know of zero-day
vulnerabilities in Tor Browser that might not be present in other
browsers.  (Some critics have pointed out that more potential attackers
probably have zero-days against the current Tor Browser at a given
moment than against, say, the current Google Chrome; at least, they
typically wouldn't have to pay as much money to buy them.)  But you
probably can't mitigate the second two concerns above on your own, which
might always mean more trackability and less anonymity of a certain kind
when using another browser with Tor.
* If you use something other than Tor Browser, you can get confused
  about when you are or aren't using Tor, or accidentally enable or
  disable it in the middle of some other activity, leading to several
  kinds of contamination between Tor and non-Tor sessions.
Very sophisticated and disciplined users might not trip over this
particular issue, but it's a relatively high risk and a lot of people
using the old TorButton setup definitely ran into this kind of problem.

@_date: 2018-02-08 12:54:09
@_author: Seth David Schoen 
@_subject: [tor-talk] catastrophe: ip-api.com sees me 
You still can't mitigate the browser distinctiveness issue through
expertise or caution, so you can't get the same level of cross-site or
cross-session unlinkability that Tor Browser users can get.  But
indeed, not everyone needs cross-site or cross-session unlinkability
for their uses of Tor.

@_date: 2018-03-20 12:47:47
@_author: Seth David Schoen 
@_subject: [tor-talk] Intercept: NSA MONKEYROCKET: Cryptocurrency / 
Or, maybe people who are privacy conscious should already have done so
following several years of academic, journalistic, and commercial work
on this subject! :-(

@_date: 2018-05-29 09:43:42
@_author: Seth David Schoen 
@_subject: [tor-talk] Post Quantum Tor 
This is because Tor only provides proxying and exit services at the
transit layer.  You can't route arbitrary IP packets over Tor, and
so you can't, for example, ping or traceroute over Tor.
Hidden services, for their part, don't even identify destinations with
IP addresses, so there's no prospect of using IP routing protocols to
describe routes to them.
There have been projects to try to make a router that would automatically
proxy all TCP traffic to send it through Tor by default.  (This would
require writing custom code, not just using existing routing tools, again
because Tor only operates at the TCP layer.)  I was excited about this
idea several years ago until the Tor maintainers reminded me that it would
expose lots of linkable traffic from applications that didn't realize
that they were supposed to remove linkable identifiers and behaviors.
For example, browsers that didn't realize they were running over Tor
would continue to send cookies from non-Tor sessions, and they would
continue to be highly fingerprintable.

@_date: 2019-03-11 12:11:44
@_author: Seth David Schoen 
@_subject: [tor-talk] Tor to become illegal in Europe? 
But this discussion is one politician's view in a keynote address at a
police congress -- which doesn't imply much about police agencies' or
legislators' agreement with this idea.  We've heard similar language in
many countries and it hasn't necessarily led to prohibitions on privacy

@_date: 2019-10-31 19:05:49
@_author: Seth David Schoen 
@_subject: [tor-talk] >170 tor relays you probably want to avoid (Oct 2019 
Very odd naming convention.  It's kind of like
(random.choice(words) + " " + random.choice(words)).replace("a", "").title().replace(" ", "")
... why no letter a?
