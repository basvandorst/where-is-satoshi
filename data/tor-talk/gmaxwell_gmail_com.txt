
@_date: 2007-12-16 18:41:12
@_author: Gregory Maxwell 
@_subject: Hello, about Best Hardware... 
This is WindowsPC thinking.
If on your server you perform a netstat -L and see that only TOR and
SSH are listening, and your 'firewall' would have let those things
through anyways, then a 'firewall' is providing no additional
security,  only some increased robustness against future
Can firewalling be useful even in this situation, yes, but to call it
"suicide" is non-sense.

@_date: 2007-12-04 16:32:12
@_author: Gregory Maxwell 
@_subject: storage privacy (was: Nice quiet, private, anonymous life??) 
If by strong you mean a super conducting magnet of the sort find in a
labratory NMR machine, then probably true. A degaussing coil that you
might find in your home, not a chance.
The amazing density of moderns drives means the individual magnetic
domains need to be very resistant to change.  It's a fairly
straightforward math exercise which has been performed by people far
smarter than I.
Any proactive action to destroy data would look very bad in court.
Using a ram drive, or simply not logging,  would be wise.

@_date: 2007-11-24 20:54:09
@_author: Gregory Maxwell 
@_subject: netstat reporting destinion IP address 
Netstat is telling the truth: You have a connection opened to foohost.
 It just so happens that there is some machinery under the hood that
intercepts the traffic and redirects it into tor, but this doesn't
change where the connection is actually going as far as the system is
Not only is netstat 'operating on a too high level to detect this', it
would be a bug if it reported anything else.
Yes. Don't do that.
it would be better if you were running something that sniffed the
network and showed the user all outbound packets that were not TOR.
Just looking at netstat may well miss short-lived (and especially
connectionless) packets which are probably much more of a significant
real threat to the user.

@_date: 2007-11-05 15:16:12
@_author: Gregory Maxwell 
@_subject: Manual exit selection and HTTP host 
It seems that when I use manual exit selection
( that my browser is sending the
node/exit name back in the HTTP request. This seems like a bad idea in
general and moreover it breaks some sites vhosting configuration.
Am I missing something?

@_date: 2007-10-01 11:31:41
@_author: Gregory Maxwell 
@_subject: netscan from exit-node 
So don't allow exit to your local subnet, but allow exit to everywhere
else. No arp storms then.

@_date: 2007-10-15 02:49:13
@_author: Gregory Maxwell 
@_subject: Tor Defcon Talks 
I don't intend to troll... but wouldn't be ..er better if playing the
videos didn't require using Flash? After all Tor users are advised not
to have the flash plugin installed in their browser if they want tor
to be able to effectively hide their identity...

@_date: 2007-10-15 14:26:03
@_author: Gregory Maxwell 
@_subject: Tor Defcon Talks 
I couldn't figure out how to download them.. The
 that BlueStar88 provided worked fine

@_date: 2007-10-01 13:59:34
@_author: Gregory Maxwell 
@_subject: funneling a wireless net's outbound connections through tor 
Instead why not consider running an exit node?   You still may hear
something about the user's dodgy traffic, but you can prove you were
running a tor exit while going on about your open wireless might just
sound like a bunch of hand-waving.
As others have mentioned it would not be nice to toss people's traffic
into tor when they haven't been given a chance to understand the

@_date: 2007-10-01 13:59:34
@_author: Gregory Maxwell 
@_subject: funneling a wireless net's outbound connections through tor 
Instead why not consider running an exit node?   You still may hear
something about the user's dodgy traffic, but you can prove you were
running a tor exit while going on about your open wireless might just
sound like a bunch of hand-waving.
As others have mentioned it would not be nice to toss people's traffic
into tor when they haven't been given a chance to understand the

@_date: 2007-10-08 02:22:56
@_author: Gregory Maxwell 
@_subject: headers in email 
There are nodes with open port 25 outbound?  I can't connect to port
25 via tor at the moment.
The better way(tm) to do this would be to just run an open SMTP server
as a hidden service, and run spam filtering, hashacash proof-of-work
challenge, whatever anti-abuse stuff you want, along with header
munging and striping, ... and advertise this server for people to set
their SMTP out to...
Then you don't have to feel bad about running a MITM node, and you
might manage to stay up for more than a few minutes before being used
to spam and getting blocked by every mail server on the planet. ;)
On this subject, it would be pretty interesting if the hidden node
support supported a client proof-of-work with server specified
difficulty in order to open a connection. If the SMTP server host were
setup to only allow one mail per connection the server could have POW
based abuse mitigation without special client software.

@_date: 2007-10-08 04:38:39
@_author: Gregory Maxwell 
@_subject: headers in email 
You wouldn't really know that in any case. Or rather you'd know where
you found out about it, and the same would be true.
More importantly, I'd rather not have the rest of the world knowing
the box is connected to tor. Tor hosts aren't normally mail relays,
and idiots blocking tor exits from mail would just discourage people
from running exits yet wouldn't stop spam.
Anything that allows mail through tor is technically an open relay.
Whats your point?
I wasn't suggesting the server be SMTP open to the internet, however.
Oh please, the purpose of making it an explictly selected service is
so that the user would be opting in and be fully aware of the
implications of using it.
I also didn't intend to suggest altering mail beyond headers, which is
something mail servers normally do.  Spam filter doesn't mean alter
the message it means reject or drop it completely.
Anyone not doing that is going to just going to get blocked right away
in any case.

@_date: 2007-10-08 04:56:37
@_author: Gregory Maxwell 
@_subject: headers in email 
It's hard to tell if your SMTP client or stunnel itself send
identifying information. :(

@_date: 2007-09-26 01:17:34
@_author: Gregory Maxwell 
@_subject: Exit enclaves and FQDNs 
I'm working on setting up a number of nodes as exit enclaves. If I use
a normal socks4 client (resulting in local DNS resolution) it works
exactly as I would expect: All traffic to the exit host uses the exit
host local tor node.
If instead I use a client with privoxy and sock4a with DNS resolution
performed via tor I find that the *first* request to the FQDN of my
exit host uses some random exit. After that my tor client appears to
have cached the result and all further http accesses are via the local
Because this first request doesn't use the exit enclave it
reintroduces in a loss of end-to-end encryption and risk of malicious
exits. While one connection isn't so bad... for http a malicious exit
could respond with a redirect to a proxy they control.
Am I missing some aspect of the configuration which removes this vulnerability?

@_date: 2007-09-26 01:27:57
@_author: Gregory Maxwell 
@_subject: Clone nodes 
What would be the implication of running multiple copies of tor with
identical configuration and duplicated private key data on a single
IP:PORT with a TCP connection based load-balancer in front of them?
To the outside world it would look like a single host/node.
Would it break the Tor network?
I've configured this on a private testing tor network and it appears
to work without problems, but it is a huge pain to build a fake tor
network big enough to do real testing.  I don't want to connect my
clone nodes to the public network if there is a risk of causing
breakage. ;)
The reasons I am interested in doing this are largely external to tor,
although if it works without trouble it could be used to scale a tor
node in the fortuitous event that there was ever enough traffic at one
point to justify it (i.e. an exit enclave running on a popular site).

@_date: 2008-04-20 00:33:41
@_author: Gregory Maxwell 
@_subject: Firefox sends your uptime 
It looks like the change just makes it send the current time. While
that should be an improvement, It's not at all clear to me that the
privacy issues of this are fixed.
Many many users do not have clocks which are accurate enough that
second level quantization hides their skew. I've successfully used
remote client time to identify trouble making users on IRC (though on
IRC I had the benefit of the returned time being local time rather
than GMT).
If the world didn't end with the client sending uptime .. could
perhaps it send some other value?

@_date: 2008-04-05 17:11:38
@_author: Gregory Maxwell 
@_subject: Firefox sends your uptime 
Why does TLS include this?  Is it used for some kind of replay prevention?

@_date: 2008-04-05 17:11:38
@_author: Gregory Maxwell 
@_subject: Firefox sends your uptime 
Why does TLS include this?  Is it used for some kind of replay prevention?

@_date: 2008-12-06 21:23:20
@_author: Gregory Maxwell 
@_subject: UK internet filtering 
I intentionally did not link to the Wikinews article: They're often junk.

@_date: 2008-12-07 20:17:02
@_author: Gregory Maxwell 
@_subject: UK internet filtering 
It's interesting to point out that Finnish Wikipedia has the image and
the UK is not censoring that:
(most non-english Wikipedias do not have it, but only because they
have no album covers at all because they only permit images under a
free content license)

@_date: 2008-12-08 09:56:56
@_author: Gregory Maxwell 
@_subject: technical solution for censorship [was: UK internet filtering] 
Make exit enclaves work better and encourage their use. Right now they
result in an extra hop. That does not solve all cases, but it would do
a lot for people running resources which they know are being censored.
... it's also not incompatible with other lower-performing but more
broadly working solutions.

@_date: 2008-12-08 16:21:47
@_author: Gregory Maxwell 
@_subject: technical solution for censorship [was: UK internet filtering] 
I thought a driving idea behind TOR was to bring together people with
different goals but a common need (or at least a set of tightly
related needs) so that we form a larger anonymity pool and benefit
from common investment in infrastructure.
Many Tor users regard some of the uses to be unfortunate but tolerable
side effects.  I run nodes primarily because I want to help people
avoid censorship, I regard anonymity (except to the extent that it is
needed to avoid effective censorship) to be socially detrimental
collateral damage. (
So when you use Tor to achieve anonymity you should be thankful that
there are other people who use the same tools to avoid censorship.

@_date: 2008-12-06 19:49:58
@_author: Gregory Maxwell 
@_subject: UK internet filtering 
I've confirmed the reports of UK ISPs censoring Wikipedia using some
UK tor exists.

@_date: 2008-01-07 17:02:07
@_author: Gregory Maxwell 
@_subject: What to do at IP number change? 
I think this is unwise and an example of bad net-citizenship.
Making TOR sessions hard to detect for people that have an actual need
to block TOR will only result in more paranoid witch hunting against
Tor (like we see with some anti-spam RBL lists) and needless
If this or things like it continue people who see a need to block (or
otherwise treat differently) users from TOR will simply run probing
hosts across the tor network, wasting TOR bandwidth, and applying
their tor blocks to big proxy servers. The resulting overblockage will
increase general anamosity towards tor. ("Those guys who got my school
blocked from editing wikipedia!").
Basically .. trying to hide exists like this is an arms race that you
can not win, and the price will be anger where understanding and
cooperation are needed most.

@_date: 2008-01-24 23:36:37
@_author: Gregory Maxwell 
@_subject: Child pornography blocking again 
Forget ISPs blocking it. Sites that falls into the category of
unspeakable awfulness are *already* illegal and are shutdown anywhere
laws are enforced.  Such sites shouldn't exist long enough to make it
into your blacklist. If they do the inability of TOR to block access
to them is the least of the worlds problems.
I'd also argue that the ability of people to use tor to access those
kinds of sites is actually beneficial. It allows private individuals
to seek them out in order to report them with reduced risk of being
mistakenly identified as a pervert themselves. Tor also enables law
enforcement to evade blocks of obvious law enforcement IP space and
potentially penetrate deep into underground groups creating and
circulating the stuff.

@_date: 2008-01-26 14:42:04
@_author: Gregory Maxwell 
@_subject: Child pornography blocking again 
It would have been better if you had, but you would have still
received a negative response.
Further splitting the anonymity set just wouldn't be a good thing
unless it was *really* needed.

@_date: 2008-01-26 16:15:10
@_author: Gregory Maxwell 
@_subject: Child pornography blocking again 
Some f'ing paedophile is responsible for being a pervert, but the
invasion of your home, the home of an innocent person, is the fault of
your government, not some pervert.
Even if it could be done it would not address the root cause of your concern.

@_date: 2008-01-28 08:39:16
@_author: Gregory Maxwell 
@_subject: How to remove some useless nodes 
It would be interesting if tor exits used passive connection
monitoring to figure out if they are on a content modifying or
censoring network, then made a note of it in the directory. Users
could then choose to avoid that exit while people interested in
censorship or neutrality would have a shortlist to do research from.
Some types of censoring are pretty subtle and couldn't easily be
detected this way, but the Great Firewall is pretty obvious.

@_date: 2008-03-02 21:18:51
@_author: Gregory Maxwell 
@_subject: Defeat Exit Node Sniffing? 
On Sun, Mar 2, 2008 at 6:34 PM, Michael_google gmail_Gersten
The 'right' way to do this would be to signal it in DNS.  By signaling
it in DNS you'd avoid another round trip, etc.    The problem with
doing it in DNS is that DNS isn't widely authenticated.   ... which is
ashame since it could be.. dnssec exists for that purpose.

@_date: 2008-05-15 20:20:27
@_author: Gregory Maxwell 
@_subject: lots of DMCA request's... (1/day) 
Okay, what I've seen lately is that people are connecting to
Bittorrent trackers via TOR.   This causes the tracker to see the TOR
exit as the client IP, but the actual data is exchanged directly
between the peers without TOR's involvement.    As such *no
copyrighted data crossed the tor node*.  This makes a lot of sense for
the user since it hides their IP from people trying to enforce
copyright, but doesn't really slow down their transfers.
With this in mind, Are the DMCA notices coming from the same few parties?
If so, the following tactic might be helpful:
A valid DMCA complaint includes a sworn statement that the issuing
party, to the best of their knowledge, believes the claim of
infringement to be accurate. The copyright complaint is not accurate
in these cases, but that may not be clear to these DMCA-bots.  (They
shouldn't be making claims based on tracker reports ... since tracker
use != copyright infringement, but no court has yet slapped their
wrists for it...)
If you make sure that the appropriate party at each company has actual
knowledge of the nature of your IP as a anonymizing proxy that hosts
or transmits no copyrighted materials belonging to them and that
complaints of this type are going to be invalid and they continued to
send claims that you are distributing their works then they are at
risk of being found guilty of perjury.
Of course, there is no precedent for this kind of fight yet., but it
might be effective to make that argument in a certified letter to
their legal offices.  At the least it should cause them to figure that
you are more trouble than it's worth.
Obviously, it would be most wise to consult an attorney. I am not an attorney.
Socially, I don't see any problem with defending the use of TOR to
connect to trackers anonymously.  Connecting to a tracker and
exchanging data is not a crime, it's not copyright  infringement.
You're not disturbing the rule of law by anonymizing an actual crime.
The DMCA-bots should limit their complaints to cases where there is
more direct evidence of a crime available.  .... although the risk of
this line of thinking is that requesting the copyrighted data could be
argued to be inducement (or not a violation since it was sending the
data to the owners agent), so it may form an argument increased
surveillances powers by copyright holders.  :(    But I guess
copy[rf]ight philosophy is going pretty off-topic for this list.

@_date: 2008-11-18 10:10:41
@_author: Gregory Maxwell 
@_subject: Limiting hops 
I think we can safely say that for most people using Tor makes it
*more* likely that their unencrypted traffic will be sniffed.
But TOR would usually change who is sniffing the traffic:  Some guy in
his basement, some foreign government, or some spammer/scammer
harvesting email addresses and credit card numbers from your traffic
may be preferable to the person's school, employer, or government all
of which may have a more direct and specific interest in the person's

@_date: 2008-10-08 23:31:45
@_author: Gregory Maxwell 
@_subject: same first hops 
Yes. A Timing/Sizing attack.  He sees the last hop exit.

@_date: 2008-10-08 23:41:36
@_author: Gregory Maxwell 
@_subject: same first hops 
Sorry, I accidentally hit send.
Consider: Nothing prevents you from running multiple tor nodes. A well
funded party might run dozens or hundreds.  If the attacker controls
both the entry and the exit that you are using he can look at the
unencrypted traffic leaving the exit and correlate it with the timing
and sizes of the data on the the entrances he controls.  He could also
do things like intercept your TCP connections leaving the exit and
stuff them with megabytes of junk data and then watch for the traffic
spike on any of the entrances he controls.
If you think about it for a bit you'll realize why changing entrances
all the time would maximize your exposure to this attack. Eventually
you would land on the bad guy's entrance and he could track you down.

@_date: 2008-10-09 00:11:57
@_author: Gregory Maxwell 
@_subject: same first hops 
Hidden after the first hop.
The first hop knows your IP, otherwise it couldn't talk to you.

@_date: 2008-09-05 11:48:33
@_author: Gregory Maxwell 
@_subject: Google's Chrome Web Browser and Tor 
Why aren't more people using virtual machines for anonymous browsing?
If your VM can't access the outside world except via TOR, and it has
no knowledge of the outside world information (because TOR itself is
running on the real machine) then pretty much all possible leaks are
closed and you're only vulnerable to leakage between multiple
anonymous things. Very simple, very clean.

@_date: 2008-09-09 20:12:30
@_author: Gregory Maxwell 
@_subject: Reduced Tor Traffic [was: Re: peculiar server...] 
Considering the implications of exit-node operation, I don't expect
that pattern to end any time soon.
Perhaps it would be reasonable to do more promotion of hidden
services, OnionCat,  exit-enclaves, and other non-exit limited
services to make good use of what is likely to be an excess of
middleman capacity, and help drive more tor adoption?

@_date: 2009-04-03 15:29:37
@_author: Gregory Maxwell 
@_subject: Fwd: [Wikitech-l] Planning to tighten TorBlock settings 
On Fri, Apr 3, 2009 at 12:34 PM, Paul Syverson
So I need to ask you to take a leap of faith here: I need you to simply accept
* There are people involved in the Wikimedia project who care deeply
about TOR relevant issues like the freedom from censorship.
* That these people generally understand the issues, the technical
factors of how Tor works, and have spent a considerable amount of time
thinking about these issues, conducted measurements, etc.
* That these people understand the issues facing Wikipedia better than you do.
If you can't accept these things, at least for the purpose of
discussion, then there is no point to discussing anything further, you
will simply be ignored by the Wikipedia crowd and the TOR community
will continue to have virtually zero pull to influence their
With that out of the way?
I'm quite confident that it would not be pointless.
At any given moment the English Wikipedia is blocking write access
from a considerable portion of the total IPv4 address space.
Persistently write-blocked access includes almost every widely known
anonymization service (including the commercial offerings), the entire
address blocks of many of the lower priced colocation/shell providers,
many tens of thousands of open or partially open HTTP proxies, a large
number of ISP transparent proxies which have not been registered as
trusted XFF header sources. Frequently short term blocked ranges
include popular broadband providers that allocate from large
non-geographically specific pools, as well as entire universities and
branches of government.  Last I looked there was a good number of /16s
The site administrators can selective grant block bypasses to users,
so if a regular good contributor is impacted when her entire
university gets blocks she can be exempted by simply requesting an
Because of the aggressive use of both long and short term write access
blocking made possible by a general community indifference to
collateral damage against users who are not regular contributors (whom
can be exempted) the existing tools are *highly effective* at halting
repeated aggressively disruptive behaviour.
We must distinguish casually disruptive behaviour and aggressively
disruptive behaviour. For the former, someone scribbling nonsense into
an article, TOR is simply not a factor? these people are not going to
use Tor. If they had enough brains to get Tor installed they'd
probably come up with a more productive use of their time.  Few people
involved with Wikipedia think they are fighting that problem by
limiting Tor.  The latter type includes aggressive fairly intelligent
people who are willing to put in considerable time and effort. These
sorts of people will write their own attack software, and yes, they'll
use Tor.  These people produce damage in great disproportion to their
Without Tor these people will have access to some number of yet
unblocked anonymization services, internet cafes, broadband provider
subnets, dialup accounts, etc.  They crop up cause some damage and the
avenue is closed with a (frequently permanent) write access block.
Once they have exhausted their available means finding new proxies is
time consuming and costly, especially since English Wikipedia has so
much exposure that all the low hanging fruit is blocked. The attacker
will usually give up, or at least be greatly slowed.
With Tor permitted they can simply continue to make their trouble
continually and without abatement for as long as they like ? and can
do so at high rates of speed relative to the overall editing activity
on the site through massively parallel automated operation.
Prior to the existence of Tor awareness in mediawiki Tor exits were
blocked randomly as troublemakers cropped up on them. At any given
time only about 300 Wikipedia-reaching exits were blocked, which was
actually sufficient because most attackers were not quite smart enough
to figure out how to visit Wikipedia with named exits (which a serious
usability problem with Tor? because it's non-trivial to get exit
syntax working with vhosted sites).  ... but this also meant that
legit Tor users also couldn't edit either, as they were even less
likely to find and use working exits than the attackers.
This presumes a fairly narrow view of abuse, and I think a greatly
underestimated view of the total cost of escrowing.
Common forms of abuse include things like flooding history and logs
with abusive messages and the repeated posting of private information
which Wikipedia chooses not to disseminate for legal or ethical
reasons.  In fact these kinds of attacks probably now constitute a
majority of the activity from the aggressive attackers, as their goal
is usually to piss off the Wikipedia community, and these methods are
actually more effective to that end then screwing with the content. (A
particular perversion of the Wikipedia community, perhaps, but it is
what it is?)
Escrowing can't fix flooding, nor does escrowing make sense for
non-content materials.
Even for content escrowing isn't simple. When the most recent edit is
escrowed, hitting edit does what? Does it fork the non-escrowed text?
Does it subject the editor to the (possibly offensive or browser
crashing) escrowed material? If it was forked, now an approver must
not just approve but now must manually merge the text.  It becomes
messy quickly, and the Wikipedia community has already clearly sent
the message that however much it cares about Tor it does not care so
much as to do a lot of additional work to support it.
That was then, this is now.  In any case, Jimmy touches on a lot of
areas but isn't deeply involved with dealing with these problems on a
daily basis, the people whom are have significantly different
I'm aware of the nymble system. But it is completely integrated on the
Tor side. To make use of it you have to conduct some complicated dance
of copying around cryptographic blobs between websites, running
javascript cryptographic engines, etc.
Frankly, it was a dance that I'd never bother going through. It's
something which we could only reasonably expect troublemakers to use.
(And? perhaps a few Chinese dissidents? except their needs are
frequently met already by friends in other countries running closed
access https proxies for them)
(I believe it also, or at least one of them required the whole site to
be via HTTPS, which is pretty much a non-starter for economic reasons;
but I could be misremembering)
Neither of these systems are or can ever be a panacea: In the best
case they are a least a 2x force multiplier for attacker, though in
practice even more since they they also inhibit Wikipedia from
utilizing range blocks.  So allowing these systems systems would be a
compromise? once you factor in a decent amount of additional code to
maintain on the WP side, and that these systems were too complicated
for almost anyone except an attacker to use due to a lack of Tor
integration?  of course there hasn't been much interest in deploying
If, instead, there was some magic torbutton nymthing that interfaced
to websites via a standard remote authentication mechanism like OpenID
which someone is actually going to maintain,  then perhaps you'd see
people actually using it. (and perhaps still not Wikimedia sites, but
at least there would be a greater chance).  It's certainly bound to be
more effective then some protest of blocking read access.

@_date: 2009-04-03 09:54:14
@_author: Gregory Maxwell 
@_subject: Fwd: [Wikitech-l] Planning to tighten TorBlock settings 
---------- Forwarded message ----------
en.wikipedia.org and others have seen a rash of abuse coming via Tor in
the form of account creations with abusive names and such; this is
taking up a large chunk of CheckUser and oversighter time and effort,
which is apparently not too fun.
It looks like the current settings don't generally restrict various
actions to a logged-in user when accessing through Tor; is there any
objection to tightening this up to restrict edits, account creations,
etc via Tor except when the account is explicitly excepted?

@_date: 2009-04-03 12:03:53
@_author: Gregory Maxwell 
@_subject: Fwd: [Wikitech-l] Planning to tighten TorBlock settings 
Wikipedia's problems with TOR are purely practical ones? when editing
is enabled via TOR a few bad people will cause major problems from
time to time and Wikipedia has no means to inhibit this abuse short of
blocking write access from Tor entirely.
This is generally regarded as very unfortunate by many people involved
with Wikipedia and by people in the Wikimedia foundation itself?  as
this project is full of people who care deeply about free speech.
(Heck, they hired Mike Godwin as their general council)
There are some Wikipedia users whos views on Tor are not rational,
they don't recognize that a significant part (but not all!) of the bad
activity that comes from Tor just comes from other channels when tor
is blocked, or they attribute tor-unrelated trouble makers to Tor?
But I don't think these people are a majority. More involvement from
the tor using community in Wikipedia may help that.
To solve this issue I believe that TOR needs a strong pseudo-anonymous
system built in and available to users.  Something where Wikipedia can
block a single misbehaving user just as easily as they can without Tor
and that user can't simply mint 100 more accounts in a few minutes.
There have been various proposals for systems to accomplish this in
the past, but unless one is integrated and easily supported by
websites it will do no good.
I do not believe that any lesser measure will be sufficient.  If you
blocked access entirely it would simply be pointed out that the reason
for the write blocking is justified by practical concerns and that the
read denial is not under Wikipedia's control?  Many many other
websites wouldn't care if read access were blocked at all.

@_date: 2009-08-16 03:42:30
@_author: Gregory Maxwell 
@_subject: IPv6 
Do you mean making IPv6 connections via Tor or using IPv6 as a
transport for TOR?
These things are serve distinct purposes, have different challenges,
and are separable and I'd guess would be implemented separately.

@_date: 2009-08-10 14:55:13
@_author: Gregory Maxwell 
@_subject: Comcast throws down gauntlet to residential accounts 
VZN's residential AUP prohibits "servers" along with a number of other
offensive prohibitions which they don't currently enforce. (For
example, you're prohibited from using your VZN broadband for anything
"sexually explicit").
As I recall the business FiOS AUP had it's own set of ridiculous
terms... but it didn't attempt to prohibit you from running "servers".

@_date: 2009-08-10 14:55:13
@_author: Gregory Maxwell 
@_subject: Comcast throws down gauntlet to residential accounts 
VZN's residential AUP prohibits "servers" along with a number of other
offensive prohibitions which they don't currently enforce. (For
example, you're prohibited from using your VZN broadband for anything
"sexually explicit").
As I recall the business FiOS AUP had it's own set of ridiculous
terms... but it didn't attempt to prohibit you from running "servers".

@_date: 2009-01-02 13:25:00
@_author: Gregory Maxwell 
@_subject: Maximize Anonymity Services but Minimize 'Abuse Like' Behaviors... 
And manage to make yourself look like someone doing a targeted MITM
attack on tor users.  :(
The above use cases should be handled by improving the exit enclave
support and convincing these sorts of major sites to run their own
If you'd like to cut abuse without looking like an attacker yourself
you're best off limiting your exit to ports which cause few abuse
complaints.  IRC and other chat protocols are probably good examples.

@_date: 2009-01-10 23:34:57
@_author: Gregory Maxwell 
@_subject: SMTPD Hidden Server 
Spam free?  Don't make it open!
If you run a public mail gateway, eventually someone will realize its
there and start sending spam through it.  You could try to run spam
filtering, but you'll just frustrate legitimate users.

@_date: 2009-07-02 23:12:23
@_author: Gregory Maxwell 
@_subject: Safe destinations 
There are many people who would like to run tor exits but whom don't
because of the inevitable flood of abuse complaints.
At the same time, there are a great many high traffic destinations on
the internet which have little to no complaint potential because they
are effectively read-only or are otherwise understood to be
tor/anonymity friendly.
Examples include most news sites, virtually all CDN services (used to
distribute images by large sites), freenode IRC, Wikipedia, other
anonymity services, search engines, and probably most instant
messaging networks (?).
Right now nodes can attempt to exit to only to safe locations and
protocols by carefully crafting their exit policies but this takes a
fair amount of work to maintain, clutters up the directories, and
risks making the exit look like a single-purpose-password-sniffer.
How awful would it be to create a community managed list of 'safe
destinations' distributed by the directory servers as a single object
which exit operators could include in in their exit policies and
further refine with local rules?
Some exit operators would likely switch to safe-mode, reducing the
total amount of universal-exit capacity but if the safe list included
enough high traffic sites it would probably more than offset the loss
and arguably anyone who switched was likely to quit in any case.

@_date: 2009-07-04 15:35:38
@_author: Gregory Maxwell 
@_subject: Safe destinations 
So, I don't know that opt it will either map well enough to the set of
sites that won't be problematic for tor exit operators nor do I think
such adoption would be widespread enough to provide a real increase in
effective exit bandwidth.  If the safe list doesn't include some of
the highest bandwidth sites then its introduction (in whatever form)
would probably decrease the available "can reach anything" exit
I think some of the most many safe destinations are sites which are
old, inactive, and generally not likely to add some tor permission,
though having a way for sites to opt-in would also be useful.
Though if a site operator wants to facilitate tor they can run an exit
enclave i.e. run a node which only exits to themselves and tor will
use it preferentially. I don't think the importance of this technique
for increasing TOR's performance for your users can be overstated:
It's the only technique which prevents exit shenanigans for unsecured
protocols. (And separately, this is a functionality which should be
improved so that people do not feel forced to use hidden services to
get this functionality)
An interesting thing I've seen mentioned a few times in the thread is
webmail services. Are these really safe exit locations, or is law
enforcement going to come seize your node after the first dimwit sends
a bomb threat via your node and yahoo mail?  My thinking was primarily
around service which were read only or nearly so or at least where any
writing is among friends.

@_date: 2009-06-30 21:56:05
@_author: Gregory Maxwell 
@_subject: Firefox video tag 
and  should have exactly the same attack surface as  has.
Thats one of the benefits that firefox's approach of building the
codecs internally rather than invoking an external media framework
(like safari does) should have.
I've been hoping very much that tor would not ultimately need to filter these?

@_date: 2009-06-03 10:11:44
@_author: Gregory Maxwell 
@_subject: SCTP 
SCTP works fine in Linux. Has been there since around the kernel 2.5 days.

@_date: 2009-06-03 11:12:53
@_author: Gregory Maxwell 
@_subject: SCTP 
Everyone else thinking that way is why it never will.

@_date: 2009-03-26 12:51:06
@_author: Gregory Maxwell 
@_subject: News from my Tor Server raid 
(since he may be too bussy to reply?)
My understanding is that he was raided this time because he is the
holder of wikileaks.de and wikilinks has copies of things like the
Australian censorship list (which includes things claimed to be child
porn). Not related to TOR.  But he was previously subject to a TOR
related taid.

@_date: 2009-03-19 05:28:13
@_author: Gregory Maxwell 
@_subject: Bridge scanning resistance 
People are unlikely to spend $$ to give their fake https sites real ca
signed certs. Its easy to test for, impossible to fake, and given how
the browser vendors handle self signed certs someone could claim they
are trying to defeat security risks by blocking self signed
So I would guess that would put an upper limit on the level of disguse
the common node would get. The ability to multiplex with a real ca
signed https server might allow a few nodes to achieve better cover.

@_date: 2009-11-19 15:05:23
@_author: Gregory Maxwell 
@_subject: HTML5 deanonymization attacks 
It's not clear from the slides exactly how the video tags are supposed
to be bypassing tor. Is this saying that the poster attribute bypasses
the proxy settings?  It doesn't appear to do so here for me in
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-23 02:43:47
@_author: Gregory Maxwell 
@_subject: The Case for Banning Reduced Hop Count Implementations 
I presume you can back this assertion up with simulation results, at a minimum?
I look forward to reading your paper.
Which is why twiddling the hop count isn't attractive for them.
It is attractive for IRC, for example, because with the current hop counts
it can be difficult to keep a TCP connection up for long.  Long lived
connections don't benefit much from the longer paths in any case because the
provide ample opportunity to simply correlate entry and exit traffic and ignore
the interior path.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-23 11:25:55
@_author: Gregory Maxwell 
@_subject: The Case for Banning Reduced Hop Count Implementations 
On Mon, Nov 23, 2009 at 10:05 AM, Paul Syverson
Reduction to one is obviously quite terrible. The reason I trimmed off the
Lucky's message was that I thought it was just making a argument against
one-hop as endangering operators which I previously agreed with and had
argued here myself.
Thank you for taking the time to elaborate on the two-hop case.
I hadn't previously considered the entry node as valuable data worth hiding
from the exit node, but now that you point it out I find it to be a
convincing argument.
I'm not confident how real the the capacity consumption concerns are, or that
they couldn't be addressed by some other means (if you have some blinded method
of determining the minimum path length, then you could use it to
prioritize longer
path traffic by an amount sufficient to prevent it from being out competed too
I find it quite disappointing that two-hop isn't a reasonable measure to improve
performance for some users. As I've argued elsewhere I think it's important
that TOR carry a significant amount of perfectly ordinary traffic both
to provide
cover traffic, and to ensure that there is sufficient public support, as it's a
lot easier to turn a blind eye on a service you haven't used personally?
To make the point more forcefully:
I'll assume here that 'questionable origin' here is primarily talking about the
people illicitly downloading movies and the like.
I find it interesting to see the file transfer case as "comparatively low risk
behavior". The reason people have used tor for this in significant numbers is
that their activity is very likely to result in legal threats and disconnection
from their ISP, as those consequences have become common. This isn't a
risk these people face it's a real one, certainly more real than any that I've
personally had for using Tor.
(You don't have to even support the illegal propagation of copyrighted works to
support people engaging in downloading?  for example, someone might download
an album to recover material on a damaged CD, or they might be
recovering a track
they purchased but has been made available to them after the closure of a DRM
key provider,  and these use cases are no less likely to bring lawsuit than the
people who are downloading copyrighted works for which they have not
been licensed.)
Of course there are people with greater anonymity needs than the file
but if you are prepared to classify someone merely at risk of a costly
lawsuit and
disconnection from their ISP as someone who is insufficiently worthy
and guide them
and all the others with even lower needs to another service then would
TOR even come
close to the level of cover traffic required to provide anonymity to those more
strongly in need?
[The file downloading on Tor isn't a good thing: it's not good because
tor isn't the
best design for bulk transfers where latency isn't relevant... some
other design could
handle them better (and probably provide greater anonymity at the same
time). The
copyright-violating download case also has the problem that it doesn't
eliminate risks
it merely shifts them to the exit operators. (Because the copyright
holders are perfectly
happy to take the same actions against the exit operators, and many
ISPs are perfectly
happy to harass them)... but these are separate matters that have
little to do with
circuit length or the reality of the users desire for anonymization]
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-25 12:21:39
@_author: Gregory Maxwell 
@_subject: AN idea of non-public exit-nodes 
So non-disclosed bridges work because the entrance node always knows who
you are, so having to arrange something with someone doesn't disclose
much more information. It doesn't disclose where you are going.
In the case of an exit the knows where you're going but not who you are.
If you must arrange for access to the exit then the exit gets the opportunity
to learn who you are.  Once the exit knows who you are than the whole purpose
of tor is defeated.
I can imagine a couple of possible cryptographic methods which would make a
private exit unusable until there is a sufficiently large clique of people
who could use the exit... but everything I can think of would be highly
vulnerable to attack by setting up additional conspiring nodes.
It seems to me that the cases where a private exit would be useful could
be equally served by running a separate tor network.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-25 13:12:45
@_author: Gregory Maxwell 
@_subject: AN idea of non-public exit-nodes 
On Wed, Nov 25, 2009 at 1:08 PM, Paul Syverson
Okay. I'm now running a HTTP forwarder to LJ as a hidden service.
Email me for the hidden service address and port number.
I'll be posting the mapping of the LJ accounts and passwords of
everyone who uses it
to their email addresses the end of the week.  ;)
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-17 09:58:23
@_author: Gregory Maxwell 
@_subject: Reduce hops when privacy level allows to save Tor network  
I thought there were plans to offer officially offer a length-two mode?
In particular the current routing is annoying for hidden nodes and
exit enclaves as they get an extra hop.
The alternatives you suggest have advantages including improved
performance, decreased probability of being blocked, less load on the
TOR network, and possibly lower chances of funny business by unethical
exit operators.
But the user loses an opportunity to contribute to the TOR anonymity
set and further pigeonholes TOR into niche, borderline, and outright
socially harmful use cases.
There are a great many people who have merely encountered one too many
examples of the ubiquitious tracking on the Internet. For example,
Google's abuse of JS fake out the link target display and intercept
outbound links on search has been driving me nuts lately as it makes
it impossible to copy and paste links from the search results. This
makes me aware of and irritated by Google's surveillance.  If I take
up using TOR in response, I add to the anonymity set, I add to the
justifiable use cases, and I add a voice against inhibiting TOR
(either governmental inhibition or internet site operators blocking it
because its a frequent source of problem users).
I'd expect the performance impacts of casual users to be
self-limiting: People who don't really need TOR's properties are the
first to turn it off as it becomes slower.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-18 19:47:34
@_author: Gregory Maxwell 
@_subject: single hop proxy? 
This is a 'single hop':
[user]----[exit node]---[internet site]
If single hops were allowed then there would be some exit nodes which
could contain logging data which directly identified a user.
The existence of single hop traffic would encourage people to
compromise, spy on, or capture exit nodes in the hopes that the
traffic they are interested in happened to be single-hop traffic.
It would also provide a ready excuse for parties with no genuine
expectation of obtaining useful information to harass exit node
operators with equipment capture or records requests. Since single hop
connections are always denied any exit operator can always honestly
answer that he does not have and could not have any records related to
the users of his exit, and he can provide this answer without any
analysis or consideration.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-09-16 17:26:31
@_author: Gregory Maxwell 
@_subject: "I Write Mass Surveillance Software" 
The hostility on reddit is odd and unfortunate.
The obvious sidestepping is MITM-ing connections for users then shove
manipulated binaries at them which disable encryption, leak key
material, or intercept keystrokes  ... or simply perform degradation
attacks, either forcing protocols to less secure modes, or simply
blocking or massively slowing secure connections to make the user
switch to something insecure.
These have the enormous downside of being detectable active attacks.
Not something you could afford to apply frequently against general
public unless you were willing to tip off your primary target that you
were watching.  Then again? with ISPs like comcast injecting RST
packets, would a degradation attack be distinguishable?
Less obvious sidestepping would include things like simply monitoring
the remote side with the expectation that they won't be as prudent
with security as your primary target.
Black-helicopter mode sidestepping would be having pre-arranged back
doors in popular operating systems or client software.

@_date: 2009-09-16 17:26:31
@_author: Gregory Maxwell 
@_subject: "I Write Mass Surveillance Software" 
The hostility on reddit is odd and unfortunate.
The obvious sidestepping is MITM-ing connections for users then shove
manipulated binaries at them which disable encryption, leak key
material, or intercept keystrokes  ... or simply perform degradation
attacks, either forcing protocols to less secure modes, or simply
blocking or massively slowing secure connections to make the user
switch to something insecure.
These have the enormous downside of being detectable active attacks.
Not something you could afford to apply frequently against general
public unless you were willing to tip off your primary target that you
were watching.  Then again? with ISPs like comcast injecting RST
packets, would a degradation attack be distinguishable?
Less obvious sidestepping would include things like simply monitoring
the remote side with the expectation that they won't be as prudent
with security as your primary target.
Black-helicopter mode sidestepping would be having pre-arranged back
doors in popular operating systems or client software.

@_date: 2009-09-30 00:49:59
@_author: Gregory Maxwell 
@_subject: Tor server "nami" taken by the German Police 
One way to achieve a "safer exit" is not so much to exit to particular
ports but to particular destinations.  For example, read only sites
and quasi read only site (i.e. news, search engines, archive.org,
wikipedia), places unlikely to generate complaint, sites that that are
aware of tor and would tell people demanding IPs "It's a tor exit?
don't bother".    Though if many people did this it would bloat the
directories and look suspicious. (An exit only good for some
destinations looks like a snooping attempt).
I was under the impression that at least some of this seizure activity
is triggered by access to childporn honeypots, if so a destination
limited exist list should be pretty effective at avoiding them while a
port based one wouldn't be.
I don't think you have anything to worry about SSH warez sites? though
I'm sure they exist they aren't likely to ever be found by anyone!
(and thus there is little reason to run them over tor, except perhaps
as a hidden service)... exiting to ssh might, unfortunately, bring you
some unwanted attention for cracking attempts but since there are so
many zombie systems doing that I doubt anyone would knock on your door
about it.
As I think I've said before:  Keeping your exit node out of your
residence is a good idea; when the powers that be discover that their
trouble IP is in a datacenter it will cause them to reconsider their
assumptions... they might learn about TOR and give up on that
approach. Even if they do not, you're more likely to hear from them in
a business like manner, rather than in the form of officers raiding
your home hoping to find something incriminating.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-09-02 16:31:51
@_author: Gregory Maxwell 
@_subject: More important: Bridges or ORs 
On Wed, Sep 2, 2009 at 3:47 PM,
Hm? but if it's dynamic the people who need to use your bridge will be
unable to find it.

@_date: 2009-09-10 16:03:06
@_author: Gregory Maxwell 
@_subject: Why you need balls of steel to operate a Tor exit node 
I think there should be an easily found list of best practices for
reducing trouble:
(1) run the above notice
(2) make the reverse DNS obvious "tor-exit.foo.bar.net"
(3) Log nothing about the traffic you carry. Logs wouldn't be useful
to anyone anyways, since if they are bothering you they already know
everything the logs would contain, but it can be tempting to go
fishing none the less. Its easier if you can simply state "there is
nothing there; the system is read-only"
(4) Keep the Tor exit on a separate piece of hardware; even some old
trash can PC should be able to keep up with the traffic most exits run
and doing so it makes it possible to compartmentalize the harm if your
node is sized, also makes the threat of seizure less worrying.
(5) Avoid running an exit from your home; not only is a raid of your
home the least desirable outcome but keeping it out of a private
residence will cause law enforcement to reconsider their assumptions,
it makes the notice from item 1 more plausible.
(5a) Institutional operation is even better. If you don't happen to be
staff at a research university perhaps you could get a local Linux
Users group to sponsor and operate an exit.
I don't think that any of these pieces of advice risk reducing the
usefulness of the Tor network unlike advice to only exit to safer
services might.

@_date: 2010-08-17 19:08:59
@_author: Gregory Maxwell 
@_subject: TLS NPN (Next Protocol Negotiation) 
s/network operator/someone with access to the network/
A protocol which places the service type outside of the crypto isn't
_only_ vulnerable to the formal operators of the network it's just
simply vulnerable.  If you can trust that people with access to the
network are trustworthy then why are you using TLS at all?
If the IETF wishes to make the protocol subject to control by network
operators then they should incorporate an explicit cryptographically
secured backdoor (i.e. something similar to key escrow). This would be
bad from a privacy and security perspective, but because it would be
explicit it would still be arguably superior to INTENTIONALLY MAKING
THE PROTOCOL IMPLICITLY VULNERABLE NOT ONLY TO THE PEOPLE YOU ARE
EXPECTED TO TRUST BUT TO THE ENTIRE WORLD. ahem.
I feel better now.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-21 20:55:49
@_author: Gregory Maxwell 
@_subject: Tor + SELinux sandbox = leak proof without VM overhead? 
Greetings, I've searched my copy of the lists and can't find any
discussion of this. If there has been, please direct me to it.
I think it's obvious that the best way of using tor is running your
torrified apps in a VM which can only access the outside world via
TOR. This provides the highest protection from network leaks and also
partially thwarts fingerprinting.   But I can only assume that the
'cost' (performance, complexity, etc) of using a VM for tor is too
high for many people? otherwise we would insist that anyone who wants
anonymity operate that way.
Has anyone looked into using the SELINUX sandbox
( to prevent leaks?   The
sandbox provides a high degree of application isolation.  It looks
like it would be pretty much trivial to add an option to the sandbox
front end program to only allow accesses to the tor socks port from
the isolated app.
With this users on a supporting platforms wouldn't have to use
wireshark to figure out if, say, pidgin, is leaking via DNS. They
could simply run the app inside the sandbox and be sure of it.
Does this sound like a practice which should be refined and recommended?
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-24 16:06:34
@_author: Gregory Maxwell 
@_subject: [Bulk] Re: The team of PayPal is a band of pigs and cads! 
Your ISP knows who you are? so you wouldn't be "lost in the crowd".
Someone who could observe your network could potentially correlate the
times, durations, amount of data sent, etc to paypal with other data
about you in order to learn things which you might prefer that they
not know.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-25 10:38:39
@_author: Gregory Maxwell 
@_subject: Google and Tor. 
Really?  This isn't obvious?
People are running automated datamining queries _via tor_ in order to
gain control of more IPs and avoid being blocked.
Even if they weren't, they'd certainly start if Google exempted tor exits.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-25 12:42:03
@_author: Gregory Maxwell 
@_subject: Google and Tor. 
For example? a friend of mine was querying google maps to find out
their estimated travel time between every pair of US cities over some
size threshold.  After about a month of this they blocked her IP and
she moved to using tor, spreading the traffic across many exits
(which, as far as I know they never ended up blocking).
People do bulk google queries to look for sites to spam (e.g. by
googling for UI elements from wiki software plus keywords useful for
their spammish purposes). These are the datamining things I was
referring to.
Another example, some people have operated fake search engines which
do nothing but serve their own ads/malware and then direct the real
queries back to google.
I'm sure that there is a ton of potentially abusive behaviour which
I've never seen or thought of but which google is aware of.
I think it would be nice if captchas and blocking weren't the only
anti-DOS/anti-abuse mechanisms used on the web today, but this is the
world we live in.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-29 04:55:43
@_author: Gregory Maxwell 
@_subject: Tor seems to have a huge security risk--please prove me wrong! 
If this is so? that people are trying to attack tor with size
fingerprinting but failing because of the size quantization and then
failing to publish because they got a non-result? then it is something
which very much needs to be made public.
Not only might future versions of tor make different design decisions
with respect to cell size, other privacy applications would benefit
from even a no-result in this area.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-30 00:21:29
@_author: Gregory Maxwell 
@_subject: Tcpcrypt and tor 
Tcpcrypt ( proposes a new extension to TCP to
enable opportunistic encryption with optional authentication. From a
features and performance perspective, it's probably exactly what we
need to get away from the almost-everything-in-the-clear Internet that
we have today.
Unfortunately, it won't interact well with tor as tor is today: It's a
TCP level technique and with tor the TCP sessions don't cross the
network. This means it would provide security between an exit and the
destination but not end to end security.
I spent a little time thinking about this and trying to figure out if
there were some socket options that could be added to tcpcrypt in
order to make it run in a purely proxy mode where the data is end to
end encrypted but the TCP still runs on the exit. However, I don't
think this is possible:  It integrates deeply with the TCP state
machine. For example, it uses TCP's sequence numbers as the counter
and replay prevention. It also uses TCP retransmission (with it's own
MAC) to deal with forged data.
I don't like the idea that a future layer-3 transport in tor is the
solution to this: Today tor gains a lot of fingerprinting immunity by
isolating the layer 3/4 and it's also nice that the tor software
doesn't need access to weird raw sockets so that it can inject
So perhaps someone smarter than I can see a way that tor could gain
end to end crypto in a world using tcpcrypt, perhaps with some changes
to tcpcrypt?
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-01 23:02:53
@_author: Gregory Maxwell 
@_subject: Padding again Was: Practical web-site-specific traffic analyses 
The overhead of padding schemes that I've seen, either end to end
type, or hop-based for free routed networks as presented above, are
simply too large to be practical.
I'd also guess that there might also be a negative social effect where
people would be less inclined to run relays if they knew that only a
small fraction of the traffic was actually good-put.
I think this makes a good argument for combining tor with a
high-latency higher anonymity service? so that the "padding" for the
most timing attack vulnerable traffic can still be good traffic
sourced from high latency mixes stored at nodes. ... but this wouldn't
be simply accomplished, and I'm not aware of any ongoing research
along these lines.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-11 19:10:05
@_author: Gregory Maxwell 
@_subject: Restricted Exit Policy Port Suggestions? 
This is, however, bad for the diversity of the Tor network. Ideally
there would be exists as widely spread as possible in order to
minimize the return on investment for attackers.
It seems to me that there exists an opportunity to collaboratively
build a list of destinations which are "safe"? in that the probability
of an ISP complaint or an unfriendly law enforcement visit is
effectively insignificant.
Safe destinations might include things like some network services
(DNS, esp if tor moves to the TCP dns stuff which has been discussed
lately), human rights organizations, other anonymity services,
read-only web resources, services which already have special handling
for tor (e.g. Wikipedia, which is effectively read-only for Tor exits,
IRC networks which identify and specially handle Tor), and services
which are known not to keep logs.
While these destinations would only amount to only a tiny fraction of
the Internet they could amount to a reasonable portion of the overall
exit usage thus freeing up the rest of the exit capacity for
everything that can't use these limited exits and providing increased
performance and diversity for things that can.
This is something that would require some technical infrastructure.
Currently nodes don't get an exit flag unless they are fairly broadly
open... and thousands of nodes each running a different idea of the
safe destinations would create a computational burden on circuit
creation as well as significant directory bloat. Setting the exit flag
on nodes with very narrow exit policies would also facilitate the
creation of targeted exit spying nodes.
To avoid these problems a single template exit list could be
distributed with the directories then included in node exit lists.
I don't have any great answer on how to create and manage such a list?
a small one is fairly easy to manage but I don't expect a large one to
But I think the bigger question is: would the existence of this option
discourage the creation of full exits to such an extent that it would
hurt the tor network overall?   At least in the US and soon, with the
ACTA, perhaps most of the developed world I think the answer is no.
The difficulty in establishing network connectivity which won't be
immediately shutdown due to overzealous notice-and-takedown
conformance is already so great that anyone running a full exit
instead of a relay is obviously putting out a special effort to do so.
The existence of an easy limited-exit option shouldn't change the
incentives much.
There are other things which could be done to increase the usefulness
of the tor network in the face of an increasingly difficult exit
climate, for example improving the exit enclave functionality would be
helpful (putting services which do not need anonymity themselves
behind hidden services is far from optimal both due to performance and
name discovery issues), but I don't think this would provide as great
or as immediate a benefit as simply increasing the real exit capacity
to selected destinations.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-11 19:10:05
@_author: Gregory Maxwell 
@_subject: Restricted Exit Policy Port Suggestions? 
This is, however, bad for the diversity of the Tor network. Ideally
there would be exists as widely spread as possible in order to
minimize the return on investment for attackers.
It seems to me that there exists an opportunity to collaboratively
build a list of destinations which are "safe"? in that the probability
of an ISP complaint or an unfriendly law enforcement visit is
effectively insignificant.
Safe destinations might include things like some network services
(DNS, esp if tor moves to the TCP dns stuff which has been discussed
lately), human rights organizations, other anonymity services,
read-only web resources, services which already have special handling
for tor (e.g. Wikipedia, which is effectively read-only for Tor exits,
IRC networks which identify and specially handle Tor), and services
which are known not to keep logs.
While these destinations would only amount to only a tiny fraction of
the Internet they could amount to a reasonable portion of the overall
exit usage thus freeing up the rest of the exit capacity for
everything that can't use these limited exits and providing increased
performance and diversity for things that can.
This is something that would require some technical infrastructure.
Currently nodes don't get an exit flag unless they are fairly broadly
open... and thousands of nodes each running a different idea of the
safe destinations would create a computational burden on circuit
creation as well as significant directory bloat. Setting the exit flag
on nodes with very narrow exit policies would also facilitate the
creation of targeted exit spying nodes.
To avoid these problems a single template exit list could be
distributed with the directories then included in node exit lists.
I don't have any great answer on how to create and manage such a list?
a small one is fairly easy to manage but I don't expect a large one to
But I think the bigger question is: would the existence of this option
discourage the creation of full exits to such an extent that it would
hurt the tor network overall?   At least in the US and soon, with the
ACTA, perhaps most of the developed world I think the answer is no.
The difficulty in establishing network connectivity which won't be
immediately shutdown due to overzealous notice-and-takedown
conformance is already so great that anyone running a full exit
instead of a relay is obviously putting out a special effort to do so.
The existence of an easy limited-exit option shouldn't change the
incentives much.
There are other things which could be done to increase the usefulness
of the tor network in the face of an increasingly difficult exit
climate, for example improving the exit enclave functionality would be
helpful (putting services which do not need anonymity themselves
behind hidden services is far from optimal both due to performance and
name discovery issues), but I don't think this would provide as great
or as immediate a benefit as simply increasing the real exit capacity
to selected destinations.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-14 11:34:14
@_author: Gregory Maxwell 
@_subject: DuckDuckGo now operates a Tor exit enclave 
Exit enclaves need a lot of work.  E.g.  Your node can't tell if an
exit enclave exists for your destination until after its done the DNS
resolution. They also add an extra in-network hop.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-14 13:32:31
@_author: Gregory Maxwell 
@_subject: DuckDuckGo now operates a Tor exit enclave 
Why don't you search the archives? The exit enclave functionality has
been discussed many times.  It only happens when the service the user
is connecting to and the exit have the same IP.
Moreover, the attack you're describing already exists? though I don't
know if I should encourage people shove beans up their noses by going
into the details here.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-15 15:13:20
@_author: Gregory Maxwell 
@_subject: DuckDuckGo now operates a Tor exit enclave 
Except that users often won't use the exit enclave due to limitations in tor.
The first connection to a destination will not use the exit enclave
because prior to the first connection the node will be unaware of the
destination IP and thus unaware of the existence of the enclave.
Incomplete directory information can also cause nodes to not use enclaves.
Exits with falsified DNS will cause nodes not to use enclaves.
These weaknesses could all be reduced or eliminated, but I don't think
people have cared too much about the exit enclave functionality.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-17 01:31:35
@_author: Gregory Maxwell 
@_subject: Tor Project 2008 Tax Return Now Online 
This is neither fair nor reasonable.
When Wikimedia broke into the top _10_ most popular sites, with
something like 100 million unique viewers in a month the annual income
was comparable to the tor project. It only broke 1m in fundraising at
the very end of 2007. It takes time to scale up an organization so
that it is able to spend large amounts of money in an efficient and
responsible way.
The Free Software Foundation 2008 990 reflects 1m in income and the
FSF has been around for 25 years and supports many initiatives.
Mozilla Foundation's 2008 990 reflects 1.2m in income (this isn't the
whole story, Mozilla's finances are greatly complicated).
I'm sure if I looked around I could find some initiative to give
blankets to kittens which raises more than all the orgs I've mentioned
combined, people allocate their charity in weird ways... and it's
isn't like homeless shelters are unreasonable things to support.
But more importantly, all of these orgs are growing. I expect that
they couldn't efficiently use all they could possible bring in, and
people are still coming to terms with the idea that software projects
can be deserving charities. "Software... thats the stuff that made
that Bill kid one of the richest people in the world, right?"
So cut the Tor folks a little slack, and if you see a way that you can
make a real contribution to improving the situation then speak up.
Everyone is a master fundraiser in their own mind.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-12-18 06:08:39
@_author: Gregory Maxwell 
@_subject: Adding voip to torchat 
Pretty much all VoIP protocols use UDP? which makes sense for their
normal realtime operation.
For voice over tor you'd really want some kind of push-to-talk
moderate latency system that uses TCP. There really isn't a lot like
that out there, but developing one wouldn't be hard.  For the audio
coding codec2 ( would be the obvious choice right
now? in order to keep the load on the network nice and low.
It sure would be nice if tor had mutually authenticated peer to peer
connections. So that every X-over-tor protocol doesn't have to invent
some crazy binding protocol like torchat uses to mutually authenticate
a hidden service. E.g. something to allows a hidden service key to be
used as "caller-id" while calling another hidden service.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-12-18 06:08:39
@_author: Gregory Maxwell 
@_subject: Adding voip to torchat 
Pretty much all VoIP protocols use UDP? which makes sense for their
normal realtime operation.
For voice over tor you'd really want some kind of push-to-talk
moderate latency system that uses TCP. There really isn't a lot like
that out there, but developing one wouldn't be hard.  For the audio
coding codec2 ( would be the obvious choice right
now? in order to keep the load on the network nice and low.
It sure would be nice if tor had mutually authenticated peer to peer
connections. So that every X-over-tor protocol doesn't have to invent
some crazy binding protocol like torchat uses to mutually authenticate
a hidden service. E.g. something to allows a hidden service key to be
used as "caller-id" while calling another hidden service.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-12-29 02:21:06
@_author: Gregory Maxwell 
@_subject: 27C3 on Tor 
We can do some useful "back of the envelope calculations" so that we
can say _something_ useful about the rounding.
I spent a few minutes now contemplating this, and I thought I'd make
the data available that I used for anyone else for anyone interested
in studying this.
contains the uncompressed sizes of the wikitext for the 3.5 million
English Wikipedia articles (as of Wikimedia's 2010-10 dump).
Here is how we can use the data to reason about this attack:
Assume that the attacker knows the target is browsing Wikipedia, and
that they know the exact size of the pages loaded and want to know
what articles the person is reading. Based on this data we can compute
the  entropy and to discover how much they will learn about each page
load.  We can then study how much quantization the size reduces
Of course, attackers have a number of additional avenues to increase
the usefulness of the data they obtain: They may have some assumptions
about the prior probabilities (other than "user is browsing
Wikipeda"), they may also reason about the interlinkedness of
articles? e.g. a second page load is very likely a page linked from
the first load. You might conservatively estimate that each and every
request adds its total to the attackers aggregate knowledge.
There are a number of limits to this line of study? Wikipedia articles
are served in HTML form (not wikitext) and in the gzip encoding. I can
wave my arms and say that I don't expect the conversion HTML and HTTPS
transport to change the entropy much, and that I expect gzip to
decrease it (because smaller sizes have intrinsically less entropy).
Normally articles contain inline images? the loading sizes of these
objects probably increase the entropy enormously. These probably
aren't important compared to the fact that Wikipedia is not the whole
internet. :) Still, it's a starting point.
Here is some data,
Using the James-Stein shrinkage estimate of entropy (which gives
slightly larger results than the empirical entropy):
log2(Cell size)    Entropy in bits
0	13.48422
1	12.48014
2	11.47869
3	10.47837
4	9.478465
5	8.478762
6	7.480331
7	6.48253
8	5.493885
9	4.507543
10	3.526705
11	2.551070
12	1.599523
13	0.8287433
14	0.3627942
15	0.1329697
16	0.03448373
17	0.004374095
18	0.0002002991
19	1.336822e-05
20	6.684109e-06
(there is a single page of size zero, otherwise 20 would have 0
entropy. Over a real transport the size would never be zero, so a unit
of 2^20 would be sufficient to reduce the leakage to zero for this
So for this data, changing the transmission unit from 512 (4) to 1024
(5) would only decrease the information learned by an unbiased
attacker from one request by one bit.  (Unsurprisingly, the entropy of
the pages sizes is not concentrated in the least significant bits)
If you make any assumption that the attacker accumulates data from
request to request (e.g. due to page linkage) then I think that a
change from 512 to 1024 does not effectively thwart this attack
against this data set. If the attacker does not have that ability then
the current transmission unit already provides a substantial, and
probably sufficient, reduction in information leaked.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-12-08 10:50:18
@_author: Gregory Maxwell 
@_subject: leaker-optimized versions of Tor 
It is strictly necessary that the bad guy not control 100% of the
forwarding nodes.
On a realtime onion network anonymity is bounded by timing attacks?
even if you could tolerate the delay of having a zillion middle nodes
the attacker could just watch the entrances an exits and correlate
timing. So adding a great many hops would not significantly increase
A mix network can tolerate higher delays and, hopefully, eliminates
the timing attacks. So additional hops can be beneficial.
The down side is increased vulnerability to DOS attacks if flooders
can generate cheap round-the-world messages.
The creating a hidden service based overlay network, as was suggested
here by Karsten N., was what I thought when I read the thread? but I
was concerned that if the network identity of all/most of the nodes is
hidden that an attacker could spin up thousands of fake mix nodes
without even needing a lot of network resources. They could make it
far more likely that all your hops were controlled by one party.
Although the risk exists for non hidden service based designs, it's
probably much easier with an anonymity layer in between. Any design
using hidden services would specifically need to address this risk.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-25 20:02:12
@_author: Gregory Maxwell 
@_subject: A suggestion to TOR [a proxy server] 
(1) If the user can't install the regular tor package that means that
someone else has enough control over his system that he can't trust
any validation on his system. Short of abusing the treacherous
computing for good, there is no real way to have confidence in any
validation system running on an untrusted machine.
More practically important,
(2) If the user can install the torbutton software he either could
install tor directly or a version of torbutton can be shipped
_including_ tor itself.
(3) If the server in question provides the torbutton it could easily
provide a modified copy of it. So this doesn't eliminate the
bootstrapping problem.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-25 22:39:56
@_author: Gregory Maxwell 
@_subject: A suggestion to TOR [a proxy server] 
Firefox extensions can and do include arbitrary binary native code,
e.g. Firefogg includes ffmpeg, and I'm sure many others include native
code too. "Just an extension" is not a great way of thinking about
If people subject to policy restrictions really can't install
"software" but can install extensions then an extension might be an
excellent way of getting tor software to people... perhaps a stripped
down end user proxy only distribution of Tor.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-25 17:50:21
@_author: Gregory Maxwell 
@_subject: A suggestion to TOR [a proxy server] 
If you do not control the computer you are using then you have already
lost the privacy/censorship battle and TOR can't help you.
If someone wanted to run an open proxy network which exited via tor
there is no way that we could stop them... but they should NOT use the
word "tor" to describe their service because such a service would NOT
and could NOT provide the anonymity protection which tor is intended
to provide.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-25 18:07:02
@_author: Gregory Maxwell 
@_subject: A suggestion to TOR [a proxy server] 
Bridge-relays do no good for people who can't load the tor software.
That is specifically what I was responding to.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-06-23 10:03:50
@_author: Gregory Maxwell 
@_subject: Automated threat messages force limitation of Exit Policy  
FWIW, it appears that a provider which fails to let you respond in
objection to a DMCA complain loses their ?512.g.1 [1] indemnity.
Providers forcing you to take down legal but complaint generating
services is really against the intent and letter of the law. Sadly,
playing hard-ball with the ISP isn't likely to do you any good, since
they can just find another reason to terminate your service.
ISTM that every exit operator should probably make the effort to SWIP
their blocks prior to the generation of these nasty-grams in order to
head off this problem.
[1] To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-03-12 09:57:14
@_author: Gregory Maxwell 
@_subject: filling a network with Tor traffic 
Why bother with the VMs?
Bind all IPs to the host. Start up 700 instances of tor, each bound to
a separate IP.  Bind them do a variety of ports if you like.
If you have a lot of ram, this should work. :)
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-10-18 14:49:07
@_author: Gregory Maxwell 
@_subject: What about private & Public Keys 
No, Tor uses perfect forward secrecy. The session key for every node
to node link is encrypted with one-time ephemeral keying.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-09-12 20:28:33
@_author: Gregory Maxwell 
@_subject: When is the 'MyFamily' setting unnecessary? 
A unique AS would be a much better restriction than unique /16...
unique /16 is really just a weak proxy for unique AS.
Even ignoring the other useful disclosure properties of myfamily you
shouldn't skip disclosing the family even if the nodes are in the same
(e.g. same ASN) in the future.
It is a bit unfortunate that myfamily adds a fair bit of configuration
load (N^2) as the number of nodes becomes large, it's especially
annoying if you slowly add nodes to a family since you have to go back
and hit the all the other configurations for every node you add.
Has anyone previously suggested using a shared secret for family
configuration?  The protocol might look something like this:
The user configures a secret per family which the node is a member of.
For each family the secret is processed with key strengthening (such
as PBKDF2 or, better, scrypt) and a (say) 64-bit family ID and a
128-bit family-key are derived.  Nodes publish the family IDs.  Upon
discovering a new node with a common family ID the node contacts the
matching node and uses the non-advertised family key in a handshake
(this could be a zero knowledge protocol like socialist millionaires
or just encrypting a concatenation of nodeIDs and nonces) to prove
that the key is shared.  After proving the secret is really shared the
nodes store the results and update their family advertisements.
This would simplify family configuration down to setting a single
common secret string per family but wouldn't create any change in
behaviour for non-family nodes and could also exist side by side with
the old mechanism.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-09-12 23:36:30
@_author: Gregory Maxwell 
@_subject: When is the 'MyFamily' setting unnecessary? 
There we go?
Perhaps the signature could be shipped only to the directory
authorities but left out of the published descriptors, no? (obviously
they'd need to be left outside of the part signed by the nodes, so
obviously some reworking is required there). Directories would ignore
nodes that claim families that they can't back up with a valid
signature. This would open up some attacks by a conspiracy of evil
directories but it would be detectable and no worse than other kinds
of attacks available to similarly compromised directories.
With the signatures left out of the descriptor and 511 bit keys the
break-even point for descriptor size is four nodes in a family.  A
very quick check with my cached descriptor data locally suggests that
this would reduce the aggregate descriptor size significantly compared
to the current scheme.  (there are enough families with _many_ nodes,
to offset the fact that most families are small)
Making families more scalable would also admit things like semi-public
families.  E.g. you could share a family key with all the node
operators in a common building. Detecting things like same network can
be done automatically with enough reliability, likewise for very
coarse geographies, but fine geographical configuration would take
manual intervention... I don't know if anyone would bother configuring
this, but it would be nice if the system scaled well enough to support
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-09-13 00:26:02
@_author: Gregory Maxwell 
@_subject: When is the 'MyFamily' setting unnecessary? 
The client needs to see the public key for sure, since thats
effectively a family ID. Does it need to see the signature if instead
it trusts the bridges to have validated the signatures and correctly
ignored/invalidated only and all the nodes with invalid signatures?
If that was workable it would halve the amount of advertised data required.
I was assuming hex, like the current families. 512/160=3.2  Obviously
base>16 would do even better... With smaller ecc and base85 it would
be rather close in size to the existing fingerprints. (assuming the
signature was omitted)
Agreed, that they can be small. Though changing them would require
per-node configuration. They ought to at least be strong enough to
discourage mischief, though 159-bit is still harder than anything that
I'm aware of being cracked and would probably leave guessing the
secret as the low hanging fruit.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-09-20 11:00:41
@_author: Gregory Maxwell 
@_subject: The best way to run a hidden service: one or two computers? 
I think you've missed some points.
* The (Virtual) machine running the hidden service should probably
also have no _outbound_ network connectivity except via tor.
This is because it can be even easier to trick a software on a server
into making a network connection than it is to remotely compromise the
server. E.g. your GNU/Linux distribution may have installed some extra
CGIs in your webserver that you are unaware of...
And here is a potentially controversial suggestion, lets see what
others say about it:
* You should run your hidden service behind tor bridges rather than
directly connecting to the tor network.
The rationale for this suggestion is that it may make it more
difficult for a network observer to enumerate a list of tor clients in
order to apply things like the clock-skew attack or subject them to
additional network surveillance.
I fear that you're overstating the security provided.
For example, I think that if you managed to piss off the ISP community
vigilantes that go after spammers and botnets that they would have a
decent chance of tracking you down in spite of your efforts to stay
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-08-07 02:37:08
@_author: Gregory Maxwell 
@_subject: [tor-talk] reddit.com wants EFF to disable HTTPS??? 
The bug is that its probably overloading their site, and/or pushing
traffic onto very expensive specialized hosting.
I think this position is silly. If HTTPS everywhere says no to
reddit's request, the site will just make it not work. Then users will
be even _worse_ off, since at least people can manually go to
pay.reddit.com while reddit gets their https upgrade done.

@_date: 2011-12-05 20:28:53
@_author: Gregory Maxwell 
@_subject: [tor-talk] How important is it that the MyFamily option be set 
But it's N^2 work if you add servers one at a time, which is annoying
and failure prone.
It would be nicer if the family option took a secret string for each
specified family that was hashed (e.g. via PBKDF2) and then used as a
private key. Then the node ID is signed using that key (e.g. with
ECDSA) and the signature is published in the directory.
Nodes could then validate the signatures and then treat all nodes with
the same public key as the same family. Because the security of this
isn't terribly important a fairly small field could be used.
This would make directories bigger for small families but smaller for
big ones. It would avoid the constant update work and make it less
likely that well meaning people would misconfigure.
Sadly doing something like this w/ RSA would be very bloating.

@_date: 2011-12-17 12:52:58
@_author: Gregory Maxwell 
@_subject: [tor-talk] Is Taking Checksum of Packet Payloads a 
You're mistaking the normal purpose of entry nodes.
Normally if Alice is using Tor then she is running it herself. If she
is running it herself the traffic is encrypted between her and the
subsequent nodes. I don't just mean the end to end https? Tor itself
encrypts the traffic so that the traffic leaving her node can only be
read by the next hop. Effectively, for this purpose, the traffic
'enters' the tor network inside Alice's  computer.  The packets
observable as not identifiable as the ones leaving the network later.
If Alice was in fact not running Tor herself, then the 'entry' node
could completely compromise her privacy without checksums or exit
sniffing or anything like that, which is why Tor is not used that way.

@_date: 2011-02-22 16:29:38
@_author: Gregory Maxwell 
@_subject: [tor-talk] Is "gatereloaded" a Bad Exit? 
For that you need an exit policy to yourself, not to the internet.
I suggest an alternative question: How much better is it having more
nodes choose not to exclude 443 so that tor will have more 443
capacity, providing faster 443 service and fewer reasons for people to
use unsecured http?
How does that compare to the potential harm of throwing out a near
zero number of nodes with high suspect policies (which are probably
either misconfigurated _or_ are sniffing) and which were not
previously taking considerable exit traffic in any case? Certainly
there are other bad nodes out there, but that doesn't really make it
any better.
It's a small benefit in each direction.  "An incentive for more people
to offer 443" vs "a small amount of additional probably tainted
capacity".  Sensible people might go either way. What sensible people
won't do is participate in an epic argument about it (and I apologize
for my participation)?
Of course, until you factor in the information we received later which
is that a researcher has apparently been using a technique to discover
"passively" eavesdropping nodes, and the node in question here came
up. Sort of mooting the whole discussion until the research is
There is already support for geographic targeting in the tor software, fwiw.
It's been a long thread so I can understand why you've missed it? but
there is an _enormous_ smoking gun reason on why this should be to be
somewhat centralized.
Consider: What is more anonymous: two anonymity networks each with one
user or one anonymity network with two users?   The first has no
anonymity at all, the latter has a little.  This pattern plays out
with larger numbers.
If you can split up the users of a anonymity network you make it less
anonymous.  This is a called a partitioning attack.
If you have user selected exit subsets then you are partitioning the
network and reducing its anonymity.
It's especially bad if you know in advance who is in which subset,
E.g. "I told everyone except bob this exit was bad, so if someone is
using it it's probably bob",  but it's can be a bad attack blind  e.g.
 "Mystery person X uses exits 1,2,3 but never 4,5,6,  and
thecarp at gmail.com also uses the same mixture. I bet mystery person X
is the same persona as thecarp"
Of course, users will sometimes do things which distinguish themselves
but the software ought not _encourage_ anonymity weakening behavior
like this especially when the implications are subtle and not the
practical effect is not especially well understood.  It would be very
sad if someone thought "I need to be extra secure, so I'll turn all
these knobs" and by the resulting unique mix of exits used they ended
up uniquely identifying their client.
So ideally the default behavior should be as broadly acceptable as
possible, and then people who need different behavior should be able
to do so thought hopefully minimum changes which result in the least
anonymity loss. (And hopefully not without understanding the increased
risk that they're taking)

@_date: 2011-02-11 01:47:45
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
As far as I can tell this is a completely spurious strawman argument.
Where is this person with a legitimate reason why they can allow :80
and not :443? What is their reason?
If anyone was showing up expressing this as a serious constraint with
a legitimate cause, then it might be reasonable to reconsider.
Certainly if there were many of them.
If you want myopia, you need to look no further than this obsession
with some speculative hypothetical universe where someone is going to
be gravely injured by the tor network not choosing to use their ":80
but not :443" exit. There are a great many ways to contribute to the
tor network, so it isn't even as though their contributions are being
turned away completely.
Tor already has a great many tweaks and heuristics. Why are you not
complaining about the exit load-balancing heuristic that denies the
exit flag to nodes which don't exit to at least a /8 of several
important ports?  It impacts a great many more nodes.  Or why not
complain about the countermeasures against one hop usage that make
nodes seizure targets and takes an unfair share of the bandwidth?
Why are you not outraged about the hundreds and hundreds of bridge
operators who are getting no use _at all_ because their identities are
being held in reserve in order to build up decent pools of bridges for
use with differentiated distribution?
Will this contingent next be advocating not blacklisting exits known
to insert malware or advertisements in the traffic because without
this activity the exit operator can not afford to keep their exit
If running an exit is somehow so imposing on someone that they feel
the need to impose bizarre (even inexplicable) restrictions on its
behaviour then they really should be helping the tor network in some
other way ? by running a bridge or a regular middle node. Or finding
something else to do with their scarce resources.  Tor needs people's
help, sure, but it doesn't demand their blood. Why not let the "rich
white people in the north" that you seem to have so much disdain for
take a larger part of the exit burden?
I personally run a node with an oddball exit policy (well, it's down
at the moment due to a hardware failure). I wouldn't have any issue
explaining the exit policy to someone who asked. (basically I have a
node that exists to a collection of hand selected 'read only'
websites, plus tcp dns to some dns servers, and a number of other
assorted things that I know should will be free of complaint
generating outcomes)
But I don't care that it barely gets used as an exit (due to various
technical details), nor would I care if tor changed in a way that
would prevent it from it ever being used as an exit.
Do you really think that someone who doesn't provide contact
information is more likely to care about this sort of thing than me?
You could employ that same argument against _every_ _single_ technical
or policy decision in the design and operation of the tor software and
network. It's an information free argument.
Ultimately the tor network is _not_ anyone's personal spy-business
playground. It provides a real anonymity service to a great many
people, and real censorship avoidance to a great many people.  If it's
also useful for things outside of this, then great, but you don't get
to argue in favour of these non-primary uses when you can't even
disclose what they are.
There are too many real constraints on Tor already for people to worry
about imaginary much less unimaginable ones!
I am greatly disappointed by Tor's failure to steganographically embed
all cells in pseudo-random LOLcat pictures. My unimaginable plans for
world domination have been chopped off at the knees, for sure.  Yet
I'm here complaining about it, so certainly we should put a higher
priority on implementing LOLcat steganography than on the unimaginable
requirements of people who can't don't even bother showing up here to
express them. No?
Why do people keep bringing this up without even making the slightest
attempt to explain why these things aren't terrible from a network
partitioning perspective?
Tor is for anonymity. Anonymity doesn't work for one person alone, it
requires many different people with many different kinds of uses too
all look _the same_ to an outside observer.  If only the FBI used tor,
then every user coming out of tor would be the FBI (which is probably
all you really care to know about them).  Likewise, if users pick
subsets of exits based on all those crazy factors they become much
more identifiable.
As a result tor will always need to lean a bit towards the least
common denominator a bit, just so that everyone can stay consistent.
This is, e.g. why nodes that inject ads/malware manipulate into
cleartext http traffic or forge DNS results must be handled even
though a sizeable portion of the tor userbase probably does not care
about these things and would happily use these nodes.
The exact boundary on how much behavioural conformity is required is
unclear and could be debated (or better, studied) but it is absolutely
infuriating to see these dismissive strawman arguments placed over and
over again without even an attempt at having that debate.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-02-12 18:13:02
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
This argument has fallen on deaf ears because in isolation it produces
nonsense results. That it is continually pressed while the substantive
arguments required to actually make a decision are ignored by the
people promoting this view makes them look like fanatics, at least in
my eyes. (And I was happily ignoring the thread until someone
commented that they had be swayed by the continued arguments, which
I'd not been bothering to refute)
If we were to take this argument openness/flexibility argument as hard
doctrine why would we not provide a facility for Tor to execute
arbitrary code shipped over from arbitrary users?  That would make tor
a truly "open, arbitrary system".
Of course we don't do this because it's highly insecure and no one
would run a tor daemon which did that. So in practice we must weigh
the benefits of the speculative features mobile code many provide vs
the costs of turning tor into a opensource botnet, and in _this_
analysis the "openness" argument provides little input, because it's
the _specific_ consequences of the decision which determine the best
The "pro" you're proposing is not useful as a _specific_ pro, because
it fails completely as a specific _pro_ as the absolute majority of
the infinite number of arguments I could make under it would actually
be really bad ideas: It doesn't really do much to accurately separate
the space of ideas into good and bad ones? especially in the domain of
security a lot of bad software is bad because it was too flexible for
the wrong users.
Flexibility is also sometimes mutually exclusive and what you exclude
(no buffer overflows!) can be as much of a feature as what you include
(run arbitrary code!).  As Geoff points out? this class of argument is
fundamentally a pascal's wager.  Which God? Worshiped how? Which
flexibility? Implemented at what cost?
I grant that flexibility is a useful general principle, but one so
general that it would never be useful in making a decision by itself.
Not a specific benefit.  For example, I'd use the flexibility to say
that this policy should not be implemented in every single client,
which take forever to upgrade, but instead should be signaled via
directories? so that the decision could be change quickly and easily.
So back to the case in question: We must look at the cost of excluding
an infinitesimal piece of flexibility (the conceivable uses of four
non-exit flagged exit nodes, is I believe what this policy would
impact today), vs a tiny piece of social policy (if you want to run an
exit node to :80, you're going to allow it to exit to :443 as well or
no one will use it, thus subsidizing port 443 capacity on the back of
port 80 capacity) and decreased incentive for tor users to run
personal exit filters (which would result in network partitioning and
reduced anonymity for everyone if widespread).
Like my botnet toy example, ? the more flexible system would be
preferred all things equal, but all things are almost never equal and
so we fall to the simple balance of specific benefits. And it's very
difficult to argue for specific benefits resulting from permitting
nodes to exit to commonly non-encrypted ports while rejecting their
commonly encrypted counterparts, while the specific benefits of
rejecting these nodes are easily explained (if not all that
One of the side effects of the suggestion of this policy which I was
not expecting is that it caused some participants on this list to
expose their previously held mistaken belief that the Tor network's
technical inability to prevent exit sniffing was actually an explicit
approval of this unethical and probably unlawful activity. To the
extent that policy which is overtly sniffing hostile, if actually
ineffectual at preventing any sniffing, makes it more clear that this
activity is considered regrettable and not permitted then that can
only be a good thing as well.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-02-13 11:52:12
@_author: Gregory Maxwell 
@_subject: Excluding exit nodes 
On Sun, Feb 13, 2011 at 11:39 AM, Tomasz Moskal
This depends on the network near you and what risks you're worried
about being safe from.
If you're concerned about anonymity then sure, tor should pretty much
always be safer. (Though will you have anonymity when you're "logging
in"? It depends?)
As for security against eavesdropping? I think you can say that tor is
more secure in that regard than a network where you _know_ it's
happening, and less secure against that than most networks where you
are unsure.
In some cases, however, even if eavesdropping is happening it's better
if the eavesdropper is someone socially/geographically far away.  I
might be more happy about someone in japan, who mostly just wants my
passwords, reading my private messages than the sysadmin at the local
ISP who knows some of my friends personally.  Eavesdropping is also
usually far less damaging if the traffic has been successfully
Really, it comes down to this:  If you do not use end to end
encryption your traffic can be monitored or manipulated by a great
many people? by hackers with access to the network between you and the
other end, by the staff of network providers, potentially by
commercial agencies that ISPs have sold feeds of customer data to, by
governments along the path, etc. This is true regardless of Tor.  If
you use Tor than the people who can do these things are changed (e.g.
some other ISP instead of yours) and possibly increased (the exit
operator might be doing something nasty).
What Tor provides is the aspects of privacy that encryption can't get
you, but it doesn't replace end to end encryption.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-02-13 14:55:17
@_author: Gregory Maxwell 
@_subject: Scroogle and Tor 
You should. The current public exit service is demonstrably incorrect.
Although it's also important to know why it's incorrect.
For example, one reason that the DNSEL is incorrect is a side effect
of that fact that they are tested to see what address they _really_
exit from. Sometimes an exit is placed behind some proxy and the
address that it claims to be is not the address anyone else sees.
But? if an exit has a policy so narrow that it can not be tested by
this process then it will not show up in the DNSEL results.
So, e.g. if I ran a scroogle only exit, it wouldn't be in the DNSEL
results.  I'm pretty sure this is the wrong failure mode for the
testing process.
Though this issue means that your non-testing based results will also
be incorrect, just in another way.
There may also be other issues with the DNSEL result which I am
unaware of. The daily/weekly cycle part just sounds like the pattern
of nodes hitting their transfer limits and shutting off.  Perhaps the
DNSEL is promptly delisting these nodes when there should be a hold-up
because the DNSEL results are cached.
As far as performance goes, you can download a list of nodes which can
reach a particular address at
but, these results have the same problem with omitted nodes that I
As far as the annoying requests from tor goes, it would be better to
subject them to a captcha than to block them completely. Then again,
the big reason people use scroogle via tor is, as I understand it, to
avoid the annoying captchas that google often subjects tor exits to...
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-02-13 21:42:48
@_author: Gregory Maxwell 
@_subject: Scroogle and Tor 
This one can be kind of lame, because some requests to an enclaved
host (in particular, the first one always) will hit some random exit.
Depending how you do the blocking this can give unexpected results.
It would be nice if there were some roadmap to fixing this, since it
really diminishes the usefulness of enclaves as a mechanism for
reducing problems due to misbehaving exits. Likewise, the extra hop
probably washes out a lot of the benefit of an enclave as a
performance enhancement (though not as much as a hidden service).
It can also be tricky to run an enclave when you DNS load-balancing
(especially with multiple datacenters): You must have an 'apparent'
Tor node on every IP that your DNS returns.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-02-14 17:05:30
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
Here's an argument tip: When you think you've spotted some enormous
hole in the other side's argument, there is at least a small chance
that you're actually instead spotted a hole in your understanding of
their position. You should probably take a moment to reflect and make
sure you're confident that you know where the error is before hitting
send.  I refrained from answering this the first time you asked it
because I thought if I gave you more time you might realize that it
wasn't really a useful question.
No one has suggested every unencrypted port must be matched.  There
are some very clear matches which do exist (e.g. HTTP/HTTPS) and for
those matches action can be taken.  Nothing requires anything to be
done about all the other cases where such nice and popular parallels
are not obvious or where the protocols are unpopular enough to begin
with.  HTTP is an overwhelming popular port, and there really isn't
anything wrong with special casing _just_ that, if thats all that it
ever came to.
Your examples aren't the best though, SSL SMTP is on 465? and it's
probably common enough that a similar rule could be enforced if anyone
cared. IRC ports aren't all that consistent even without the
introduction of security, so there isn't much that can be said there.
Then they need to not run an exit. If running an exit is probably
going to get you killed or put in jail you should not be running one.
If you're right and the decision to allow wacko exit policies
discourages people with their life on the line from running exits,
then I could imagine no better policy.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-31 20:39:29
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
The website also advises that sniffing exit traffic is potentially a crime.
So I very much think you are mistaking the perspective of the website
operators. Please don't confuse the realistic position of "We can't
prevent this from happening" from the "we approve of this activity".
Can you exactly point out places where the website seems to suggest
that "anything goes" is actually permitted?  Any such text should be
corrected, because it's not correct. It isn't permitted to do these
things, to the extent that the tor developers and community can stop
these activities they will (but the extent is very limited).
The chances of receiving email from the weirdos here can be minimized
by unsubscribing from this mailing list.
Just saying! :-)
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-01 20:46:34
@_author: Gregory Maxwell 
@_subject: Key length and PK algorithm of TOR 
So? if someone had asked me about this I would have also pointed out
that using anything other than moderately sized RSA in the transport
security would make it impossible for Tor to look at all like a random
SSL (e.g. a http client/server) and thus be more vulnerable to
blocking by even the laziest attackers.
I haven't seen this point raised in this thread, so I'm wondering if
I'm misunderstanding or if it's just not being mentioned because even
ignoring the ciphersuite selection blocking tor based on the
on-the-wire behavior isn't especially difficult.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-04 10:13:00
@_author: Gregory Maxwell 
@_subject: Tor uses swap? 
OT, I know, but this is information that all tor node operators should have.
If you have a separate swap partition it is very easy to encrypt it on
all GNU/Linux systems.
If you use an ephemerally keyed swap you don't even have to provide a
password at boot? it will use a new random key at every reboot.
First edit /etc/crypttab, and add a line (or create the file):
swap /dev/sda9 /dev/urandom swap,cipher=aes-lrw-plain,size=256
(replace "sda9" with the name of your swap device, "swapon -s" will
tell you. It is important that you get this right.)
Then edit /etc/fstab and change the swap line to
Reboot and your swap will be encrypted (cryptsetup status swap will
give you information on the volume).
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-29 22:12:20
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
It's not realistic to think that people will maintain their own
excludelists? a few extremists will, but the bulk of the users won't.
Instead, I think that nodes which exit _only_ to the unencrypted
version of a service (e.g. 80 but not 443) should be excluded from
operating as exits entirely (except as enclaves).  In this way these
nodes would be force to "pay their way".  We can't stop them from
sniffing, but at least we can make them carry traffic they can't sniff
as part of the cost of doing their evil business. They could do things
like severely throttle encrypted traffic, but that is activity that
testing could detect.
To some extent the exit flag criteria approximates this, but it's
mostly a load balancing criteria and it's actually really easy to
trick, even though this node has not successfully done so.  (E.g.
Accept 224/4:*)
As far as that exit policy goes, the RFC1918 blocks might be there in
an ignorant attempt to trigger the exit flag for completely benign
reasons, though sniffing sounds more likely.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-31 12:47:37
@_author: Gregory Maxwell 
@_subject: Is "gatereloaded" a Bad Exit? 
I'd disappointed that you're not responding to the argument I initially posed.
We should do this not because the node looks suspicious but because we
want to shape the behaviour of exit operators.
There are legitimate reasons why tor supports an operator controlled
exit policy,  but no real suggestion has been made for a _legitimate_
reason to allow 80 and block 443.
So we make carrying 443 part of the price of being a port 80 exit.  So
this exclusion shouldn't be seen as something that will eliminate bad
guys,  ? it clearly won't ? but will instead force them to behave more
like we want them to, by adding 443 capacity and making tor faster for
https users? as part of the price of operating an exit.
The best argument against this would be that it makes it harder for
people to spot these probably-bad nodes based on the exit policy and
exclude them for themselves.  I think this downside is inconsequential
because almost no one is actually going to do this, and anyone that
sophisticated can be trusted to protect themselves in other ways.
Tor has a great many behaviour shaping incentives in the protocol and
implementation.  This would not stand out as too unusual.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-01-04 12:29:57
@_author: Gregory Maxwell 
@_subject: Tor uses swap? 
Our commands differ in the chaining and IV selection mode.  Mine
should be a fair bit faster. Both should provide adequate security.
The LRW mode I'm suggesting wasn't added to the kernel until a few
years after essiv support, which explains the prevalence of essiv in
I'm not sure what the defaults are if no parameters are specified. I'd
be concerned that it may use plain CBC, which is vulnerable to
watermarking attacks[1].
[1] To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-03-09 13:35:21
@_author: Gregory Maxwell 
@_subject: [tor-talk] Making TOR exit-node IP address configurable 
If you start inspecting and screwing with third party traffic you will
be bad-exited.
Save yourself and the tor users some trouble and don't provide exit
services if you can't do so without engaging in screwy traffic

@_date: 2011-03-09 17:58:16
@_author: Gregory Maxwell 
@_subject: [tor-talk] Making TOR exit-node IP address configurable 
On Wed, Mar 9, 2011 at 5:29 PM, Fabio Pietrosanti (naif)
Tor has currently has no facility for those users who are happy to
have random third parties screw with their traffic to opt-into it, or
those who would want to avoid it to opt out.  This means that anything
you to the traffic will have random inexplicable effects on tor users.
 Even if such a facility existed its use would likely reduce the
anonymity provided by ... partitioning the userbase (is there an echo
in here?)
The tor system does have a facility for dealing with this? flagging
the trouble nodes so that no one will use the exit at all.  If you are
lucky this is all that will be done to your node(s).
If you are unlucky tor users who have been harmed by your tampering
with their traffic may begin legal action against you, and/or people
harmed by traffic exiting your node may argue that your traffic
tampering has deprived you of any applicable legal protections as a
neutral service provider...

@_date: 2011-03-22 12:17:55
@_author: Gregory Maxwell 
@_subject: [tor-talk] Iran cracks down on web dissident technology 
I hesitated in responding because it's just so easy to run of an infinite
series of explanations. While any particular reason might not actually
be valid, there are enough plausible ones that your argument of
inconceivability can not be support.
Because governments are not monolithic entities, because people don't have
perfect foresight, because the benefit to your interests can outweigh
the benefit against your interests, and communications technology arguably
disproportionally benefits larger groups.
Interests outweighed:  Funding something like TOR may be the most cost
effective way to achieve a particular end. In particular, a US government
only anonymity network would likely not be very useful ("I don't know who
this is, but it's a fed").  Regardless of it helping the enemy too, it
can still be a net win to support.
Not monolithic entities:  If you have an organizational unit charged with
accomplishing X they will work to accomplish X. Sometimes they may work
so hard at it that stop another unit from accomplishing Y, even if Y was
more important to the overall mission.  This happens frequently in
all kinds of large organizations.
No perfect foresight:  It's not always obvious to everyone that some move
may turn net negative in the future. E.g. the US supporting the Taliban.
Larger groups:  If just you and I want to communicate with secrecy we
can do so without something like TOR? we can send coded messages hidden
in innocuous usenet posts or Wikipedia articles. The value of a network
is related more to the square of its communicating members. If you're the
bigger party it can help you more than it helps your smaller enemies.

@_date: 2011-11-04 12:14:15
@_author: Gregory Maxwell 
@_subject: [tor-talk] Don't use Google as default search in Tor Browser? 
On Fri, Nov 4, 2011 at 10:54 AM, Christian Siefkes
Anonymity is not an either or thing. If it were tor would provide
little value indeed because being completely anonymous while actually
doing anything important is almost impossibly difficult? everything
you do leaks information.

@_date: 2011-11-07 09:19:49
@_author: Gregory Maxwell 
@_subject: [tor-talk] Youtube? 
Go to  and enable html5.  Most videos should then
work in a recent firefox without flash.

@_date: 2011-10-23 20:51:45
@_author: Gregory Maxwell 
@_subject: [tor-talk] Legal or not on monitoring traffic at a Tor exit? 
This is in the FAQ.
Don't confuse capability with legality.  You should expect the same
laws which make it unlawful for your ISP to do these things make it
unlawful for exit operators.

@_date: 2011-10-26 17:34:03
@_author: Gregory Maxwell 
@_subject: [tor-talk] Rumors of Tor's compromise 
Or I could make your software start spewing an inexplicable and novel
error message? and then I look to see who googles for that error
Operational security is hard, and also not something tor can fix.

@_date: 2011-09-07 23:18:15
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor spying 
For some reason the moral standards people abide to online are unlike
the ones they'd apply in other contexts.  I'm doubtful Moxie
Marlinspike would go around jiggling the doorknobs of his neighbors or
hold their mail up in front of candles  (or at least do so without
fear of having a really bad weekend in a police office as a result).
But on line? people do. Oh well, not much we can do about that.
It's unfortunate and unlawful for people to monitor or modify exit
node traffic. You should not do so.  At the same time, _all_ internet
users should do what they can to protect themselves. These attacks
aren't just limited to Tor: regular ISPs perform them too, and if we
can't stop it there we certainly can't stop it for tor.
It's worth pointing out that Wikileaks and Jacob have refuted and
rejected the claims that (at least as far as they could be aware)
Wikileaks documents came from sniffing tor exits.
Of course, its impossible for anyone to prove they haven't been, and
thought its possible to do so no one has proven they did.  At best its
unfounded rumor, at worse its an active smear.
I find it somewhat ironic that you complain about the ethics of
obviously well intentioned security researchers while simultaneously
spreading a reputation destroying rumour.
At least we learned something useful from the sslsniff research that
might educate us about building practical secure systems. What did we
learn from your post?

@_date: 2011-09-09 06:36:58
@_author: Gregory Maxwell 
@_subject: [tor-talk] Dutch police break into webservers over hidden 
OpenVPN-based "anonymity services" ~= snake oil.
If you're running a hidden service you've already got a perfectly good
network anonymity service running.

@_date: 2012-04-18 12:07:10
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor's critique of Ultrasurf: A reply from the 
This is of more interest then their 'response' itself:

@_date: 2012-08-10 22:32:19
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor as ecommerce platform 
It's odd that this thread started with a discussion of some sketchy research
which worked by running an automated spider moving enormous amounts of
So the very thing that inspired the conversation ruins the proposed methodology.
Tisk tisk.

@_date: 2012-08-11 21:59:38
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor as ecommerce platform 
Unless I understood the paper, their measurements appear to be based
on watching listings go up and down,
which only provides a upper bound on the public activity.
When Bitcoin is correctly used the sources and sinks are one-time-use
pseudonymous locations and the standard operational practices for
private? much less, "I'm a target for wealthy adversaries"? usage is
to run bitcoin over tor.  the most obvious vulnerable points are on
the goods and inexplicable income ends? like in cash.
With poor use the activity could be very vulnerable to correlation via
compressed sensing techniques.  I and the other developers have found
it to be surprisingly hard to convince Bitcoin users how non-private
their activity can be, even with pointing them to public tracking
sites. Regardless, I still expect the high profile trouble making
users to eventually succumb to fairly boring police work rather than
fancy technical analysis, as usual.
This is what I really responded to correct.
In the last 4 hours the Bitcoin network processed 291,326 BTC in
transactions? about 3.3million USD at the current trading prices. In
_four hours_.  And this doesn't include the significant amount of
off-network BTC changing hands inside exchanges and bank like
services, though it may well be double counting coin that effectively
moved multiple times. (Which cant be measured, because it's not always
the same coins moving even if its the same 'value' moving, or the
As long as at least the parties are trusted to not doublespend against
their counter parties (bad dealing which can be trivially proven to
ensure that a cheater's reputation is destroyed) it's perfectly
possible to perform unbounded amounts of party to party transactions
totally invisibility to the network too, or to form join transactions
which concurrently settle multiple parties in a single act, and other
weirdness which makes even estimating the true activity level
difficulty.   Bitcoin transactions are just a few hundred bytes, and
there often is no need to make them public in a hurry.  I can think of
little else of value which could be made more immune to timing
analysis, if people cared to do so.
I already think these estimates of underground black-market volumes
are exaggerated, but it's impossible to know for sure. But the data
simply does not suggest that this is a substantial chunk of the
Like Tor, Bitcoin suffers from a fair amount of people eager to play
up the most controversial uses: Some do so to attack it, some because
it resonates with their juvenile desire to 'stick it to the man', but
most importantly: its a lot more exciting to present it by emphasizing
those things, regardless of how (in-)significant they are or how much
many of the users and developers wish they'd go away.
Whatever the reasons, skepticism is healthy all around.
If you had any doubts before:  Welcome to the future.

@_date: 2012-08-12 01:23:57
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor as ecommerce platform 
Wrong mental model.
You're assuming a "lawful" attacker.  This is just fundamentally
incompatible with any definition of attacker that I care about. A real
attacker doesn't follow rules that can be bent or broken.
In this case you're assuming a particular threat set?prosecution by
law enforcement in a place where the rule of law is largely effective
and at least somewhat just.  While that may apply to people
trafficking banned goods in the US, those aren't the only users of
tools where these attacks could be applied.  If the analytic tools
reliably identify their targets, that may be all that's required for
someone to go out and kill them.  The threat against people promoting
disapproved-of political positions or religions, or people disclosing
evidence of unlawful and unethical acts by powerful parties, can be
expected to be more like the latter than the former.
Even so, fancy correlation isn't used as evidence for a conviction.
It's used to identify the actual parties, then regular focused
evidence gathering and investigation does the rest. In the US,
potentially it gets used to generate probable cause for a search, as
the bar there is so low as to be almost non-existent and there is no
before the fact adversarial process to challenge them. Even absent it,
it's trivial to manufacture ample probable cause against anyone, but
doing so doesn't scale as an investigative tool unless the targeting
has been highly focused first.
Perhaps most importantly: a child sex trafficking ring doesn't need to
convince a court of law that a new customer is certainly law
enforcement before deciding not to do business with them, and I very
much want the people doing socially important enforcement work to have
good tools and operating procedures so that they can enjoy the full
investigative benefits of privacy technology. The fact that evidence
guidelines may make non-cheaters working for social good weaker is
actually good motivation for developing protection against techniques
which are mostly useful to attackers who don't care about following
those rules.

@_date: 2012-02-04 21:35:04
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor security on EC2 
I'm unqualified to say anything about the specific questions wrt VM
system security... but I thought it might be worthwhile to offer a bit
of caution related to risk saliency.
Whatever risks you decide exist in EC2 here probably also exist in
many other services (certainly ones that are similar to EC2, but
probably also in ones that look less like it). Arguably they exist in
all cases where the operators don't have physical control over the
If these risks are discussed as risks of EC2, rather than more general
risks of virtualization, or systems owned by third parties then people
may avoid EC2 in favour of alternatives which are less secure in
If I were a hostile force which was able to compromise some hosting
providers but not EC2, raising public concerns about the security of
EC2 specifically would be a smart tactic on my part. :)
Food for thought.

@_date: 2012-01-05 10:07:45
@_author: Gregory Maxwell 
@_subject: [tor-talk] Deterministic builds? 
This isn't generally a challenge which is unique to Tor, though the
different dependencies and targets may make for some different
Bitcoin is using gitian for this purpose   though
it's still very early on in its attempts to solve the binary builds
problem, it does at least manage independently reproducible linux

@_date: 2012-01-08 17:44:11
@_author: Gregory Maxwell 
@_subject: [tor-talk] Hoax? 
Perhaps this list should be moderated to at least filter out the
crackpots/disinformationists that are hardly even trying?  :-/
This sort of trash isn't worth the time it takes to delete.

@_date: 2012-01-09 19:12:48
@_author: Gregory Maxwell 
@_subject: [tor-talk] On the arm race with chinese 
He's welcome to send patches to evade its effects too.

@_date: 2012-07-01 15:38:53
@_author: Gregory Maxwell 
@_subject: [tor-talk] Anonymous Publishing Is Dead. 
I had a similar experience.
When I decided to publish a large collection (30gb) of previously
paywalled (but public domain) JSTOR documents[1] I initially planned
to do so anonymously? simply to mitigate the risk of harassment via
the courts. Ultimately, after more consideration I decided to publish
with my name attached and I think it made more of an impact because I
did so (even though quite a few journalists reported it as though it
were a pseudonym)? though if I didn't have even the prospect that I
could publish anonymously I can't say for sure that I would have
started down that road at all.
I perused anonymous publication for some days prior to deciding to not
publish anonymously and I encountered many of the same issues that
Anonymous Person above named at every juncture I hit roadblocks?
though in my case I already had bitcoins, but I couldn't find anyone
to take them in exchange for actually anonymous hosting especially
without access to freenode.   If I'd wanted to emit a few bytes of
text fine? but large amount of data, no.
It's also the case that non-text documents can trivially break your
anonymity? overtly in the case of things like pdf or exif metadata, or
more subtly through noise/defect fingerprints in images. I think I can
fairly count myself among the most technically sophisticated parties,
and yet even I'm not confident that I could successfully publish
anything but simple text anonymously.
The related problems span even further than just the anonymity part of
it.  Even once I'd decided to be non-anonymous I needed hosting that
wouldn't just take the material down (for weeks, if not forever) at
the first bogus DMCA claim (or even in advance of a claim because the
publication was 'edgy').  I ended up using the pirate bay? which
turned out pretty well, though there were some issues where discussion
of my release was silently suppressed on sites such as facebook
because they were hiding messages with links to the pirate bay, and it
was blocked on some corporate networks that utilized commercial
So I think that the problems for anonymous publication on the Internet
are actually a subset of a greater problem that there is little
independence and autonomy in access to publishing online. You can't
_effectively_ publish online without the help of other people, and
they're not very interested in helping anonymous people, presumably
because the ratio of trouble to profit isn't good enough.
About the only solutions I can see are:
(1) Provide stronger abuse resistant nymservices so that things like
freenode don't have to block anonymous parties, thus facilitating
person to person interactions.
(2) Improve the security and useability of things like freenet and
hidden services, so that they are usable for publication directly and
provide strong anonymity.
I'm disappointed to see some of the naysaying in this thread. It
really is hard to publish anything more than short text messages
anonymously, at least if you care about the anonymity not being broken
and you want to reach a fairly large audience.
[1]

@_date: 2012-07-01 16:46:38
@_author: Gregory Maxwell 
@_subject: [tor-talk] blocked exit node IP because of spam 
There are things the tor project and surrounding community could do to
help here.
For example, If I could anonymously donate $10 to a charity and in
return receive a persistent nym which I could use to get around those
kinds of blocks... I'd be hesitant to misbehave and get my nym
blocked.  (And forums should feel good about whatever small residual
amount of spammers who do buy donation nyms, because even though they
spam their need to keep buying nyms support the charities).
But no practical software infrastructure exists for this sort of thing today.
And until it does any education/advocacy will not go too far because
it doesn't offer much in terms of real alternatives.  "It's not really
so bad." "Yes it is, or we wouldn't have bothered putting in the
blocking in the first place" "er.."

@_date: 2012-07-02 00:27:35
@_author: Gregory Maxwell 
@_subject: [tor-talk] blocked exit node IP because of spam 
It's this kind of thinking that will result in the web continuing to be largely
read-only for Tor users.
People running services that block Tor aren't blocking Tor because they
Hate Freedom?, or because they can't help but staying up at night
trying to come up with ways of screwing people over.
Blocking tor isn't trivial, especially to do it well... and many of
the people who have been involved with blocking tor at major
sites are themselves Tor supporters and bridge/relay operators and
only block tor when it is clear that they must.
They block write access from Tor because when an abusive user
is blocked their inevitable recourse to evade the block is Tor (if
not their first choice).  After the umpteenth occurrence of
whatever antisocial jerkwad assaulting the site via tor it simply
has to go.
Arguing that a problem doesn't exist is unconvincing to people
who are dealing with it, arguing that blocking tor is ineffective
or involves unacceptable tradeoffs is unpersuasive to people
who have made the changes and measured the results.
One of the great forces which makes online communities
viable and not all trivially destroyed by a few byzantine
troublemakers is that the cost of excluding people is low,
but when tor makes the cost of evading the exclusion
nearly zero? the balance is upset.
Even captchas are a pretty weak tool: Commercial services
will solve them for pennies each, and targeted trouble
makers aren't deterred by them at all.
Perhaps most importantly, ? this has been the ongoing
approach used by the Tor community and it is demonstratively
ineffective: Write access via tor is frequently inhibited.
And yes, sure, there are cases where nym use doesn't
solve things. But there are a great many where it does.
The Tor project absolutely has done this in the past.
Though as far as I can tell it has not hat  much success except in
areas where the Tor prohibitions are sloppy (blocking read access,
blocking relays instead of just the relevant exits).
You're making a grave error to characterize the people who've
made different calls than you have as foolish or insensitive.
I'm sure it's true in some cases, but even the well informed
frequently make the dispassionate, considered, and
rational decision to block write access from Tor.

@_date: 2012-07-09 16:15:15
@_author: Gregory Maxwell 
@_subject: [tor-talk] hidden service on same location as public service 
It's also useful to run as an exit enclave for these purposes. You
configure yourself as an exit but only to your public IP address.
Then tor nodes will switch to using you to exit to you even when they
use your non-onion address.
There are advantages and disadvantages of exit enclaves vs onion
hosts, but I don't see a reason to not do both on a site which is
accessible both on the public internet and as a onion hidden service.

@_date: 2012-07-09 17:21:39
@_author: Gregory Maxwell 
@_subject: [tor-talk] hidden service on same location as public service 
Right. Enclaves work for people using the global domain names, onion
addresses do not.
I would always run an enclave for such a service even if all it did
was detect tor use and punt people to the onion url.
Exit enclaves have a number of limitations. For example, they're just
by IP but if the user uses your DNS name they'll make their first
request out some other exit (which could MITM redirect them) before
switching to the enclave.
They also add a hop compared to regular exiting (easily made up for by
being able to avoid congested exits)... but fewer hops than hidden
The only concern I'd see if that you may have some problems sorting
out which users are enclaves vs onion, so you wouldn't know what
internal absolute URLS to use internally.  Though if you gave people
who showed up via the enclave onion URLs for further links that
wouldn't be the end of the world.

@_date: 2012-07-09 18:14:44
@_author: Gregory Maxwell 
@_subject: [tor-talk] hidden service on same location as public service 
Bummer, they still work on old nodes (or at least I just tested and it
works for me).
I liked them for unloading exists and narrowing the exposure of
non-targeted / non-"malicious" (censorware) exit misbehavior, and for
increasing performance (which they did considerably when I tested them
years ago).

@_date: 2012-07-09 21:30:05
@_author: Gregory Maxwell 
@_subject: [tor-talk] HTTPS to hidden service unecessary? 
I was under the impression that browsers had generally stronger cookie
and cross domain policies for SSL sessions but maybe I'm imagining

@_date: 2012-07-11 14:48:56
@_author: Gregory Maxwell 
@_subject: [tor-talk] hidden services 2.0 brainstorming 
It depends on how you got the name of the site you're visiting.
(1) You get the name from a trusted source over a secure channel.
 -  Onion has complete MITM protection
 -  Selfsigned can be owned up by MITM an active network attacker near you
 -  CA is also secure, if the CA is good.
(2) You get the name from a non-trusted source or over an insecure channel
 - Onion buys you nothing over self-signed
 - Selfsigned is still completely insecure against active attack
 - CA model provides little security, even if the CA is good!
(e.g. knowing that you've connected to "gaypal" with certainty isn't
helpful if it was really "paypal" that you wanted but didn't know the
right name)
So in (1) onion beats self-signed, and in (2) even a CA is not secure.
The (2) case is kinda helpless.

@_date: 2012-06-22 00:13:06
@_author: Gregory Maxwell 
@_subject: [tor-talk] Forbes article: Tor and Bitcoin 
A word to the wise: Perhaps this is an earnest effort, but it's
impossible to tell.  From appearances it is indistinguishable from a
scam which will accrue a large amount of third party owned bitcoin and
either vanish or get "hacked".
Promotion on the Forbes.com site shouldn't be taken to signify any
evidence of reputability. I've seen first hand that they do not do
much research for this sort of thing, and articles there have
previously plugged services operated by people known to have stolen
from others.
Anonymity can an important tool for social good, but it can also be
misused people should take great caution in handing over control of
valuable information to parties that operate under the veil of
anonymity. Many people have been robbed under similar circumstances.
The open source Bitcoin client software runs excellently over Tor. If
you want to use Bitcoin anonymously its a good combination and you
don't need services like this website.  The next major release of the
Bitcoin software will feature much better support for inbound
connections in hidden services and automatic hidden service peer
discovery, making it work even better.

@_date: 2012-06-27 08:51:13
@_author: Gregory Maxwell 
@_subject: [tor-talk] [Bitcoin-development] Tor hidden service support 
On Wed, Jun 27, 2012 at 2:33 AM, Fabio Pietrosanti (naif)
There are no plans to do this currently.
Maybe it makes sense, but I'm somewhat doubtful about that.
This would be nice? there would need to be some way (preferably over
the socks port?) for bitcoin to request a hidden service be created
and to discover the address of it. Unlike torchar we don't need access
to the keys. It would need to have some way of returning the same
address if called multiple times for the same destination service so
that the hidden service address doesn't change with every restart.
Although there would be some tricky security issues to work out with
this functionality? e.g. what happens when some rogue java code you
accidentally run starts creating hidden service backdoors to your
I thought this was already a proposed tor feature, but I'm not finding
it right now.
It does use the onion addresses? they just need to be mapped into an
IPv6 address in order to be carried by the existing bitcoin p2p
protocol. The mapping is a bijection and externally to bitcoin it's
identical to using the onion addresses. E.g. the command-line
parameters deal with .onion addresses, the logs record .onion
addresses, and .onion addresses are passed into tor.

@_date: 2012-03-29 19:48:51
@_author: Gregory Maxwell 
@_subject: [tor-talk] Choosing a name for a .onon 
Using a brute force search tool like I'd advise against it? while I don't have a study to back me up I expect
'readable' names like that discourage good security practices? that
they cause people to use addresses (spread in that look like yours, perhaps)
without verifying the source? and when people do compare they are probably
more likely to just compare the readable parts.
sure, the computation is a bit of a barrier? but it's easier for the
attacker (who
may generate fake onions for many sites at once) then it is for the defender.

@_date: 2012-05-30 09:18:09
@_author: Gregory Maxwell 
@_subject: [tor-talk] Data storage in cached-descriptors 
On Wed, May 30, 2012 at 9:07 AM, Fabio Pietrosanti (naif)
And the directory authorities could freely attack such usage at will.
They're trusted to document tor nodes? but why would you trust them to
publish other data?

@_date: 2012-11-08 14:01:49
@_author: Gregory Maxwell 
@_subject: [tor-talk] Reduced latency transport for TOR 
This work could be _very_ productive for future transport for TOR:
As opposed to a raw datagram transport it still gets through the
firewalls and nats that TCP/TLS does and still looks like HTTPS to

@_date: 2012-10-16 14:35:44
@_author: Gregory Maxwell 
@_subject: [tor-talk] registration for youtube, 
It isn't just the phone? the effort required to perform that set of
activities was a non-trivial cost? but one acceptable to a person with
an earnest need of increased anonymity?,  which also created
geographic restrictions which limited the use of cheap labor in other
locations. Not to mention the cost of the knowledge of how to do
whatever workaround you provided, the cost of convincing an internet
privacy expert to help them, etc...
Maybe at some point someone will build an industrial infrastructure
that abuses the ability to buy disposable phones and resell them and
then Google will have to adapt. But at the moment?
Fundamentally all of these attacks and their defenses are operating in
the space of a constant linear work factor.  You must do one unit of
"effort" to send a valid email, the attack must do N units _or less_
to send N units of spam/crapflood/etc.   No system that puts an
attacker at merely a simple linear disadvantage is going to look
"secure" from a CSish/cypherpunk/mathmatical basis. And in
consideration of the total cost, the attacker often has great
advantages: he has script coders in cheap labor markets whilee your
honest activist is trying to figure out where the right mouse button
But the fact of the matter is that the common defense approaches are,
in general, quite effective.   Part of their effectiveness is that
many service providers (including community driven/created ones like
Wikimedia) are broadly insensitive to overblocking. Attackers are far
more salient: they are adaptive and seek out all the avenues and are
often quite obnoxious, and there are plenty of honest fish in the sea
if you happen to wrongfully turn away a few.  When considering the
cost benefit tradeoffs one attacker may produce harm greater than the
benefit of ten or a hundred honest users? and almost always appears to
be causing significant harm? and so it can be rational to block a
hundred honest users for every persistent attacker you reject
(especially if your honest users 'value' is just a few cents of ad
income). This may be wrong? in terms of their long term selfish
interests and in terms of social justice? but thats how it is, and
it's something that extends _far_ beyond TOR. For example, English
Wikipedia blocks a rather large portion of the whole IPv4 internet
from editing, often in big multi-provider /16 swaths time. Tor is
hardly a blip against this background of over blocking. Educating
services where their blocking is over-aggressive may be helpful but
without alternatives which are effective it will not go far beyond
just fixing obvious mistakes.
And? the effectiveness is not just limited to the fact that the
blocking rejects many people (good and bad alike) there are many
attacks like spamming which are _only_ worthwhile if the cost?
considering all factors like time discounting, knowledge, geographic
restrictions? is under some threshold.. but below that threshold there
is basically an infinite supply of demand.  The fact that the level
abuse is highly non-smooth with the level of defense makes it quite
attractive to make some blunt restrictions that screw a considerable
number (if only a small percentage) of honest users while killing most
of the abuse.
I think this common but flawed thinking that prevents progress on this
front.  You're thinking about this in terms of justice. In a just
world there wouldn't be any abusers... and all the just rules you can
think of to help things won't matter much because the abusers won't
follow them, and we don't know how to usefully construct rules for
this space that can't be violated. (...and some alternative ideas like
WOTs/reputation systems have serious risks of deeper systematic
kafkaesque injustice...).  And of course, _anonymous_ is at odds with
_reputation_ by definition.
The whole thinking of this in terms of justice is like expecting
rabbits and foxes to voluntarily maintain equilibrium population so
that neither dies out. That just isn't how it works.
Is it possible that all the communication advances we've made will be
wiped out by increasing attacker sophistication to the point where
turing test passing near-sentient AIs are becoming best friends with
you just to trick you into falling for a scam and we all give up this
rapid worldwide communication stuff? Will we confine ourselves to the
extreme group-think of exclusive friend-of-a-friend networks, and
excommunication for friends who make the mistake of befriending the
extreme? though after seeing the latest generation spambots over the
past few years that manage to do _worthwhile_ tech support on forums
by splicing together search engine harvested answers from
documentation and other forums it seems somewhat less farcical to me
than it used to.
If we're to avoid those risks we need to be willing to relax the
stance on justice a bit and accept that distrusting strangers,
disallowing complete anonymity, and making identities costly are
_effective_...  We must be sympathetic to the people who are suffering
with the abuse and struggling to get it out of their way and think
about what acceptably effective alternatives are the most just even if
they a still a little unjust.  e.g. being required to 'buy'
pseudonymous identities by making charitable contributions or putting
up a bond of valuable limited supply tokens, when we really think and
prefer that true anonymity ought to be allowed and unencumbered with
barriers to prevent abuse. Until there are easily deployable options
"block any proxy or anonymity network that you observe causing a
problem" is a simple and largely effective measure which we can
continue to expect to see used as a first option defense against

@_date: 2012-09-18 10:35:11
@_author: Gregory Maxwell 
@_subject: [tor-talk] How dangerous are DNS leak? 
It's very dangerous IFF it's real, and IFF you're expecting tor to
hide what sites you're visiting and services you're using from someone
who can watch your network connection.
But it's not always real, some services connect by IP and tor will
produce spurious warnings about these.

@_date: 2012-09-18 11:21:13
@_author: Gregory Maxwell 
@_subject: [tor-talk] How dangerous are DNS leak? 
On Tue, Sep 18, 2012 at 11:01 AM, Paul Syverson
I meant if and only if, thank you very much.
I grouped my clauses poorly indeed.  It is dangerous if and only if
(the warning is not spurrious && your threat model cares about them).
In the future grammatical criticisms are best delivered off-list, so
that one need not feel defensive and waste everyones time with yet
another tangent email. :P.

@_date: 2012-09-18 12:20:48
@_author: Gregory Maxwell 
@_subject: [tor-talk] How dangerous are DNS leak? 
[bouncing back to the list because I think it's useful]
On Tue, Sep 18, 2012 at 12:10 PM, Paul Syverson
You're absolutely right. I was talking about the platonic ideal user
with a Rational and Informed grasp on their threat models.
I absolutely should know better, since I'm often chiding people that
they can't know their real threats in privacy areas until its too
late, the very nature of most surveillance means that it's _secret_.
My only defense: My primary reason for posting was to point out that
Tor's leak alarms are sometimes false.  It's a frequent spurious
complaint for Bitcoin, because the p2p component of it connects by
address for obvious reasons.
The sorts of error myopia can cause are funny.
I agree that it's good and important to not let assumptions of
non-existent ideal users with non-existent complete information stand.
Thanks for your patience.

@_date: 2012-09-19 02:05:30
@_author: Gregory Maxwell 
@_subject: [tor-talk] Hidden Services 
And they should all move to places where they won't be killed for
disfavored political views, and we should all personally audit the
source that we run, and we should anticipate any attack or abuse...
It seems to me that there is a common expectation is that onion urls
provide a degree of name privacy? generally, if someone doesn't know
your name they can't find you to connect to you. If someone violates
that expectation it risks harming people until the new risks are well
known (and still even then some, as no matter how well known it is
some people will miss the fact that something enumerates the darn
Perhaps the convention is dumb. But that doesn't make it right to act
in a way that can be expected to harm people when you know better and
can avoid it.
Hopefully some kind of NG onion would include addition data in the
link which is used for introduction so rendezvous collection couldn't
get usable addresses (e.g. something as simple as an additional secret
used to complete a challenge-response knock with the end host, or as
complicated it could pack in a small ECDSA private key, the onion site
provides the RP with the public key, and for a connection to proceed
the connecting host must sign a permission slip to get past the RP,
before even getting to knock).  Though this wouldn't do anything to
prevent a service like tor2web from data harvesting.

@_date: 2013-04-05 13:57:48
@_author: Gregory Maxwell 
@_subject: [tor-talk] NSA supercomputer 
You really should take just a _moment_ to do a little figuring before
posting to a public list and consuming the time of hundreds or
thousands of people.
Lets assume that decrypting with a key and checking the result is one
"Floating point operation" (since you're asking us to reason about
apples and oranges, I'll just grant you that one apple stands for all
the required oranges).
To search a 128 bit keyspace on a classical computer you would expect
that on average the solution will be found in 2^127 operations.
2^127 'flops' / 10 exaflop/s =  2^127 flops / 10*10^18 flops/second =
17014118346046923173 seconds = 539,152,256,819 years.
...Or, about 39x the currently believed age of the universe.
Surely with a lot of computing power there are many very interesting
attacks? particularly in the domain of traffic analysis, weak user
provided keys, discovering new faster than brute force attacks, etc.
But to suggest that they're going to classically brute force a 128 bit
block cipher is laughable, even with very generous thinking.
Honestly, these other things are arguably far more worrisome but
they're all just handwaving... which is all any of this discussion

@_date: 2013-04-08 12:50:19
@_author: Gregory Maxwell 
@_subject: [tor-talk] NSA supercomputer 
For signing, ? If you are willing to have large amounts of data:  (and
you can almost always move public key bytes into the signature by
making the "public key" a hash of the real public key).
(1) You can use merkle signatures, which have stronger security
properties than the common asymmetric schemes (simply because they
already all use a hash function in a way that a second pre-image is a
complete break on the signature). They're also stupid fast, and as a
class generally secure against hypothetical quantum computers.
(2) You could use multiple schemes e.g. RSA && Ed25519 && merkle &&
lattice such that the composition is no less secure, ... and even if
all of the schemes can be attacked the cost of building the distinct
attacks may be powerful.

@_date: 2013-04-13 21:46:22
@_author: Gregory Maxwell 
@_subject: [tor-talk] Deterministic Builds - was: Bridge Communities? 
There is no Gregory Disney involved with Bitcoin as far as I know.
They're included with the source:
Yes. It may take a little jiggling to get the builds to actually be
deterministic for any particular package, but they should be
applicable to anything.

@_date: 2013-04-18 13:45:12
@_author: Gregory Maxwell 
@_subject: [tor-talk] CloudFlare 
On Thu, Apr 18, 2013 at 1:01 PM, Matthew Finkel
It's the same old story:  There are persistent highly annoying trouble
makers? not even many of them? who are effectively deterred by
blocking whatever proxies they use. Eventually they hit tor, and thus
tor must be blocked from editing.  This abuse isn't imaginary.
The various magical nymtoken ideas would probably be acceptable? they
just need to make it so that an unbounded supply of identities is not
any cheaper than it already is? but they need to be implemented and
not have a high deployment or operating cost.
There are some people who hold the position that instant doubling of
identities (w/ and w/o tor) that attackers would get is not acceptable
but with things like
 and Tor's effectiveness at evading censorship I expect that most can
be convinced that it's worth it.  Harder would be the fact that
English Wikipedia (and many other larger Wikipedias) blocks most data
centers and VPS services with large rangeblocks as they get used as
account multipliers by socks and an obvious nym implementation would
partially defeat that.

@_date: 2013-04-18 18:20:56
@_author: Gregory Maxwell 
@_subject: [tor-talk] CloudFlare 
Tor exits were not banned prior to their use for abuse. At the point
automated exitlist banning was performed a substantial portion were
manually blocked. (Which had the three way bad effect of not
completely blocking the trolls, while blocking most use by non-free
users, while also blocking ex-exits and punishing people for even
trying out being an exit).
There is no particular blocking efficiency gain that comes from using
exitlists relative to other kinds of abuse sources. The site can and
does block /16's all by itself.  (
Your approach is why the tor community will make absolutely no
progress on this subject.  Telling me that you don't think the problem
is imaginary doesn't help when everything else you say shows that you
believe it is.
You might think you're being only slightly insensitive to other
people's needs, but I am here to tell you that I am inside the both
communities and you are coming off as a clueless jerk.  This is
actually hard and it involves real trade-offs.
This attitude of "oh it's easy and you're just being a reactionary" is
embarrassing to people who know better... and to people who care less
about enabling access than I do it's so completely misguided that it
will just get you ignored.
I suggest you familiarize yourself with the previously proposed
solutions before responding.
The purpose of any anti-baddness system must be to distinguish between
good and bad users.  Things like time delays actually select for _bad_
users:  Good users are unlikely to tolerate the delay. Bad users can
just pipeline to hide the latency.  That things like this reduce
badness is only an artifact of that fact that they reduce everything.
I'm not sure where to begin here.
All I can say is that if the Tor community will allow people to
approach this issue with this kind of response it "should expect

@_date: 2013-04-18 19:34:00
@_author: Gregory Maxwell 
@_subject: [tor-talk] CloudFlare 
The flag is called ipblock-exempt
You can see the the list of uses on english wikipedia that have it here:
(bot accounts and administrators also inherit this ability without the
ipblock-exempt flag)
(As an aside, your own account was previously flagged this way, (by
Wikimedia's chairman of the board), but the flag has since been
removed because your account has been inactive:
I've been generally unable to convince people that surveillance of
Wikipedia access is both happening and actually important. The people
participating in the creation and administration Wikipedia (and
likewise those employed by the Wikimedia foundation) enjoy the
privileged of having the greatest intellectual freedom that has ever
been enjoyed by anyone anywhere. This is unsurprising: People without
substantial freedom of all kinds are not the most likely to go about
assembling a Free Encyclopedia. Like any other privileged it's not
always obvious to the beholder.
The idea that someone's Wikipedia editing (or, much less _reading_)
habits might be highly private and personal and likely to cause harm
if monitored isn't really appreciated by people who really find that
kind of monitoring hard to believe (even, ironically, when it's
currently happening to them? the illusion of intellectual freedom is
greater than the actual intellectual freedom)
I was unsuccessful in the last major datacenter reworking convincing
the technical staff to adopt an architecture which could reasonably
scale to supporting SSL always on for all readers (one where SSL
wasn't handled by a separate cluster but was instead run in parallel
on the existing non-ssl frontends).
Unfortunately, I think it will probably take someone being killed for
reasons considered unjust by western standards before the considerable
expenditure necessary to HSTS the entire site will be justified.
Pressure on this front needs to come from activists, not from
technology people.
Account creation via tor is explicitly and intentionally disabled.
The blocking of tor (and other IP) addresses is never intended to be a
part of the regular "disagreeable behavior for otherwise well meaning
and sane contributors" process. It doesn't aid in that process.
In theory blocking is really only a measure against people who are
malicious or (temporarily?) mentally ill.  Wikipedia will try to
reason you out of doing something, and if that fails, _tell_ you to
stop doing something, and then only block you if you don't listen.
Mostly the really automated behavior is not that huge of an issue? the
thousands of wiki administrators have access sophisticated  to
automated behavioral blocking tools (I think the rule expression
language in abusefilter is turing complete), account creation requires
solving a captcha... and marketers have discovered that spamming
Wikipedia can have certain unexpected negative effects once caught
(like completely disappearing from search engine indexes), so only
idiot marketers spam overtly.
But what is an issue is an issue is _non-automated_ or semi-automated
jerk behavior.  A single bored kid or irate mentally ill person can
easily fully saturate the time of ten or more Wikipedia volunteer
editors with a barrage of fake identities making subtle undermining
edits or over massive scale one time automated attacks. To some people
this kind of thing is just a really excellent MMORPG, this is, no
doubt, amplified by the fact that most of the sites operation is
conspicuously performed by human hands and minds. Much of the bad
behavior is benign but time consuming, though some is quite concerning
and violent (e.g. blasting pages with images of child porn mixed with
photos of contributors children).  Beyond the pure time consumption,
it is demoralizing and dehumanizing to the volunteer editors to
constantly be non-consensually made a target in some jerks MMORPG-fun.
There aren't many of these jerks, however? I'd guess that for any
major language there are only dozen or so world wide any any time
(they either change obsessions, grow out of it, or end up incarcerated
(no kidding), so they seem to be constantly shifting).  Because of
this aggressively blocking every IP address they have access to is
actually _quite_ effective.  You eventually get all the networks they
have ready access too (in some cases where the problem has come from
an institution, Wikipedians have traded blocking the whole institution
for eliminating the problem with disciplinary action), including
whatever open wifi they can easily reach... the first one to have paid
for botnet access gets the botnet largely blocked and so on.  It's
demonstratively effective... and in cases where overbroad blocks hit
established users, they can be exempted on an account by account
So if creating an account that can edit via tor is as simple as
solving a captcha then it will be impossible to stop these abusive
people? they will happily pipeline out account creation as fast as
whatever rate-limiters will allow them, jump through whatever hoops,
they have nearly unbounded time and motivation ... and then they can
continue to victimize Wikipedia contributors (and readers, though the
readers don't seem to take bad information of Wikipedia personally)
without consequence.
Sometimes you can be victimized by forces outside of your control and
there is just nothing you can really do about it.  But thats not the
case here, blocking every proxy the jerks use _works_. It has
collateral damage of unknown magnitude, but the part that is
specifically known can be largely solved with exemptions. The harm it
solves is insanely salient: the jerks rub your face in their success,
the harm is causes is invisible (since the visible parts get solved
with exemptions).
There are so many hurdles to equitable participation: Access to
computers, _literacy_, educational differentials, perceived societal
roles, social norms within the community making some people feel like
outsiders ...  the people excluded because they are not free and for
whom the exemption process is inadequate seem like something of a
rounding error by comparison? especially to people who find that whole
not-freeness thing to be a kind of vague and distant concept.  Doubly
so when it's easy to ignore the importance of participating in that
culture and say "for your own protection, if editing Wikipedia would
put you in danger we prefer you to not do it!"

@_date: 2013-08-05 23:47:03
@_author: Gregory Maxwell 
@_subject: [tor-talk] Is Tor still valid? 
I believe it would have had to also include a local privilege
escalation exploit and tails specific code to do the bypass.
This is basically the threat model that whonix's isolation is intended
to address, it would be good to see tails improve wrt this.

@_date: 2013-08-11 16:33:16
@_author: Gregory Maxwell 
@_subject: [tor-talk] HS drop 
Unfortunately, due to mildly design limitations in hidden services you
don't need to scan the 80 bit HS space in order to monitor which
hidden services are active.

@_date: 2013-12-28 03:46:08
@_author: Gregory Maxwell 
@_subject: [tor-talk] Improved HS key management 
One of the current unfortunate properties of hidden services is that
the identity of the hidden service is its public key (or the
equivalent hash, in the current setup), and this key must always be
available for signing on an online host (usually the HS itself, though
potentially on a bastion host).
This is pretty bad for prudent key management? the key is very high
value because its difficult to change, and then stuck always online
constantly being signed with? even on demand by a hostile attacker.
Then the matter is made even worse by there being no systematized
mechanism for revocation.
It would be preferable if it were possible to have a HS master key
which was kept _offline_ which could be use to authorize use for some
time period and/or revoke usage. The offline key could be used to
create an online key which is good for a year or until superseded by a
higher sequence number, and every 6 months the online key could be
replaced. Thus if an old copy of the HS media were discovered it
couldn't be used to impersonate the site.
Sadly the homomorphism proposed to prevent HSDIR enumeration attacks
cannot be used to accomplish this, as knoweldge of the ephemeral
private key and the public blinding factor yields the original private
I can describe a scheme to address this but I'm surprised to not find
any discussion of it.

@_date: 2013-12-28 04:14:40
@_author: Gregory Maxwell 
@_subject: [tor-talk] Vanity onion attacks 
With the advent of super fast onion address generators it's become not
too uncommon for hidden services to use vanity addresses, but this
seems to have brought about some vanity attacks where people grind out
lookalike addresses to setup fake sites. People then do a poor job
visually comparing them as the vanityness practically demands.
I've heard from some people getting tricked by some of these, but I
don't have any idea how common it is in general.
I wonder if anyone is enumerating hidden services and can post some
stats on how many low edit-distance names they're seeing in the

@_date: 2013-12-28 15:13:34
@_author: Gregory Maxwell 
@_subject: [tor-talk] Improved HS key management 
It absolutely is difficult to change? you can only "just change it" if
no one uses it.  Otherwise you're chasing people to change addresses
on websites and in software, and the static addresses in people's
bookmarks are vulnerabilities? both if the key falls into an attackers
hands but also because if users become used to the URL just changing
they'll believe it when an attacker DOS attacks the URL while
publishing a new one. Copies of the old name lurk around for years
hitting unsuspecting people, etc.
Sure, it's not the end of the world. Life goes on, and even with good
key management possible, many won't use it.

@_date: 2013-07-19 09:33:45
@_author: Gregory Maxwell 
@_subject: [tor-talk] Network diversity [was: Should I warn against Tor?] 
It's _very_ hard to reason about this subject and act safely.
It is common for ISPs to use segments in their network which are
provided by third party providers, even providers who are almost
entirely facilities based will have some holes or redundancy gaps.
Because these are L1 (wave) and L2 (e.g. ethernet transport) they are
utterly invisible from the L3 topology.
You can make some guesses which are probably harmless: a guard that is
across the ocean is much more likely to take you across a compromised
path than one closer?    but going much further than that may well
decrease your security.
These concerns should be reminding us of the importance of high
latency mix networks... they're the only way to start getting any real
confidence against a global passive observer, and the are mostly a
missing item in our privacy tool toolbelt.

@_date: 2013-07-19 13:42:03
@_author: Gregory Maxwell 
@_subject: [tor-talk] Network diversity [was: Should I warn against Tor?] 
So have low latency ones, some things fail.  Today you'd answer that
concern by running your high latency mix network over tor (or
integrated into tor) and so it cannot be worse. Answering the "you
need users first, and low latency networks are easier to get users
for" concern.
The point there remains that if you're assuming a (near) global
adversary doing timing attacks you cannot resist them effectively
using a low latency network.  Once you've taken that as your threat
model you can wax all you want about how low latency mix networks get
more users and so on.. it's irrelevant because they're really not
secure against that threat model. (Not that high latency ones are
automatically secure either??but they have a fighting chance)
On Fri, Jul 19, 2013 at 10:03 AM, Jens Lechtenboerger
Because you're lowering the entropy of the nodes you are selecting
maybe all the hosts themselves are simply NSA operated, or if not now,
they be a smaller target to compromise.  Maybe it actually turns out
that they all use a metro fiber provider in munich which is owned by
an NSA shell company.
In Germany this may not be much of a risk. But if your logic is
applied to someplace that is less of a hotbed of Tor usage it wouldn't
be too shocking if all the nodes there were run by some foreign
intelligence agency.

@_date: 2013-07-30 06:48:37
@_author: Gregory Maxwell 
@_subject: [tor-talk] BitMail 0.1 - p2p Email 
And source code that looks remarkably like GoldBug.
Also being promoted on various list for crypto/privacy minded people
by parties who seem to be pretending that they just "found" it and are
curious about it.

@_date: 2013-10-25 21:26:21
@_author: Gregory Maxwell 
@_subject: [tor-talk] x.509 for hidden services 
==Background== (you can skip to the Tor section if you don't care)
The Bitcoin universe is in the process of creating a specification for
digital invoices called "the bitcoin payment protocol". (More info:
The payment protocol allows someone to request someone else pay
Bitcoin for specific things, instructing them to pay specific amounts
in specific ways, and allows the receiver to provide things like
instructions for sending a refund if the transaction is for some
reason aborted... all sorts of extra metadata which doesn't belong in
the public Bitcoin network for scalability and privacy reasons.
One of the things these invoices have is an optional signing mechanism
for authentication and non-repudiation. Normally these requests would
be sent over an authenticated and encrypted channel which provides
confidentiality and authentication, but not non-repudiation.
The non-repudiation will provide cryptographic evidence to the
participants which can be used to resolve disputes: e.g.
"He didn't send me my alpaca socks!" "Thats not the address I told you to pay!"
"He told me he'd send my 99 red-balloons, not just one!"  "No way,
that was the price for 1 red-balloon!"
The payment protocol is extensible and may someday commonly support
many kinds of signatures, but the initial implementations only support
signing with internal x.509 certificates and verifying those
certificates with standard CAs. As _horrible_ as this is, it's better
than nothing, and the primary users asking for this functionality have
SSL websites today.  We don't believe that any other PKI mechenism is
actually functional enough to be usable (e.g. as evidenced by the fact
that downloads of our GPG signatures, is on the order of 1% of the
downloads of the Bitcoin software; and probably only a small portion
of those users have actually done anything to verify the signing keys)
today, so other options haven't been a priority.
However, the need to use the known insecure CA infrastructure for this
(optional!) feature has seriously spazzed out some people. A lot of
this is pure confusion, e.g. people thinking that all payment requests
would have to go via the CA (no kidding!), but its surprisingly hard
to convince people who are responding emotionally of the subtle
tradeoffs involved especially when they have the luxury of saying
"it's your problem to go figure out, figure it out and go write a
bunch extra of software for me". So having some alternative on day one
would be useful in helping the more conspiracy minded understand that
this isn't some effort to cram the use of CAs down their throat.
==Where Tor comes in==
One of the downsides if using x.509 certs to non-repudiate here is
that sites hosted as tor hidden services can't participate.
It occurred to me that this could be fixed with very little code:
Take the HS pubkey, pack it into a self-signed x.509 cert with the
cn=pubkeybase32.onion.  And specify that .onion certs have a special
hostname validation rule that hashes and encodes the key.
Then the whole process would work for .onion, we'd have a non-CA
option available and working, etc.
I'm aware that HS pubkeys have been used for application level
authentication in Tor elsewhere (e.g. I believe torchat does this) so
it's not entirely unprecedented. I'm not aware of anyone packing them
in x.509 certificates. If anyone has, I'd like to use the same
encoding style for greater compatibility.
The biggest reason I can see not to do this is that it will not be
compatible with future editions of hidden services which aren't based
on RSA keys. (e.g. the EC point addition stuff to protect against
enumeration attacks wouldn't fit into this model). I don't think this
is a serious concern: if HS x.509s do become widely used we could add
a new authentication type for the new onion addresses when those are
Does anyone else see any other reasons not to do this?
Are there other applications which would benefit from having x.509
certs for onion names?

@_date: 2013-10-26 01:53:23
@_author: Gregory Maxwell 
@_subject: [tor-talk] x.509 for hidden services 
Link please. :)
At least in one (early) version it needed to access the HS keys so it
could sign with them and identify itself on outgoing connections. I
didn't mean to imply it used x.509, but rather just that something
else had used a HS identity key for some application level auth.

@_date: 2013-10-27 20:15:06
@_author: Gregory Maxwell 
@_subject: [tor-talk] x.509 for hidden services 
Fantastic point, and just as easily done. As obvious as it is, I'd
forgotten about people keeping the HS keys on separate hosts.
It also raises the point that perhaps future Tor HS should also
support delegation
so that the HS master identity key could be kept offline.  E.g. you
have a HS identity
key, and it delegates to a short term HS key which has a lifetime of
only 1 month,
and perhaps has some kind of priority scheme such that a key with a higher
sequence number takes precedence. E.g. if someone compromises your key you can
instantly throw up a new service which people will connect to instead...
If your HS (bastion) host is compromised you wouldn't completely lose
control of your HS identity.
Might even be useful to pre-define a maximum sequence number such that
an announcement with
that sequence number blocks access.  So if your site is compromised
you can announce a pre-signed HS revocation which forever kills the
address so long as someone keeps periodically rebroadcasting it to
Yea, I'm not too worried about that. If the Tor usage becomes very
common we could simply extend the protocol with a tor-specific
extension that supports them.  My thinking here is that at least for
today I can do something to make existing HS identities work with very
little effort.

@_date: 2013-09-07 20:09:24
@_author: Gregory Maxwell 
@_subject: [tor-talk] NIST approved crypto in Tor? 
I believe Schneier was being careless there.  The ECC parameter sets
commonly used on the internet (the NIST P-xxxr ones) were chosen using
a published deterministically randomized procedure.  I think the
notion that these parameters could have been maliciously selected is a
remarkable claim which demands remarkable evidence.

@_date: 2013-09-08 06:44:57
@_author: Gregory Maxwell 
@_subject: [tor-talk] NIST approved crypto in Tor? 
Okay, I need to eat my words here.
I went to review the deterministic procedure because I wanted to see
if I could repoduce the SECP256k1 curve we use in Bitcoin. They don't
give a procedure for the Koblitz curves, but they have far less design
freedom than the non-koblitz so I thought perhaps I'd stumble into it
with the "most obvious" procedure.
The deterministic procedure basically computes SHA1 on some seed and
uses it to assign the parameters then checks the curve order, etc..
wash rinse repeat.
Then I looked at the random seed values for the P-xxxr curves. For
example, P-256r's seed is c49d360886e704936a6678e1139d26b7819f7e90.
_No_ justification is given for that value. The stated purpose of the
"veritably random" procedure "ensures that the parameters cannot be
predetermined. The parameters are therefore extremely unlikely to be
susceptible to future special-purpose attacks, and no trapdoors can
have been placed in the parameters during their generation".
Considering the stated purpose I would have expected the seed to be
some small value like ... "6F" and for all smaller values to fail the
test. Anything else would have suggested that they tested a large
number of values, and thus the parameters could embody any undisclosed
mathematical characteristic whos rareness is only bounded by how many
times they could run sha1 and test.
I now personally consider this to be smoking evidence that the
parameters are cooked. Maybe they were only cooked in ways that make
them stronger? Maybe????
SECG also makes a somewhat curious remark:
"The elliptic curve domain parameters over (primes) supplied at each
security level typically consist of examples of two different types of
parameters ? one type being parameters associated with a Koblitz curve
and the other type being parameters chosen verifiably at random ?
although only verifiably random parameters are supplied at export
strength and at extremely high strength."
The fact that only "verifiably random" are given for export strength
would seem to make more sense if you cynically read "verifiably
random" as backdoored to all heck. (though it could be more innocently
explained that the performance improvements of Koblitz wasn't so
important there, and/or they considered those curves weak enough to
not bother with the extra effort required to produce the Koblitz

@_date: 2014-12-11 22:29:04
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor and solidarity against online harassment 
The intensity of the language such as "Further, we will no longer hold
back out of fear or uncertainty from an opportunity to defend a member
of our community online", immediately caused people in two different
communities I'm a member of to express concern that this was basically
a declaration of war and that the Tor Project and the signing parties
might engage in activities like releasing backdoored software in an
effort to return fire.
I was only able to respond that I didn't think that was the case, but
nothing in the document provides a strong basis to support that... and
also pointing out that these people could always be coerced and so
that risk exists regardless of any statements of intent, and so we
must audit and count on the auditing of others.
A counter argument given was that the auditing by others is not
worthwhile when they are also part of the "war".
Is there a similarly strong statement that the software will never be
intentionally backdoored by the same parties that I can point people
to?  I don't wish to deflect from the serious concern about online
harassment, but it seems this statement can easily be misconstrued
(perhaps maliciously) as a statement of abandoning prior values.

@_date: 2014-12-12 02:28:48
@_author: Gregory Maxwell 
@_subject: [tor-talk] Tor and solidarity against online harassment 
I expressed no view that I had any issue with denouncing harassment.
Without context I'm at a bit of a loss as to what exactly it all
means, but I am certainly not okay with harassment.
I am upset that you thought it appropriate to accuse me (or people I
associate with) of being okay with harassment, simply because I asked
for a clarification that this new (?) intolerance of harassment didn't
extend to undermining prior values.
The statement of support was vague-- how would things change? how was
harassment tolerated before? What can I do to help? etc., no doubt
justifiably in the interest of not exacerbating the problems.
Some people read it as saying that it meant things I was sure it
didn't mean. It was very helpful to get a clarification to help put
people at ease.

@_date: 2014-06-29 18:16:10
@_author: Gregory Maxwell 
@_subject: [tor-talk] High-latency hidden services (was: Re: Secure Hidden 
If such a facility existed, e.g. a "mailbox delivery to a hidden
service" we would use it in Bitcoin, at least optionally for
broadcasting new transactions (We already use hidden services). Many
seconds of delay are basically always acceptable for transaction
broadcasts and privacy conscious users would probably not mind using
hours in a reliable system, at least if they could reduce the time
when they wanted to do so.
Designing this well may be tricky.  To prevent DOS attacks (e.g. I
send you a gigabyte of messages and the network dutifully keeps
delivering them to you) you may want to have the recipient per-approve
incoming traffic... but if if that would allow linking otherwise
independent messages from users it would break some applications.
This is fixable, e.g. connect once to get the recipient to blind-sign
a bunch of return-envelopes with the current HS key... then use them
to authorize delivery, but thats getting into non-trivial
cryptographic design.
Have you seen  ?  Part of the argument is that having a store of high latency
messages to send through can improve privacy for everyone (e.g. by
padding out links with useful traffic to make passive traffic analysis
harder on the realtime flows).
Nor does it prevent the pond server operators from learning general
traffic matrix data about their pseudonymous users... e.g. it doesn't
use PIR techniques to check the mailbox.
Bitmessage, which uses flooding (and thus can be seen as the simplest
kind of PIR) could have complete privacy in the receive direction (but
doesn't right now due to braindead protocol design that lets you send
parties messages that they'll automatically respond to), but in the
send direction users are fairly vulnerable to traffic analysis
So both pond and bitmessage? systems which have traffic analysis
resistance? would still be improved by this kind of tool.
If the recipient of messages has a way to rate limit them, he should
be able to choose to not allow traffic levels that would be
conspicuous (e.g. out of line with the level of padding that they/the
network can support).

@_date: 2014-05-08 16:22:21
@_author: Gregory Maxwell 
@_subject: [tor-talk] Satori (this crazy app thing I've been working on) 
You might be interested in some of the ideas that have been floating
around in Bitcoin land about better tools for distributing software
updates, I've collected the ones I think are most important here:
Note that it's not about automatic updates, it's about automatic
update staging? the user stays in control there... but the goal is to
advance the art so that users aren't just pulling updates from some
website in a way that any MITM could compromise too easily... but
without introducing centralized gate-keeping either.
I think some of these ideas might be pretty important when
distributing software specifically to 'interesting targets'? e.g. it
would give pretty good dividends to rubber hose the guy who can issue
the updates to a bunch of activist, so both for the users and the
operators safety something more robust ought to be done.

@_date: 2014-11-24 01:57:16
@_author: Gregory Maxwell 
@_subject: [tor-talk] Propsal for decentralization of the Tor network 
It's far from clear to me that substantially stronger decentralization
is practically possible for this application; at least not without
additional assumptions and exposure to new and concerning attack
I think a better short term goal would be improving review and
auditability... which I think can be done.  E.g. better tools for
providing convincing evidence that the directory authorities are not
misbehaving, and additional protections against misbehaving, better
automatic handling should authorities misbehave. (E.g. making it so
that authority signing is moved into a HSM which at least enforces the
constraint that only a single signature will be given for a particular
time period, or the like;  making proof of a misbehaving authority
forever ban that authority, beyond a threshold misbehaving should shut
down the network until manually overridden, etc.).

@_date: 2014-11-24 03:17:26
@_author: Gregory Maxwell 
@_subject: [tor-talk] Propsal for decentralization of the Tor network 
You haven't specified the decentralization mechanism.  So I guess I get to pick?
Okay. Instead of believing the directory authority signatures, instead
you have nodes connect out to as many nodes as they can find, and add
any entry returned by a majority of nodes to their local directory.
Oops. The attacker is a local network and only lets them connect out
to their own nodes, which perform a sybil attack and limit the tor
client's view to just the attackers hosts.  Client security is lost
Q.E.D. ...
There are many ways you can go about trying to be 'decentralized'
most are _profoundly_ insecure in an active adversaries attack model.
Usually the main failure mode is inadequate sybil resistance.
This isn't to say that I don't think useful things are possible,  I
don't know. I have not seen a proposal which even makes an argument
for its own security for this application. Saying "decenteralized" is
easy, tendering a concrete proposal which achieves useful security
properties is much harder.  And "decenteralized" isn't something that
can be deployed or analyzed for its security, specific concrete
proposals are.
If you don't want your emails being made public you should consider
not sending them to a public mailing list.

@_date: 2014-10-30 04:51:13
@_author: Gregory Maxwell 
@_subject: [tor-talk] 
So?  80 bits is superior to the zero bits of running over the open internet?
(You should be imagining me wagging my finger at you for falling into
the trap of seeming to advance not using cryptographic security at all
since it's potentially not perfect)
Hah. Well here modern TLS seems to be mostly a cluster@ of
complexity and resulting protocol an implementation failure. :)
But thats not here nor there, because it isn't actually a choice offered.
Sure, though thats a one time transfer common to all Bitcoin users.
Which the user may have already had most of previously, or obtained
from some other source.
At worst, that traffic has just identified you as someone who has
started up a Bitcoin node.
Bitcoin core intentionally obscures the timing of its transaction
relaying and batches with other transactions flowing through. It could
do much better, the existing behavior was designed before we had good
tor integration and so didn't work as hard at traffic analysis
resistance as it could have.
In some senses Bitcoin transaction propagation would be a near ideal
application for a high latency privacy overlay on tor, since they're
small, relatively uniform in size, high value, frequent... and already
pretty private and so are unlikely to gather nuisance complaints like
email remailers do.
Bitcoin has a fair degree of robustness against network sybils, and
even if all your peers are a single malicious party their ability to
attack is gated by the several thousand dollar per block costs (and
the risk that the receiver will realize something is wrong when it
takes days to get six confirmations).
(New client software comes with foreknowledge of the work in the real
network, so you cannot even provide a replacement alternative history
without doing enormous amounts of computation, e.g. 2^82 sha256
operations currently to replicate the history).
More mechanisms to reduce sybil risk are important for onion peers and
IPv6 where address scarcity are unavailable and people have been
experimenting with various ideas to address those and related
concerns, e.g.  and
 but the system
already assumes that the peers are attackers generally.
As above, at least the 'trusted' operator has considerable costs to
attack you... This is arguably a much stronger security model than
using tor in the first place due to tor's complete reliance on
directory authorities, for all you know you're being given a cooked
copy of the directory and are only selecting among compromised tor
nodes. This is one of the reasons that a some amount of work has gone
into supporting multi-stack network configurations in bitcoin, so that
you can have peers on each of several separate transports.
Normally when used with tor bitcoin nodes will use both HS and non-HS
peers, and if non HS peers are available it will not use more than 4
non-HS peers.
However, because of the way tor's exit selection works the non-HS
peers usually end up entirely connecting through a single exit, which
is pretty pessimal indeed. We'd certainly like more control of that,
but the ability to create hidden services over the control port would
be a higher priority IMO... since right now it's almost pointless to
improve robustness to HS sybils when so few people go through the
considerable extra configuration to enable running a hidden service.
There is a whole separate document on this, I'm not sure what else
you're looking for:
I'm not sure where you found "tor=" as that hasn't been a
configuration option for over a year. Originally it was the setting
for specifying a proxy to be used exclusively to reach HS peers-- a
somewhat advanced usage (e.g. when you want to be able to connect to
HS but the regular clearnet IPv4/IPv6 internet for your other
connections), and clearly documented as such... but users in a rush
were sometimes setting it. That option is now called -onion.
Good to hear about the reduced exit policy, in general there have been
virtually no(*) reports of bitcoin node operators being harassed.
(*) one exception:   but
here if it was even real it was from someone listening, it seems, not
making outbound connections.
