
@_date: 2005-04-22 12:39:06
@_author: Paul Syverson 
@_subject: Google messages 
It appears that some people are sometimes routing transactions through
the Tor network that Google categorizes as abusive.  Your experience
is likely part of their response to that. Attempts to abuse Google are
a given whether through Tor or otherwise, thus Tor developers are
currently talking to people at Google to seek approaches that further
enhance mutual benefit between Tor and Google and their users.
-Paul Syverson

@_date: 2005-08-19 14:32:08
@_author: Paul Syverson 
@_subject: So TOR is NOT  really anonoymous!? 
Right. In case it is useful, the simple metaphor that I have
used to express this for many years is that
  Onion Routing (Tor) is about anonymizing the communication pipe, not
  the data that goes through it.
Less precise, but perhaps easier to grasp.
Note also that this separation is intentional. There are circumstances
where one would like to sanitize the data, and times when one would
like to be authenticated, e.g., when connecting via ssh to a limited
access system.

@_date: 2005-08-19 14:32:08
@_author: Paul Syverson 
@_subject: So TOR is NOT  really anonoymous!? 
Right. In case it is useful, the simple metaphor that I have
used to express this for many years is that
  Onion Routing (Tor) is about anonymizing the communication pipe, not
  the data that goes through it.
Less precise, but perhaps easier to grasp.
Note also that this separation is intentional. There are circumstances
where one would like to sanitize the data, and times when one would
like to be authenticated, e.g., when connecting via ssh to a limited
access system.

@_date: 2005-08-30 10:22:22
@_author: Paul Syverson 
@_subject: Tor on USB 
You might also see the following commercial distribution that
bundles Tor, a tiny linux, and related software on a USB stick
Looks cool and got favorable reviews, but I haven't used or examined
it first hand. This is a pointer, not an endorsement.

@_date: 2005-08-30 15:38:54
@_author: Paul Syverson 
@_subject: Injecting client data through your own server 
Just to add, usually you would want to run the OP on the same machine
as the application, but you may need for it to be in a different place.
For example, in a corporate environment where local communication is
subject to monitoring you might want the OP and OR both at a firewall.
Or if you are unable to run an OP locally but can set up an encrypted
connection to somewhere trusted to proxy for you.
Much of this is discussed in "Onion Routing Access Configurations"
This paper is pre-Tor so will have some obviously dated aspects.
I also have the slides describing access policies and illustrating
about a half dozen different configurations if anybody
wants to see them.

@_date: 2005-12-22 11:53:09
@_author: Paul Syverson 
@_subject: recent tor stream timeout errors... 
The interrelation between p2p, QoS, file-sharing, incentives,
performance, and anonymity is complicated.  Beyond the simple things
that are already being done, many of the things one might want to do
to encourage, to discourage, or to interact with a particular use of
Tor (such as p2p file-sharing) has anonymity or performance
implications, or both.
You might want to look at
for some of the research issues that we are trying to solve to address

@_date: 2005-01-25 17:09:20
@_author: Paul Syverson 
@_subject: Wikipedia Manifesto 
Nihil sub sole novum. Some of us have been making this point in print
and in our systems for a decade or more. The stated purpose of onion
routing right from the start in 1996 was to separate identification
from routing.  And supported applications have always included
authenticated connections over an anonymous channel.  Unfortunately I
don't think Geoff's perfectly valid points will of themselves have
much affect on Wikipedia for a long time to come, possibly a decade or
more. IP addresses are the SSNs of the internet, squishy
pseudo-authenticators that work adequately well for those who choose
to use them as such. (Canadians please substitute `SIN' wherever I say
``SSN'', and others from elsewhere do likewise.) And they really do
work: the primary cost of identity theft is born by the victim, not by
the financial, commercial, or other entities that accept the pseudo
authenticated transactions. It has taken billions of dollars of fraud
losses over years to prompt institutions to merely _supplement_ the
SSN as an authenticator (cf. my "The Paradoxical Value of Privacy").
Suppose that by blocking all Tor servers, Wikipedia cuts out ten
percent of their perceived anonymous abusers of the less resiliently
persistent type. (This is probably very generous even with the
qualifiers. I say ``perceived'' because we have directory
servers. There may not be more anonymous posts from Tor; it may just
be that `the light is better under Tor's streetlamp'.) That's still a
short-term win for them as Roger and Geoff have pointed out.  At
what cost?  There is little immediate visible cost to the Wikipedia
maintainers. (There is cost to the readers and posters, but that as I
said is separate and hard to measure.)
Instead of just throwing up our hands at ignorance, we should
recognize that until someone provides them with something that will do
the job that their current system does for them with something close
to the same cost/benefit advantage, they have no rational reason to
change. (Not just paper designs or vaporware, but stuff that really
works at least fairly well.) There may come a time when Tor or other
systems have grown to a point when many systems that would rather not
accept anonymous connections will have to, like it or not. (Hey I can
dream.) Or there may come a tipping point in the
IP-address-as-valid-identifier assumption. But given the persistence
of SSNs, even then we can assume a supplementation rather than
abandonment of IP addresses as identifiers. To get anonymous
connections more widely accepted in many places where there is
resistance, we need to point out the benefits and the costs of not
doing so. We are already doing that some. But we may also need to
develop authentication and authorization techniques that work with
anonymous channels that function (in practice not just theory) as well
as the squishy authentication and authorization techniques that are
used today.

@_date: 2005-01-13 08:59:47
@_author: Paul Syverson 
@_subject: Tor exit nodes by country 
You are right that it is difficult to design a usable workable
low-latency widescale system that hides its nodes but provides OK
anonymity to its users. Until such a design is worked out by someone,
Tor is less ambitious. The Tor network is not meant to be anonymous
itself or provide censorship-resistant publishing (location hidden
servers notwithstanding), rather it provides (some degree of a type
of) anonymity to its users. (Location hidden servers could be thought
of as part of the network, but it is probably better to think of these
as one application of the network.) Tor's main resistance to directed
attacks against its nodes and their operators is to have enough nodes
in enough jurisdictions that these sorts of pointed attacks become,
well pointless.

@_date: 2005-07-03 08:41:54
@_author: Paul Syverson 
@_subject: Sniffing OR-OR connections by rerouting them 
yes, quite cool.
Yes. This is exaclty what I had in mind.
Makes sense, and I also prefer keeping it as simple as possible, especially
when it is more flexible. I assume it is the determination of the alternative that is the bulk
and risk.  That is, it would not add much bulk to add an "incorrect
inbound IP address encountered" error message that Bob sends back to
Alice before hanging up, yes? Should the criteria include correct IP address
but wrong port? I mean something's gone wrong in that case, and dvorak
can maybe find another bug.

@_date: 2005-07-21 12:57:35
@_author: Paul Syverson 
@_subject: chaining JAP and Tor 
I don't intend to comment about the architecture (other than to say
that it seems probably OK at the level you described it) but rather
to ask what the larger goal is here.
First an aside: I want to respond to a JAP FUD comment that has
already been posted. I do not think that JAP is backdoored right
now. If it were backdoored though (as in the past) I doubt they could
tell us legally. Same is true of Tor by the way, which is why
independent inspection of the source and alertness to sudden
disappearance of the source is recommended. But, even with source code
signed and in hand, you have no direct evidence that the Tor server or
JAP server through which you route is running it; although you can
compile the client yourself. And the source code for JAP is
available. You just have to go to the JAP site to see. It beats idle
Back to the main question: what's the point?
We don't have to argue about who has the better/more-realistic/...
system or threat model. You can just take the two threat models
as givens.
Tor's security derives from the adversary not knowing the endpoints of
communication. But if you feed a Tor circuit into a JAP cascade,
anyone can know where it's coming out. Thus, someone who wants to see
where you are connecting can just watch the end of the JAP cascade and
confirm the responder to which you are connecting.
JAP's security derives from persistent clients with similar traffic
profiles (e.g., via padding, although I don't know if that is deployed
currently). Having a bunch of Tor circuits that open and close at
varying times for relatively short intervals with varied
traffic profiles will not satisfy these assumptions and, it should
be relatively easy to bridge the cascade for exiting traffic based
on Tor circuits. So you derive no benefit from sticking a JAP cascade
at the end of your Tor circuit, or put the other way round: you would
ruin the protection of using JAP.
Now, you suggested putting the JAP client before the Tor circuit which
then leads into the cascade. This could govern the sorts of things I
raised in the last paragraph. The problem is that Tor is not designed
to work with this level of traffic padding and regularization. Whether
or not it is practically feasible (of which I am dubious), it seems
likely that it would chase away many of the volunteers that run our
Also, I don't think it would have reasonable payoff even if feasible.
That's partly because of my views in an ongoing debate between the
free-route and cascade advocates. Fortunately, we can note the
incompatibility of the current systems with their current threat
models without getting into who is right in that debate.

@_date: 2005-07-21 17:16:00
@_author: Paul Syverson 
@_subject: chaining JAP and Tor 
Hi Ben,
Well I was raising it as a challenge: it would seem this suggestion
has problems, so what envisioned advantage are you suggesting? But if
you are a neophyte, that challenge isn't exactly fair. It may be that
there is a way to build something good out of this suggestion.
  Aside: It is in fact somewhat similar to the hierarchical or
  hydra---respectively two-headed hydra--design that has been part of
  onion routing discussions going back ten years, although I don't
  think any of that is in officially published stuff.  The advantage
  is to have Tor-like hiding of source (and in the two-headed case,
  source and destination) but traffic aggregation advantages by
  running it through larger pipes (possibly in a cascade) in the
  middle (or at one end). If you don't follow what I'm saying just
  ignore this aside.
But, on the face of it I see putting JAP after Tor as hurting (not
just neutral) because you have a more predictable exit point for your
traffic. And unless you abuse Tor, you will be worse than JAP alone
because you cannot have constant behavior clients in a relatively
persistent anonymity set. (That's in theory. I'm not sure I believe
JAP gets you that protection in practice even by itself, but I'm
trying to avoid going into a JAP vs. Tor debate on effective
protection. I'm sure the JAP team would say something quite
Bottom line: For the current systems I think it is worse not neutral
to combine them.
Running your system through two anonymizing systems rather than one
is not going to help your speed, or was that what you meant?

@_date: 2005-06-14 09:36:06
@_author: Paul Syverson 
@_subject: Why TOR Operators SHOULD always sniff their exit traffic... 
Umm, just to make it even more confusing: I think what this person did
would illegal in much of the US, but by state rather than federal
law. IANAL, don't speak for my employer, yada yada...

@_date: 2005-06-28 07:08:22
@_author: Paul Syverson 
@_subject: Sniffing OR-OR connections by rerouting them 
A default URI in the directories rather than an IP address takes care
of dynamic IP and the like. Then we can just rely on secure DNS ;>)
For these and the reasons Adam raises it could also make sense for
Alice to tell Bob the IP address she has for him. If it is acceptable,
no action from Bob need be taken other than continuing the connection.
If it is unknown/unacceptable, Bob can send back a currently acceptable
alternative and then kill the connection.

@_date: 2005-03-05 09:53:01
@_author: Paul Syverson 
@_subject: exit node only server 
It might also be helpful to look at "On the Economics of Anonymity"
Or perhaps it will just leave you more confused. Anyway see the
tor.freehaven.net/anonbib/ or  for download.  It
explains (some of) the issues that have arisen in this discussion in
terms of anonymity incentives rather than just performance incentives.

@_date: 2005-05-25 18:02:46
@_author: Paul Syverson 
@_subject: FW: I still do not understand... 
Hi Manuel,
Two points. First, if nothing about the nodes is published or
otherwise known to you the Tor client, there is no basis to think that
the network (or the part presented to you) consists of anything but a
hundred nodes run on a few highspeed machines by a single
individual. Actually there is not even the need to setup a hundred
distinct nodes. Basically there is no anonymity protection at all.
Second, anonymity and censorship resistance are related but not the
same. One could want to provide an anonymous communication network
without a concomitant desire to provide censorship resistance. Also,
censorship resistance is hard, and anonymity is just one of the
building blocks for it. So, if you do want to build censorship
resistance, you need to build good anonymity. And, you need to have
researched, tested, and understood the anonymity provided to know
to use it in the censorship resistance context.

@_date: 2005-05-04 15:08:41
@_author: Paul Syverson 
@_subject: Onion Routing Project webpages overhauled 
I have overhauled the webpages at
Please take a look, and let me know if you have any comments or

@_date: 2005-05-05 15:43:43
@_author: Paul Syverson 
@_subject: Onion Routing Project webpages overhauled 
Hi Chris,
Thanks for the comment. Since I have to ask my sysadmin to
post changes, I'm going to wait a few days, collect some, and
give them to him all at once.
Probably update it when I get back from California.
See you Tuesday,

@_date: 2005-11-29 15:04:00
@_author: Paul Syverson 
@_subject: use of routing information in anti-fraud mechanisms 
As is blocking of calls from caller-ID blocked phones, at least in
some US jurisdictions. The receiving phone does not ring, and the
initiating caller gets a recording saying that the number that is
being attempted does not accept calls from blocked phones.
For many years I've imagined this would escalate to the point that you
would get a call from an operator who would tell you, "I've got a guy
on the line who offered ten bucks to talk to you.  What will you pay
to make him go away?" In the ensuing bidding war, the carrier is
always the only real winner.

@_date: 2005-10-18 13:56:59
@_author: Paul Syverson 
@_subject: Broadband Reports: Tor Network Bogged Down by P2P 
For a stab at these and related questions, generally noting why they
are hard and why the "obvious solutions" to many of them are not so
obvious, people might want to look at the "Challenges in Low-Latency
Anonymity" paper at particularly the sections on file sharing and scaling.

@_date: 2005-10-20 09:26:59
@_author: Paul Syverson 
@_subject: SSL fro hidden services 
It's unnecessary. All communication is over Tor circuits that are
created at both ends of the communication which are mated at an
Introduction Point to establish contact and at a Rendezvous Point to
pass data. So even the edges of the communication (between client and
Tor network, and between hidden server and Tor network) are multiply

@_date: 2005-10-03 08:32:44
@_author: Paul Syverson 
@_subject: Hooking nym to wikipedia 
Hi Jason et al,
You might want to have a look at our UST (Unlinkable Serial Transactions)
It was published in ACM TISSEC
(or .ps)
There was also an earlier version published at Financial Crypto.
(It lacks the proofs and some improvements to the protocols)
(or .ps)
1. I think it is much less complicated than the other things you raised,
   but of course has other tradeoffs.
2. The papers are it. There is no current code worth looking at.
3. Thanks for the reminder. It too is patented if not patent-ridden,
   but we should be able to cope with that. Basically you shouldn't put
   huge work in assuming that there are no encumbrances to address, but if
   you are interested given 1 and 2 after you look at it, let me
   know. I can then explain the issues regarding the patent situation.

@_date: 2005-10-05 09:10:47
@_author: Paul Syverson 
@_subject: Hooking nym to wikipedia 
Note I'm not on the cryptography at metzdowd.com
Feel free to forward if you like.
This is not true. Let me explain a bit further. The paradigm example
in designing UST was for accessing material in a paid subscription,
e.g., to an encyclopedia service. But, it applies more broadly as we
also described in the UST papers. Whatever the application, the token
is typically not returned until the transaction is completed, and what
that means is up to the application. So, if you want to check posts
for abusive content before permitting them, this can be part of the
transaction.  The new token need only be issued after the transaction
is complete.
For Wikipedia posts from Tor this is almost certainly not worth the
effort over my earlier suggestion to just hold any posts from Tor
until they are reviewed. Jimmy suggested that there was plenty of
manpower available for that. And, as I mentioned, incentive to abuse
should essentially disappear once it is known to be pointless. This
approach is not perfect, but there are no pseudonymous profiles as in
others. It has perfect forward anonymity without needing UST because
we aren't worried about limiting available subscriptions (i.e.,
accounts to post from) because we have other means to manage the
incentives we care about.
We are actually in a good position wrt Wikipedia. They are already
blocking Tor. So if they started unblocking Tor with this reviewing
step in place, there is not likely to be much abuse showing
up---because it won't result in any posts.
Why might UST be useful here? Well I already suggested that people
angry enough to try to deface Wikipedia entries might get angry and
try to attack the service when they cannot, redirecting that anger to
the service infrastructure itself rather than the content that is
posted. As Mazieres and Kaashoek noted in their design of the MIT
nymserver, (1) blame the messenger (i.e., identity protecting service)
and attack it does happen when the actual source of frustration cannot
be accessed, and (2) when it does, it is important to make sure it is
more effort to attack a service then defend it.
UST can help with this by requiring that the poster show up with a
valid token. One could imagine that if abuse from Tor drops way off,
the effort to filter submissions through Tor could be not worth it
and turned off as well. When an abuse happens, Tor posts could be
escrowed again. With UST however, the abuser(s) can be more quickly
filtered off (by not reissuing tokens to them). They can of course
get another token chain; this will be as difficult as you want to
make it (IP address based, email address based, captchas, SSNs,
retinal scans, whatever you want), but that will be separate and
will not create pseudonymous profiles of honest users.
Also, it might be useful if Wikimedia (or others) wants to ever use a
limiting device that, unlike Tor, is not delineated by the IP address
from which the post emerges, or if they want to use other IP
BUT, I think that this is R&D thinking rather than
solving-the-problem-at-hand thinking.  I reiterate that a basic escrow
of submissions from Tor should be simple and solve immediate problems
The issue has been raised about getting this to the people who
actually develop and maintain the code (and the sites) for Wikipedia.
So far they have been referred to as though those of us on this plane
of existence have no normal means of communicating with them.  I
assume that is not true. I don't know how the code development and
maintainance and network maintainance is all managed, but I bet Jimmy
can at least give us contacts if not have larger input.  (And
apologies to Jimmy if the characterizations of his relation to the
coding and maintainance do not reflect realities. Make that "that the
characterizations of his relation..." because I know from experience
that there are all kinds of things that are assumed from outside, but
munge reality.)
If the code is available, the easiest solution might be to have the
enterprising people who want this to happen code up a simple escrowing
interface and present it to them. (OK and work with them so it
actually happens.) Easy for me to say, as I don't have the time or
skills to do it. Or there might be more sensible approaches.  But the
first step is probably talking to the people who can change this
rather than about them. Any help there Jimmy?

@_date: 2005-09-27 11:18:31
@_author: Paul Syverson 
@_subject: Wikipedia & Tor 
No. Cf.
I share frustrations that the statements attributed to Jimmy Wales in
the record below and in previous messages seem to show some fundamental
misunderstandings and willful ignorance of Tor, and more broadly of
identity, identifiers, reputation, authentication, etc. in open
network communications.
But, it's easy to misread both content and intent from terse email
excerpts.  As we said in the "Challenges" paper, besides their
short-term focus and limited effectiveness, IP address blacklists
constitute "a loss for both Tor and Wikipedia: we don't want to
compete for (or divvy up) the NAT-protected entities of the world."
A potential for cooperation is the proposal below for authenticated
access to Wikipedia through Tor. I will not speak to any particular
design here, but if Wikipedia has a notion of clients trusted to post
to Wikipedia, it should be possible to work with them to have an
authentication server that controls access to Wikipedia through Tor.
This would limit the type of anonymity that such posts could have,
probably pseudonymous at best, but it would be doable. A win for

@_date: 2005-09-27 14:49:07
@_author: Paul Syverson 
@_subject: Hello directly from Jimbo at Wikipedia 
Agreed. And glad to struggle to common understanding as long as good
faith seems to be coming from both sides (which to date it mostly does
despite mutual frustration).
OK. I was letting out some frustration there. One of the main reasons
for this is the raising of the standard spam red herring. You appear
to have raised it again below, and I still don't understand why.  I
take spam to mean the mass sending of unsolicited email. I don't want
to get into quibbles about 'commercial' or criteria for what counts as
solicited. But this does not seem to be what you are talking about at
When we first designed and fielded Tor, we decided that even though it
would be a lousy delivery vehicle for spam, we would set a default to
block port 25 (the only avenue over which spam has been sent at least
at the time). Even though this reduced functionality for legitimate
users and had just about no effect on spammers, we didn't have to
explain subtleties. We could just say, "It cannot be used for
spam. Period." But many, e.g., the SORBS people, seem to just not care
about the facts. You say below that you deal with it [spam] regularly,
but how does blocking Tor servers (more properly, any and all that
share an IP address with a Tor server) from posting on your web
interface reduce the amount of unsolicited email you receive? I'm not
trying to bait you here. It's just that we are always saying Tor isn't
used for spam and is designed to be especially spam unfriendly, and no
one ever provides a shred of evidence to the contrary.  I honestly
don't understand how anyone could bring up spam unless they were
willfully ignorant of Tor design and deployment strategy. That's
why I said what I did. But let's get past the strong wordings.
When you bring up spam coming over Tor to Wikipedia, what sorts
of traffic specifically are you talking about.
I don't claim to speak for the community. But as the originator of the
underlying Onion Routing concept, and as one of the designers of Tor,
I can tell you that we are aware of the tradeoffs. We discussed them
in our "Challenges in deploying low-latency anonymity" paper.  I quote
the relevant section from that paper.
   It was long expected that, alongside legitimate users, Tor would
   also attract troublemakers who exploit Tor to abuse services on the
   Internet with vandalism, rude mail, and so on.  Our initial answer
   to this situation was to use ``exit policies'' to allow individual
   Tor nodes to block access to specific IP/port ranges.  This
   approach aims to make operators more willing to run Tor by allowing
   them to prevent their nodes from being used for abusing particular
   services.  For example, all Tor nodes currently block SMTP (port
   25), to avoid being used for spam.
   Exit policies are useful, but they are insufficient: if not all
   nodes block a given service, that service may try to block Tor
   instead.  While being blockable is important to being good
   netizens, we would like to encourage services to allow anonymous
   access. Services should not need to decide between blocking
   legitimate anonymous use and allowing unlimited abuse.
   This is potentially a bigger problem than it may appear.  On the
   one hand, services should be allowed to refuse connections from
   sources of possible abuse.  But when a Tor node administrator
   decides whether he prefers to be able to post to Wikipedia from his
   IP address, or to allow people to read Wikipedia anonymously
   through his Tor node, he is making the decision for others as
   well. (For a while, Wikipedia blocked all posting from all Tor
   nodes based on IP addresses.) If the Tor node shares an address
   with a campus or corporate NAT, then the decision can prevent the
   entire population from posting.  This is a loss for both Tor and
   Wikipedia: we don't want to compete for (or divvy up) the
   NAT-protected entities of the world.
   Worse, many IP blacklists are coarse-grained: they ignore Tor's
   exit policies, partly because it's easier to implement and partly
   so they can punish all Tor nodes. One IP blacklist even bans every
   class C network that contains a Tor node, and recommends banning
   SMTP from these networks even though Tor does not allow SMTP at
   all.  This strategic decision aims to discourage the operation of
   anything resembling an open proxy by encouraging its neighbors to
   shut it down to get unblocked themselves. This pressure even
   affects Tor nodes running in middleman mode (disallowing all exits)
   when those nodes are blacklisted too.
   Problems of abuse occur mainly with services such as IRC networks
   and Wikipedia, which rely on IP blocking to ban abusive users.
   While at first blush this practice might seem to depend on the
   anachronistic assumption that each IP is an identifier for a single
   user, it is actually more reasonable in practice: it assumes that
   non-proxy IPs are a costly resource, and that an abuser can not
   change IPs at will.  By blocking IPs which are used by Tor nodes,
   open proxies, and service abusers, these systems hope to make
   ongoing abuse difficult.  Although the system is imperfect, it
   works tolerably well for them in practice.
   Of course, we would prefer that legitimate anonymous users be able
   to access abuse-prone services.  One conceivable approach would
   require would-be IRC users, for instance, to register accounts if
   they want to access the IRC network from Tor.  In practice this
   would not significantly impede abuse if creating new accounts were
   easily automatable; this is why services use IP blocking.  To deter
   abuse, pseudonymous identities need to require a significant
   switching cost in resources or human time.  Some popular webmail
   applications impose cost with Reverse Turing Tests, but this step
   may not deter all abusers.  Freedom used blind signatures to limit
   the number of pseudonyms for each paying account, but Tor has
   neither the ability nor the desire to collect payment.
   We stress that as far as we can tell, most Tor uses are not
   abusive. Most services have not complained, and others are actively
   working to find ways besides banning to cope with the abuse. For
   example, the Freenode IRC network had a problem with a coordinated
   group of abusers joining channels and subtly taking over the
   conversation; but when they labelled all users coming from Tor IPs
   as ``anonymous users,'' removing the ability of the abusers to
   blend in, the abuse stopped.
Huh? See above.
Yes we are, but that's not the only security you could implement, and
I hope no one would suggest it. But getting someone else's IP address
is no harder than getting someone else's credit card number. In fact
much easier since they are explicitly not unique to individual people
most of the time, and there are even less attempts to protect them
than to protect credit card numbers. I think I can safely speak for
the main Tor developers and designers when I say that We would be glad
to work with you to develop Tor-compatible authentication mechanisms
that are more appropriate qua authentication mechanisms than you now
have. And you can rest assured that we would be at least as concerned
about protecting the identity of those using it as you would be.

@_date: 2005-09-27 16:17:16
@_author: Paul Syverson 
@_subject: Wikipedia & Tor 
Nobody should understand me as claiming that Tor has never been
a vehicle for abuse in any way. But Gah, No! I think this is an awful meaning creep of `spam' that
makes it harder to talk succinctly and clearly about the different
types of abuse.  If I'm behind, and that's what `spam' means now. Oh
well, another loss in the war for clarity. We'll just have to hack a
new term ;>)

@_date: 2005-09-27 17:39:03
@_author: Paul Syverson 
@_subject: Wikipedia & Tor 
Maybe some of the more high-tech solutions, something involving
blind signatures, etc. will ultimately be shown useful, designed
and coded.
In the meantime, I suggest something to try that I believe is both low
tech and easy to implement.  It turns into advantages some of the
putative limitations we've been talking about
(1) Tor is set up and managed so Wikipedia can easily recognize
    stuff coming through Tor.
(2) Wikipedia doesn't have a lot of human or other resources available
    to check these posts for what it considers abuse. (Relates to Nick's
    points.)
Anyone attempting to post via Tor gets a response that explains that
posts through Tor are not automatically posted. Because of previous
experience, posts through Tor will be screened by a human being. There
are not alot of human beings available.  This is both a reality and
intentional so this could take a while. (Once experience is
accumulated an expected wait time in hours/days/etc can be given.)  If
the client would still like to submit a post, he can continue and do
so. The post is then stored till a person gets to it, or till its time
waiting in storage for a person to check it expires, or maybe there's
some scheme to limit the potential abuse of disk space or
whatever. There are some perameters to tweak, but I still think it's
pretty simple.  Even if there's only fifteen man-minutes a day for
this in the Wikipedia organization, it could still be effective.
People who want to abuse a particular entry or even arbitrary entries
will simply not succeed through Tor at all. They will have to come in
by some other means. People can still abuse Wikipedia in general by
wasting this resource, but Wikipedia can intentionally limit the
resource to the amount it wants to give to this. If there's lots of
such abusers out there who don't care about defacing entries, they
just want to mess up the Wikipedia organization in a way that is not
even publicly visible, then this resource all goes to waste.  But it's
conceivable to me that most (all?) who know they cannot succeed in
creating an abusive post will not bother, leaving the resource for
those who truly want to remain anonymous. If way more people
nonabusively post through Tor so that the Wikipedia human or other
resource is overwhelmed by legitimate anonymous posts, well that's
something Wikipedia would probably like to find out about too. By then
maybe we'll have a better technical solution or maybe Wikipedia will
be able to find more man hours if it's seen to be that important.
I think this is simple, both conceptually and to implement;
it might work;  and it's an improvement over the status quo.

@_date: 2005-09-28 09:47:52
@_author: Paul Syverson 
@_subject: Wikipedia & Tor 
Great. Mostly I didn't want to just presume lots of resources from
you for a solution. How much you have available is really orthogonal to my points though,
one of which was that this puts you in control of the resources you
want to devote to it.  I was also raising a point not about the abuse
you now face but about the potential abuse (overuse of the human
resource by people frustrated that they can't attack your entries) you
might face if you implemented the suggestion, and how that isn't
necessarily a problem.
I want to emphasize a central aspect of my suggestion: The goal is not
just to provide a filter for abusive posts, it's to change incentives.
We can't know for sure without running the experiment, but my guess is
that if abusive posts through Tor never succeed (OK perhaps virtually
never), and if the process of posting through Tor informs posters of
that fact, then Tor will become worth it for your admins. The abusers
will disappear or greatly diminish because they will know from being
warned, and if necessary from experience, that their attempts will
fail. Posts through Tor will then mostly have value (in the sense of
not being abusive in the ways that prompted this discussion.)
Yes, I know (and I'm sure Jimmy knows) that this won't solve the
longterm underlying issues. Abusive posters will just move on to
another avenue than Tor. But I think it will be a quick, cheap, and
big win for both Tor and Wikipedia.
1. It remains completely anonymous.
2. It doesn't require proofs of work, pseudonyms, authentication,
etc. (OK you might want to put in captchas or something to prevent
automated consumption of Wikipedia resources.)
Yes, as Marc Abel suggested you could implement passwords, pseudonyms,
or hell ZKPs.  But this is stepping onto the slippery slope of trying
to solve the more longterm problem that using IP addresses in the way
Wikipedia does is a temporarily useful kludge. (Kludges are great, but
function creep is dangerous and can make for bigger problems in the
long run.)
3. Wikipedia can put as much or as little resources as they want into
it. If my guess is right, the resources needed should be small. If I'm
wrong (about the abusers diminishing), then it would take a lot of
resources to filter the abusive stuff. But if wikipedia devotes a
minimum of resources, we're still better off than the status quo at
relatively low cost. And this allows patience in case it be a change
that takes a long time to manifest. If in the end, it is deemed simply
not to work, it will still have been a worthwhile and relatively
low-cost experiment.
Great. It is an imperfect solution in the best spirit of onion routing.
Cf. Nick's wonderful footnote {2} in his message. I'll be glad to
talk with you about how we might pursue that.
P.S. I remain against the expanded use of `spam' beyond email bulk
sending of unwanted commercial(whatever that means) messages, except
perhaps metaphorically. Sooner or later I will probably lose, and even
conservative dictionaries will change. (So far even the liberal ones
mostly agree with me, but not---big surprise---Wikipedia.) Such is the
nature of language, but that doesn't change the value of resisting.
Note that even under this expanded use of the term, the types of abuse
we've been discussing are not spam since they are not bulk: they are
posted one at a time by the abuser. If even that part of the meaning
is gone, somebody should update the Wikipedia entry for
`spam'. (That's meant to be ironic.  I sincerely hope it is not
updated in that way.)

@_date: 2005-09-28 23:10:32
@_author: Paul Syverson 
@_subject: Hello directly from Jimbo at Wikipedia 
Hi Steven,
I agree with you about it being a bad idea to add such things to Tor.
I don't see how your proposal solves the Wikipedia problem as I
understand it. Jimbo, please correct if I'm wrong, but I get the
feeling that people are abusing by defacing certain entries that they
don't like. My guess is that it would not be hard for them to gather
at least a couple of puzzle server accounts per day. If not it is
probably too onerous for honest users wanting to post through
Tor. Then when bad guys want to abuse some entry, they can deface it at
least up to the number of accounts they've accumulated. Admittedly
this proposal raises the bar a bit and requires the abuser to do some
planning and limits his rate of abuse. Maybe that's enough to deter
the abusers, but I'm dubious. Please let me know if I've missed

@_date: 2005-09-29 09:56:19
@_author: Paul Syverson 
@_subject: Hello directly from Jimbo at Wikipedia 
Some points of clarification that I hope will help:
(1) On anonymity and authentication/pseudonymity/etc.
All versions of Onion Routing, including Tor,  were designed to separate
identification from routing.  The slogan way that I have put this for
the last five or six years is:
 Onion Routing is about anonymizing the communication pipe, not what
 goes through it.
The devil's always in the details, but as one-line summaries go, I
think that sums it up pretty well.{1}
(2) On various pseudonym authentication or anonym
authentication{2}, etc. approaches to solving the problem at hand.
Some of this is ultimately necessary for various applications,
especially once the Internet looks as Geoff described it. (In fact I
think it's one precondition to realizing anything like that vision.)
But I'm dubious about any of those proposed to date here providing
enough friction to identifier acquisition to deter abusers but not
honest users in this context. They may be worth trying.  Roger's
suggestion about the temporary IP blocks and Steven's about the
separate puzzle servers come to mind, probably some others I'm
forgetting just now.  But as Roger says, somebody's gotta code them
up---and probably much more work---deploy them, maintain them, and
evaluate their effectiveness, all on the Tor-Wikipedia frontier.  I
suspect that the abuser who goes postal as Jimmy described is willing
to waste lots of time acquiring IDs, but perhaps stereotypes about
attention span are close enough to true for some of the proposals to
be effective.  I had my own proposal that doesn't rely on any of this,
and that could be implemented and deployed in a few days (OK after
spending at least a few months or so thinking about the design, the
engineering, and the implications.) In the spirit of mutt: All these
ideas suck; I just think that one sucks a little less.

@_date: 2005-09-29 14:41:35
@_author: Paul Syverson 
@_subject: Abuse resistant anonymous publishing 
Hi George, Ben, et al,
George and I have talked about using some of these ideas for anonymous
communication a handful of time over the last few years.  I think they
have clear potential for communication and maybe also for the
tradeoffs of abuse resistance and anonymity in publishing, but I'm a
little more hesitant on applying them in the Wikipedia context.
Note also that I consider the things George mentioned as an attempt on
the longterm problem, not a relatively quick fix as some of the others
are intended to be.
The human approach strengthens trust and I'm guessing it will prove
one of the few good solutions to sybil issues, but can increase
vulnerability to rubber-hose cryptanalysis depending how it's set
up. The trust/responsibility trail is also pseudonymous rather than
anonymous, which exacerbates the problem.
If Alice makes a post that someone doesn't like, they can apply
directed pressure along the path to Alice. People are then put in a
position of either revealing the next hop in the chain to Alice or
falling victim to whatever countermeasures the coercers would apply to
them or their loved ones. This might be ameliorated by making it a
system of threshold entities, but if those entities are still
manageable enough to function I think they will still be rubber-hose
vulnerable, not to mention that you probably just increase the
intersection information if it is indeed based on who-you-know
For the latest posting on Lie Groups or something, you probably don't
need to be worried about the above concerns. (Although recall
Kaczynski and the guy who's name escapes me that took out his wife
with a ballpeen hammer about a decade ago because there was an error
in the algebraic topology book he wrote. Mathematical's got postal
beat all to hell ;>) But many people have been raising examples of
dissidents in repressive societies. I don't think any of the proposals
to date will provide such people good protection. A trust tree that
cuts across jurisdictions would help, but would also be hard to form.

@_date: 2005-09-05 06:00:29
@_author: Paul Syverson 
@_subject: selecting Node lists by  criteria? 
Nick focused on the difficulty of making this work. It is important
to also remember that the anonymity implications of these choices is
far from clear (even farther from clear than even the choices that
are already in the design ;>). In this respect it is even
worse than padding. People can play with attempts, but until there
is a clearer understanding of why as well as how, this is not likely
to be a focus of primary developer attention.

@_date: 2006-04-26 13:30:38
@_author: Paul Syverson 
@_subject: Weird behavior of my server 
It's more like 200 exit servers (c. 40% of the network) and has been
for months (going back to when 200 nodes was 50% of the network).  See

@_date: 2006-04-28 15:11:11
@_author: Paul Syverson 
@_subject: Is three hops enough? (was Re: Tor client over a SOCKS proxy, and Tor client running through another Tor Circuit) 
To reinforce, most of what has been raised in this thread has been
raised (often years ago) and subjected to lots of published
analysis. The tradeoffs between anonymity and practicality continue to
evolve, but if you have a thought off the top of your head, there's a
good chance it's already reflected in the literature. Keep thinking
about it and raising questions/comments, but do also have a look at
the extensive research that's been done in this area.
One paper that isn't posted in the anonbib (yet) shows some timing
attacks confirmed via experiments on the Tor network and
countermeasures to them. See
There were also a different sort of timing attack conducted on a
younger and much smaller Tor network. See
"Low-Cost Traffic Analysis of Tor" in the anonbib.

@_date: 2006-04-12 15:26:38
@_author: Paul Syverson 
@_subject: possible security hole(unsure)(really minor) 
Both. Tor does not get security from mixing of traffic at a node but
from the low probability that there is no attacker observing both
endpoints of a Tor connection. While some trivial attacks are thwarted
by the presence of other traffic through the same node, for the most
part timing attacks can easily separate it. This was expected and
described in the Tor design paper, and indicated in simulation
elsewhere.  It has now been empirically shown for at least for hidden-server
connections on the Tor network, cf.,
Note that the latest versions of Tor are not vulnerable to the described
attacks because of countermeasures implemented earlier this year.
Relatedly, see last year's "Low-Cost Traffic Analysis of Tor"
available at The attacks in that paper only identify the Tor node endpoints not the
client, and only when a client visits a hostile web site. And the
attacks were conducted when the network was less than a tenth its
current size; it is an open question if they would scale to the
current network.  Nonetheless, these two papers illustrate that one
should not be thinking of Tor as a sort of mixnet, as it is often
described, because that conveys an impression of mix-based security
that Tor does not provide.

@_date: 2006-04-15 13:54:38
@_author: Paul Syverson 
@_subject: Cover Traffic 
Actually, no it doesn't
And such attacks can be real

@_date: 2006-08-18 08:25:29
@_author: Paul Syverson 
@_subject: My ExcludeNodes list...post yours 
Depending on what constitutes authentication (and encryption).  If the
encryption adds integrity to the authentication (if not there already)
and prevents an eavesdropper from being able to trivially learn what
is needed to masquerade as you, then it has value against adversaries
not sophisticated enough or motivated enough for stream
hijacking. Good enough for many purposes. But in principle and
for more sensitive usage your point is well taken, thus worth raising.

@_date: 2006-08-18 10:29:33
@_author: Paul Syverson 
@_subject: My ExcludeNodes list...post yours 
Absolutely agree on all points. Of course in what you've described the
cookies are part of the authentication mechanism, or you couldn't do
the attack you mentioned. So cookies should be encrypted as well to
really follow my point. But this shows that appearing to protect
authentication and actually protecting it are two different
things. I'm guessing there's violent agreement there between you, me,
and M. I was mostly reacting to the statement that "[e]ncrypting
authentication has _no point_ if rest of the communication is
unencrypted" (my emphasis). I was, however, being a bit persnickity,
probably a vestige of my early years designing modal logics to analyze
authentication protocols.
Paul Syverson                              ()  ascii ribbon campaign
Contact info at    /\  against html e-mail

@_date: 2006-08-21 11:20:05
@_author: Paul Syverson 
@_subject: Tor and chained web based proxy sites? 
Hi John,
I'm sending my reply to the or-talk list, since that is the
place from which this exchange springs.
You raised the Tor vs. JAP design question. Your example, however, is
compatible with both depending whether the chain is chosen once and
used by everyone or its links are chosen by the client and changed
fairly frequently (or conceivably you could be suggesting something in
between, e.g., a client picks a route and sticks with it
persistently). I'm not sure what specifically you are asking, but I
can respond to the comparison between your two example scenarios.
You are generally much worse off in the second case. There is a single
point, webproxysite-1.net, that can watch both ends of the circuit.

@_date: 2006-08-30 08:53:22
@_author: Paul Syverson 
@_subject: Holy shit I caught 1 
All the points raised above and subsequently about self-signed and
other dubious/bogus certificates that I have cut for space are well
But the real threat, or perhaps I should say another significant
threat, is that the certificate may not be self-signed or bogus.  If
the URL takes one to an attacker web site, the attacker may have
obtained a perfectly valid Verisign or other recognized authority
certificate.  Some phishing attacks work this way. So one must be sure
of the address one is attempting to reach, be aware of unicade
attacks, probably only type URLs in yourself rather than follow
hyperlinks when it's important and risky etc. Avoiding phishing in
general is way beyond the scope of this message, but the point is that
a valid certificate obtained from a recognized authority through due
process may not be giving you the security guarantees you
expect---which is why such attacks are  effective.

@_date: 2006-12-17 22:20:35
@_author: Paul Syverson 
@_subject: More onions on TV: NUMB3RS this time 
Mike Reed just mailed me this link. I didn't catch the show, which
aired last Friday, and know only what's on the associated page.

@_date: 2006-12-18 08:26:03
@_author: Paul Syverson 
@_subject: Directory Server 
It's actually multiple directory server's that allow a vote and the authoritative directory server public keys are in the client
you download
It's not perfect, but the problem you are concerned with should not arise.

@_date: 2006-02-09 22:01:50
@_author: Paul Syverson 
@_subject: restricting circuits to top 30 
Apropos to this, you might want to look at the attacks in
"Reliable MIX Cascade Networks through Reputation" that can
be found at or

@_date: 2006-01-18 23:10:01
@_author: Paul Syverson 
@_subject: Tor Diffie-Hellman question 
This has some similarity to the generation 0 and generation 1 designs
of onion routing. The problem is that if someone saved all the traffic
that went to that OR and then later broke or otherwise obtained the
private key, they could now get the AES key with which data between
Alice and that OR was encrypted. Using ephemeral DH gets us perfect
forward secrecy. I.e., this sort of attack won't work.

@_date: 2006-06-10 08:03:07
@_author: Paul Syverson 
@_subject: [dnyberg@premier1.net: Re: The Cost of Running a Server] 
As the most practical contribution to Tor you could make at this point
I recommend following Nick's suggestions (repeated below since it was
unclear to me if D would receive them).  As a decision about what to
do personally about your IRC situation I offer the following analogy,
which is limited as all analogies are.  It's also straying way out of
tor-ops territory so I have sent it to the wider or-talk list.
Say you frequent a nightspot, The Chat Club, where you meet all your
friends.  It's been an integral part of your regular routine for
years.  One night you are astounded to discover you are barred entry.
Upon further enquiry you discover that the establishment had
previously had problems with patrons from the Orthodox Church of
Scallions (always noticeable in their distinct Tor T-shirts) getting
drunk and starting fights.  You try to point out that even more fights
are started by others, and the relative percentage of fighting people
wearing Tor T-shirts is no higher than in the general population; it's
a tiny fraction. It's just the bright green color that makes their
activities more salient. Furthermore, you were not actually wearing a
Tor T-shirt when banned, and you're not actually in the Orthodox
Scallion Church.
Well they don't know about that; they have a rent-a-cop company who
they contract to that decides who to bounce.  So you find out that
this company lists you (correctly) as a member of the Reformed Church
of Scallions whose members all wear traditional black ZKS Freedom
T-shirts (these are all pretty geeky religions). It doesn't matter
that Reformed Scallions have all taken a vow of nonviolence and
teetotaling, and there is not a single instance of one ever being
involved in a fight in a bar. They're not interested. And, your
favorite nightspot doesn't care either.  Even though you point out
that it is trivial to instruct their bouncers to distinguish bright
green from black, it's just not worth their trouble.
So you can either quit your religion and meet your friends at The Chat
Club as you alwoys have, or you can decide your princples are too
important, and you will need to meet them elsewhere, even if there is
no one other place that they all tend to congregate regularly. You
might try to convince them all to start frequenting the place up the
street, Talkers. Even if they all agree that the Chat Club owner is
being an incredible jerk and a bigot, he does run the place where all
the cool people are and that has the best music.  Plus your friends
may be vaguely uneasy to be seen supporting a religious group they're
not actually in, even against a bigot. What do you do? Probably there
is no one answer that is the best answer for everybody in that
situation. Tough call.

@_date: 2006-03-10 13:37:59
@_author: Paul Syverson 
@_subject: DNS leak check 
This is probably true for most cases where you are not certain, but
not necessarily. I sometimes use connect to ssh through Tor to
specific locations for which I have supplied a known IP address. I of
course still get the message.

@_date: 2006-03-20 11:31:25
@_author: Paul Syverson 
@_subject: How to contact tor-ops? 
Hi Robert et al,
Yes. My mail servers were down over the weekend.
It's not my position to tell you what to fear, but I can inform your
choice. Tor was conceived, designed, developed, and deployed entirely
by a combination of career employees of the Naval Research Laboratory
(me), and people under contract to NRL to work on it (Roger and
Nick). (Not to mention many significant volunteers, especially over
the last few years.) It is the third version of onion routing, the
first two of which were done entirely by career employees of the Naval
Research Lab.  All of this has been publicly posted for many
years. You can see more history and background at
 Since 2004, ongoing development of the
Tor network per se has not been part of any NRL project.  The bulk of
funding for that during 2005 was from the EFF. Work specifically on
hidden services over Tor is ongoing at NRL, and FreeHaven was also
funded to work on that (to a much smaller extent than previously)
until early 2006.
Hope that helps,

@_date: 2006-03-22 14:14:25
@_author: Paul Syverson 
@_subject: Hidden service question 
Still incomplete and a little off. The node the hidden service sets
up is called an introduction point. The client creates a circuit to
a separate rendezvous point, then sets up a circuit to the introduction
point and sends information about the rendezvous point down that.
Path length is six Tor nodes not seven (for both introduction and
rendezvous circuits.)
(1) HS --> 1 --> 2 --> IP
(2) C  --> 1 --> 2 --> RP
(3) C  --> 1 --> 2 --> 3 --> IP (send RP to HS through circuit (1) )
(4) HS --> 1 --> 2 --> 3 --> RP RP mates circuits (2) and (4) so total Tor nodes between C and HS is six.
HS = Hidden Server
C = Tor client
IP = Introduction Point
RP = Rendezvous Point
1, 2, 3 are Tor nodes but unlike HS, C, IP, and RP, the numbers 1, 2,
3 stand for positions not necessarily for the same Tor node in each
circuit above. (They kind of do, which is why I didn't give them
different names. Ask me about entry guards if this isn't complicated
enough already.)
There are other pictures of this in slides at
and at
under "Design Documents"
The above slides don't include aspects of the latest versions of Tor.
However, there are more pictures and detailed discussion of hidden
services in "Locating Hidden Servers", cf.
I think you are still running introduction point and rendezvous point
together here. If you mean rendezvous point, this was the actually
configuration used to conduct the attacks described in "Locating
Hidden Servers" above (attacks no longer possible in the current
versions) since the client was less concerned about protecting
her anonymity than in finding the hidden server.
Yep. Actually several variants currently being considered.

@_date: 2006-03-22 14:37:38
@_author: Paul Syverson 
@_subject: Hidden service question 
Absolutely. We're each trying to avoid too many details. (Guess at
least I'm failing there.)  The hidden server can decide whether it
wants to talk to the client or not. There are options in the code, but
nothing yet implemented, that allow the client to authenticate to the
server (when it doesn't want to be anonymous from the hidden server,
although perhaps from external observers).
Of course this implies no anonymity from either the HS or a network
observer. Just as should not limit use of Tor only to the cases when
you're browsing to sensitive sites (lest you flag when that is
happening), you don't want to only use Tor when you're browsing to
sensitive hidden sites.
I agree. I just meant variants on hidden service protocol
design and configuration in general.

@_date: 2006-11-30 14:17:50
@_author: Paul Syverson 
@_subject: Appeal for class-action lawsuit against connection data storage in Germany 
Everyone on the list is enriched by hearing about important relevant
issues, perhaps especially for things from beyond the US (and all the
more so for those of us in the US).
Google web page translation does an OK job. It won't win any awards
for prose, but one can generally understand what is being said.

@_date: 2006-11-13 09:52:37
@_author: Paul Syverson 
@_subject: Anonymous Blogging 
Note that your protection depends on what you mean by "surveillance".
I realize you may not know, but here is a quick description of the cases.
- If an adversary is _only_ monitoring the plaintext content of your
traffic as you send it to/receive it from the internet, then Tor should
protect the communication because it is encrypted.
- If an adversary is _only_ monitoring the plaintext content and the
traffic pattern (i.e., the pattern of packets or bytes that go by) of
your traffic where you connect to the internet, then you should be
protected because it will not see where your traffic exits the Tor
- If an adversary is _only_ monitoring the plaintext content of your
traffic as you send it to/received it from the internet, and the
plaintext content at the other end of the circuit, e.g., as it is
received at blogger.com, then Tor should protect the communication
because it is both encrypted and changing appearance from the spot it
leaves your Tor client until it emerges after bouncing around the Tor
- If an adversary monitors _only_ the internet connection of
blogger.com, but not your connection to the internet, then Tor should
protect the communication because the adversary will not see where it
entered the network, i.e., emerged from you.
- If an adversary monitors the traffic pattern of your traffic where
you connect to the internet, and monitors the traffic pattern where
you exit the Tor network, e.g., is observing the internet link of
blogger.com or the internet link of the last node in your Tor
connection to blogger.com, and if the adversary does simple analysis
on those patterns, it is likely to confirm that this is indeed your
traffic. (That is, with high probability, you are the source of that
post to blogger.com. I have no idea what sort of official deniability
remains. IANAL in any country.)
- If an adversary keeps track of the patterns of when you connnect to
the Tor network and when posts are appearing at your blog, over time
they will be able to confirm (as in the previous paragraph) that you
are the source of the communication. This is a trickier attack involving
more complicated statistical analysis and probably implies the adversary
is doing intense targeted surveillance.
Our soundbite way of expressing this is to say that Tor guards against
traffic analysis, not traffic confirmation.  As a rule of thumb if an
adversary can watch both ends of a circuit and do basic timing
analysis of it, they will be able to correlate these two. To say
anything more specific gets us into a bunch of math.

@_date: 2006-10-20 16:16:03
@_author: Paul Syverson 
@_subject: "Practical onion hacking: finding the real address of Tor clients" 
These hacks are very ancient news. We first wrote about them in I
think 1998, and many of them especially concerning Java, Javascript,
and ActiveX were not original to us even then. We were also all aware
of GUIDs being imbedded in Office Docs, Windows Media Players, Real
players, etc.  Mike Reed wrote a snoop server that we used to have
posted on the onion routing site back then. A nice one that was new at
the time was to embed into the HTML a call to use the RTSP protocol to
load shells of movies into Quicktime, and other media players
specifically to identify the IP address of the sender. There were
other snoop servers and similar demo pages available from Anonymizer,
Digicrime, JAP, and others. I don't know first who put up a demo of
the obvious point that using an anonymous pipe does not imply an
anonymous data stream nor the prevention of opening up a nonanonymous
pipe if one doesn't shut down all other pipes or calls too them through
the anonymous pipe.
So what?
1. It's pretty annoying that every few years someone announces a big
discovery in which they re-invent a wheel that we and others had
invented, implemented and announced many times. Then some press report
jumps all over it like it's a new discovery that surprised the
anonymous communications people unawares or something like that.
2. If the appalling lack of scholarship is annoying, the concerns are
real. It's simultaneously true that it's unfair to yell at someone
still trying to get a core TLS implementation done right for not
having solved all the phishing attacks that might occur over
applications that use it and true that people will get hurt by a
browser that simply offers an OK crypto interface but doesn't cope
with all the exploits that come from not understanding what it
protects and what it doesn't (that's a metaphor, don't take it
literally as about current Tor issues). What we don't need is anyone
else telling us that there's a problem as if we didn't know
that. People have to reinvent wheels a bit as they learn about
something. That's fine, and they should be encouraged and coaxed not
ridiculed. But they shouldn't be tolerated if they put themselves
forth as experts showing something new to the world while refusing to
read any of the documentation, the specs, the code, or the scientific
literature. What we do need are answers.
3.  This is forever an arms race, and, once you get beyond the early
adopters or systems for specialized use, telling people to RTFM is
always nonanswer.  What exactly is an answer? I don't know. Many
people who are on this list have hints of ideas that will help
somewhat and they have been raising them, implementing them, analyzing
them in papers, etc. I make one suggesting here so that I'm not just
grousing, even constructively.
It might be good to have a testing page that is part of the setup
wizards in some way as well as being fairly prominent on the homepage.
Apologies if someone has already suggested that and I forgot (and
especially apologies if that someone was me). There's lots of issues
implicit in this suggestion, but nunc scripsi totam pro publio, da
mihi potum.

@_date: 2006-10-20 19:12:58
@_author: Paul Syverson 
@_subject: "Practical onion hacking: finding the real address of Tor clients" 
Expanding what I was suggesting in my previous message, which I know
is just one little piece of the pie, is that when you run the
installer wizard for your favorite OS. Part of it is to run a
configuration check.  This causes you to connect to a snoop server
that identifies you every which way it can think of both within and by
bypassing the anonymized circuit, maybe compares to info from an
unanonymized connection from you. There should then be
warnings/suggestions/links/more-wizard-dialogues/etc for things to do
if you come up found. I think there are already servers out there that
do a moderate job at this, and there are at least a few people I can
think of to suggest other things to do (who no doubt have plenty of
time to do this ;>). There can also be a link from the Tor
home page for general use, and/or for periodic rechecking, etc.
Gotta go,

@_date: 2006-10-24 09:08:01
@_author: Paul Syverson 
@_subject: "Practical onion hacking: finding the real address of Tor clients" 
I have not been able to find the old onion-router.net snoop server
among the old project files unfortunately. It seems to be lost and
But check out digicrime.com
Don't know.

@_date: 2006-09-08 13:23:19
@_author: Paul Syverson 
@_subject: Tor server question regarding hidden services. 
Just to be complete: you do not need to run a Tor node at all to run a
hidden service. Your hidden server can just be a client as far as the
Tor network is concerned.

@_date: 2006-09-10 23:10:32
@_author: Paul Syverson 
@_subject: Tor Defense Fund...an idea. 
Tor, as with the previous generations of Onion Routing, was designed
by myself and others from NRL. It's the first system that was partly
rather than entirely done by NRL personnel, Roger and Nick were under
contract to NRL for design, development and initial deployment of Tor.
Funding for related research is ongoing, e.g., for analysis and
improvement of hidden services (which led to improved security for all
Tor circuit building cf.
Development and maintenance per se of Tor has passed beyond being
a research project; that aspect is no longer an NRL project.
You might want to look at
     in general, and
   in particular.
Funding from EFF for Tor development ended about a year ago, though
they still provide the web site and support in other indirect ways. So
the sponsors page is a little dated in that respect.

@_date: 2006-09-10 23:49:41
@_author: Paul Syverson 
@_subject: Tor Defense Fund...an idea. 
I'm not sure I understand the question. Onion Routing was originally
invented and developed entirely by civilian (non-uniformed) employees
of NRL (not contractors).  So, if that counts as "full-on Navy", then
yes.  As I said before, Roger and Nick were contractors when the three
of us devised Tor together. Beyond that I think I already covered.

@_date: 2006-09-19 11:23:16
@_author: Paul Syverson 
@_subject: [INFO] new anonymizing software 
In (approximately?) the words of Bob Morris, a system without a
specification cannot fail; it can only surprise you.

@_date: 2006-09-20 22:55:09
@_author: Paul Syverson 
@_subject: Question about Tor 
Hello Oscar Kardima,
I am not sure I know what you are asking. By default Tor does not
ordinary permit sending of email because the default is for Tor
nodes to not allow mail to exit on port 25, which is the standard port
for email. It is possible to configure email clients to work with Tor, and
that may require additional software..
However, there are subtleties and security implications of this.
There have been discussions recently about that on the or-talk
mailing list. See
under the threads
   Tor-compatible secure email systems
   Using Gmail (with Tor) is a bad idea
Your question is actually better addressed to that list since many
people may respond to you. onion-info just goes to me at this point,
and I'm overwhelmedly busy and unlikely to respond further than
this. You will need to subscribe if you want to post future messages
to it. See  to subscribe.
Hope that helps,

@_date: 2007-04-27 16:21:37
@_author: Paul Syverson 
@_subject: AHBL and TOR 
I also find this a little surprising. Not just at an abstract business
plan consistencly level but from having talked to some of their folks,
although that was several years ago. Has anyone who is concerned about
accessing e-gold actually spoken to people at e-gold about this? It
could be this is just a configuration mistake, a bad iinterpretation
of policy by an admin, or it could be that they made a choice they can
articulate. There could be simple solutions to some real concern of
theirs short of blacklisting.  (In general there are almost always
better solutions.) But it would be worth finding out what they are
doing and what they are trying to accomplish rather than debating in

@_date: 2007-04-20 13:43:28
@_author: Paul Syverson 
@_subject: Importance of HTTP connection keep-alive 
It's a reference to Marvin the Paranoid Android in H2GG.

@_date: 2007-04-20 13:45:32
@_author: Paul Syverson 
@_subject: Importance of HTTP connection keep-alive 
Err that's HG2G. Where's control-T when I need it.

@_date: 2007-02-13 14:19:40
@_author: Paul Syverson 
@_subject: PHP coder needs Tor details 
This  off-topic thread has gone on for too long.
Please stop this thread now.
Paul Syverson                              ()  ascii ribbon campaign  Contact info at    /\  against html e-mail

@_date: 2007-02-23 09:54:02
@_author: Paul Syverson 
@_subject: onion-router.net down temporarily 
Since at least one person (Steven Murdoch) seems to find
onion-router.net useful and noticed when it went down, I thought it
worth a note about status.
The domain name expired on January 28th. Since Network Solutions was
sending notification only to someone who has not been at NRL for
nearly ten years, the impending expiry was not noticed before the
fact.  Since only the principal account holder can pay for an
extension, since the process of changing the principal account holder
information is far from any of rational, quick, or painless, and since
they will only grant a single fifteen day extension, it will be down
until I can get this resolved. I will continue to spend far too much
time on this until it is fixed, which I hope will be soon.
-Paul Syverson

@_date: 2007-02-27 14:27:25
@_author: Paul Syverson 
@_subject: building pages with tor in mind 
I believe all the things raised in this discussion were in the old
"snoop server" we used to run here at NRL starting in 1996 until about
'99 or 2000. Unfortunately the code for it disappeared from NRL when
Mike Reed did a few years after that, so I can' say for sure what was
on it. I know he had set up javascript exploits along with many other
things, and I believe at least one of the javascript ones would return
IP address through the anonymous circuit but I can't swear to it.
(There were some that did return IP address; I just can't recall if
they were javascript based.) The snoop server also had multiple ways
of opening up connections that bypassed the anonymity network
entirely, such as via flash, real audio, etc., that is unless the
Redirector that we had for Windows NT in '97 ('98?) was used. It sent
all connections through onion routing, but was much more intrusive on
the OS. Of course that wouldn't protect against any of the attacks
that ran through the anonymous pipe, rather than around it.
You might also look at some of the exploits Kevin McCurley has on
the digicrime site. I don't think he's updated them for years, but
they're still there. James Muir has already pointed to some of the
similar exploits he's done.

@_date: 2007-02-27 15:34:41
@_author: Paul Syverson 
@_subject: Norwegian DNS compromized 
It's not about one's opinion about censorship. We can leave that
entirely aside. Two major questions loom: first, Tor is an
international network.  Sometimes the node operator and the hosting
site are not even in the same country. Whose definition of illegal do
we go by? The Netherlands, Iran, the US, Norway, ...?
Much more importantly, once you start filtering for content at all
even really poorly wrt what you are targeting, you are implying (in
the sense of the law, rather than logic and rationality) that you
_can_ filter content and therefore are obligated to filter whatever
the local/regional/national authorities du jour want you to
filter. (Admittedly this might be more important in the US but not
elsewhere; I'm not as up on that as I might be.)

@_date: 2007-02-28 12:13:01
@_author: Paul Syverson 
@_subject: Norwegian DNS compromized 
Please stop this thread.
Whatever shred of relevance it might have had is long gone.
Please stop this thread.
Paul Syverson                              ()  ascii ribbon campaign  Contact info at    /\  against html e-mail

@_date: 2007-02-28 14:13:17
@_author: Paul Syverson 
@_subject: [tor-philosophy] Protecting rights. 
OK. Just changing the title of the thread and adding the word
"philosophy" does not make this relevant.
This list is not about censorship, or religion, or technology, or
ethics, or social policy, or even anonymous communication in general,
_except_ if where these directly impinge on the design, deployment, or
use of Tor and similar systems.  (And you can't torrify your message
simply by asking things like WWJDWATN*?)
We don't have to be straight-jacketed, but extended digressions in
philosophical topics (and I have an MA and a PhD in Philosophy so I
can recognize them when I see them) as have been going on are
*What would Jesus do with a Tor node?

@_date: 2007-01-02 11:23:19
@_author: Paul Syverson 
@_subject: Tor and Thunderbird: Outgoing Email Unsafe? 
Actually we would expect Tor to be a relatively inefficient vehicle
for spam even with port 25 open, so hardly a paradise. The decision to
block port 25 by default was based partly on the expectation that spam
is the most known and widely despised source of abuse and we expected
people to raise this as a concern.  We wanted to be able to say that
Tor was simply not a vehicle for spam since it is default configured
so as not to be able to transmit email at all. (I know that's too
quick, but we're reasoning at the level of soundbites.) Even with
that, it has not prevented ignorant statements in various places from
making the association that Tor is a spam engine. Imagine how much
worse the image problem would be if there were even a grain of truth
in these statements.  We were also partly concerned about network

@_date: 2007-01-24 17:01:30
@_author: Paul Syverson 
@_subject: tor and p2p 
See "Challenges in deploying low-latency anonymity" available on the
Tor docs webpage via a link in the URL Roger gave in his last message
or at
for a preliminary discussion. Cf. the sections "Bandwidth and file-sharing"
and "Scalability".
Paul Syverson                              ()  ascii ribbon campaign  Contact info at    /\  against html e-mail

@_date: 2007-01-01 12:33:39
@_author: Paul Syverson 
@_subject: flooding attacks to discover hidden services 
You've roughly described the attacks we carried out that are described
in "Locating Hidden Servers". Hidden servers and Tor clients in general
are much less vulnerable to this since the introduction of entry guards
about a year ago. See
also to counter flooding to introduction points and related issues

@_date: 2007-01-01 17:34:39
@_author: Paul Syverson 
@_subject: flooding attacks to discover hidden services 
Right. I was misreading you at first as repeatedly flooding requests
to the hidden server and having hostile Tor nodes detect when they are
on the path. I think though what you ask is much closer to the attack
described in Steven's paper than to the attack in the paper I cited.
He has already noted the main pros and cons of how the hidden server
is configured (wrt the Tor network) and how it behaves wrt this
attack.  A further note is that the attack in Steven and George's
paper was successful when the Tor network consisted of about 35 nodes
and for routes consisting of relatively low bandwidth nodes. It is an
interesting open question if something comparable could scale up to
the current network. In principle it should, but I suspect the
engineering of it would be much harder and would involve synching many
attack-flooding clients.  It might cause other problems for the Tor
network before it succeeds in general, if it can at all. But, it could
also be interesting to see if this succeeds substantially more often
than roughly c^2/n^2 because the perecentage of attackable paths has
some nice properties (from the attacker's perspective).

@_date: 2007-07-20 11:32:22
@_author: Paul Syverson 
@_subject: RIP Anonymizer Web service 
Not sure exactly what has happened here, but probably the first major
advocate for anonymity on the Web has gone away (sort of).  A moment
of reverent silence would be appropriate.

@_date: 2007-07-12 16:08:09
@_author: Paul Syverson 
@_subject: magic Wednesday 
I noticed within the last few days a periodicity in the bandwidth
usage graphs for the whole network that goes back for at least a month
and seems to reflect what Olaf described, except that it looks like
the Thursday to Friday midnight is the one that doesn't dip as much.
I have no explanation, but then I seem to mostly be the subject
rather than the source of conspiracy theories ;>)
Cf.

@_date: 2007-06-08 16:29:54
@_author: Paul Syverson 
@_subject: The Hidden Wiki is Gone ... so how can I find TOR-only web pages? 
Good point. However, I assume you meant to say that the notice narrows
down the country from which the person hosting the hidden wiki derives
cultural context, which might but might not tell us much about where
s/he stashed the server. Now is there enough in this message for Nick
to do a linguistic analysis and tell us exactly who it is ;>)

@_date: 2007-06-01 14:36:21
@_author: Paul Syverson 
@_subject: Sampled Traffic Analysis by Internet-Exchange-Level Adversaries 
If the traffic patterns can be stored and analyzed offline rather than
in real time, it just makes my point stronger.  Assume
someone with the ability to do truly global monitoring, watching
every connection from every client everywhere in the world through
every tor node everywhere in the world to every server everywhere in
the world (Note that I was effectively assuming the filtering you mentioned.
I don't care if the adversary watches non-Tor traffic. I assume they
have already made that separation. As you note, it is trivial to
recognize traffic going to/from/between Tor IP addresses.)
What I am saying is that it is nuts to assume that someone could have
monitors on all of these places but can do nothing active at all,
not even doing something as trivial as killing a targeted circuit
and watching to see if a suspected circuit dies elsewhere. It doesn't
even have to be targetted. The adversary can simply arbitrarily induce timing
channels in various places or kill circuits or whatever
and watch for those patterns elsewhere (in the stored
data if this is done offline).
Lasse and I saw how incredibly easy it was to find patterns with very
limited resources. George and Steven showed how you could induce patterns
gross enough to even monitor them by interference (albeit on a much
smaller and generally lower bandwidth network). I'm not comparing a global passive adversary with a global active one
and claiming that global active is more realistic or practical.  I'm
saying that it is a mistake to posit a truly global adversary (not
just a really big adversary watching, e.g., eighty percent of all the
communication we would ever be talking about) that cannot do even the
tiniest local thing actively. Nonetheless, that is the adversary from
much of the literature.

@_date: 2007-03-07 09:29:43
@_author: Paul Syverson 
@_subject: one less onion skin 
I asked just this question recently in another context. As you noted
in a later message CREATE_FAST allows us to avoid the handshake
(besides the one that formed the TLS link), so it's really just the
k_1, d_1. I suppose it's worth considering. My kneejerk response is
that (a) the overhead from this vs. everything else in Tor is very
small, and (b) maintaining it provides for a consistency and
flexibility should we want to change the protocol in some other ways,
and to quote Needham's oftquoting of I think Strachey "It's impossible
to foresee the consequences of one's own cleverness." by which Roger
usually meant that when we try to be too clever in removing things we
don't need we can get ourselves into trouble. On the other hand he
also said (with Martin Abadi in their Prudent Engineering Practices)
"Be clear about why encryption is being done. Encryption is not wholly
cheap, and not asking precisely why it is being done can lead to
redundancy. Encryption is not synonymous with security, and its
improper use can lead to errors." For me, I think this encryption is
cheap enough that (a) and (b) above win for the moment, but I could be

@_date: 2007-03-07 15:42:43
@_author: Paul Syverson 
@_subject: Building tracking system to nab Tor pedophiles 
I don't think it was off topic. To repeat what I already said in
an individual response.
  I think it was not OT since your post addressed the reality of a
  situation for which people were designing Tor modifications and
  deployments and you evaluated their applicability to intended
  application.
I had advocated something similar some time ago. Actually what I proposed
was that some sort of test server be set up. I know there are already
many of them, but I was thinking that there could be testing stages
in an install wizard (or a post-install testing wizard)
that takes the user through various tests and what to do in response
to results. I know a lot of work, maybe another suggestion to be
listed on the volunteer page or a candidate for summer of code?
You dream big (not sure which is the bigger dream ;>)

@_date: 2007-03-07 17:16:43
@_author: Paul Syverson 
@_subject: Boulder Tech report on low-resource routing attacks on Tor 
The following are some comments on the Univ. Colorado at Boulder tech
report "Low-Resource Routing Attacks Against Anonymous Systems" that
has been getting lots of press and other web attention lately and been
somewhat discussed on this list.  It is only today that I have managed
to find time to sit down and read the paper.
The nutshell for people that don't want to read the details below:
A good paper. It does _not_ show Tor to be broken. (Nor did it ever
claim to. I only state that because of some of what has appeared in
the blagnpress, which to their credit, the authors tried to curtail.)
It is a nice contribution, especially in showing the limitations of
the current approach to entry guard selection. Overstates its novelty
over prior work, which is really unnecessary because it makes valuable
contributions of its own (and which is more or less my fault not theirs,
cf. below).
More details:
This is a nice piece of work. Its greatest contribution is in
directing attacks on entry guards. In the theory and simulation work
in which such ideas were introduced by Wright et al. they were
introduced (as "helper nodes") to reduce vulnerability.  As a recent
addition to Tor, the nature of defense they provide but also the
possible risks from how they are used in actual implementation and
deployment needed to be explored. It was understood from the start
that there is something of a tradeoff in introducing them. It was
realized that profiling without entry guards was in practice trivial
enough that the additional risk of adding entry guards and thus
simplifying and enhancing profiling for anyone who unfortunately had
an adversary guard node was clearly worth it. I don't think this paper
changes that. However, by attacking the guard selection process
itself, the research forces us to examine it more closely.
What they did was apply techniques that Lasse and I developed in
"Locating Hidden Services" to ordinary client circuits. Though we had
said this would be straightforward to do, we didn't actually do it.
Because we were focused on the deployed Tor network we could not
pursue this sort of attack there. We were also focused primarily on
what could be accomplished with a single hostile node. This limits to
cases of either a hostile website (as in Murdoch and Danezis and as
mentioned on p. 10 of this tech report) or a hostile client and a
hidden service, which is what we reported on.
Deploying a Tor network on PlanetLab and using synthetically generated
data removes some of the "in the wild" reality from the results.  But,
by accepting this limitation, it allowed them to obtain data at all
about Tor circuits for ordinary use (not hidden services). Much in the
practicality spirit of onion routing. The experimental networks were more than an order of magnituded
smaller than the current deployed Tor network. One cannot be sure
something will scale until actually trying it, but in this case there
is no reason to doubt it does scale. Still if we take the 9 percent
figure given by the authors as an arbitrary line at which attacks
become significant, that is still almost a hundred nodes in the
current network. At about twice the entire size of the experimental
networks that were set up this starts to be a bit more than
low-resource.  Still one could do quite a bit with less than 9
percent. Also, as a counter to my own point, see "On the
countermeasures" below.
On prior work:
Before I start noting all the things that the authors didn't properly
cite, I should observer that they first contacted me about their work
way back in last October and have cc'd me on correspondence with
Roger, who did read their work and respond to them. If these are
indeed omissions and oversights in their paper, then it is my fault
because they gave me plenty of opportunity to comment before it hit
the interblag.  I just didn't squeeze in the time before now.
I already mentioned that the basic techniques are similar to what was
in my paper with Lasse ?verlier, "Locating Hidden Servers".  The
authors say that they think theirs is "the first approach capable of
[compromising anonymity] before the client starts to transmit any
payload data". I believe that the code we ran would be entirely able
to do this. We mentioned it only briefly in passing in our paper, and
we didn't actually do it. The authors of this report did.
They also say they have introduced the idea of nodes falsely reporting
bandwidth and uptime. As they note this is central to the way their
basic attack works. As they do not note, this was explicitly used by
Lasse and me in our attacks. I quote from our paper, "Alice's server
node will report a false higher uptime and the maximum network
bandwidth to the directory server in order for other nodes to trust it
for their circuits. This is still possible as there is (yet) no method
for keeping reliable track of uptime at the different servers."
They also introduce the idea of selective path disruption to speed up
attacks by dropping circuits, because that will cause more circuits to
be built. This was also part of our attacks albeit since we controlled
the client connecting to the hidden service, it could be done there.
The statement that the algorithm for choosing entry guards was
"implemented to protect the first hop of a circuit by using what are
believed to be more reliable and trustworthy nodes" is false.  Using
more reliable nodes was seen as sensible because it should minimize
the number of entry guards a client uses, which is the whole
point. Nobody thought that this made them more trustworthy. In fact
the general threat of adversaries running reliable nodes (in both
onion routing and mixnets) to attract more traffic is well recognized.
On the threat model. While the design document does use the c^2/n^2
basic result from "Towards an Analysis of Onion Routing Security"
as a starting point. This was not thought to be accurate once we
had substantial deployment and was acknowledged as such.
Cf., "Challenges in deploying low-latency anonymity"
On the countermeasures:
Rather than discuss all of them in detail in an already overly long
post, I'll just note that I think they may help against an adversary
that has only several hundred to a few thousand dollars in resources.
The authors note that they are considering just that case, and it is
certainly worthwhile to see what it takes to mount an attack and what
works against a low resource adversary. That was also part of our
motivation to show what could be done with a single bad node.  But, an
adversary that has a few tens of thousands of dollars can simply run
many reliable high bandwidth nodes and thus mount the attack
invulnerable to any countermeasure against lying. Michael Gersten
noted the threat of this attack in a separate context in a post to
this list yesterday (March 6). And it has long been recognized as a
potential threat to Tor in general. I have begun to look at
countermeasures that should work unless the adversary owns major hunks
of the network, e.g., social network based, but will not get into that
further here.
More than you wanted to read. Hope it was useful anyway.

@_date: 2007-03-07 22:35:54
@_author: Paul Syverson 
@_subject: Building tracking system to nab Tor pedophiles 
Yes. Three cheers. I think this is a fine interim thing to do.  Maybe
I'm overly sold on install wizards but I think a step in the install
that says something about not being secure against responding web
sites by default and a pointer to a couple of things to do before
continuing is probably going to catch more people than anything on the
download page. Of course there will still be some (most?) people who
will just say "yeah, whatever" and click continue. But this is an
interim idea. (Now someone has to write installers in every
language. Perhaps _that_ could be added to the volunteer page.
In the interim interim, something on the download page will get
caught be our volunteer translators sooner than anything I said

@_date: 2007-03-09 09:00:35
@_author: Paul Syverson 
@_subject: Warnings on the download page 
I was going to note this, but Roger beat me to it.  This is exactly
what I do. I can configure Safari to be relatively safe, but mostly
use it when I really don't care or when I must/want to use some
plugin, etc. I use Firefox with everything shut off together with
Tor, when those things don't apply.
But this is an easy option for people to understand. For this reason
it might be best from a usability perspective (I'm thinking noobs
here.) If most of our users are on Windows, they can be instructed to
use a clamped down Firefox (with and without Tor) when they care about
security and IE when they don't. We could still recommend that they
try to do various things to be more safe when using IE. I _know_
people think that IE is just a big security hole, etc. I don't want to
get into that debate. I am just assuming that users will be willing to
do minimal things. Telling them to install Firefox and install Opera
(I know it's proprietary) or whatever (fine, tell them to use the
Windows port of lynx, that'll have alot of traction) is probably a
non-starter. This could be a relatively simple instruction that they
are relatively likely to get right wrt configuration and that will
make them much more secure than they are now and more secure
than they will be if they attempt some more subtle alternative and
get it wrong.

@_date: 2007-03-04 12:41:02
@_author: Paul Syverson 
@_subject: exit node back to user 
As the person who did that design I would have to say, err no, that's
not true. It's always been designed for communication over a single
bidirectional circuit. There have been some alternative things
discussed that I won't raise here. But, except where raising possible
alternatives, the design documents and publications for all three
generations have described circuits and flows as having this

@_date: 2007-03-24 08:52:52
@_author: Paul Syverson 
@_subject: Ultimate solution 
I don't understand this statement. Tor was reasearched and developed
by and for the US DoD as an onion routing project, the explicit purpose
of which is security for DoD and other communications:
traffic analysis resistance, DoS resistance, personnel protection,

@_date: 2007-03-26 11:40:44
@_author: Paul Syverson 
@_subject: Ultimate solution 
I'm even more confused. Are you saying anonymity is not a security
property? By "security" are you limiting yourself to confidentiality?
There are many aspects of security, rarely all addressed at once by
any system. One of these is anonymity, which is why one finds
anonymity as a listed topic in the CFP of virtually every major
computer security conference.

@_date: 2007-03-26 12:29:31
@_author: Paul Syverson 
@_subject: Ultimate solution 
There are (at least) two things going on through this discussion that
are run together and should be separated. 'Free' and 'open source'
have long ago become technical terms in the context of software and
programs. Technical terms allow a community to agree on some things so
that it is not necessary to spend many paragraphs explaining
terminology every time one wants to say something. This does not rob
the terms of their broader meaning.  And people can use the terms as
they wish, but if they have a discussion in a context for which the
terms have specific meaning, then they should expect to be
misunderstood and yelled at a lot for confusing people and wasting
their time. And, if they refuse to conform to the vocabulary of the
community they can expect to eventually be simply dismissed as too
much trouble to deal with. I would be subject to the same criticism if
I started talking about free algebra, free groups, free variables,
etc. using 'free' in some nonstandard way and simply insisting that it
is correct.
One can of course question the standard usage that has arisen and give
arguments why other ways of speaking are more appropriate. But that is
a topic for a forum where there is discussion about the basic meaning
of those terms, not in a context where they are simply assumed and
used, such as this one.
Arrakis can do what he wishes with his software. If he calls it free
software in a broader population, well it will be interpreted however
it will.  But, if he calls it free software in a software development
forum, he is simply misusing an established term. But, after one or
two posts, the meaning of "free" is off-topic for this forum and
should be taken to one where it is appropriate.  (People may also then
get angry that the public will be misled and run together things that
are not the same at all. This happens all the time when science hits
the broader world. It is also part of what is going on here.)
If it is important to continue a discussion of the different types
of licenses that Tor software or other related software is under, e.g.,
because it affects decisions about further development and distribution
of that software, then fights over the correct meaning of 'free' should
be taken elsewhere. We can for these purposes adopt something like
standard-free and arrakis-free for these discussions without quibbling
over which of them is really free.
-Paul (P.S. For more, please see my critical analysis of that moving
treatise on the impact of Frege's Begriffsschrift in the years
after the American Civil War,  _Freedom's Lament: A History of the
Bound Variable in America_. )

@_date: 2007-03-26 12:47:31
@_author: Paul Syverson 
@_subject: Ultimate solution 
Yes. I understand what you mean now. But you are misusing 'secure' by
saying something is not secure if it is not confidential. As an
imperfect analogy, people sometimes say "home computer" when they are
really only counting it as a computer if it's a wintel box.

@_date: 2007-03-26 16:06:09
@_author: Paul Syverson 
@_subject: Ultimate solution 
Or hence "free as the fremen" (sorry couldn't resist).

@_date: 2007-03-07 08:59:54
@_author: Paul Syverson 
@_subject: UDP over Tor [was Re: blog about tor and skype] 
============================== START ==============================
You could simply do that, but you would have transport with all the
efficiency and flexibility of TCP with all the reliability of
UDP. It's a quick and easy (err, sort of) way to incorporate UDP
without having to redo Tor. But to really channel UDP and get some of
its advantages, it's not just the DTLS we would need that Nick
mentioned. That will only take care of the links. We would need to
have a block cipher based system that worked against replay and a
bunch of other issues. As mentioned in the Challenges paper
( but not on
the wiki, the Freedom network did this. So we know it is in principle
possible. But I don't think the code was publicly available and
documented nor the protocol fully specified (although the Freedom
Protocol 1.0 architecture paper spelled out much
web.homeport.org/~adam/zeroknowledgewhitepapers/arch-notech.pdf ) They
were also forced to reimplement alot of TCP anyway.  Ultimately
since Freedom had a different way of maintaining users and nodes,
and wasn't around long enough, we don't know in principle
if it's possible to do well enough for our needs. I think this is
really a big hunk of work to do anything like an OK job, but I
encourage people to continue to go after it because it would be great
to have. What nobody has yet mentioned is that carrying UDP traffic would also
involve alot of tradeoffs in design as well as needing to design for
incentives and economic issues that would arise. It won't just be Tor
only better and able to do more.

@_date: 2007-05-29 08:56:55
@_author: Paul Syverson 
@_subject: Sampled Traffic Analysis by Internet-Exchange-Level Adversaries 
As the person (or one of the people?) who first started to complain
about the GPA I thought I should note that my objections were against
both adjectives, global and passive. A global adversary is too strong,
even if you do limit to just the internet links. I don't think that is
quite as strong a statement as when I first made it many years ago:
(1) the line of work that prompted this thread shows that if it's too
strong to posit a truly global adversary, the scope of a potential
realistic adversary is pretty large indeed.  (2) relatedly, underlying
layer networks change over time, lots of consolidating. Some things
seem more feasible...  Anyway, the main reason I'm writing is that my objection was not just
that the GPA was too strong but that it was too weak. Thinking you
could have an adversary powerful enough to monitor all the links
necessary to watch your whole large network but not able to do any
active traffic shaping at all anywhere seems obviously nuts. This is
one reason why padding on an open low-latency (lossless) network is
problematic: an adversary with any active capability at all can induce
a timing channel easily.

@_date: 2007-11-17 10:14:00
@_author: Paul Syverson 
@_subject: encrypting your communications?! 
This is better. Although Much like the original concern that Robert
raised, this could be interpreted as saying that using Tor prevents
the sites you visit from learning where you're coming from (especially
since it says just that ;>). The tricky thing is how to succinctly and
clearly say to the general user that it is the networking address
information implicit in the act of connecting that is hidden, but
that's not the whole story.  The onion routing project home page
phrases it thus in the opening paragraph.
   The focus is on practical systems for low-latency Internet-based
   connections that resist traffic analysis, eavesdropping, and other
   attacks both by outsiders (e.g. Internet routers) and insiders
   (Onion Routing servers themselves). Onion Routing prevents the
   transport medium from knowing who is communicating with whom -- the
   network knows only that communication is taking place. In addition,
   the content of the communication is hidden from eavesdroppers up to
   the point where the traffic leaves the OR network.
This is too geekspeakish for the intended purpose here. But it gives a
hint perhaps of what could be said. Also, apropos to Robert's
complaint the last sentence does two things: it does let people know
that traffic is encrypted against eavesdroppers within the
network. More importantly, even for people who aren't thinking about
encryption one way or the other and for people that might have been
confused by the sentence Robert noted, it succinctly and clearly tells
them that there is a part of the communication path that is not
encrypted against eavesdroppers---a part that is outside of Tor.
So a suggested revision
  Tor protects you by bouncing your communications around a
  distributed network of relays run by volunteers all around the
  world: it prevents somebody watching your Internet connection from
  learning what sites you visit. Even the Tor relay you connect to
  doesn't learn that. Tor comes bundled with other protections that
  combine with Tor to hide your location from the sites you visit
  too. And, Tor hides what you are saying from eavesdroppers anywhere
  between the point your connection leaves your computer to the point
  it leaves the Tor network and heads to the site you are visiting.
I think the sentence about bundling lets even the people who can't
look two short paragraphs down know that there is more to the story,
but it still says there is a basic protection from responding sites
that they get from Tor (And, it doesn't end with a preposition ;>) I'm
torn about whether the last sentence is worth it. It's a really
important point for the reasons that prompted this exchange and other
reasons too, but maybe it is just one point too many for an opening

@_date: 2007-10-26 05:20:01
@_author: Paul Syverson 
@_subject: Spam over Tor 
I disagree with that. There still has been no spam over Tor in the
scenario you described. Rather someone has logged into a system over
Tor and used _that_ system to send spam. I'm not saying Tor would not
have been used in any way by the person in this scenario. I am saying
that no spam was sent over Tor in what you described. You could also
claim by your logic to have shown that the statement that there is no
spam over HTTPS is history.

@_date: 2007-09-20 10:26:25
@_author: Paul Syverson 
@_subject: A Tor Typo 
After one exchange offlist, I am sending this response to ortalk
in case whoever wrote the warning comment wants an authoritative
response for future reference or if someone wants to observe the truly
compulsive in action.
Here, in its entirety, is the first entry in _A Dictionary of Modern
English Usage_ by H.W. Fowler (Second edition revised by Sir Ernest
Gowers), Oxford University Press, 1965.
   _A_ is used before all consonants except silent h (a _history_, and
   _hour_); _an_ was formerly usual before an unaccented syllable
   beginning with h andi sis still often seen and heard (_an
   historian, an hotel, an hysterical scene, an hereditary title, an
   habitual offender_). But now that the h in such words is pronounced
   the distinction has become anomalous and will no doubt disappear in
   time. Meantime speakers who like to say _an_ should not try to have
   it both ways by aspiring the h. _A_ is now usual also before vowel
   letters that in pronunciation are preceded by a consonantal sound
   (_a unit, a eulogy, a one_). Before letters standing for
   abbreviations or symbols the choice is determenined by sound of the
   letter, not of the word it represents, e.g. _an R.A., an M.P.; but
   that is the sort of thing about which we ought to be allowed to do
   as we please, so long as we are consistent.
This a British text, but has discussions of American and British
English and pays a decent respect to both.  _The Elements of Style_
had nothing on the issue. My copy of the _Harbrace College Handbook_
has an entry on a vs. an, but is annoyingly silent on what to do
before letters standing for abbreviations.  The same was true of the
two editions of _The Chicago Manual of Style_ I consulted.  If one
wants a reglur 'mercun authority, however, what could beat the _United
States Government Printing Office Style Manual_, 1984?  On page 71 it
  When a group of initials begins with _a, e, f, h, i, l, m, n, o, r,
  s, or x_, each having a vowel sound, the indefinite article _an_ is
  used.   an AEC report                         an NSC (en) proclamation
  an FCC (ef) ruling                    an RFC (ahr) loan
After consulting these physical references I signed up for a free
thirty day trial of the _The Chicago Manual of Style Online_, 15th
Edition, which does have an entry that specifically addresses the
issue, viz, 15.9. It agrees with my original post, as well as Fowler,
and the U.S. Government Printing Office. The terms of usage for my
trial subscription preclude posting content outside of my
organization, however; thus you will need to trust me on that one.
So, no need for our two great peoples to be divided by a common
language on this point.  Usage evolves, and I believe the flexibility
expressed in last line of the entry from Fowler to now be
dated. Nonetheless, its sentiment is most important. I'm going with
Fowler, Sir Ernest, _and_ the U.S. of A.

@_date: 2007-09-24 09:56:27
@_author: Paul Syverson 
@_subject: Servers and the "Named" flag (was Re: time needed to register a serve) 
Not that it matters much for present purposes, but I would say that
these primary reasons were actually clear ancillary benefits that grew
to be the important reasons. The original motivation for putting this
man-in-the-loop element in there by design was a kluge to have a
simple if weak check on the number of servers run by a single
authorities rather than to make sure servers were up and running
properly (which was an issue whether you were known or not). In
practice this started as Roger-has-to-know-you-out-of-band.  Once we were pleased to scale beyond that being feasible, we (i.e.,
Roger) were still manually deciding whether to take a server into the
network, so could avoid or manage-as-it-arose multiple servers
obviously controlled by the same person, and we could have warm
fuzzies that we made it at least a bit more work if someone wanted to
do this non-obviously.  Throughout this process, even when everyone
was known, there will still interactions of the
we-don't-seem-to-be-able-to-reach-you or
we-don't-seem-to-be-able-to-make-circuits-through-you type.  But, as
the authorization aspect came to be less manageable and wasn't a
functional issue, it ceased being something that was addressed at all
in joining the network. I think even before Weasel took over this job from Roger it had
entirely moved to an issue of functionality rather than preserving
anonymity that was being addressed by having registration.  As scaling
continued, whether for server reachability/functioning or for
authorization of who could join what to the network, this moved beyond
what Weasel or anyone could feasibly manage in this way. We ultimately
arrived at the current situation.
The automation and usability of configuration continues to improve
steadily (if much too slowly for the impatient).  Managing who is in
the network and/or their control of path endpoints is something that
remains much trickier since the nature of the network is itself
evolving. And what is theoretically justified, practical, and doesn't
break some other aspect is itself very murky and the subject of
ongoing research.

@_date: 2007-09-26 14:39:06
@_author: Paul Syverson 
@_subject: Clone nodes 
Minor historic quibble: Router twins did start in the day of onions to
lay circuits, but they lived at least for a while into the onion skin
(incremental path) era. More importantly, they weren't just for
replies: there were multiple motivations, e.g., network and client
overhead. You didn't need to rebuild a path from scratch if a node on
the path was down (you could go to the twin). Note that you could do
this with telescoping by just going to another node (with another
onion key), but (1) it would still involve additional exchanges and
exponentiations, so savings wouldn't be as great (2) it's a bad idea
from an anonymity perspective (which is the main reason why such
circuit building was quickly abandoned from the code once twins were
gone). Load balancing for both network performance/overhead and for
anonymity were also considerations. In the end we didn't have time to
properly figure out the tradeoff implications of twins so left it as
something to possibly consider again in the future in our copious free

@_date: 2007-09-26 15:34:46
@_author: Paul Syverson 
@_subject: Clone nodes 
In the early days of onion routing, roughly the first two generations,
paths were built by a data structure called an onion that comprised
several layers: each one was public-key encrypted for one node in the
path. Within each layer was session keying material to be used once
the actual data started flowing and the identity of the next hop in
the path.  The whole thing was just layers with nothing in the middle,
which is why I called it onion routing (aside: although later people
seemed to apply the phrase to anything that had a layered data
structure.  To me the central aspect making it onion routing is that
the public-key actions are used to lay a cryptographic circuit. Then
the data flows over that. I regard anything else as a misuse of the
term. Note that 'onion' was used to refer to layered data structures
independently at about the same time as we were inventing onion
routing, e.g., in the Babel paper. However, they were talking about
data in a mix network, not onion routing.) Nowadays, we extend the
path one hop at a time: each node does a DH key exchange with the
client to get the session keys, which are used when data starts
flowing, but also to encrypt the one-layer onion (the "onion skin" as
I called it above) to extend to the next hop. This has a few drawbacks
from the old design but some major advantages: the largest are
probably that there is no need to keep digests of onions around at
nodes to guard against replay and forward secrecy once a circuit is
closed.  More about the earlier designs and other info can be found at
 Graphic depiction of onions is in the slides is at
 These describe mostly generation 0 onion routing, but some aspects of
generation 1 are mentioned, e.g., separating clients from server
nodes.  There is also a brief description of the points I make here
together with graphics at
These slides mostly describe generation 2 onion routing (Tor) as of
c. late 2003.

@_date: 2008-04-18 09:34:39
@_author: Paul Syverson 
@_subject: [OT] mitigating or defeating syntax analysis 
To be clear, this is both a recognition of where the hard problems are
and a design choice not just fatalism.
Onion routing has always been about separating anonymity of the channel
from anonymity of the data and providing the first but leaving the
second up to the needs of the application. Every iteration, including Tor, has stressed that just anonymizing the
channel is a hard enough job. So it has always been the primary focus
while doing things about anonymizing transmitted data is important
but not as immediately central.
In the days when proxies were relatively novel and controls for
cookies and active content in browsers were nonexistent or minimal,
we had both anonymizing and nonanonymizing http proxies. For all
anonymizing communication networks, data anonymity depends on channel
anonymity: if you are identified by the channel, it doesn't matter
that you anonymized the data. But usually the other direction is
independent. When Crowds came along it was a nice clear contrast
because their channel anonymity against the nodes carrying the traffic
depended on anonymizing the data.
On the specific topic of text patterns I would be curious to know if
there is any analysis on how well the jumping frog technique suggested
above works in practice. And to add one more reference to check out,
at the other extreme from what we've been talking about, cf.
"Natural Language Watermarking and Tamperproofing" by Atallah et
al. in Information Hiding 2002.

@_date: 2008-12-19 08:05:52
@_author: Paul Syverson 
@_subject: UDP and data retention 
(Sorry couldn't help myself. And sorry I don't have a substantive
answer to your question. -Paul)

@_date: 2008-02-13 13:10:13
@_author: Paul Syverson 
@_subject: OSI 1-3 attack on Tor? in it.wikipedia 
Err no. That's exactly wrong. It is in fact "The Onion Routing".  When
Roger started to work with me on this c. 2001 or 2002, he told people
he was working on onion routing. There have been various other efforts
since we started onion routing in the mid nineties that are versions
of onion routing (as well as various things that people call "onion
routing" that I, as the coiner of the expression, think are very
different from onion routing). "Onion routing" to some extent rapidly
became the bandaid, xerox, jello, aspirin, (google ?)... of anonymous
communcation. So people would ask him when he said this, "Which one?"
He would tell them, "No, _the_ onion routing, the original project
from NRL.", or something like that. (Roger feel free to correct. I
think it is Rachel who actually noted its appropriateness as an
acronym---and Roger who then turned it into the proto-acronym it is
today.) "The Onion Router" would make no sense in any case. It would
imply that there was one amongst the c. 2K onion routers currently out
there carrying traffic that is the genuine article.

@_date: 2008-02-13 14:42:14
@_author: Paul Syverson 
@_subject: OSI 1-3 attack on Tor? in it.wikipedia 
Yeah well I respond to stuff if I see it in a message and (more
significantly) have a moment. Life's much too short for me to delve
into wikipedia because I don't think it has something right.
Fair enough. Although it still doesn't seem right with me.
What is that web server running on? The Apache web server.
What are you proxying through? The squid proxy.
Notice that `server' and `proxy' are part of the question.  Perhaps if
people asked questions like, "Which anonymity router is on that node?"
and there were onion routers and crowds routers or freedom routers or
something I could hear "The Onion Router" without cringing. But so far
that seems unlikely. 'Tor' is a nice because it is ambiguous between
software, network, project, 501(c)3, etc. not to mention its parsing.
(I like the recursive reading of it best.) And don't get me started on
regretting going along with the title of the Tor design paper,
multiple things wrong with that title. ;>)

@_date: 2008-02-16 08:42:19
@_author: Paul Syverson 
@_subject: Compromised entry guards rejecting safe circuits (was Re: OSI 1-3 attack on Tor? in it.wikipedia) 
might also give some insight into theses same points, although applied
to different network structures, especially in its "Robustness of
Anonymity" section, although

@_date: 2008-02-19 09:36:54
@_author: Paul Syverson 
@_subject: OSI 1-3 attack on Tor? in it.wikipedia 
Was cleaning my spam traps and several messages in this thread were
in them, so I reread the thread and had a further reflection on
why "The Onion Router" just seems wrong and makes me cringe when
I hear it. (The cringe came first. The explanations will dribble out.
It's kind of also how I do math or system design.)
Another important distinction between, e.g., Apache and Tor is that
the Apache software is for a web _server_. The Tor software is for
both an onion router and a client. In the first generation of onion
routing these were integrated in a much more P2P way (although not
intended to be P2P in the usual sense because we were focused on
enterprises and enclaves running it, not individuals).In the second
generation we separated the need to always run an onion router if you
are running a client. In the third generation (Tor) this separation
continues. So, for first generation onion routing, one could perhaps
sensibly refer to the software as "The Onion Router". But since about
late 1996, it has made no sense for another reason than the
ones I gave above: If "the Apache web server" referred to software
that was used both for the web server and (far more widely) for a web
client then it would be analogous to onion routing (second or third
gen). But if that were the case, then I don't think anyone would refer
to the software as "the Apache web server" since it would not be only
the web server, just as Tor is not only the onion router but also the
Yours persnickety,

@_date: 2008-02-04 11:48:33
@_author: Paul Syverson 
@_subject: The use of malicious botnets to disrupt The Onion Router 
Those are all nice starting points. Roger also has some research in
this area that goes as far as anything to date in looking at the
issues. It may have been mentioned on this list or not ready for the
public (can't recall which). I've been talking to George and others
about this... In other words, there are people who know Tor quite well
who recognize the issues and are thinking about this area along with
others. People should please continue to discuss. The only thing I
wanted to point out is that this has at least as much of hard basic
problems to solve before its fully ready as say, switching to UDP or
some of the other basic issues, probably more.  (Cf. the challenges
paper  where
some of the difficulties are raised.)  But there are already some
smart people looking at it (and me too). So I'm not holding my breath,
but I'm hopeful.

@_date: 2008-01-11 17:36:18
@_author: Paul Syverson 
@_subject: Possible attack method?? Question.. 
The mantra statement I've been using since '96 is: Onion routing
protects against traffic analysis not traffic confirmation.  Roughly,
if the adversary controls both ends of the connection (entry and exit
node, entry ISP and exit node, entry node and destination web page, or
any variant of these or similar combinations) they will be able to
pretty trivially confirm by packet counting and timing that a stream
at both ends is linked and "discover your true ip".  If they only
control one of those ends, they cannot do such linking. There are
caveats and subtleties, e.g., website fingerprinting can have some
limited statistical success for Tor circuits if only the entry side is
known. And there are other subtleties.  But it remains rough
approximation of the true picture.

@_date: 2008-01-28 16:15:59
@_author: Paul Syverson 
@_subject: quick circuit tear down question 
One example concern we had was that someone who owned the first two hops
could kill the last part of the circuit and hope it was rebuilt to
a compromised node. Put much too succinctly, this makes the anonymity
roughly 1 - c^3/n^2 rather than 1 - c^2/n^2 , where c is the number
of compromised nodes out of n nodes total. That statement rides roughshod
over many important points. But there were enough concerns with this
and other aspects of leaky-pipes that we decided we should put off
deploying them until our analysis was holding water a little better.

@_date: 2008-01-02 23:24:24
@_author: Paul Syverson 
@_subject: [OT] more from Cryptome on NSA, Windows firewals, mail services 
At the risk of jumping into a conversation I barely skimmed, it sounds
like you're talking about something Nicko van Someren described at the
rump session at CRYPTO 98 and expanded on in "Playing 'Hide and Seek'
with Stored Keys", a paper with Adi Shamir at FC 99. My remembered
impression of the presentations was that it was pretty interesting
stuff that was well researched and analyzed.  Apologies if those
references have already been mentioned (or are not actually what
you are talking about).
Hope that helps,

@_date: 2008-01-03 08:48:31
@_author: Paul Syverson 
@_subject: Google becomes usefull for us again 
Another suggestion. Check out ixquick. The main reason is that they
have the best privacy and cookie policy of any search/metasearch
engine that I have found. They even act as a minimal IP address anonymizer not just of search
but also of resulting links followed if one uses their automated
highlighted results setting. As an aside their rankings are less
easily gamable since they count the rankings of the different engines
independently. That said, they don't include google in the engines
they metasearch, and there is much that I like about google. I use

@_date: 2008-03-09 21:01:02
@_author: Paul Syverson 
@_subject: consult on Tor(from a student) 
Hi Gates Chen,
Thanks for your interest. There are several Tor directory servers.  I
wouldn't claim they are immune to DoS but they are fairly resistant.
Besides there being an increasing number of them in diverse domains
and jurisdictions they tend to be on fairly high bandwidth
connections, etc.
The design of directory servers is now on its third main version, and
it continues to improve. Besides the servers there are many more
directory mirrors (perhaps enough to genuinely resist any DoS) so even
if all the directory servers were successfully attacked the network
would continue to function for a while---hopefully long enough to
recover from the DoS or to set up alternative servers.
HTH. I've cc'd or-talk which is the best place for such questions.
You caught me at a really rare moment when I had both attention
to see your query come in and time to respond. Usually from sheer
business I respond horribly late or never.

@_date: 2008-11-06 08:31:44
@_author: Paul Syverson 
@_subject: DDoS attacks via TOR? 
You uncaffeinated clod ;>) (Sorry I was talking to myself, I need more
too.) I think you mean the introduction points. The rendezous point
is chosen by the client, not the hidden server.
Apologies if this isn't what you meant, but you may be thinking of my
paper with Lasse Overlier, "Valet Services: Improving Hidden Servers
with a Personal Touch" from PETS 2006 and available at
 or
We discuss precisely that issue there.

@_date: 2009-04-03 12:34:30
@_author: Paul Syverson 
@_subject: Fwd: [Wikitech-l] Planning to tighten TorBlock settings 
Building this into Tor would be pointless because, as you observed, it
would not block misbehavior via the many other ways to be anonymous
widely used by abusive people. Systems that require protection against
abuse need some sort of authentication (possibly anonymous or
pseudonymous) required for any access. Otherwise people can just
manufacture identities at will and use them over any other path than
Tor.  More accurately I should have said that it would be pointless
to reduce overall abuse; it would just give one warm fuzzies that
abuse via Tor was moved to the other mechanisms not designed to be
as visible.
It is also not necessary to have a technical solution. Simply escrowing
all edits coming via Tor until some editor is willing to check them
would prevent abuse from ever getting to Wikipedia and would allow
edits only at whatever rate Wikimedia chooses to devote resources.
Because of the incentives from failing ever to succeed via this
path would mean that abusive submissions should be minimal; although
that is moot from the perspective of Wikimedia resources devoted to
it. I hashed this out with Jimbo several years ago, and he entirely
agreed with me in the end.
All that aside, my understanding is that precisely what you suggest
has been designed, built, and proposed for integration to wikimedia
more than once and was basically ignored: first Jason Holt's nym and
then later the more fully featureful and developed nymble from some
folks at Dartmouth. I wasn't involved, so I may not have the details
correct. And, it's been a while. My point is that it's already pretty
much there any time Wikimedia wants it. Or it was, I don't know how
much its been maintained. But they are apprently not interested
(enough). I'm going on very little sleep over the last few days and
apologize if I have run roughshod over a misunderstanding that could
be resolved if the right people (not me clearly) had a chance to
connect and work things out.

@_date: 2009-04-03 21:08:10
@_author: Paul Syverson 
@_subject: Fwd: [Wikitech-l] Planning to tighten TorBlock settings 
It's no leap of faith. I would take the first and last points as given
and am happy to know the points in the middle.  I apologize again if
my tone was off (even more than usual).
I have trouble believing that, if they are indeed as tech-savvy as you
say, because I'm thinking that there is a virtually indefinite
supply of fruit that hangs pretty low for anyone willing to do the
things you outlined. I suppose this is the source of our divergence
on my "pointless" statement above.
But, if it is really working for you, and they haven't just learned to
slip below the radar, good for you (relatively speaking).
I don't think I had a narrow view of abuse. Rather a broad view of edit.
All of these imply something more than read access to public pages,
which is what I was trying to capture succinctly  by "edit".
Right. My point is that you set up however much resource you care to
for escrowed material. I don't mean to imply a single resource if it
is more convenient to set up a content escrow and any other escrows
for thing that can reached and affected through Tor. However much all
of that amounts to is up to you. Once the escrow repository(ies) is
full, you just drop any more input via Tor until you have had a chance
to look at it. Similarly for the available time of human editors.
Assuming it is anything above zero, it is useful. They will only put
in as much effort as they care to. Until they get around to it, no
input via Tor. There is no need to put "a lot of additional work". The
point is that abuse via this vector now comes to Wikipedia only at the
rate the community wants to take it. Abuse does not overload the
Wikipedia resources or community, it just prevents honest inputs via
Tor. _But_ because it _never_ succeeds in successful attacks on
Wikipedia, the incentive to persist doing it goes down over time
(assuming that preventing honest edits via Tor is not itself
Please let me know how what I said above is incompatible with
any new positions.
Umm. Been a while for me too, but I think you just described nymble,
or at least part of it. I also don't see how it would affect range
blocks because they're just orthogonal from each other.
You seem to be saying that you are actually managing against abuse
roughly OK with completely open unauthenticated access via various ad
hoc techniques such as range blocking, except for sophisticated
attackers who tend to be the ones that come via Tor and a couple
of other vectors that you know how to dry up. Is that correct?
That would be surprising but good to know.
2AM here. G'night,

@_date: 2009-04-19 10:59:10
@_author: Paul Syverson 
@_subject: exit counts by port number over 61 days 
Right, just to underscore the point, this was a central use case that
we had in mind when we invented onion routing and when we designed
Tor. Also, if I want to, e.g., ssh from a hotel room while traveling
or maybe even a local Starbucks somewhere into my system "back home"
but don't want to announce to any local observers who/where my remote
system is.  The nutshell way we've expressed this since '96 is to say
that the purpose of the network is not to provide anonymity per se but
to separate identification from routing. If you want to be anonymous
from the remote server, that is a related but separate technical
problem to solve from the fundamental service of an onion routing
network. HTH.

@_date: 2009-04-21 08:34:47
@_author: Paul Syverson 
@_subject: Information at exit node. 
To extend this last point, it is not just what the exit node can see,
but what it can do. Encrypted traffic is also typically
integrity-protected in various ways. The exit node can insert anything
into the traffic in either direction, and, if the traffic is
unencrypted, probably endetectably. this can give you all sorts of
bogus data, or, as Scott noted, insertions of malicious code---either
to identify you or for other purposes.

@_date: 2009-12-30 16:18:25
@_author: Paul Syverson 
@_subject: Why governments fund TOR? 
I'm not speaking for any government, including my employer or my
funders, but I can say something about why we, the inventors of onion
routing and designers of Tor, did what we did. We were as explicit as
possible as to what we intended and why with funders, management and
others. Presumably some of it was agreeable since we received support.
The above is largely correct, so I am only clarifying where I thought
there was room for misinterpretation.  The primary purpose for which
we proposed and designed onion routing networks (including Tor, which
started life in some of my NRL onion routing projects) was to separate
identification from routing, as we note in the first onion routing
publication "Hiding Routing Information" in 1996 and at
  Jim's speculation on the above cited motivation
was not something we ran across through experience but rather a design
motivation from the very beginning. We argued fifteen years ago that
to protect private traffic when going to and from a public network you
needed to carry traffic for others not just yourself, which meant that
they had to trust the network, which meant that you had to diffuse
trust by letting others run part of the infrastructure and that you
had to let them see the code. I think this is essentially stated in
our early onion routing publications. This was also part of the reason
we sought and received our first publication release for public
distribution of onion routing code in 1996. We were open source before
that phrase was in general use. My comments apply only to the funding
I received and the motivations we had. Other later goals of, e.g.,
censorship resistance and other funding of Tor I have not been part of
and should let others comment.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-02-13 10:35:37
@_author: Paul Syverson 
@_subject: use of SCTP in volunteer relay networks (a bit off-topic) 
I can't swear who first thought about it. It was a conscious decision
to use TCP for both Tor and earlier versions of onion routing (from
NRL, ZKS Freedom chose UDP over TCP) vs. unreliable transport.  You
can see discussion of some of the issues we were considering several
years ago when discussing moving Tor to UDP in
There is a recent master's thesis from Joel Reardon at Waterloo
and a doctoral dissertation from Camilo Viecco at Indiana that each
discuss transport issues for Tor.

@_date: 2009-02-17 15:39:58
@_author: Paul Syverson 
@_subject: another reason to keep ExcludeNodes 
I'm not commenting on the specific relative merits of continuing to
support ExcludeNodes, but I do want to strongly reject the principle
of always giving more choice and power to the users.  Whatever its
merits in general, this is a dangerous principle for anonymity
systems. It is easy to allow users to configure their systems in ways
that allow an adversary to uniquely identify them (or at least
dangerously narrow it down). How this can occur is subtle, and it
sometimes surprises the experts. The user (even a fairly savvy user)
has even less chance of grasping what is a safe configuration. For
this reason, we chose Tor design to minimize the number of
configuration choices even when we didn't have specific attacks in
mind. When we thought we had a countervening reason to allow options
we have done it hesitantly and with eyes as open as possible, rather
than doing it as part of a principle we enthusiastically embraced.
This point has been made in numerous published papers over the years,
including the Tor design paper from USENIX Security 2004.

@_date: 2009-02-09 22:36:04
@_author: Paul Syverson 
@_subject: Time Warner bad / VPS recommendations 
The main problem with this suggestion is that it is more likely to
decrease than increase anonymity.  Tor gets its security from the
difficulty that an adversary has being able to observe or infer both
ends of a connection between source and destination.  If you
redirected traffic from many Tor exit nodes to one proxy or a handful
of proxies then you are not doing extra anonymizing---just the

@_date: 2009-03-09 13:45:33
@_author: Paul Syverson 
@_subject: Clock problems 
Scott's analysis is no doubt correct. But just on the off chance
that it is something simpler. I have been having all kinds of
wierdnesses on OSX 10.4.11 earlier today. Some web searching
indicated that people had similar problems a few years back
with OS Xafter doing a security update a few days before a timechange.
I tried rebooting in case some parts of my system hadn't got the
word about the time change. This fixed some of them. Still
tracing down others, but thought since your problems seemed
to start at the right time, that you might look into it.

@_date: 2009-11-23 10:05:49
@_author: Paul Syverson 
@_subject: The Case for Banning Reduced Hop Count Implementations 
Thank you Lucky. I had been meaning to write something like your
Even if Lucky's basic points are eventually born out, you are right
that more analysis of latency and incentives would be valuable.  To
get an idea about incentive issues in anonymous communication in
general and Tor in particular you might want to look at "On the
Economics of Anonymity". Also "Anonymity Loves Company: Usability and
the Network Effect" both available from the Freehaven anonbib. Also,
"Deploying Low-Latency Anonymity: Design Challenges and Social
Factors" which is available from the onion-router.net publications
This has nothing to do with how long the connections are. Onion
routing going back even before Tor acknowledges that if the entry and
exit nodes are controlled/observed then an adversary will quickly and
trivially link them. The nutshell way we have said this is that onion
routing [including Tor] guards against traffic analysis not traffic
confirmation. This was acknowledged in the original Tor design paper
and was later born out by analysis ("Passive Attack Analysis for
Connection-Based Anonymity Systems") experiments on the live Tor
network ("Locating Hidden Services") and in simulation on PlanetLab
("Low-Resource Routing Attacks Against Tor"). These confirmed that
correlation was fast and easy. "Sampled Traffic Analysis by
Internet-Exchange-Level Adversaries" showed that it could also be done
sampling a tiny fraction of the traffic passing through an IX.  This
is also why onion routing's security is said to be roughly c^2/n^2,
where c is the number of compromised nodes in the network and n is the
total number of nodes. (Yes that is a little too quick, and you can
raise questions. See "Towards an Analysis of Onion Routing Security",
"A Model of Onion Routing with Provable Anonymity", and "Probabilistic
Analysis of Onion Routing in a Black-box Model" for details.)
The "Low-Resource" paper is especially telling wrt your point: the
attacks were done during connection setup _before a single data cell
was transmitted_ (and with vanishingly few false-positives). You just
don't need to have a long-lived connection to fall victim to this.  So
why bother with multiple hops? One part of the answer is already given
above: it reduces the threat quadratically. But why three hops instead
of just two? This comes back to Lucky's other point that you skipped
over. And this one is not subtle at all. Three hops is the minimum to guarantee that all an exit node knows is
that a circuit came from someone using Tor. The exit cannot say even where
in the Tor network the circuit started. Similarly, all an entry node
knows is that the circuit is headed somewhere. (Yes, this too is
actually more subtle; cf. "How Much Anonymity does Network Latency
Leak?" But, a priori, given ordinary log information, it is correct.
(Of course honest Tor nodes do not do any such logging.))
So, reducing the number of hops means that exit nodes have
significantly more information about connection origins. Reducing hops
to one means that they know everything about the origin of a
connection (up to the IP address from which the connection entered the
Tor network, which is all that Tor is designed to hide.)  That makes
their deniability of what they know about traffic exiting through them
no longer plausible (because, well now it will be false). That any of
the connections going through the network are single hop thus
increases incentives to attack any exit node, also any entrance
node---which basically means all the public nodes. Details would
depend on likelyhood that a given circuit is one hop and on the
incentives, legal considerations, resources, etc. of the
adversary. But absent such details, it would be unwise to allow such a
fundamental threat to the infrastructure itself.
As Lucky observed, this is a threat to the public Tor network itself
and should be treated as such. There are other drawbacks that could be
noted, but that is the central one.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-25 13:08:11
@_author: Paul Syverson 
@_subject: AN idea of non-public exit-nodes 
Two words: Hidden service
Some more words: If you set up a hidden service to function as a Tor
exit, then your above concern about defeating the point of Tor goes
away. I haven't done any thorough analysis but it seems obvious that
there are lots of ways to attack this, such as quoted from Roger
above. As usual you would need to specify what your threat model is to
know if this is adequate for intended purposes.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-13 12:25:45
@_author: Paul Syverson 
@_subject: all traffic through a VPN on top of tor, done! 
Ignoring what the underlying network can observe, the value to having
three hops is that the first and last ones don't know about each other
directly (so immediately know who to attack to completely deanonymize
a connection; they instead need to iterate such an attack). But if you
enter and leave the network via nodes you control, the only thing you
are getting from adding a "public" hop in the middle is a greater
chance of an adversary observing you. The problem with your design is
that if anyone discovers the nodes are under your control, then things
emerging from/entering them will be suspected of being associated with
you. (It was similar considerations that led us to recommend even in
the onion routing designs that predated Tor that the network not just
be run by/for the DoD.) Worse still, if you add just a middle hop that
is not yours, you make things worse, not better. Any time it is you
going to a destination observed by your adversary and via a middle hop
owned by the adversary, he will be right in guessing the connection is
more likely to be yours than are arbitrary connections through the
network. He will get this without needing to see your entry connection
into the network.
These are tricky questions, and we are doing ongoing research about it
now. An initial result we have is not quite to answer this question
but instead to look at how you should do routing to avoid compromised
entry and exit nodes if you trust some nodes more than others and
where the difference in trust and percentage of trusted and untrusted
nodes are input parameters. Published in the IEEE Computer Security
Foundations Symposium, cf.
I think I will have a better, but not complete answer, to questions
closer to yours within several months. But it will involve some
complicated analysis. For now, I suggest you follow Andrew's
advice---or just take your risk if speed matters more than security
for you. But know then that you are entering uncharted and especially
ill-understood waters and that any guesses you might have for X (or
even that this is the right question) are likely to be wrong, and you
really will have no idea what kind of protection you are getting.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-11-17 10:14:27
@_author: Paul Syverson 
@_subject: all traffic through a VPN on top of tor, done! 
The more careful analysis still to be done will hopefully say
something more about how difficult such correlation is and whether
things like locale make a difference. (For a related but distinct
example, see my recent paper with Matt Edman "AS-awareness in Tor Path
Selection", available at  )
But two related immediate concerns: Irrespective of network analysis
and usage finding relations among/with these relays, this all depends
on your ability to keep hidden exogenous information about those nodes
being related. (I'm talking about the sorts of management things you
just mentioned.) How hard that is probably depends both on how careful
you are (and how you are careful) and who you are trying to hide from.
Relatedly, you may face issues of what makes a good torizen since you
will not have disclosed your ability (or the ability of those who can
coerce/corrupt you) to de-anonymize by yourself their circuits that
start and end with your relays.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2009-09-17 16:34:31
@_author: Paul Syverson 
@_subject: good troll, intelligence psyops, or the genuine article? you 
As I've said for a decade and a half, onion routing guards against
traffic analysis, not traffic confirmation. If your adversary has
already identified suspect endpoints to a communication, then they are
trivially confirmed. There's other subtleties, e.g., website
fingerprinting, latency attacks, etc.  but if someone is talking about
a vulnerability to traffic confirmation level attack, then this is
something explicitly acknowledged about Tor since the beginning of the
design (and before).
I must confess that I only glanced through and didn't follow much of
what was said (or not said ;>).  But whether it's crap or not, I do
think this thread has mostly strayed pretty offtopic and request that
it be voluntarily dropped before it must be involuntarily dropped.

@_date: 2009-09-23 10:59:03
@_author: Paul Syverson 
@_subject: Random chaff [was: more work for Grobbages] 
Yes. But packet counting can also play a role. Cf, "Passive Attack Analysis for Connection-Based Anonymity Systems"
at It's not. Cf. my "Locating Hidden Servers"
wherein we had zero false positives on any timing attacks conducted
in finding hidden services, which generally was very quick.
(That such attacks existed were known for years. That they were not
just possible but so fast and effective using merely a single
node in the network was the reason that guard nodes were introduced
into the Tor network.)
And building on that see, "Low-Resource Routing Attacks Against Tor"
where timing attacks with epsilon false positives
were based simply on circuit setup and were shown on general
Tor circuits, not just for hidden services.
There's been a lot of research on this. I think Nick pointed at
some. Cf. the anonbib.
Research against timing attacks continues. (I'm doing some myself.)
But so far, any "chaff" strategy in the literature is both too
expensive and not at all effective against active attacks on
general low-latency systems for wide use, such as Tor.

@_date: 2009-09-23 11:29:36
@_author: Paul Syverson 
@_subject: Random chaff [was: more work for Grobbages] 
You're trying to turn it into a mix network. The order uncertainty
doesn't matter at this level of latency. The Bauer et al. research I
mentioned showed how to do timing attacks based just on setting
up the circuit. You don't even need to send any data.
Whatever solution (if one even exists) is out there, most of
the straightforward ideas and many of the not so straightforward
ideas have already been extensively researched. Cf. the papers
Nick and I mentioned before and others in the Freehaven anonbib.

@_date: 2009-09-24 12:56:35
@_author: Paul Syverson 
@_subject: Random chaff [was: more work for Grobbages] 
I did, but I don't get the sigh.
I was trying to succinctly say that this is a component of a different
system architecture with different assumptions. In the second
generation onion routing system we developed, i.e., the one before
Tor, we actually included mixing for experimental purposes.  The
lessons so far has been that it isn't worth it and we did not bother
to put that in Tor. That could change, but so far there are no
positive indications from the research.
Yes of course. You say that like it's trivial (to design, implement, etc.)
rather than huge.
Plus, keeping the existing network nodes synched even just to the point that
things don't actually break has not been 100 percent successful, and
this would imply much tighter synchronization not just across the
nodes but across all the clients as well. And the synchronization is
not just to keep things running but now becomes security-critical.  Wah!
More importantly, it is trivial to beat this with an active attack.
Just delay circuit setup packets slightly and watch for the pattern
at the other end. Or if the circuit is established, stomp some bits
at one end and see if the other end has junk come out shortly thereafter.
I'm not saying it's forever hopeless. The things I've mentioned and
more have been considered and people have design and evaluated
countermeasures to them and continue to do so. As Nick said, the
problem isn't that padding doesn't work. It's that it doesn't work
nearly well enough (at least so far).
Ermm. The stuff that Lasse and I did _was_ on the deployed Tor
network. Now that is not today's network. The network then was
much smaller, it didn't have guard nodes, etc. Testing in the wild in general is very tricky because Tor _is_ an
operational network, and you don't want to do anything that would
inadvertently create problems. This is also an ongoing research
challenge. We would like to understand and improve performance by
gathering data but without doing anything to increase risk to users or
operators. Karsten and others have been working on that.
Yes. There might be. But you would first have to justify the overhead
cost to the network by giving at least some reasonable argument that
it might work reasonably well, at least better than anything that's
been considered to date.  "Hey we don't know this won't work unless we
try," is not an adequate justification. Vetting ideas through the
research community seems like a reasonable first step. You would also
have to adequately analyze the impact on client and relay performance
and security before deploying. Again, nobody's discouraging research
into these questions. They just want answers before deploying. So far
none of the research has been giving encouraging answers.
Sorry. I thought that was standard. It means 'Cf.' means _confer_, i.e.,
see here.  Woops, "i.e." stands for  'id est' which means _that is_.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-17 17:29:55
@_author: Paul Syverson 
@_subject: Tor Project 2008 Tax Return Now Online 
Since you're (rightly) not worried about stepping on some toes, I'll
do likewise: You have given at least part of a response about FSF
since from both a code and a community/culture standpoint there
probably would not be a linux without them. And firefox serves as a
platform (from testing through to deployment) for lots of security
ideas that would not be where they are without Mozilla. Also, it is
much easier for IBM and Oracle to understand the RoI in linux than for
such players, qua corporate position, to see the RoI in Tor, but even
that still occurred over years of footdragging, hedging bets, etc.
I have been evangelizing versions of onion routing including Tor to
VCs etc. since before we started calling this version Tor. My
experience is that if they want to put serious money in (or sometimes
not even), they want to be able to generate revenue from that in a
short period of time, perhaps a few years. Similarly for other sources
of funding even if they aren't expecting direct immediate financial
return but are not paying for prototypes, research, and improvements
to what's there now; although the story changes somewhat in each
case. They may not be looking for financial return, but they have
unrealistic expectations about what would happen if they abruptly
threw lots of money at someone or added fifty percent to the
infrastructure at once. When you describe ways that things could
improve with an investment in the 50K USD to a million range, they
become less interested. They actually seem to prefer to hear promises
to roll out whatever random stuff from someone who would be happy to
get an instant ginormous influx of cash or adopt their plans to put a
thousand new nodes up from their corporate network.
There are little ways in which big entities are funding Tor or working
on collaborating. And it would be good for that to expand and
improve. Also, if someone (or better, several distinct someones) were
to double Tor's funding over two years, that would be great, but my
guess is that if anyone were to throw a tenfold jump in funding at the
Tor Project, Inc. all at once right now, the result would be
profoundly disappointing and frustrating for both the Tor Project and
the funder. And probably damaging to the Tor Project in the long
run. Similar comments if some one entity wanted to contribute, e.g.,
300 fast nodes to the network at once (except add that there would be a
swift major reduction in security).  I am pleased that when I
brought Roger on he had the right balance of interests between making
good stuff (and documenting that), deploying it, and selling it. And
this balance has continued with choices that he and now Andrew and
others have made. Tor _has_ a development director (Hi Karen), and
while more (and more diverse) development and funding are important, I
don't share your view that they are way off of where they should
be. And, just to be clear, the opinions I have just given are my
own. I'm not speaking for the Tor Project or anyone else but me. My
nutshell statement in that regard is I'm not from the Tor Project, the
Tor Project is from me.
Please continue to speak up. And, I'm sure you've looked at
 for starting ideas for
things to work on, and then discussing your prospective ideas with
the right people, e.g., on IRC.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-24 16:18:18
@_author: Paul Syverson 
@_subject: [Bulk] Re: The team of PayPal is a band of pigs and cads! 
Perhaps, but you are revealing to an eavesdropper anywhere between you
and the Paypal server that you, or at least someone at your IP
address, has a Paypal account, is logged into Paypal right now, etc.
Maybe for Paypal you personally don't care, but some no doubt will.  A
related but perhaps clearer threat: if you have an account at some
small credit union where you used to live/work, logging in from
wherever you now live might be identifying of you to an eavesdropper,
could open you up to more plausible spear phishing, targetted DNS
redirection, etc. There are many scenarios one could suggest. Some will apply to
you. Some won't.  Maybe you don't think it is anyone's business at
Paypal whether you are at home right now as you log in or not.  Or
maybe you are unhappy that someone who gets ahold of your account info
can now spoof you more convincingly because they also know your usual
IP address. If you are sure you have thought of all the current and
future threats for your particular usage and have decided that Tor is
unnecessary for your expected risk in that scenario, you could skip
it. To some extent you make those guestimates every time you step out
the door, or heck, into the bathtub. Tor is just another mechanism to
protect you in an uncertain world. But Tor was created to separate
identification from routing, and authenticated connections over
anonymous pipes was one of the intended uses.
Some examples of where that applies are at
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-28 11:20:41
@_author: Paul Syverson 
@_subject: Tor seems to have a huge security risk--please prove me wrong! 
Hi Hikki,
What you describe is known in the literature as website fingerprinting
attacks, and there have been several research papers published about
them. Consult freehaven.net/anonbib or type "website fingerprinting"
in your favorite search engine. I think the most recent paper on this
is "Website fingerprinting: attacking popular privacy enhancing
technologies with the multinomial na??ve-bayes classifie" by Herman
et al.  at the 2009 ACM CCSW (Cloud Computing Security Workshop). It
will cite much of the relevant previous literature.
Roughly, while Tor is not invulnerable to such an attack, it fairs
pretty well, much better than other systems that this and earlier
papers examined mostly because the uniform size cells that Tor moves
all data with adds lots of noise.
The ability to identify destinations without seeing the destination
end of a connection (even with pretty low probability of typical
success) remains worthy of continued examination and analysis.
But end-to-end correlation remains the most significant
fact-of-life for all practical low-latency systems, including Tor.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-28 23:02:04
@_author: Paul Syverson 
@_subject: Tor seems to have a huge security risk--please prove me wrong! 
OK I'm confused. Sorry for being terse initially but I just wanted to
get out that website fingerprinting is a known problem not a new
surprise. But it sounds like you think you are contrasting with what I
said rather than extending the same points. I said Tor is not
invulnerable to the attack, only that the published research (I wasn't
talking about the abandoned projects) shows it's a lot less vulnerable
than other deployed systems examined in that research, like jondonym
or various VPNs.  Yes, of course that's subject to the experiments and
assumptions conducted so far. I also said that it's worthy of
continued examination and analysis even if it is not the demonstrated
problem for Tor that end-to-end correlation is.  Since it's a pretty
open research area, we cannot say some significant attack isn't around
the corner. That's always the case.  All we know yet is that the few
published results there are show a small fraction of websites seem to
be uniquely identifiable via existing techniques. What am I missing?
Yes. Exploring defensive techniques would be good. Unlike correlation,
fingerprinting seems more likely to be amenable to traffic shaping;
although the study of this for countering correlation (as some of us
recently published at PETS ;>) may be an OK place to build on.
Personally I still think trust is going to play a bigger role as an
effective counter than general shaping, but one place we seem to be in
sync is that it all needs more study.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-29 14:01:11
@_author: Paul Syverson 
@_subject: Tor seems to have a huge security risk--please prove me wrong! 
I disagree. Most of what you say about base rates etc. is valid and
should be taken into account, but that is not the only thing that is
going on. First, you have just stated one reason that correlation
should be easier than fingerprinting but then tried to claim it as
some sort of methodological flaw. Truncating the lower 8 bits does
have a significant impact on fingerprinting but little impact on
correlation because of the windows and datasets, just like you said.
But way more importantly, fingerprinting is inherently a passive
attack. You are sifting through a pile of known fingerprints looking
for matches and that's all you can do as an attacker. But its easy to
induce any timing signature you want during a correlation attack. (It
seems to be completely unnecessary because of point 1, but it would be
trivial to add that if you wanted to.) Tor's current design has no
mechanism to counter active correlation. Proposed techniques, such as
in the recent paper by Aaron, Joan, and me, are clearly too expensive
and iffy at this stoge of research. This is totally different for
fingerprinting. One could have an active attack similar to
fingerprinting in which one tries to alter a fingerprint to make it
more unique and then look for that fingerprint.  I don't want to get
into a terminological quibble, but that is not what I mean by
fingerprinting and would want to call it something else or start
calling fingerprinting 'passive fingerprinting', something like that.
Then there is the whole question of how effective this would be,
plus a lot more details to say what "this" is, but anyway I think
we have good reason to treat fingerprinting and correlation as different
but related problems unless we want to say something trivial like
"They are both just instances of pattern recognition."
We don't have the luxury of chemistry or even behavioral stuff like
population biology of some species of fish to just hand out full
traces. There's this pesky little thing user privacy that creates a
tension we have that those fields don't. We could also argue more
about the nature of research and publication criteria, but I suspect
that we will quickly get way off topic in such a discussion, indeed
have already started.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-08-30 08:52:59
@_author: Paul Syverson 
@_subject: Tor seems to have a huge security risk--please prove me wrong! 
But there might be significant value to solving just passive
fingerprinting relative to cost whereas the value of solving just
passive correlation seems really tiny if it leaves active correlation
untouched. More below.
I don't share your belief about global external passive adversaries on
the current Tor network. I do find it plausible that there could be
(but no idea if there actually are) widespread adversaries (internal
and/or external) capable of attacking double-digit percentages of the
network; however I don't think they would be anything approaching
global. But we can agree to disagree on our speculations here.  Your
paranoia may vary.
My main concern is that your characterization implies a false
dichotomy by assuming an adversary's capabilities are uniform wherever
he may exist, either active everywhere or passive everywhere (and
let's ignore for now that each of 'active' and 'passive' cover a
variety of attackers).
This is central to the distinction between fingerprinting and
correlation. An adversary who is interested in what you are looking at
via Tor only has to be active at your location (your network
connection to Tor, your guards, whatever) and passive everywhere else
(global for you, many places but somewhat less than global for me) to
defeat any measure that only works against passive adversaries. (I
think we agree that all the published research indicates this is
currently unnecessary---passive attacks are enough to indentify Tor
connections at present.) My point here is one that I have not changed
for a decade, viz: adding countermeasures against a passive adversary
can be trivially defeated by a minimally active adversary. And by
'minimally' I mean both the nature of the attack (just playing a bit
with the timing of traffic. Published research already indicates that
one can do so in a way that creates an undetectable signal) and the
distribution of the attack (the adversary only has to be active at one
Looking at the destination side, an adversary wondering who is
visiting a particular web server need only be active at the
destination end. He can be passive anywhere else. But such a minimally
active adversary cannot attack connection sources via destination
fingerprinting.  (This is not to deny the usefulness of continuing to
research fingerprints of existing destinations that are common or
particularly interesting, nor the usefulness of researching actively
making a destination fingerprint more unique to facilitate attack---the
active fingerprinting that we have already agreed is not what either
of us meant by "fingerprinting" in this discussion.)
Have you just asked for it? Other than for privacy purposes or because
of some intellectual property or other release headaches I don't think
this should be a problem.
I agree. I meant arguing in terms of value generalizations about the
broad areas of study and their research/publication standards rather
than talking about what we need from research to solve our problems
and what Tor needs to do so that researchers will want to work on
them. In that sense Tor has had a very good track record to date.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-02-15 11:27:56
@_author: Paul Syverson 
@_subject: Path-spec - fast circuits 
This is a good point, but it's hard to gauge both the urgency and the
significance of a threat you discover. In the case of one that had
been shown to work on the live Tor network and that was easy to do, it
seemed clear that some remediation was needed quickly.  Note that the
Bauer et al. simulation was a year after Lasse Overlier and I
demonstrated this attack on the live Tor network (not simulated) using
just a single corrupt Tor node that lied about its bandwidth to find
hidden services, as we described in "Locating Hidden Services" in
2006. Of course we structured things so that we would only attack
ourselves without affecting others. See the paper for details.  This
prompted a couple of changes. One was the capping of allowed claimed
bandwidth, another was entry guards (which Bauer et al. showed could
also be used to create attacks without the caps).
Your comments on and suggestions for (and even complaints about ;>)
how to measure the network and how to use that information in routing
are a welcome part of the picture, but this remains a complicated
balancing act that we continue to refine as best we can, including
in that the considerations that you have raised.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-02-24 10:09:58
@_author: Paul Syverson 
@_subject: TorChat is a security hazard 
Ermm, sort of. It depends whether you want to be anonymous from the
far end or just from (some part of) the communications infrastructure
between you and it.  The nutshell way we've been saying this since '96
is that onion routing is not to make you anonymous from the far end
but to separate indentification from routing. If you do that, then you
can choose whether to authenticate to (e.g., ssh to a trusted location
over Tor) or be anonymous to (e.g. sensitive queries to a search
engine) the responder. Or, as is being discussed, whether you are
communicating with the same persistent pseudonym as previously.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-01-06 09:11:36
@_author: Paul Syverson 
@_subject: Speaking of cryptography 
As a start on that research: we published some advantages of an
MQV-like protocol in "Improving Efficiency and Simplicity of Tor
circuit establishment and hidden services"
Though we mention reasons to be hopeful about its security
we have not done an actual security proof yet (which I'll get to in
my copious free time), without which it is of course not to be
recommended for use in deployed Tor or perhaps even for more detailed
design exploration than we have already done.
Paul To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-01-08 15:57:55
@_author: Paul Syverson 
@_subject: Anonymity risks of 2 vs 3 hops 
While I think that the cost/benefit tradeoff comes down strongly in
favor of three hops, I'm sorry to say that I don't think this argument
is correct.
End-to-end correlation attacks are trivial. Examples of how to do them
were given in the first onion routing paper in 1996, and this has been
shown in experimentation and simulation numerous times in numerous
ways.  The nutshell way we have always said this is that onion routing
prevents traffic analysis not traffic confirmation.  People seem to
keep missing this point and think coming up with yet another way that an
adversary at distinct points in a Tor circuit can recognize that
circuit when encountered in both places is worth publishing. It
isn't. The last worthwhile contribution to this general area was in
2007 when Bauer et al. showed that correlation could be done with very
high confidence and virtually no false positives using just circuit
initialization, i.e., before any application traffic is even sent.
The point is that an adversary who can watch your circuits enter the
network and kill them when he does not see them exiting the network
does not need to have entry and exit directly connected to do it.
(Cf. also "Synchronous Batching: From Cascades to Free Routes" and
"Denial of Service or Denial of Security? How Attacks on Reliability
can Compromise Anonymity" for related points and analyses of how
effective such attacks are on various anonymity networks---not just
Tor. Both are available at the FreeHaven Anonymity Bibliography.)  The
extra effort to recognize a circuit (or recognize that ones nodes have
failed to do so and kill it) is real but not significant enough to
matter in practice. Also, for a tiny toy example such DoS to
DoAnonymity is significant. Even with the 800 node Tor network that
Borisov et al looked at in 2007 in the above paper, this attack only
started to matter when more than twenty percent of the nodes were
To answer the original question:
There are many arguments for and against a three-hop minimum.
I think the most compelling arguments in its favor are
(1) If an exit node sees a circuit of interest it now knows one
of the entry guards for that client. It thus is worth trying to attack
or eavesdrop on that entry guard now to try to see who its clients
are either to see the list or if, e.g., the adversary is a destination
to catch and identify the IP addresses of clients that use that
entry guard and visit that destination again.
(2) Because of (1) and also just because he does know the entry
nodes of circuits, an exit's claim to know nothing useful about the
origin of circuits exiting through it goes away. Thus even if an
attack based on (1) does not succeed it could still be an attack/problem
for the exit node. As exit nodes are more scarce, this amounts to
an attack on the Tor network itself. (I made this second point,
but not the first in a post to this list on Nov. 23. Cf. the discussion
under the thread titled
"The Case for Banning Reduced Hop Count Implementations"
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-13 07:01:36
@_author: Paul Syverson 
@_subject: Torbutton Documentation - Adversary Capabilities. - fork: 
Tor doesn't do any batching or delaying.  This is just another way you
could be identified by timing attacks. Tor provides no resistance to
timing attacks, and so far there are no countermeasures that have
been identified as working against a passive, much less active, adversary
without imposing unacceptably high overhead or limitations. Most have
these limitations and still don't work.
See the blog post
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-13 13:38:32
@_author: Paul Syverson 
@_subject: Torbutton Documentation - Adversary Capabilities. - fork: 
Even assuming your description of the evolution of Tor network
communication processing is correct, I don't understand what increase
in network speed (throughput?) or bandwidth have to do with making it
more feasible to protect against timing attacks. Faster networks
should just make timing attacks more effective, and we know that we
were already unable to do anything useful when such attacks were less
People should continue to work on this hard research problem.  (I
myself have a paper on it to be presented in the Privacy Enhancing
Technologies Symposium next week, "Preventing Active Timing Attacks in
Low-Latency Anonymous Communication ".) But as the blog post I pointed
at noted, nobody has yet made a suggestion that clearly improves the
situation (even in theory) and would clearly be feasible and practical
to deploy on the Tor network as it stands.
And just as there is no such thing as a secure system---only systems
secure against a given adversary conducting a given class of attack
provided that the implementation, deployment and environment satisfy
certain assumptions, so to there is no such thing as an anonymous
system. In that sense, the answer is no, "anonymous" should not mean
anonymous, or rather it depends what _you_ mean by anonymous and a
whole bunch of other things that must be stated.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-07-14 20:31:04
@_author: Paul Syverson 
@_subject: Torbutton Documentation - Adversary Capabilities. - fork: 
This is not attitude; it's an explanation of science. It's how
'secure' is understood by anyone that I know who works on security
analysis and design from those who write the textbooks on computer
security to those who hack, from those who try to secure major defense
command and control systems to those who try to make secure web
browsers to protect consumers against phishing.
To protect communication. And Tor does that pretty much better than
anything else available. (The professional philosopher in me feels
obligated to acknowledge that there are adversaries and contexts for
which other systems are more secure, but I believe that they are less
secure in ways that are significant and lack pathways to change that,
unlike Tor.)  And lots of us are working as hard as we can to make it
better still.
Tor is on the road to being more secure now. I have tried to point you
to the research in the area of timing attacks that is being done. I
reiterate that there is no evidence to date that the sorts of things
you are proposing actually work. That is not to say we shouldn't keep
looking at timing attack resistance as a research question. But it is
still very unproven research. I think I have taken this as far as
anyone, and it's still a long way from practical. But having worked on
it and many other aspects for a long time, there are many things that
can be and are being done---also research that will lead to
improvements IMO long before timing-attack countermeasures ever
produce anything but much larger overhead, much smaller anonymity sets
and thus worse anonymity (less entropy if you prefer). Why are you so
focused on timing attacks?  There's plenty of positive changes to work
on where the expected payoff is better.
To make better Tor (amongst other things).
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-03-22 10:31:15
@_author: Paul Syverson 
@_subject: Anti-Virus software for windows server 
More to the point, this is an apples and oranges comparison.
We would like everyone who runs a Tor server to have as secure
a machine as possible. And, for at least those running on some
version of Windows, antivirus software is a significant part of
good security practice.
Voting machines are special purpose: running in a restricted
environment with restricted interfaces to do a restricted set of
operations. And anything they _can_ do should have had a level of
formal verification and testing way beyond what could be applied to
generically available OSes and configurations that most volunteer Tor
operators should be expected to provide.
The point of the XKCD comic is that if a voting machine is being
operated as intended, and that involves using antivirus software at
all, then it looks to be a fundamental failure of the development or
the manufacturer's recommended usage. Tor servers should be as secure as they can, but even if that is very
secure, they are run in a much more hostile environment (the internet)
and must be much more accessible than voting machines. Also, they are
run by volunteers on systems that we cannot expect will always have
had the level of careful scrutiny or restriction to minimally
necessary functions before they became Tor servers that we should
reasonably assume our voting systems have had throughout their
lifecycle. In particular, many of them are run on the sort of systems
for which antivirus makes sense. We could restrict to just those servers run by vetted operators and
running on a properly stripped down version of say SE Linux deployed
in a verifed configuration that is fully inspected by authorized
personnel. The trouble is that a ten node onion routing network
doesn't actually provide much anonymity protection.
(P.S. To be fair to the voting systems, they have there own harsh
limitations.  E.g., they spend much of their lives locked in a utility
closet or wherever there is space that every municipality can spare
with whatever security that municipality can muster. That would seem
to cry out for designing these systems so that they simply cannot be
susceptible to viruses of the sort that infect much of the internet,
which would actually be the easy part of making them secure. But
getting into the morass that is electronic voting is fortunately not
our problem on this list.  We have our own morasses.)
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-05-20 08:57:52
@_author: Paul Syverson 
@_subject: Family specifications (was: Re: perfect-privacy.com, Family 
Glad I read this thread through to the end. This was what I was
going to say, only not as well as Andrew.
It is possible however, to have some value to allowing some asymmetry,
viz: if Alice lists Bob_1, ..., Bob_1000 in her family, but no Bob
lists Alice, then a path selection that chooses both Alice and Bob_i
will be rejected, but one that lists Bob_i and Bob_j will be just fine.
This is not how MyFamily works now (or am I wrong ?).
But that could change. The only paths Alice could then
affect would be ones that choose her.
The point is not just that the attacks Andrew mentioned are no
longer possible. It is also true that someone who mananged to set
MyFamily on only some of his nodes would still cause those paths to
be avoided. S/he may or may not have covered the entire family by this
process, but s/he can cover the entire family by setting MyFamily
in half of them, perhaps a little less overhead.
Perhaps this is what various people were alluding to when they said
that there is no attack in letting one node set MyFamily and having it
only affect itself thereby? The above is not an unqualified
recommendation, however. Besides the fifty percent configuration
overhead savings for people with large families. I like that a
partially set family provides some of the intended function rather
than just failing completely, but (a) It's off the top of my head, (b)
There may be other more subtle attacks than the obvious ones Andrew
was mentioning, (c) The added complexity of it being OK to do
something less complete than setting this at all nodes may lead to
more people getting it wrong more often so that the graceful failure
is more than offset. (d) I haven't thought about implications for
complexity of path selection, distribution of directory info, etc.
They may render any benefit too expensive. All that said, it is perhaps worth at least considering.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-05-20 15:12:38
@_author: Paul Syverson 
@_subject: Family specifications (was: Re: perfect-privacy.com, Family 
Your interpretation of what Bruce said makes sense. But it is not
how I parsed, "BelongToFamily xyz" in his message. I read it the same
way it seems that Roger did, as giving a list: node x, node y, and
node z.  And then we're off and running. I think what Bruce/you
suggest is better than what I proposed to avoid the problems Roger and
Andrew noted. As I said before, it's not how MyFamily now works. And I
believe Andrew/Roger/me/others were addressing trying to use the
existing functionality in a different way, which was another
disconnect. Anyway, this is certainly an idea worth considering.
Now, should you ever say you are in multiple families at once?
And should there be a lattice structure for families, hmmm? ;>)
Thanks for clearing things up,
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-05-20 16:04:55
@_author: Paul Syverson 
@_subject: Family specifications (was: Re: perfect-privacy.com, Family 
Right. But I only saw the message in the context of Roger's reply
where he seems to have read it that way. To be generous, he was
reading an email proposing something associated with MyFamily, not
code. Also, in my experience 'xyz' (with or without spaces) is usually
used to mean a list of things rather than as a name variable like
'foo'.  To be even more generous, hey we all make mistakes and I've
probably explored this mistake's origins a fair amount farther than
interesting or productive. So stopping now.
I totally agree, albeit having not thought long or hard about
it. Again, partly I was simply trying to explain to myself and perhaps
others how we managed to misread the suggestion.
This and my other question were meant as jokes. Hence the emoticon.
(Some of us haven't slept recently and are a bit punchy.)
But yeah, many a theorem or system design started as jokes
while goofing around.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-05-28 08:25:09
@_author: Paul Syverson 
@_subject: HTTPS Everywhere Firefox addon 
But you could set this as a default for most people most of the time
with an option to disable it and use site-specific rules for those
who need that.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-11-19 08:11:30
@_author: Paul Syverson 
@_subject: Anonymity easily thwarted by flooding network with relays? 
Sorry. This doesn't work. Besides the tremendous overhead added to the
network of full-length padding to whatever perceived maximum across
all client or destination flows go in either direction on circuits
and/or the performance hit of adding delay to smooth flows that are
too high a data rate on a network already perceived as too slow by
many of its users, it is trivial for the entry node to induce a timing
signature whatever the client has done. And this signature will be
discernible by the exit node. Lots of work has gone into trying to do
something to deal with this question (my own last contribution in the
area is here  So far
all of it is merely theoretical. And I believe that will always be
true for the majority of use cases.
As hinted above, adds overhead that is not affordable on a usability
level, and is easily defeated by trivial active attacks.
Your reactions are good. It's just that many people have had the
same reactions so we've explored this, and nobody in all of the research
done has yet produced a viable version of what you suggest.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-11-19 10:33:38
@_author: Paul Syverson 
@_subject: Anonymity easily thwarted by flooding network with relays? 
No. It just increases unnecessary network overhead. If an adversary
owns some given fraction of the network, the probability that he owns
the first and last node does not change whether the path is longer or
shorter.  (If you mean that by having longer chains you decrease
thereby the probability for a given circuit that an adversary
occupying nodes in that circuit is occupying first and last positions,
that is true. But that is like reducing the likelyhood of an attack by
given adversary by using an algorithm that chooses ten nodes instead
of Tor's usual three for a circuit and then using three as normal and
telling the other seven of them to do nothing at all on the
circuit. The only difference is the latter has less network overhead.)
Better go have another espresso ;>)
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-11-19 11:14:06
@_author: Paul Syverson 
@_subject: Anonymity easily thwarted by flooding network with relays? 
Dare to be stupid. is a motto I've lived by for decades. (Just ask
anyone who knows me.) But I'm not kidding. I try to put in appropriate
effort learning background so as not to waste people's time, but I
also try to resist the fear of asking or suggesting something off the
cuff because it might be stupid. I'm smarter than smart people in this
respect because I know to surround myself with smarter people than me
to draw from when I make such suggestions, while they obviously are
dumber since they are relying on me.  This is hardly the worst
already-settled-that question raised on or-talk.  And don't worry
about the MIT affiliation. I've known too many to be impressed.
(Shades of Westley talking to Inigo Montoya.) OK, clearly distracting
myself from pressing matters. Back to it.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-11-07 07:54:44
@_author: Paul Syverson 
@_subject: You know you have arrived in yet another sense 
In the article linked to below in the Baltimore Sun, "Protect yourself
against Internet eavesdropping", there is interestingly no mention of
Tor as a network or as a privacy tool that could itself be used to
protect you, but
   If you use the Firefox browser, you could also install the "HTTPS
   Everywhere" extension developed by the Electronic Frontier
   Foundation and the Tor Project, dedicated to improving Web privacy.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-10-18 22:16:16
@_author: Paul Syverson 
@_subject: What about private & Public Keys 
What others have said about forward secrecy and the rotation of
different kinds of Tor keys is all valid and reasonable, but I wanted
to back up a bit and note that you are expressing concern about an
attack that requires more effort than other attacks an adversary as
stong as the one you describe is more likely to be able to mount.
Unless you choose a path for which he has broken all three onion keys
an adversary won't be able to decrypt your Tor traffic.  And because
of the forward secrecy others noted, he will only be able to do this
if he sits at or between the client and the first Tor node _while the
circuit is being built_ . But more importantly, if he has indeed
compromised a large number of Tor nodes, he is likely to be in a
position to observe your traffic entering and exiting the Tor
network. Then he does not have to read anything to break anonymity
(link source and destination); timing patterns will make that
trivial. And if he can watch the same traffic entering and leaving the
Tor network, only end-to-end application encryption will matter for
confidentiality of that traffic; whether or not any Tor keys are
broken or nodes in the path are compromised, he will know who you are
talking to and what is being said. (And if that weren't enough he can
compromise the integrity of the traffic in that case too.)
So yes, it is important to protect against the threats you noted.  And
with enough keys, an adversary in a single optimal position could
break anonymity (and confidentiality).  But Tor has those fairly well
covered. And an adversary compromising a large chunk of the network
can do more damage without needing to use the keys he thus
obtains. Fortunately, this requires a significant adversary to be
widely effective.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2010-10-03 12:12:05
@_author: Paul Syverson 
@_subject: beneficia versus maleficia 
You are right about the design options and this part of their
motivations.  These considerations and incentives have been a factor
in Tor's design since the beginning, Cf.  "Onion Routing Access
Configurations" in DISCEX 2000 or "Deploying Low-Latency Anonymity:
Design Challenges and Social Factors", in IEEE Security & Privacy
Magazine 2007.  Both can be found at
As you noted, incentives and thus opportunities to participate
continue to evolve; more recently Tor has provided the option of being
a bridge to the configurations in which one can participate and
support Tor.  This offers still other tradeoffs to those mentioned in
the above papers (less bandwidth or public visibility, supporting
specifically users needing to circumvent blocking of the Tor
Well that's the goal anyway. Those who cannot participate in any of
the above ways but want to help can provide financial support,
positive publicity, etc.---if any of those are intelligent choices
consistent with their beliefs.
To unsubscribe, send an e-mail to majordomo at torproject.org with
unsubscribe or-talk    in the body.

@_date: 2011-04-01 11:14:25
@_author: Paul Syverson 
@_subject: [tor-talk] Another security program 
Not quite. We created onion routing to separate identification from
routing. Whether you want to be anonymous from the server you
connect to or (mutually) authenticated with it is an important though
distinct issue from Tor's central function.
You might want to authenticate to your bank (you don't want someone
else able to access your accounts!) but hide that you are connecting
to it from anyone observing the connection. Maybe you don't want
people to know where you have a bank account to reduce chances of
identity theft or similar problems. Maybe you don't want someone
watching your internet connection to know that you are logging in to
your bank at that time, etc. Whether the bank has your "real name"
associated with the account or a pseudonym has other choices and
motivations (and potential legal implications depending on
jurisdiction and whether by "real name" you mean legal name in that
jurisdiction). For a significant fraction of their connections, most
Tor users will want to be anonymous to the server they're contacting
as well as any network observers. But this is not always the case.
Note: This is responsive to to the comment about anonymity being
broken if you identify yourself using your "real name" to a server you
access.  I don't have any comments about the original question. Sorry.

@_date: 2011-04-18 09:14:39
@_author: Paul Syverson 
@_subject: [tor-talk] To Toggle, or not to Toggle: The End of Torbutton 
The job of Tor has always been to separate identification from
routing. And that is how we've expressed it since the
beginning. Keeping this in mind makes it much easier than trying to
capture technical definitions for "anonyymity", which is pretty
complicated and subject of much research as well. When we do talk
about anonymity it helps to say that Tor anonymizes the communication
pipe, not the data that passes through it.
This isn't just to narrow the scope of the problem Tor addresses,
although that modularity is also a good thing. It is because we often
want to identify ourselves through the anonymous pipes Tor creates.
It is easy to forget that if you only think about looking at web sites
when you don't want the webserver to know it is you looking.  But, if
someone wants to login remotely to a system of hers, or to update her
blog without giving away where or who she is to anyone watching, then
she wants to both make sure that she is talking to the right system
(to make sure she isn't sending things to a receiver she doesn't want
getting those things) and that the system knows it's her (to prevent
any arbitrary person from doing what she's doing or pretending to
speak on her behalf). So she wants to identify and authenticate the
system, and she wants the system to identify and authenticate her.
Actually we've had the terminology and conceptual machinery for at
least fifteen years. What I said above I also said in talks and papers
in 1996.  For example, "Our goal here is not to provide anonymous
communication but, to place identification where it belongs, The use
of a public network should not automatically reveal the identities of
communicating parties" --quoted from "Hiding Routing Information"
Tor separates identification from routing so that your communication
gets where it needs to without identifying you---anonymizes the
communications pipe if you prefer. Torbutton toggles whether your
browser communicates through that pipe or not and helps filter and
manage some of what goes in or out of that pipe. This helps prevent you
from inadvertently sending identifying information through that pipe
and also helps prevent what comes through the pipe tricking your
computer into sending something in a way that bypasses the pipe and
its protections.

@_date: 2011-08-21 08:52:26
@_author: Paul Syverson 
@_subject: [tor-talk] de-anonymization by correlating circuit changes 
You mentioned having traffic logs from all possible servers.
If by "servers" you mean 'honestly and properly run Tor relays',
then those logs do not exist, so what you are saying is not possible.
But if the adversary is watching both ends of the connection, he will
know which user IP address is connected to which destination. This is
much easier than following the stream through the relays. "Watching
both ends" could be any of many things. Here are a few. He could be at
the user ISP and at the destination server. Or he could be at an AS or
IX between the user and the first Tor relay and also between the last
Tor relay and the web server the user is connecting to. Or he could
have compromised or simply own the Tor relay at both ends of the
circuit at the time the connection is made. If any of those are true,
he does not need to look at all the relays in the circuit. He can
easily correlated the traffic patterns at both ends to determine which
connections match up. In a 2007 paper, Bauer et al., showed that it
was not even necessary to send any traffic over the circuit to do the
correlation. It is enough to watch the circuit creation.
The too-terse way we have said this since about 1996 is that onion
routing protects against traffic analysis, not traffic confirmation.
The countermeasure you suggested is one of many that have been
investigated. State of the art is probably the following.
 But nothing that is both adequately practical and effective has been
discovered by any of the researchers who have investigated it, nor do
I think ever will be, at least for general purposes.  As Curious Kid
noted, Tor does not attempt to prevent this because there is no
practical way for it to do so.

@_date: 2011-12-21 19:40:12
@_author: Paul Syverson 
@_subject: [tor-talk] Automatic vulnerability scanning of Tor Network? 
Ah, perhaps they have read [1] and are trying to roll out such an
attack below the radar. Who _do_ they work for? ;>) On a less
facetious note, people might want to look at our trust work as a more
constructive response to the diversity of geolocations, jurisdictions,
OSes, operators, Tor versions, hardware etc. [2], although it is still
research and I do not pretend to have all the pieces to make this
fully practical without several more years of work.
[1] freehaven.net/anonbib/cache/ccs07-doa.pdf
[2]

@_date: 2011-03-10 22:05:09
@_author: Paul Syverson 
@_subject: [tor-talk] How does Tor REALLY work? 
This is definitely not my area of expertise for Tor (which is why I
didn't say anything earlier), but when you iniitially raised this
thread my kneejerk reaction was to suggest exactly this. Now we
can wait for Roger or Nick to say why it's not the good idea it seems.

@_date: 2011-03-21 00:46:30
@_author: Paul Syverson 
@_subject: [tor-talk] Iran cracks down on web dissident technology 
People seem to need a periodic refresher on this.
I will just state the long public and published facts.
Interpret them as you like. You can read more details at
but here's a quick summary:
I invented onion routing at NRL with David Goldschlag and Mike Reed in
1995-96 as a US Naval Research Laboratory project with initial funding
from ONR. All of us were NRL employees time. Our first deployed
system was in 1996 and source code for that system was distributed
later that year. (Code was entirely US government work by US
government employees, so not subject to copyright.)
As part of a later NRL project, I created the version of onion routing
that became known as Tor along with Roger Dingledine and Nick
Mathewson starting in 2002. I have been an NRL employee throughout all
this.  Roger and Nick were contractors working on my project. NRL
projects funded by ONR and DARPA were the only funding they had to
work on Tor until 2004. The first publicly deployed Tor network was in
2003, which was also when the source code was made available and
publicly licensed under the MIT license.  The first funding Roger and
Nick got to work on Tor that was other than as part of an NRL project
was from the EFF starting in 2004.
Tor got funding from a variety of sources after that, including several
U.S. government projects, both before and since becoming a US 501 (c)(3)
nonprofit. You can find a summary at

@_date: 2011-03-21 11:07:49
@_author: Paul Syverson 
@_subject: [tor-talk] Iran cracks down on web dissident technology 
This is a reasonable concern, but I think you are oversimplifying the
assurance and risk management available to those who are not tech
savvy. If they are just going to look at one or two poorly researched
articles in a
blog/credentialed-news-publication/whatever-medium-you-want that
confirm their expectations, well there's not much more you can do to
help them. Whether they trust you or not, their beliefs will not be
very well grounded.  But if they do have the interest and time (lucky
them), they don't have to be able to read the source code themselves
or pay someone (and why trust the guy you are paying to read it
anyway?, and how do you know that this is the code running on all of
the relays out there?, or the code you downloaded, and ...)
There are good answers to the latter of these for people who
are tech savvy, but how do you get trust those answers short of
a significant self-education? Here are just a few of many possible
The Tor source is available and people are encouraged to check it out,
but that's _not_ the whole story. Tor is also fairly well documented
(meaning that description of what the different parts of the source
code does is available) which encourages people to look at it more
than if it was just this pile of code goo to wade through.  And lots
of independent people _do_ look at the source code. One way you can
tell this is that they find mistakes, sometimes some fairly bad
ones. (Fortunately not too bad very often and generally fixed
quickly.) You can look at the posted history of the announced versions
 and see
acknowledgments of who found flaws and look them up. Lots of times
these are researchers at some reputed place. Lots of times these are
smart people with no credentials you would recognize. In either case
you could look them up and see who they are. Ask them their experience
reporting a flaw and getting it fixed and what their overall
impression of Tor is. You can do this even if you have no idea what
the flaw is that the release notes are saying they found or how the
Tor people fixed it.
There's also lots of academic researchers looking at Tor all the time
(somewhat overlapping the people looking at the source) and poking
holes in the design, the deployment etc. testing its strengths and
weaknesses, suggesting improvements, which often do get incorporated.
This is also all well documented and vetted by publication in
peer-reviewed scientific venues. It is also work done at reputed
institutions of higher learning in various countries, if you want
to base anything on that. You could contact the authors of these.
There are also people at places you've never heard of if you don't
trust people at big institutions.
If you don't know anyone you trust who is tech savvy, you could
contact your favorite computer science department by looking them up
on the web and ask around till you get directed to someone who knows
something about Tor and ask them.
Yes, maybe someone bogusly directed you to a simulated website of
Enormous State University with fake phone numbers in it, and whoever
you talk to there might inadvertently link you back to the Tor cabal
rather than getting some random professor or savvy student's opinion,
and maybe all those publication venues and researchers and
universities are in on it, and the supposedly independent researchers
who found code flaws were also in on it (or sock puppets created by
Roger to create credibility). But at some point you have to look at
the size, diversity, and entrenchment of the conspiracy you think is
there. At some point there is only so much we can do to reassure
you. (I'm talking about reassuring you that there is no
conspiracy. That the stuff is good is a related but independent
question that the above suggested checks should help with.)  If the
above or some of the many other things you might do to check into it
yourself without needing to understand the technology doesn't convince
you, then probably you have already decided what to believe and no
evidence is going to change that.
And yes there's always things to do to improve
transparency/trustability/usability/etc. People worth trusting
probably have a processes to do that and a relatively independent and
confirmable history of doing it.

@_date: 2011-03-21 15:39:52
@_author: Paul Syverson 
@_subject: [tor-talk] Iran cracks down on web dissident technology 
Last comments for a while. (All I have time for, sorry.)  I'm just
going to respond to specific issues about system threats and the
like. I will not join in the speculation about what governments do or
That's why I mentioned design, code, deployments, what the nodes
are running, etc. These are all subject to well documented scrutiny,
(and could of course use more and better scrutiny).
Yes. This was an acknowledged tradeoff in design that is well discussed
and documented. And many people have tried to better assess its
significance and what to do about it. This connects to usability,
incentives to running a relay, many things. See for example
the original Tor design paper, also
Anonymity Loves Company: Usability and the Network
Effect. I have done some work myself on understanding and trying to counter
endpoint eavesdroppers in both theory
"More Anonymous Onion Routing Through Trust"
and practice
"AS-awareness in Tor Path Selection"
Addressed above and in previous messages.
Also addressed above. Not claiming it's completely understood,
but I am claiming it is being publicly examined.
 Advanced govts aren't prone to

@_date: 2011-03-22 09:27:57
@_author: Paul Syverson 
@_subject: [tor-talk] Iran cracks down on web dissident technology 
I think you also did a nice job of finding the Tor relevance buried
therein. I'll respond to those parts where I think I might have
something to contribute.
Distributing trust is also not just the number and diversity of users
(and relay providers) but how they are related in intentions and other
things. When going up against The Man*, you can't just assume a
uniform distribution on relays, users, and network links between those
wrt likelyhood-of-being-run-by-a-hostile/resilience-to-attack/etc
Which means numbers and even diversity isn't the whole picture. I go
into more on this in "Why I'm not an Entropist". It is also the basis
of the trust-based routing we have been working on, which is basically
how do you route if you consider the possibility that significant
portions of the network might be under the view/control of your
adversary even if the network has 10000 relays.
And since I'm really going to try to resist responding any more to
this thread, Thanks Mike for your other message containing the stab at
a soundbite-sized and coherent expression of what I was trying to say
about how the non-tech-savvy could trust Tor with the best
justification to effort ratio.
Right. See above.
*My name for a nation-state/organized-crime/your-favorite-big-scary
adversary. Gratis to Nick for enthusiastically liking this name in a
partially related discussion on trust based routing models and thus
encouraging me to use it.

@_date: 2011-05-21 08:52:11
@_author: Paul Syverson 
@_subject: [tor-talk] Using passwords with TOR 
Tor is about separating identification from routing. Nobody can tell
where you are from or who you are just by seeing your Tor
connection. But you can use Tor to talk to someone you want to identify
and authenticate you, at least in the sense that you probably don't
want them letting someone else into your personal accounts.
Whether or not your password is safe depends on whether your
connection to the server you are logging into is properly protected
once it leaves the Tor network. If you were connecting directly to
your account using an unencrypted open wireless connection at a coffee
shop, anyone around you could get your password once it left your
computer. Similarly, Tor will protect your password through the Tor
network, but the last part between where it leaves the Tor network and
the server is protected by whatever protection you have for
that. (Often that protection is SSL encryption, arranged between your
Firefox and the server you are connecting to.)
See also Hope that helps,

@_date: 2011-09-27 09:49:47
@_author: Paul Syverson 
@_subject: [tor-talk] <<EntryGuards>> 
Guards are chosen automatically. You don't need to do anything but run
Tor. (I'm assuming you don't need to use a bridge to get to the Tor
network.  )
Relying on one is generally too unpredictable.  Your Tor software
randomly chooses a default set of three entry guards which are used
for each of your circuits. These are used as long as everything
remains basically stable and the same. Over time, some may be
gradually replaced for various reasons.

@_date: 2011-09-27 12:11:33
@_author: Paul Syverson 
@_subject: [tor-talk] <<EntryGuards>> 
Yes. When your Tor software picks three guards, it picks them
from the ones that are labeled as Guard. If you mean why are
some labeled and some not, there is an attempt to pick nodes
that are neither too flaky nor too slow to be relied on regularly
as guards. See more details at.

@_date: 2012-04-05 08:41:23
@_author: Paul Syverson 
@_subject: [tor-talk] access sites 
We created Tor to protect military communications. Much like other
things invented at NRL (e.g., the joystick controller for remote
control---patented in 1923! or GPS) it also has widespread civilian
use. For most of those, the civilian, business, or other government
use is icing on the cake of the purpose that prompted research on
them.  For some, e.g., IFF (identification friend-or-foe) their
development into civilian use (ATCRBS, the air traffic control radar
beacon system) importantly facilitates military use of the shared
space. For onion routing, we argued publicly right from the start that
the diversity of users was an essential element in effective use of
the technology---even way back when we were just calling our systems
onion routing, rather than _the_ onion routing (Tor) to distinguish
from instances of onion routing developed elsewhere.
As you can see, I'm not averse to touting these creations. But I am a
researcher who does publicly published research in this area and whose
work largely benefits from visibility. As Roger and others have
pointed out earlier in this thread, people who rely on Tor to protect
sensitive communications are rarely going to be happy to have anything
revealed about their usage. You are simply not going to hear from
(most of) those people, and you are definitely not going to get a
representative sample of such use. At best you are going to be lucky
to have anecdotal examples or even just anecdotal claims of usage
from which to extrapolate.
You seem to be asking for a statistically accurate demographic study
of all users. But I am sure there are whole classes of users who don't
want even their class of activity on Tor, much less their specific
activity, known. I have no idea what classes, but that just makes
sense. And on a more individual level, for every stalking victim who
both managed to connect to Andrew and decided to trust him to help her
protect herself online there are ???  others who did not have that
opportunity or were unsure enough about trust to not reveal. (Of
course ideally anyone regardless of technical background should know
about the benefits of Tor and how to use it for their needs without
having to talk to Andrew or someone. Let's not get into any of those
A well-designed user study will tell us something interesting about
Tor users. What it will definitely not do is give us a representative
distribution of Tor users by purpose (and as already touched on usage
demographics, e.g., by country can be significantly
dynamic). And inferring user distributions from traffic distributions in
studies that are methodologically controversial is not helpful.  If
that's all we've got for now, then that's all we've got. But we should
be very careful what we infer from it.

@_date: 2012-08-07 06:00:21
@_author: Paul Syverson 
@_subject: [tor-talk] Strong anonymization in a fixed group of	participants 
Note that Dissent-related work is ongoing. For a recent addition
see "Scalable Anonymous Group Communication in the Anytrust Model"
It's not on anonbib, but you can get it at See also

@_date: 2012-02-09 00:29:01
@_author: Paul Syverson 
@_subject: [tor-talk] tor-blocking sites 
Just add to your sense of inadequacy, a nice new addition was presented at
NDSS today But, yes if people can generate at virtually no cost arbitrary numbers
of new IDs from which they can register, then it won't matter what
controls are placed on the registered users by the nym system.
Not sure in practice. Incentives and tolerance for users is tricky
business. Note however that Nymble and its ilk are generally independent
of what the scarce resource is, so if your suggestion works, it should
be compatible. As to your question, a main contribution of work in this
area is that one establishes revocable credentials for clients. So if
computation is a scarce resource, it would be one that clients need
spend only rarely. Once they have the credential, they can log in without
that expense as long as they behave. I defer to others whether this
advantage is worth the costs and risks for particular cases.

@_date: 2012-02-27 01:10:40
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
Actually no. Tor is a flavor of onion routing not mixing. They have
a different design, a different threat model, and a different basis
for security. It is possible to combine them, as we did in the version
of onion routing we did between the first version and Tor. But Tor
does not use any mixing, and it did not evolve from mixing.
The main problem, besides the overhead, is that padding doesn't work
if an adversary can do something as trivial as very briefly delaying
It is too easy for an adversary to put a traffic signature on a
circuit in one place, and look for it elsewhere. If he owns, e.g., the
first node and any of the last node, the link to the destination, or
the destination it won't matter what kind of padding is done. There's
lots of published work showing this in various ways. Some already
alluded to in this thread. If nothing else the adversary can just kill
the connection at the first node and see which connection exiting the
network dies.
We did do a design that, if one had a scheme that was successful
against a passive attacker, would resist an active attacker. Though
providing provable guarantees against a class of active attacker, it
is not ready for primetime. See "Preventing Active Timing Attacks in
Low-Latency Anonymous Communication" at

@_date: 2012-02-27 08:51:28
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
I'm confused. Did you mean that you can't get to the anonbib webpage?
The link you gave is the same as the one given on the anonbib page.
(I gave the anonbib location to point people at a place where they
might see lots of related papers by others and also to give in one
location citation two possible URLs to retrieve the paper, the one you
gave plus the anonbib cached version. Both of these worked for me
when I tried them a moment ago.)

@_date: 2012-02-29 09:48:15
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
Maybe. The important thing is to understand what security is provided
and what is not. Then you can make an informed decision about whether
or not it's bad news.
The short but incomplete answer is yes. Generally, what you are
describing we experimentally verified on the live Tor network back in
2005.  See "Locating Hidden Servers" by Lasse Overlier and myself.
Available at  These sorts of attacks are what motivated us to introduce guard nodes,
also described in that paper.  We all knew, and had seen analyzed in
earlier work by Wright et al., that onion routing circuits were
subject to predecessor attacks, and that what Wright et al. had called
helper nodes would, well help.  What Lasse and I showed was that the
public Tor network as of 2005 was subject to such attacks working very
quickly (minutes) using very limited resources. You could generally
find a hidden server within minutes using just a single hostile Tor
relay (no cooperation from evil web server required). We wanted to
show what you could do with a single relay, which limited us to hidden
server circuits. If you owned at least two relays, you could attack
arbitrary Tor circuits. This was confirmed in simulation on PlanetLab
shortly after by Bauer et al.  ("Low-Resource Routing Attacks Against
Tor", also available at anonbib).
Entry guards don't change the asymptotic threat of such attacks, they
just move it around. Since you could be screwed so quickly and easily
by building random circuits, Tor was changed so that you pick just a
few relays as your entry guards. If one of them is evil, it will see
the entry side of (a large fraction of) your circuits and will be able
to associate you with your destination whenever you go to a hostile
destination or use a hostile exit. What Lasse and I showed was that
you weren't really much worse off from this than when choosing
circuits with random entry relays. And if none of your guards is evil,
an adversary can never de-anonymize you in this way. (Never say
"never". ;>) Cf. the experiments and discussion of layered guards in
"Locating Hidden Servers", and our subsequent research on building
trust into path selection.)

@_date: 2012-01-05 10:15:05
@_author: Paul Syverson 
@_subject: [tor-talk] Deterministic builds? 
I understand that memory space randomization is a runtime thing.
And I understand why we want verification. I also understand how
version diversity is a "codetime" thing used in general to guard against monoculture in a related but distinct way from ASLR etc.
(and that Tor does not currently bother to do version diversity,
and that it is a tradeoff probably not worth making). What I was
asking about was whether there is anything to be _gained_ by the
use of build diversity, either by adding yet another defense for
the multiple issues of software monoculture or something else.
Probably like n-version programming, etc. it is not a worthwhile
tradeoff, but I didn't know if this was a topic that had already
been broached. It's better to consciously reject a tradeoff than
to be unaware of it. You didn't immediately dismiss my question, so I'm cc-ing tor-talk (so if you now get what I was asking, you
can tell me it is dumb in public ;>).

@_date: 2012-01-26 10:18:00
@_author: Paul Syverson 
@_subject: [tor-talk] Fwd: ANONdroid 
JonDoNym and Tor have some significant aspects in common, but they are
fundamentally different in the basis for their security. JonDoNym is a
mix cascade design (or `MIX' as some would write it). This means that
all the messages (packets/cells) that enter the network together
(enter a node at roughly the same time) proceed through the network
together in a batch and leave the network together (exit at a
predictable but distinct node all at the same time). In principle,
this means that an adversary will have a difficult time separating
messages in a batch from one another.  (In practice, research
indicates that the situation for low-latency traffic is, let's just be
generous and say, more complicated.) This is the basis of the security
they provide.  Onion routing designs like Tor derive their security
primarily from the unpredictability of routes: just seeing a message
from a client enter the network does not tell you where it is going to
come out.
The pros and cons of these are a matter of long debate. (I miss you,
Andreas.) I come down strongly on the side of onion routing as an
approach. Many detailed research papers address lots of the issues,
but much of my position is captured in my "Why I'm not an Entropist."
(Let me know if you want to see it and can't find it.)  The basic
difference is roughly that mix cascades attempt to hide you well in a
predictable 'anonymity set'. Onion routing networks attempt to make it
hard to know where to look/attack if you want to de-anonymize, e.g.,
what website someone is browsing. And the Tor network is intended to
be big enough and in diverse enough jurisdictions that it is hard for
any one adversary to look everywhere (thousands of nodes
worldwide). That's the main idea, skipping a lot of aspects, such as
the possible benefits (or not) of combining the two approachers.
Many security researchers, including myself, are quite resistant to
the idea of revocable anonymity. Designing, building, and deploying
a system to be secure is incredibly hard even without the added burden
of building in a systematic means to selectively remove its security
properties. See, for example, Matt Blaze's work on key escrow. See
 for his recent retrospective.
But leaving that entire issue aside, from my above comment, I hope it
is apparent that there is another important difference from mix
cascades. For Tor such a surveillance order for purposes of tracking
particular communicants would be pointless, and thus presumably
unjustified. Unlike cascades, a message entering the network in one
location might come out at any exit node, anywhere in the world. And
if, e.g., one observes communication exiting a particular network node
and arriving at a destination of interest, there is no reason to think
that future communication, even from the same originating client is
likely to emerge from that same node. In fact just the opposite.

@_date: 2012-02-29 19:22:58
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
Some abbreviation I'm not familiar with for Monsignor? I'm
pretty sure I've not been called to the RC priesthood, or for
that matter the RC church in general. I prefer Sagamore or
Trismegistus if you wish to give me an honorific.
There's nothing currently available to do this. Nor am I enthusiastic
about the prospect of anything that doesn't break some essential
aspect of Tor working. (I don't mean implementation. I mean it will
fail on an abstract protocol level even before that.) At best you
might reduce this from virtual certainty to a very serviceable stochastic
But that's a good thing, because otherwise Tor would be more vulnerable
to long path attacks.

@_date: 2012-03-01 10:40:15
@_author: Paul Syverson 
@_subject: [tor-talk] on the topic of tor's weaknesses 
It wouldn't.
To protect the hidden service, not just you. You each build an onion
route of three hops (the total hops on Tor relays is actually six not
seven). You build your circuit and give the last hop to the HS as a
rendezvous point. It builds a circuit and connects to that rendezvous
point as the destination after the last hop in it's circuit. The
mated circuit has six hops and neither of you knows anymore about
the network location of the other than, e.g., a regular web server
would know of you if you were building a basic Tor circuit to it.
More description with pictures at
And more elaborate details and variants in various papers on anonbib.

@_date: 2012-03-06 16:04:10
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
Is that a typo? The suggestion was that people _stop_ working on
defeating the GPA, which is unrealistic as both too strong (global)
and too weak (passive). I've been making the same point for over 15
years, but this was an attempt to sum a lot of that up in one
place. Adversaries may be really large, but it's generally unrealistic
to consider any one of them truly global on the internet.  (In the
paper I call realistically large adversaries, The Man.) And passive
makes your mathematical proofs cleaner (and sometimes doable at all)
but assuming your adversary can't even make use of delaying packets
passing by him for a few milliseconds is ridiculous. So you what
you end up proving doesn't really tell you much about real systems
even in principle. Which is why I (and others) have been working on
better models.
I'm a mere four years behind in putting my work up on the web, and
this one wasn't co-authored so nobody else did either. I'll try to do
something about that in my copious free time this week and send a

@_date: 2012-03-06 23:14:39
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
I'll try to get to it soon.
Actually there are many papers over the last several years (e.g., at
ACM CCS and Info Hiding) showing that one can place undetectable
timing channels on flows (for some schemes provably undetectable for
others practically undetectable).  But passive correlation is adequate
anyway, even at very low sampling rates (cf. Murdoch and Zielinski,
PETS 2007). This is long known and well understood. It's why we have
always said that onion routing resists traffic analysis not traffic

@_date: 2012-03-09 10:51:39
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
And there was a star in the east...
I actually updated my webpage, not as fully as I would like, but
I updated all the dead links and added about a dozen or so papers.
In particular there is now a link to the "Why I'm not an Entropist"
paper, the "Practical Vulnerabilities of the Tor Anonymity Network"
paper that Andrew asked me to post over a year ago, my recent
historical review "A Peel of Onion", etc. HTH.

@_date: 2012-03-09 15:52:38
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and HTTPS graphic 
You misunderstand or at least misrepresent what is being argued
here. There does not even have to be anything incompatible in what you
are saying and this "insistence" as you put it.  The difference lies
entirely in the threat model. So we need to get more precise about
that (below).
Nobody's blinded to the possibility. Many of us knew long ago that
several things like this are easy to do. It's even easier to just do
bitsquashing, as we noted in the first onion routing paper in 1996
(there are tradeoffs and may be times when other tagging attacks are
preferable, that's not the point). As a more directly connected
indicator of prior awareness, Mixminion was designed by some of the
main research people who also worked on Tor, specifically Roger and
Nick together with George Danezis. They spent a significant part of
the research paper that sets out the design talking about tagging
attacks and their countermeasures to them. We're all well aware of many tagging variants here. What we're saying
about them is that (1) identifying another specific example of tagging
attack without other significant contribution is not a publishable
research contribution and (2) designing in countermeasures against
such attacks (such as the Mixminion paper and some of the subsequent
formatting work in that vein did) are not worth it because it's so
easy to attack Tor whether it's made resistant to this kind of
tagging or not. (I know you don't agree with that---yet. I'm coming to
Nobody said they were equivalent. What is actually said in [1] is
   "One of the unknowns in the research world is exactly how quickly
   the timing attack succeeds. How many seconds of traffic (and/or
   packets) do you need to achieve a certain level of confidence? I'll
   grant that if you run the entry and exit, tagging is a very simple
   attack to carry out both conceptually and in practice. But I think
   Fu underestimates how simple the timing attack can be also. That's
   probably the fundamental disagreement here."
And in that passage, they're only talking about the passive timing
attack. As noted earlier in the post and in many other places,
it's trivial to put in active timing signatures if they are needed.
Actually, the post notes that this was the maximum false positive rate
achieved in the cited simulation. In the analysis on the live Tor
network also cited, there were zero false positives in thousand of
runs of the experiment (not thousands of circuits, there were also
thousands of circuits in each run of the experiment). Nonetheless, you
are right to ask about scale and base rate, but I don't think
they undermine the effective adequacy of timing attacks
in ways that ultimately matter.
Again, you overstate. What it actually says is
     "If somebody can show us that tagging attacks are actually much
     more effective than their passive timing counterparts, we should
     work harder to fix them. If somebody can come up with a cheap way
     to make them harder, we're all ears. But while they remain on par
     with passive attacks and remain expensive to fix, then it doesn't
     seem like a good move to slow down Tor even more without actually
     making it safer."
More importantly, here's where we come to the crux of the
biscuit. What do we mean by "actually much more effective"?  You seem
primarily focused on a global passive hoovering adversary, perhaps at
a limited sample rate a la Murdoch and Zielinski. You seem to want to
show that timing correlation attack by such an adversary is not so
bad, but tagging would be effective. I think you are a little quick in
the practical conclusions you draw from your analysis (ignoring
intersections and ancillary information) and how you come up with the
numbers on which you base your analysis (what you do with them seems
fine), but I won't debate those points because I don't care even if
you turned out to be right in those aspects: I'm not very worried
about that adversary because I don't think it's realistic threat to be
that global or that passive (large is fine, even multijurisdictional,
and only making small delay patterns in passing traffic, hmmm OK
maybe. But GPA I just don't buy). In any case, even if we were worried
about the global hoover, you don't want to limit to a passive attacker
as your focus on tagging illustrates.  But once an adversary can be
active there are all kinds of active timing techniques that padding
can't address, ranging from the provably secure, provably undetectable
to the merely highly effective and practical. So the usability costs
and network overhead that countering tagging would imply would not
even help much.  So, to convince me that your analysis shows we should
revisit tagging for Tor you would have to show three things: (1) Convince me that a truly global adversary is realistically worth
worrying about, (2) convince me that an adversary that does active
timing correlation would not remain a significant threat even if
tagging were no longer possible, and (3) convince me that your numbers
correspond to reality and that the results are robust to intersection
attacks and ancillary information. (No need to bother with (3) until
(1) and (2) are established.)
I also want to comment on your consideration of an adversary looking
for the clients visiting a given website. Let's accept for the moment
the idea of full GPA and accept your numbers. Even if we accept your
EER that is at least an order of magnitude worse than experiments have
found (i.e., 99%) you come up with initial anonymity sets of who is
visiting a particular website (respectively which destinations a given
client is visiting) of around 50. That is essentially zero for a big
and powerful adversary. Then add in any ancillary information
geographic location of IP addresses, prior knowledge about the
clients, nature of the destination servers, etc. not to mention
intersections over time. Rather than undermine the adequacy of passive
correlation, you have supported its effectiveness.

@_date: 2012-11-14 09:49:03
@_author: Paul Syverson 
@_subject: [tor-talk] Guard flag vs relay bandwidth 
It isn't. One of the lessons of is that the chance and the time matter.
It turns out that if you communicate with a destination at all
regularly, the adversary will deanonymize that relationship very
quickly with very few resources. If this is a concern, then you
are much better off using guards since if none are bad then
(for this class of attack) none of your connections are deanonymized.
And you also overstate the threat of a compromised guard:
the adversary sees ALL the traffic but doesn't deanonymize
it. That only happens when he combines this with some other attack,
most notably when he owns the other end of the connection as well.
Plus he doesn't see all the traffic, even in this sense, just
a third of it on average (averaging out differences between guards).
I don't think that's Roger's point. 1. You can induce all kinds of
congestion and scheduling messes for circuits going through you if
you're optimizing for TB/month processed. 2. There can be advantages
to the performance of the network for everyone by throttling your
connections even if your node is processing everything with no
significant delay. See our paper

@_date: 2012-11-30 10:07:49
@_author: Paul Syverson 
@_subject: [tor-talk] RFC1918 addresses on outside interface 
Could you say more about why you would want to do that? I ask because
this increases those clients' risk from an AS-level attacker by
mandating an increase in the number of ASes that must be traversed
between client and entry node.

@_date: 2012-09-18 11:01:01
@_author: Paul Syverson 
@_subject: [tor-talk] How dangerous are DNS leak? 
Logic persnickitiness: 'IFF' is not a more emphatic version of 'if'.
'IFF' means if and only if. In other words, you are saying that
it's dangerous if it's real, and it's real if it's dangerous.
Also, it's dangerous if you're expecting tor to hide... and
you're expecting tor to hide... if it's dangerous.
Pretty sure that's not what you meant.
But you're main point is well taken, subject to the limitation
that most people won't know exactly what their threat model should
be for any given behavior and it's best to be cautious.

@_date: 2012-09-26 04:31:25
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and P2P 
It wasn't efficiency per se. In many ways reply onions are more
efficient: one circuit build vs. versus four, fewer hops in the final
connection, etc. (Potentially, since the intermediate generation of
onion routing (the one before Tor had variable length routes.) In some
ways they were less efficient.  The first two generations of designs
used onions to build circuits.  These didn't use Diffie Hellman for
circuit building and so didn't have forward secrecy. (There are some
circuit building protocols that get back some virtues of the original
design without all the limitations, but they're still just academic at
this point.)  Onions also required relays storing records of onions
that had been processed before to guard against replay attacks.
Either originally or in the second gen design (can't recall) reply
onions had a flag that allowed them to be set for either single use or
repeated use (more efficient but with risk of replay.  Reply onions
were also designed just to protect the one being replied to (e.g., the
hidden service although we also expected them to be usable for
mail). We also had design for rendezvous servers, (but that was more
for things like, e.g., an anonymous chat room) and reply onions could
start from the end of a circuit similar to the rendezvous point of
today's design. Not the whole story there but already more than you
wanted to know. I still think there's some nice features of reply
onions that I miss. The whole design and ecosystem of replies and
hidden services could benefit from a revisit, in or copious free time.
Depends on your goals. If you are going to a server you trust in
appropriate ways there is no need to build a path to hide your IP
address from it. But for a mutually mistrusting client and server
neither party should rely on the other to protect their anonymity.
Hidden servicss were designed with that as a presumed default.
Reply onions were more flexible in that regard and could be used
either way. I guess the closest current analogue to reply onions
is tor2web. Gotta run. HTH.

@_date: 2013-04-04 23:50:41
@_author: Paul Syverson 
@_subject: [tor-talk] NSA supercomputer 
*sigh* at the risk that I am feeding a troll rather than helping
someone wellmeaning but misinformed and the hope that some will find
these points useful despite their having been made many times before:
1. Tor not TOR  (See
    )
2. was created at NRL, by which I mean by myself (U.S. govt. employee)
   and Roger and Nick (working with me at the time as subcontractors
   for the U.S. govt.)
3. NRL is not nor ever has been a part of DARPA. DARPA is a funding
   agency that funds research at universities, private companies, and
   yes at NRL. NRL is the U.S. Navy's corporate research laboratory.
   (One of the oldest corporate research laboratories operating at
   this point.)  See  NRL scientists have
   built lots of useful stuff besides onion routing and Tor that you
   probably use every day, such as GPS. The first chapter of _Pushing
   the Horizon_ gives a fair overview of NRL research culture, and
   history through 1998
   ( Although it
   doesn't mention NRL's patent on the joystick controller in 1923 ;>)
4. Not sure who "they" is, but the fact that Tor was created at NRL
   does not carry the implications you suggest. And we worked hard to
   make that so. Since its beginnings in 1995 we argued that to
   provide the intended protection for Navy and other government
   communications, onion routing networks had to carry traffic for
   others, must let others run parts of the infrastructure, and must
   run open source code so others could inspect it to know they could
   trust it. We obtained our first publication release for the source
   for an onion routing system in 1996. See
       Thus the development and deployment process was designed from the
   beginning to preclude the sort of concerns you are
   implying. Continuing to the present day, Tor design, development
   process, and software is transparent, well documented, and heavily
   scrutinized by a variety of people with diverse goals and
   interests.
5. This is a response to the comment immediately below from Gregory
   Disney. I'm not going to try to address everything mentioned in the
   thread, although Nick gave a fine answer to the question about key
   length.

@_date: 2013-04-06 11:14:36
@_author: Paul Syverson 
@_subject: [tor-talk] NSA supercomputer 
Sorry no that's wrong. See my earlier post. Roger and Nick did not
submit a grant proposal for something they wanted to do and then this
receive funding from NRL (which is primarily a producer of research
work not of research funding, again see the links in my earlier post).
I submitted proposals and obtained the inital funding, and the three
of us worked together under the funding I received, me as an employee,
Roger and Nick as contractors.  Saying something like that they were
sponsored by NRL is akin to saying that they are now sponsored by the
Tor Project. Maybe you would want to say that, but I find it
For good or ill it is standard and common in modern English usage to
refer to a collective named entity under which people work for the
singular or collective actions of individuals. (Don't get me started
on action theory, personal identity, and causality.)  By your
reasoning the Tor Project has not done anything: Mike did this, Arturo
did that, etc.
Roger, Nick, and I created Tor in the sense that we designed it
together. For what it's worth, the earliest code base for the version
of onion routing that was to become known as Tor was taken (with
approval) from Matej Pfajfar from his undergrad project at Cambridge
U, although all of that was gone from the codebase within a few years
or so. The three of us actually started calling what we were doing Tor
when working off an earlier design done arising c.  1997 by David
Goldschlag, Mike Reed and myself (all NRL employees at the time, if
you're keeping score on that count). The Pfajfar code was written by
him definitely based on the earlier design since he wouldn't know then
about what Roger, Nick, and I designed. (You may want to read up
an the Ship of Theseus to figure out which is the real Tor ;>)
Your last point is the most salient. Lots of people with lots of
different employers, funders, affiliations, etc. have contributed.
Whether they were employees or contractors of the Tor Project,
Inc., they were all part of the Tor Project.

@_date: 2013-04-08 08:29:32
@_author: Paul Syverson 
@_subject: [tor-talk] NSA supercomputer 
Actually I didn't write any of the code: Partly everything was much
simpler if contractors rather than employees were producing the code,
but another nontrivial factor is that basically I personally don't
write code (at least not since the 70s).  I've started to a little bit
every few years or so, but there always seems to be better uses of my
time and better to have someone else do it, and I end up dropping
any attempt to develop that skill.

@_date: 2013-04-08 09:00:00
@_author: Paul Syverson 
@_subject: [tor-talk] NSA supercomputer 
covers what I said and then some, basically gives a brief history
roughly 1995-2005. Althought the site seems to be down right now.
A more indepth discussion of the design evolution and the reasoning
behind it (but less blow-by-blow than the above) is covered in
which I already pointed at in this thread. Again more focus on the
first decade of onion routing than the last several years.

@_date: 2013-04-08 15:55:33
@_author: Paul Syverson 
@_subject: [tor-talk] NSA supercomputer 
Hrmm. I was checking from home, and before I sent that I tried it over
Tor of course, and just for overkill checked
downforeveryoneorjustme.com as well.

@_date: 2013-08-07 07:28:26
@_author: Paul Syverson 
@_subject: [tor-talk] Javascript vs privacy? 
Just in case you are unfamiliar, you might also try Links. I use
both Lynx and Links but mostly prefer the latter. YMMV
Certainly that is true for some users. But the point of this
discussion and a motivation of onion routing designs from pre-Tor days
through the present has been to separate indentification from routing
while minimizing disruption of the user experience. The more Tor is
usable (and therefore used) for ordinary activities by ordinary users
the more secure all users, including the most sensitive, are against
a significant portion of the threats facing them. This is far from
the whole story, but is important to keep in mind. For more, see
"A Peel of Onion", "Why I'm not an Entropist", "Anonymity Loves
Company: Usability and the Network Effect", and "Challenges in
deploying low-latency anonymity".

@_date: 2013-08-27 11:51:21
@_author: Paul Syverson 
@_subject: [tor-talk] Many more Tor users in the past week? 
Or somebody's research experiment gone awry, or behaving predictably
but that they didn't think a concern worth mentioning, or...
Malicious stuff happens, but most of the time these things are
incompetence or similar rather than malicious intent.
Until we know more, it's important to keep that in mind.

@_date: 2013-08-30 11:33:30
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and Financial Transparency 
Right. And since no doubt at some point Juan Garofalo or someone else
in this discussion will yet again "discover" what we make every effort
to be as open and pretty loud about as is practical, Tor isn't just
funded by the U.S. government it was designed by U.S. government
employees and contractors based on technology invented by
U.S. government employees. You might see my "A Peel of Onion" for the
history.  It's on my homepage For the latest developments in making the adversary models and network models as fine-grained as possible and the subsequent analysis
you can see our forthcoming paper
"Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries".
It's not on anonbib or my own homepage yet, but one of my co-authors has
posted the pdf. See And note that some of the issues identified have already resulted
in changes to implemented Tor versions. And no Tor isn't perfect.
Design improvements are ongoing. Constructive help would be nice.
Right exactly. See all the research on the issues trade-offs, threats,
designs, etc. that Tor Project Inc. employees, government employees,
university and corporate researchers, and lots of others have done
trying to design for a diverse userbase. is a fine place to start. If you can come up with better designs,
we would love to have them. Please share those rather than the
allegations you keep making but offer no support for, such as
"Tor isn't 'subverted' - it just flawed...by design."

@_date: 2013-01-09 11:00:02
@_author: Paul Syverson 
@_subject: [tor-talk] On the Theory of Remailers 
Right. Tor isn't mixing at all. Onion routing gets its security from
the observation that even adversaries that can be anywhere typically
can't be everywhere. This is true whether one is trying to protect
properties of the communication between source and remote destination
(typically called anonymity) or protect properties of the
communication between the source and the onion routing network
(typically called circumvention or obfuscation).
Mix network security analysis typically assumes adversaries are on all
links between mixes and at some compromised mixes as well. Tor is
broken against such a model. But, and this is a huge but, while the
above is all correct, it perpetuates the mistaken view that the only
thing going on is a security/practicality tradeoff that mix networks
resolve more in favor of security and onion routing networks resolve
more in favor of practicality. In several important respects Tor is
more secure than any practically deployed, but also practically
deployable mix network. I discuss this more in "Why I'm not an
Entropist" and "Sleeping dogs lie on a bed of onions but wake when
mixed", both available on my webpage (
That said, I continue to think that combining different latency
traffic as we discussed in the alpha mixing paper mentioned earlier in
this thread can provide real benefit for some applications and
plausible adversaries (although I think what we called 'tau-mixing' in
that paper is the more likely fruitful departure point). But departure
point for someone else: there are years worth of higher-priority-to-me
Tor-related research problems to solve, so it is back-burnered
indefinitely or until somebody entices me that working on this
is worth pulling away from other things.

@_date: 2013-01-10 15:35:38
@_author: Paul Syverson 
@_subject: [tor-talk] Roger's status report, Dec 2012 
This might not be what you or I would mean by "open access", but it is
not IEEE misappropriating your term either. Theirs is a meaning of
"open access" that is long in use and not just by commercial
publishers. For example, the favorite cited instance of open access in
the wider press, PLoS, works on that model---actually like IEEE but
less flexible: there's no option to have a paywall.  You simply must
give them thousands up front.  (Although if you can claim hardship,
you can avoid author fees.) I don't disagree with your overall
sentiment, but your indignation over the term is no more likely to be
a winning approach than fighting the broader media about the use of
"hacker" (and possibly with less legitimacy since there the
misappropriation of the term from its original use is unambiguous and
well established).

@_date: 2013-07-03 16:57:08
@_author: Paul Syverson 
@_subject: [tor-talk] How intensely do you use Tor? 
Just to underscore this point: Your bank hopefully knows it's you when
you sign in, but as Roger noted, there are other entities Tor protects
against. In particular, someone watching you connect to the network
who also sees you connecting regularly to (one of) your bank's IP
address(es) can guess that this is your bank, which they then might
combine with other things they observe to construct spearphishing or
worse attacks against you and your money.  My guess is that the
concern he raises below will be of more immediate concern, but most
credit card vendors will be happy to have you, e.g., contact them and
say you will be in Italy for the next ten days. Similarly, if you want
to use Tor under such circumstances you can restrict to exits in a
given location when connecting to your bank. This then raises other
potential concerns...
Another issue is that if you only use Tor when it's important as
opposed to in general, local observers of your behavior will know they
are seeing important traffic because it's going over Tor (unless you
use bridges...).
For many users these may be outside their threat model, but you asked
why you would bother to use Tor when connecting to a destination that
knows you anyway. Another scenario of securely logging in somewhere
over Tor is if you are physically away from where you usually work,
need to connect to a system of your employer, but you don't want
locals to observe where/who that is.

@_date: 2013-03-15 00:01:34
@_author: Paul Syverson 
@_subject: [tor-talk] What would Tor v1.0 look like? 
Tell me about it. Some of us have been onion routing since 1995. ;>)

@_date: 2013-09-08 13:11:05
@_author: Paul Syverson 
@_subject: [tor-talk] What if I connect to all nsa Tor nodes? 
To do what, and secure against whom and to what degree?
It is reasonable to assume that if NSA is running Tor nodes, then they
are probably using good operational security. So against someone
breaking into those nodes and then attacking you, you are probably
more secure than using random nodes. (But see below.)
If you mean secure against the NSA node operators, then no. Any
adversary that owns all the nodes in your route should be able to
learn pretty much everything about your traffic patterns, who you're
talking to and when. This is true for the NSA or anybody else.
If you mean intentionally selecting some subset of nodes because you
trust them more or because you are trying to avoid them as adversary
nodes, there is a tradeoff between the potential better security that
might provide and what your choice might reveal about you. cf.
"Trust-based Anonymous Communication: Adversary Models and Routing
Algorithms". There's currently research advances but no simple advice
on that score.
This all assumes adversaries just live at the nodes rather than also
at the ISPs, the ASes, the IXPs, etc. It is hard to say anything more
about such an adversary without more details. You might want to see
"Users Get Routed: Traffic Correlation on Tor By Realistic
Adversaries" and some of the earlier work on this issue cited therein.

@_date: 2013-09-11 14:31:07
@_author: Paul Syverson 
@_subject: [tor-talk] Tor and Financial Transparency 
For some reason, just the message to which I'm responding below out of
this entire thread got caught in a spam trap that I rarely check (once
every month or three) because it very rarely catches messages I
want. The thread has long ago moved on. I respond inline below, but
feel free to ignore.
No idea what you have learned when, only what you say. My goal here
though was to preempt anyone participating in or reading the thread
making such a "discovery" hence "Juan Garofalo or someone else in this
discussion". This seemed a valid concern since you mentioned repeatedly
a U.S. government role but did not at all mention that larger history.
Please stop constructing rhetorical straw men. I didn't say or imply
either of those things. First, many times when we note that there are
open problems to work on, we note in general that constructive
contributions would be welcome. (Cf. many comments/posts by Roger.)
Sometimes people indicate an assumption that if they are not in some
inner cabal that their helpful contributions are not welcome. It is
thus usually good to remind everyone that the Tor community works in
quite the opposite way.  Most people involved in creating Tor
including, e.g., Andrew Lewman, now Executive Director, of the Tor
Project Inc. first got involved simply by volunteering constructive
suggestions/code/design/etc of one sort or another and then growing
into a larger position. For anyone looking to help, see for example
 Second, people who make clueful constructive criticisms of design are
usually amongst the best to ask for help in improving designs. Sorry
if what I said offended by leading you to infer that I thought you
coud be such a person.
Another straw man. Nobody said that. You _did_ say that Tor was flawed
by design without offering any support of that claim.  What you have
not done is comment in a constructive way. If you have only ad
hominems and broad criticisms made without support, then you can
comment, but apparently not usefully. You thus leave people with little
to say beyond, "Sorry you don't like our stuff, maybe you can do
something better." If you have something constructive to say, please
I see no attack in this at all, personal or otherwise. I gave a
pointer to where people can find much of the work that has been done,
which I do think is a fine place to start. And if anyone can come up
with better designs, we _would_ like to have them.
That is absolutely true, and I never said otherwise.  Many of the good
papers cited above do not provide better designs. They "take a look" at
existing systems to improve our understanding of what those systems
provide and what they do not. Pushing the science forward does not
happen simply by creating new designs. Indeed a field that only
designs and without taking a careful look at how they work and don't
work is not in a good position to determine which designs are better.
Well, much of the above mentioned work, and other work as well
attempts to spell out carefully how much security is given to how many
users of what kind. Can you be more precise about what you mean by
"seems to be impossible". As to your second statement, I simply
disagree. Unless you are talking about something steganographic
or the like, all of my research has generally come to exactly the
opposite conclusion. Cf. my "Why I'm not an Entropist".
The appeal remains. And as you noted, you don't need to have designs
to suggest.  Substantiated and constructive critical comments are
welcome too.
