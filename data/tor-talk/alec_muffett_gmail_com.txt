
@_date: 2013-08-30 10:26:01
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Financial Transparency 
economic 'incentives' can't be denied, the best course of action is to call
any critics "tin foil hats".
So you're a critic?
Ok, Mr Critic: the source code is public. The protocol is public. The
architecture is public. The threat model is public. The deployment could be
Do us all a favour and point at the bug?

@_date: 2015-05-17 16:36:30
@_author: Alec Muffett 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Lots of good questions there! I'll try and update
 later with some thoughts.
Happy Sunday!

@_date: 2016-12-08 21:39:31
@_author: Alec Muffett 
@_subject: [tor-talk] Ahmia search engine works normally again 
Well, yes, onion addresses are like any other form of network address.
Perhaps you didn't intend to go as far as saying "when you connect your
computer to the network and put a service on it, people will _use_ it", but
unless I am misinterpreting, that seems to be pretty much the implication?
That presumes someone was not acting intentionally.  From experience, even
when you _do_ something nice and helpful like that, people will still
scrape you in boneheaded, stupid ways and bring you down.
For general interest (perhaps to Juha, especially?):
* I am building a 6-node, 24-core cluster, specifically to run an
Onion-traffic-serving experiment upon.
* It's running a Debian variant - so the results/learnings should be
generally applicable to Linux - on Raspberry Pi hardware. Total cost ~ £400.
* It's a casual-fun-research thing (rather than a big academic exercise)
and the current goal is to serve half-a-gigabit from a "single" onion
address, across ~30 daemons.
* Details and diary are at:
Hopefully we should have results by end of January. :-)
    -a

@_date: 2016-12-19 10:30:16
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
I would post this to the tor-onions list, but it might be more generally
interesting to folk, so I'm posting here and will shift it if it gets too
I'm working on load-balanced, high-availability Tor deployment
architectures, and on that basis I am running 72 tor daemons on a cluster
of 6 quad-core Debian boxes.
I am then - using Donncha's "OnionBalance" to:
* scrape the descriptors of those 72 daemons
* selects random(ish) 60 of the introduction points from those daemons, and
* rebundle those 60 introduction points into 6 distinct descriptors of 10
introduction points apiece, then
* signing those distinct descriptors with a "service" onion address and
emplacing them onto the HSDir ring.
This means that, at any one time, the daemon will be able to have 60x the
CPU and network-bus bandwidth, above/beyond what is available to a typical
single-daemon instance.
Why "72"? Because it's a number >60 and I'm seeking to stress-test things a
Specifically: one eventual goal of this project is to adjust the timings a
little, so that the HSDir cache acts a little like "Round-Robin DNS Load
Balancing" - people accessing the "service" onion address at lunchtime will
receive/cache different descriptors from those who access it some hours
later, and the descriptors persist, so thereby the whole 72 daemons get
used/averaged-out over an entire day.
In my attempts to go fast-and-wide, however, I appear to have run into a
hardcoded limit:
    Dec 19 09:24:43.365 [warn] HiddenServiceNumIntroductionPoints should be
between 3 and 10, not 1
There's little point in publishing >2, and perhaps* not >1 introduction
point for each of the 72 daemons; also it makes scraping and reassembly
slower/more expensive.
So I am writing to ask whether it is possible (and whether it is wise?) to
have a mode that will bypass this (otherwise very sensible) control?
    -alec
* it would be rude to an IP to have only a single-IP-per-daemon that was
invariant over a long period, but I believe that IPs migrate over time
anyway... ?

@_date: 2016-12-19 10:47:17
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
As an aside, this is what I am currently using as a daemon config.
Comments welcome.
I'm trying not to use Guards because again it would be rude to hammer them
with vast data flows when instead the pain can be spread around a bit more;
given that my target deployments are unlikely to be truly anonymous (eg:
Facebook) this isn't much of an anonymity issue.
$ more /home/alecm/master/halfagig/hs6.d/config
DataDirectory /home/alecm/master/halfagig/hs6.d
HiddenServiceDir /home/alecm/master/halfagig/hs6.d/
# HiddenServicePort 19 localhost:8506 # chargen, eventually
HiddenServicePort 80 localhost:10506
HiddenServiceNumIntroductionPoints 3 # <--- maybe 2 or 1 here?
LongLivedPorts 19,80
CircuitBuildTimeout 60
LearnCircuitBuildTimeout 0
PredictedPortsRelevanceTime 0
RendPostPeriod 37 minutes
SocksPort 0
UseEntryGuards 0
UseEntryGuardsAsDirGuards 0

@_date: 2016-12-19 15:40:25
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
There's a big difference between your scenario and mine.
There will still be multiple introduction points.
There will, in fact, be 60 introduction points.
But they will each be attached to one of 60 different tor daemons, as
opposed to the existing best practice of 1 daemon x 10 introduction points,
replicated in 6 descriptors. (1 x 6 x 10 == 60)
I understand this is a bit hard to explain with just words, so I posted a
couple of slides at:
    ...but I could only fit 4x tor daemons on the slides, instead of 60.
I would post the slides here, but I believe that the maillist scrubs
    - alec

@_date: 2016-12-19 16:37:16
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
Hi David!
You've hit basically upon an area of experimentation - with an OnionBalance
setup there are essentially 2x RendPostPeriods - one for the "parent" (or
"service") onion, and one for the "children" (or "ephemeral") onions.
As you already know, the parent "scrapes" the childrens' descriptors, and
"re-bundles" some of the childrens' introduction points as its own.
Anyone who's an astronomer will know about sampling issues in variable star
measurements, and the same applies here: if both the parents AND children
are on a "1 hour" RPP cycle, then a kind of oscillation could be set up
where the parent refreshes its descriptor, then most-or-all of the children
simultaneously (5 minutes later) change their introduction points, thereby
invalidating most-or-all the parent/service descriptor, and be permanently
stuck on this situation.
This is probably a bit of an edge case, and I suspect Tor already contains
some workarounds for it, but until then it seems wise to take a lesson from
cicadas and run the parent and childrens' RPPs on two different cycles,
using prime numbers as multipliers.  Hence 31-and-37 minutes, or something
like that.
Ignoring all "ZOMG THIS WILL MAKE IT OBVIOUS" - assume Single Onion - what
do you reckon?
    -a

@_date: 2016-12-19 16:46:41
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
...in six distinct descriptors, each containing 10 intro points, each of
_those_ attached to one tor daemon.
also said "people accessing the service onion address at lunchtime
which lead me to believe that a single client will not
Strictly, the way that Tor works, the client will query the HSdir and
receive 1 (one) descriptor, of the 6, containing a random selection of 10
introduction points.
Each of the 60 "actual" Tor Daemons sets "NumIntroductionPoints = 1"
Then the OnionBalance software creates one more, virtual, somewhat
"parasitic" set of descriptors - which comprise the 6x10=60 alluded-to
Any given introduction point in the parasitic set of OnionBalance
descriptors, is actually a descriptor stolen from one of the 60 separate
daemons (- these daemons having set NumIntroductionPoints = 1)
Is that a better explanation?
    -a

@_date: 2016-12-20 17:17:35
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
Hi George!
On 20 December 2016 at 14:03, George Kadianakis There is a hypothetical way around that, but Donncha will need to comment,
I don't _feel_ that this is a problem which needs to be solved in the next
2 years.
What _could_ happen in the future, is.
1) the 72 workers could each set up an IP but *not* publish it in a
descriptor, and then
2) the master daemon could poll the 72 workers for their list of current
IPs via a backchannel, and then...
3) construct the "master descriptor" from that information.
This has pros and cons.
It makes the architecture more "active" - using backchannels and lots of
chattiness - which is great for physically geolocated clusters - ie:
everything in one rack - but it makes matters *really* complicated for the
kinds of physically distributed clustering that Tor would be awesome and
cutting-edge for.
For one thing, in such a scenario, it would not be possible to use the
slave onion addresses to coordinate with each other; so it would make it
really hard to build a high-availability solution like a Horcrux* (see
It's probably easier to think of this not as "432 descriptors" but instead
as "73 Hidden Services" - comprising 72x "physical" onion addresses, and 1x
"virtual" onion address using OnionBalance. This is much the same as the
Physical-versus-VIP architectures which one sees in other load-balancing
It also resonates with the slides I posted in the thread, here:
...arguing that Onion addresses are the Layer-2 addresses of a new network
With such an approach, rather than seeing Onion addresses / HSDirs as
scarce resources, we would be better to be engineering them to become
abundant and cheap, for they will become as popular and as ephemeral as any
other Layer-2 address.
tl;dr - I am doing the bonkers stuff so that nobody else has to. 72 is
above-and-beyond, especially since Facebook does it with two; but if
streaming HD Video over Tor eventually becomes a thing, something this will
need to happen. :-)
Concur. Only semi-retired enterprise architects with spare time need apply.
If you would like to talk to one of the 72 daemons, check out:
    ...which is probably okay for the next 24h or so.
I guess to improve the
Agreed, we can do that, and that's very efficient for localised clusters.
However, I had this idea the other evening*, which smells very "Tor" and
has some interesting properties.
1) Say that, instead of 72, we chose a more sensible number like "6" onion
2) We configure 6 cheap devices (Raspberry Pi?) each to have a single
"worker" onion address
3) We also configure OnionBalance on all 6 computers, so that they all know
about each others' onion addresses, plus the *same* master key; so we have
an n-squared mesh.
4) They get booted; each launches its own Worker onion, and each scrapes
the descriptors of all the other workers, synthesising a "master"
descriptor and publishing it once a day to the HSDirs.
5) This means that, for workers A B C D E F, occasionally the master
descriptor which B's onionbalance uploads to the HSDirs will get stomped-on
a few minutes later by the  from F, and then the  from D will
overwrite them, etc.
6) there is some (small?) extra load on the HSDirs this way - BUT the big
win is that to take this onion site "offline" will require killing all 6
daemons, all 6 machines - hence the "Horcrux" reference from Harry Potter.
7) this works because the 6 daemons use the HSDir as a source of truth
about the descriptors, which is an idea Donncha had for OnionBalance, and
is awesome, because it enables this kind of trusted distributed directory.
8) to make it forgery-proof as well, you'd want to use certificates, or
signing, or something; but this would be an intensely robust
High-Availability architecture.
I want to build one, for test/fun, but not until the bandwidth testing is
    - alec
* Horcrux thread:

@_date: 2016-12-20 17:21:10
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
4) They get booted; each launches its own Worker onion, and each scrapes
ps: obviously the Horcrux can synchronise data around itself (for
webserving, etc) by using rsync over ssh to the worker onion addresses, as
a backchannel.
Or something like that.
Maybe "git" even.
    -a

@_date: 2016-12-21 13:46:42
@_author: Alec Muffett 
@_subject: [tor-talk] Not comfortable with the new single-hop system 
Speaking as the person who basically instigated this feature: I call
People do stupid stuff.
The work on OnionScan demonstrates that a lot of Onion services are
shoddily implemented.
I am trying to help address that with "standardised build processes" like
But this feature is important to adoption and normalisation of Tor.
If people want to be anonymous, they should not use it, and likewise they
should not fuck up their installation to leak hostnames and IP addresses.
Let's fix that latter problem, rather than treat users as ignorant fucks
who can't be trusted not to enable a feature.
Otherwise, go work out how to ban "rm -rf /" - first.
    -a

@_date: 2016-12-21 14:33:09
@_author: Alec Muffett 
@_subject: [tor-talk] Not comfortable with the new single-hop system 
For clarity, I'm not a member of the Tor project.  I am independent,
freelance, and I am very good at what I do.
And I do own my words, but for the sake of this posting I'll try to be
"polite". In future you might want to block me, just in case.
To reiterate my position, in case you missed it:
Thesis: we should not treat users as stupid.
Second, as someone who firmly believes in Murphy's Law, I share the
So, that means you are happy to implicitly denigrate the intelligence of
people who run onion sites, yet not happy to have that perspective itself
called prejudicial, stupid and daft?
As I say above: I feel we should work from the assumption that people are
smart until proven otherwise, and that includes those who set up onion
If it proves to be a problem, later, then fix it, later.
   -a

@_date: 2016-12-22 07:50:38
@_author: Alec Muffett 
@_subject: [tor-talk] Not comfortable with the new single-hop system 
I know. Check my resume, I worked at Sun, and I was literally part of that
We decided that although you could detect someone doing something wilfully
dumb (rm -rf /) you could not, because of shell expansion, not block
something very, very similar (rm -rf /*)
We chose the mitigation to be the lightest possible block against
stupidity, akin to what Tor are doing with "you have to enable two options
to prove that you really, really mean to do this".
The OP's concerns were not frivolous.
I concur, they are not frivolous, but they were/are perceived
disproportionately, with consequent over-mitigation being proposed.
But from other posts on this thread it is obvious (IMHO) the developers
    - alec

@_date: 2016-12-22 11:54:19
@_author: Alec Muffett 
@_subject: [tor-talk] Not comfortable with the new single-hop system 
Hi Laureai,
This is a server-side feature, not a tor-browser feature, so will not be
seen nor touched by >99% of Tor users.
The default is "off".
The enablement means hand-editing a flat text file, and adding two,
separate, magical commands to it.
Both of which are named to suggest "this feature is about making the
webserver that you are about to set up, somewhat less anonymous".
And which doubtless will be documented as such.
Given this, I believe that the bigger issue, server side, is highlighted by
Sarah Jamie Lewis' tweetstorm, earlier today, which I highly recommend,
albeit a long read across multiple tweets:
    ...that a substantial, perhaps overwhelming, source of security risk is
from people using software in "default" configurations.
Rather than "extremely non-default configurations" as above.
I am trying to help fix this latter issue.  Would you like to help, assist,
or provide aid, and thereby benefit the people who use Tor?
For instance, set up an onion site using the above basic configuration, and
test it?
    - alec

@_date: 2016-12-22 14:13:11
@_author: Alec Muffett 
@_subject: [tor-talk] Not comfortable with the new single-hop system 
By default, users will be installing a version of Tor which can be
configured to run single-hop onion services. Alternatively, there could
be separate versions. Perhaps someone could explain why that option was
Perhaps first someone should establish the case for a substantial downside
that would warrant shipping something separate, complex and fiddly.
- a

@_date: 2016-12-22 16:23:40
@_author: Alec Muffett 
@_subject: [tor-talk] How hard would it be to copy an onion address? 
Very hard. There is a nice explanation at
(part 3)

@_date: 2016-12-23 11:17:45
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
You'll have to do this after prop224 because of onion key
cross-certifications, so fancy plain OnionBalance "renaming" won't work
(HSDir system is unidirectional).
I did wonder; that said, all the nodes will know about each other, so they
can chat to each other using the existing "worker" onion addresses
I agree with George, this is not good to upload so many descriptors and
building so many useless circuits.
The cheap solution is that that will go away when I can set
NumIntroductionPoints to 1; because the new architecture will _require_ me
to use onion addresses to chat between worker-systems, the number of
descriptors and circuits will be effectively irreducible, thereby solving
George's/your concern on that front, too.
If you want to retain nice "hole punching"
feature of tor why not just use backchannel onion service (one per
cluster) for this? (Donncha actually mentioned this in the docs)..
Oh, I shll use the backchannel onion addresses.
What I regret is that the new architecture significantly complicates the
"background" communications for such a deployment, turning it from:
  "scrape the available worker descriptors and publish a master descriptor,
carry on regardless"
  "an n-squared mesh of daemons which have to communicate with and
authenticate to each other using an application-specific protocol, as well
as maintain some kind of consensus of which workers are alive, which are
temporarily or permanently dead. Also: shared consensus protocols suck."
In short: not being able to use the HSDirs as a "source of truth" makes the
whole architecture a lot more complex and synchronous.
That's why.  But, y'know, if it's inevitable, it's inevitable.
    -a

@_date: 2016-12-23 11:25:21
@_author: Alec Muffett 
@_subject: [tor-talk] Massive Bandwidth Onion Services 
"an n-squared mesh of daemons which have to communicate with and
authenticate to each other using an application-specific protocol, as well
as maintain some kind of consensus of which workers are alive, which are
temporarily or permanently dead. Also: shared consensus protocols suck."
In short: not being able to use the HSDirs as a "source of truth" makes the
whole architecture a lot more complex and synchronous.
I suppose a possible solution is for the nodes to check their current
introduction points into a shared 'git' repository, which they then
cross-sync with all the other nodes, along with a shared understanding of
the content to be served from any given one of them.
It'd work, but it's still a kludge, and will have a lot of frequent
node-to-node fingerprintable "chatter"; the old-style method using HSDirs
is a bit more like a dead-drop.
    -a

@_date: 2016-12-27 23:06:00
@_author: Alec Muffett 
@_subject: [tor-talk] 33c3 and tor? 
Julius reserved a workshop room for us on day 2, from 21:30 to 23:00,
in Hall B:
Just to disambiguate, since CCC number their conference days from zero: the
time of the Tor meeting is Wednesday (upcoming day) at 9:30PM.
    -a

@_date: 2016-10-01 06:36:12
@_author: Alec Muffett 
@_subject: [tor-talk] How to  (Was:  Tor and Google error / CAPTCHAs.) 
Watching the thread that leads up to this message puts me a little in mind of the Seligman experiments[1] on "learned helplessness" - but with a critical difference:
The similarity is that a series of jolting user experiences have led to a mythology springing up:
    "this is what we observe, therefore this is how the world must work, and it must be intended to be working this way because nothing else makes sense and no other explanation is forthcoming" -
…which leads the the sort of posting that Joe posts above, essentially that some evil gods named Google and Cloudflare have, do and are, arranging for the websites of the internet to be hostile to people who need or want use Tor, by throwing lightning-bolts called CAPTCHAs at them.
The difference is that - as I tried to outline in an earlier posting - all the CAPTCHAs and so-forth are *not* caused by some bunch of omniscient corporate scientist-gods who are systematically applying electric shocks to dissuade people from using Tor, nor indeed is there any kind of permanent and invariant "logic" to the CAPTCHA behaviour which by ritual (enable this, disable that, pray harder, give up Javascript for Lent) will remediate the problem.
(Aside: It's also not typically about Tor users "crashing the site".)
Instead unlike the Seligman experiments, all these CAPTCHA-shocks are mere side-effects of a hodge-podge of code and network configuration, changing weekly or daily as it gets poked and prodded by systems-administration people who are prettymuch-the-same-as-you, their intention being to defend:
- access to their website, and - the data that the people uploaded to it
…from robots, scrapers and "bad" people who hide[2] amongst the "good" or "needful" people who use Tor.
The long-term solution is not to get caught up in a homebrew religion discussing "how to get access to  whilst it is defended by a capricious multi-headed olympian monster named CAPTCHA". The long-term solution is the much harder and slower one of politely making the systems administrators aware that you "would really like to use Tor to access [their] website, please".
I'll admit that this does not help someone who is stuck with the first-order challenge of:
  "I need to get into  to read my e-mail **tonight**    "
...but I believe it will be easier to bear, address, and eventually fix if one stops thinking that CAPTCHA is the order of the universe, and instead that "clearly some person at  is not aware that I want to use Tor to access their site, perhaps it's hard for them to accommodate my wants but if I reach out to them, maybe they can whitelist Tor exit nodes, or something."
The CAPTCHA "shocks" don't need to happen to you - or at least not all the time. A few are okay, that's what they are there for. Try to have a nice conversation with the actual human beings who run or represent the sites you want to access.  Be aware (and unwavering!) that their security mechanisms _can_ be tweaked and adapted - but also understand that the mechanisms are actually there for a probably-good purpose, and you're caught up as a side effect of using the same software that "bad people" also use to scrape their website.
"But hey, isn't that the story of the entire Internet?" :-)
This is how to foster change.
    - alec
[2]for some sites it's entirely possible that there are fewer "good" people who use the site over Tor, than there are "bad" people scraping it, leading to a kind of "hostage" situation.  One way to break that deadlock is to ask site owners to set up an official "onion" site, which - for exactly the same reason of "lack of awareness" - the scrapers are less likely to use, but still enabling the "good" people. One of my personal side-projects is to document precisely how cheap and easy it is to do this, because "a cheap and easy fix" is an attractive proposition.

@_date: 2016-10-01 08:27:36
@_author: Alec Muffett 
@_subject: [tor-talk] An example of scraping and bad behaviour over Tor 
Sharing for context: the article does not clearly say whether this scam was
entirely completed over Tor, or only partially - the "over 200 proxy
servers" sounds like come other proxy network - but it's a fine example of
the sort of thing I have been talking about and what all those CAPTCHAs we
experience are meant to be preventing, in this case: helping scammy hoaxy
e-books on Amazon:
Moore was just one of hundreds of pseudonyms employed in a sophisticated
These books were associated with a publisher's email account used to
These accounts worked together to artificially inflate the number of ebooks
The server hosted a table containing 83,899 fake Amazon accounts (an easy
Not all logins will be successful. Some are blocked or banned. If that
The *downloads would be tunneled over the Tor anonymity network*, masking
It can take just a few days for an ebook to rise up the charts and increase
Has anyone here had a CAPTCHA on Amazon over Tor, recently? This sort of
thing is why...
    -a

@_date: 2016-10-02 22:53:00
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
So do I - totally agreed.
What I find useful when anyone says "We need more of X!" is to ask:
   "How many more of X?  How many X should there be in total? And what
constitutes X?"
If we had only one "single overlay network with exits" (let's just call
this a "proxy-network") - then we would be arguing on behalf of a single
"proxy network" for the whole of the internet.
Tor is a singular easy target painted on the back of the internet.
…I will argue whether Tor is the **ONLY** proxy network for the whole of
the Internet, but I will agree that having a single proxy-network would
make it an enormous target.
Also: if we wanted a single proxy network for the whole of the Internet,
and if
I want more people on them, and by default.
...if we wanted it enabled by default, then what we are actually saying is
that there is a feature of "anonymity " which needs to be
hardcoded into TCP, because everyone would want it for every occasion.
This is clearly a nonsensical position - because I neither need, want or
desire the overhead of anonymity for the vast majority of my bytes - so
  Rewrite > "we need more than 1 proxy network, and want it easily and
widely, nearly ubiquitously available"
So now we need "more than 1" proxy network - but still, how many?
Well, if we had 7 billion proxy networks, that would clearly be too many.
With 7 billion proxy networks, there would be a 1:1 mapping of network to
person.  That would be bad, and would also be the degenerate case of
hardcoding proxy network technology into TCP/IP.
Correlation would lead to deanonymisation if every person used their *own*
So we need between "more than one" (ie: 2) and seven-billion proxy networks.
We still don't know how many.  How can we choose a number?
Well: what's beneficial in anonymity is for people to be lost in a crowd.
How big a crowd?
Well, to pick an arbitrary number, how about 1 million people?
1 million is not terribly big.  If something happens to 1-in-a-million
people per day, it happens 1350 times a day in China, 1200 time in India,
250 times in Indonesia, 318 times in the USA, 64 times in the UK, and so
But still one million people in a network would probably generate enough
traffic to make a stab at effectively burying signals and patterns in
noise.  One million people is a lot of digital poop.
Say we did a straight division, then: the 7 billion people of the world get
split into crowds of 1 million, so we need 7000 proxy networks to support
There's a number. Except: it's wrong.
It's wrong not only because we are pulling crowd-sizes (etc) out of thin
air, but also because of the divide-by-N split, and that not all of the
world uses the internet, but also that most traffic will not need anonymity
(else we'd be petitioning the IETF to amend TCP) so the at-any-given-time
proxy-network-using-communities would be smaller, yet would still need
dividing into crowds of "1 million" or whatever in order to smooth traffic
and bury access signals in noise.
So, to try and accommodate this, we need a fudge factor which (again) I
will arbitrarily guess at between 10x and 100x.
Perhaps only 1-in-10 people need anonymity (etc) at a given time, perhaps
only 1-in-100.  Perhaps even less because of the lack of penetration of
advanced internet anonymity practices into *Darkest Peru and other parts of
the world.
So that means we need between 700 and 70 proxy networks to protect the
anonymity of the world.
It's a wet-finger-in-the-air number, and subject to argument, but it's at
least a ballpark.
And now I start counting: Tor, I2P, Psiphon, TunnelBear, all those proxy
networks which are designed to let people watch TV when they are not in
their home country I am pretty confident that I could count up to 100 of those proxy networks,
which is a number which exists within the ballpark that I calculated above.
So I am not ever going to bitch about how many networks we need to have,
since my guess of how many we "need" approximates reality.
Perhaps instead I could bitch about is 'Market Dominance' of Tor?
Certainly I have seen a lot of that on Twitter lately; folk who worry about
concentrations of power and influence amongst people for whom they did not
vote (...though I am not sure that voting makes it any better, often worse)
So we should take that million+ people that Tor already has, and break it
up in order to foster more networks?
But - given what we wrote above - that sounds counterproductive to our
goals.  We want big crowds of about that size.
So: there are about enough proxy networks, and we should not fragment Tor.
What should we do?
Good question.  My take: innovate and evangelise, stop pretending that
Shoot any user-experience consultants who tell us that people can't deal
with complexity & nuance.
Use & improve Tor for access to Onions & for the clearnet.
Foster & support I2P for... well, whatever I2P is good at. I have no
interest in filesharing and a major valueprop of Tor to me is bridging to
clearnet through exit nodes, having a namespace which intersects the rest
of the web and uses unmodified HTTP - so I've not done more than fire I2P
up a few times. I'd like to go play with it but I am missing a reason to do
Create _new_ stuff.  That'd be superb.  Just don't try to be like the early
Torfork weenies, proclaiming that they would split the Tor userbase (and,
presumably, onion namespace) and that this would be "progress".
Returning to the topic:
No argument there.
 [...deletia...]
Oh, that's *bullshit* - I know you as a serious argumenter in favour of
privacy rights, Grarpamp, so let me respin what you are calling for in
terms that you might revolt against:
 " are creating databases of user interaction
behaviour - your typing speed, how long you take to solve a captcha - in
order to track you and deanonymise you"
The issue is that "authentication" and "deanonymisation" are from many
practical perspectives **exactly the same thing**.
I am with you on "graduated service enablement" as a fine goal - that if
you have only authenticated to a weak level, you should only be permitted
to do less-harmful things; but this again is an area where it is helpful to
shoot the user-experience researchers who tell you that people cannot cope
with a bank transfer failing when they try to do it over SMS but not over
Wifi, from the same app.
These have real and rather unprogrammable / rising costs
To reduce harm and cost, sometimes you will get a little of both.  The wise
company will treat blocking of known-proxy-network IPs differently from
those which are more inarguably evil.
I think you are saying similar things to me, but perhaps from a far more
judgemental place.
You can't blame people who don't know about Tor and similar technologies,
from blocking the IP addresses associated with it.
Solution: make Tor more well-known, and associated with social enablement
and do-gooding.
Chocolate is a good treatment for cynicism.  :-)
They need insider people (the Alec's) at their insider corporate
It's lovely of you to say that, but it's wrong.
Parachuting clones of me into organisations is not what changes things.
That's a quick-fix mentality which will fall short.
The solution - what I have sought to do - is explain to peers what Tor is,
and demonstrate to them with graphs and charts how... yes, there is shit
and spam and scraping which comes through Tor, but there are also these
*other* people who use the service and who need especially it in sudden
rushes when bad things happen, so we need to build things such that
accommodations are made for that.
You have to fix the *culture* and *perception*, not parachute-in a
Muffett-shaped widget.
    - alec
*Paddington Bear. Likes Marmalade. Dislikes Internet.

@_date: 2016-10-02 23:33:26
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
- person uses tor to connect to socks proxy provider
- person authenticates (?) to socks proxy provider
- person traverses socks proxy prover to connect to end service
This sound like putting a condom on top of another condom after cutting a
hole in the first one, but hey, if you think it's worthwhile and fun, knock
yourself out. :-)
The problem is that the SOCKS IP itself may be "bad" (perhaps even as "bad"
Also that.  Basically you are just shifting (say) Google's knowledge of
what IP address from
    "possibly a Tor user, amongst all the spam that Tor emanates"
    "traffic sourced from some shitty open (or authenticated?) random SOCKS
relay, who know what the fuck this is?"
In short, you're making  job of picking out good traffic from
bad, _way_ harder.
But, hey, maybe you're masochistic / sadistic / both?  :-)
Since SOCKS IPs, like exit node IPs, can be selected by country and
Geolocation is not the only the only fruit, in the authentication fruit
salad; I even alluded to this in the original blogpost ("...appears to be
connecting from Australia at one moment may the next appear to be in Sweden
or Canada") - but with this comes the realisation that geolocation can be
faked to appear normal, too.
    -a

@_date: 2016-10-03 08:09:55
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
While outreach and cooperation with some companies may work, do you not
Ah! That delightful old argument.
I've heard it a lot, and I am afraid that it is all of groundless,
incorrect and demonstrably silly.  :-)
In three bullets:
1) If less than 0.1% of the people who use your site do so "anonymously",
the amount of ad-revenue associated with them is negligible. There are
bigger leaks to plug.
2) In my experience the "blocking" that companies do to Tor (and similar)
is 100% grounded in the threats from spam, scraping, testing phished
credentials, and other forms of bad behaviour.
3) I would bet a substantial amount of beer that anonymous proxy networks
are negligible threats to advertising revenue in comparison to "People on
the Clearnet who use AdBlock+".
My perspective:
I've always felt that the "It must be because we are a threat to advert
revenue!" argument is a perfect example of the kind of conspiratorial or
religious-oppression-like rationalisation that I discussed in an earlier
For clarity - though I hope my work to date does not mean that I NEED to
say this - I am NOT saying that people who use anonymity proxy networks are
What I am saying is that anonymous users are, from the perspective of
revenue, negligible in number, and thus arguments that they get blocked for
revenue reasons are utterly specious - i.e.: plausible, but actually wrong.
I will observe that occasionally someone who is responsible for "compliance"
will worry about anonymity proxy networks.
e.g.: the banking industry have this obligation called Know Your
Customer[1] which would make them really fret about Tor, because the
regulator might spank them for recklessness, or something.
People who are responsible for compliance are really good to get "on your
side" if you are trying to make better affordance for Tor within a company:
if you can build a system for them that says:
    "This connection is coming from a Tor exit node, That connection is
...and can surface that distinction to the internal compliance-enforcing
code, they can decide what to do with connections that come from Tor.
This sort of "tagging" of Tor connections may sound counterintuitive to the
Tor community, but the point is that by building such a system you are:
1) enabling measurement[2] of how many people access your site over Tor,
2) for the compliance people you are turning the fact someone is using Tor
from an amorphous "ZOMG DARKWEB MURKINESS SOMEWHERE OUT THERE ON THE
NETWORK" - into a simple boolean signal which they can factor into their
decision matrix, so that they get to keep their jobs when the regulator
asks them "what they are doing about the darkweb which [the regulator] read
about in Wired two months ago..."
Having built this thing for your compliance people, you've also had an
opportunity to explain how important Tor is for people who _really_ need to
access your site, so it could turn into a huge "Block Tor!" thing, but it's
more likely to turn into "let's just switch off the stuff we're worried
about, for compliance reasons" - when someone accesses the site over Tor.
This latter is the kind of "Graduated Access" thing which Grarpamp was
arguing in favour of, yesterday.
    - alec
[1] See: [2] For why this is important, read this:

@_date: 2016-10-03 08:43:29
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Actually, I just want to hammer this point home with a really _large_
a sizable number of sites will always block anonymous traffic simply
When the Tor community go around saying "We are Legion - Innumerable, and a
Threat to Advertising Revenue!"
...the truth is, folks:
a) Tor Users Are Totally Numerable - exit nodes are well-documented, sites
can just switch off ingress from Tor if they want.
b) Tor Users Are Not A Threat To Ad Revenue - because there are relatively
so few* of you.
However: if you Tor users go around _claiming_ that you are a threat to
ad-revenue, then memes like that get recycled and turn into Wired and
DailyDot articles which are read by "regulators" and "compliance officers",
who will then recycle this understanding as harassment or blocking of Tor
users, on the grounds of _zero_ evidence.  This is unhelpful.
The world's mindset needs to be shifted:
1) Tor is a network used by a few million people - which, globally
speaking, is a small number - to access sites with some degrees of extra
anonymity, extra integrity, extra certainty, extra security.
2) It's an open network, so quite a lot of bad stuff flows through it too;
but then that's true of the Internet at large, so the only possible real
distinction is the density ratio of "badness" to "goodness"
3) If people criticise Tor because of that ratio, the easy solution is to
help more "good" people use Tor.  :-)
So stop talking about Tor in terms of it being "A Strike Against
Corporatism / Advertising / THE MAN !".
Really, in that space, Tor is a drop in the ocean, and you are just scaring
a bunch of non-geeks from whom you will subsequently try to demand special
treatment for Tor in some weirdly co-dependent fashion.
Talk about Tor instead as an "enabler" of "better than HTTPS"
communication, especially for people who are in need, or who face
communications blocks.
    - alec
*but so important? :-)

@_date: 2016-10-03 14:12:27
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
This thread/discussion/response is getting very fragmentary, so pardon if I
slash-and-burn a little to try and restore a theme:
So this is about proxy networks, their number, quality, diversity, function
and purpose.
To a first approximation I am in favour of maximising all of those, but
practically I feel that that's a foolhardy proposition - simply, my Netflix
viewing, or whatever, does not need to be anonymised.
It _might_ benefit from a VPN (because see all the stories of ISPs choking
bandwidth where they are not receiving a kickback) - but in such
circumstances I'd prefer the solution to be "choose an ISP who are not a
bag of dicks, and 'out' their ill-behaviour as much as possible", rather
than to re-engineer the internet for this edge-case.
Generalising: more tools, please, but let's not pretend that more than a
fraction of bandwidth will benefit greatly from anonymity technologies.
I kind-of agree, but I'm not able - on grounds of pragmatism - to tell the
world "DUMP EVERYTHING AND USE THIS NEW SUPER-PRIVATE SOFTWARE, MAYBE IT
WILL MAKE YOU THINK DIFFERENTLY"
My preference is "HERE TRY THIS NEW SUPER-PRIVATE SOFTWARE **AS WELL**, IT
HAS COOL BENEFITS AND MAYBE IT WILL MAKE YOU THINK DIFFERENTLY"
Agreed, that's why I mentioned it.
But for the 98% of the time that I *am* in the UK, I don't need (nor want)
to take the performance hit of the BBC's geolocation-restricting firewall
I was thinking about that, having pressed "send".
It seems logically impossible, as well as unwise, to try and say "Tor is
over-full, go use AltTor" when one of the key points of Tor is to strip
identity; after all, whom will you identify to tell this too, and how?
I feel that we just have to let market demand, and ability to scale &
deliver value, be the deciding factor in what is available...
...as opposed to pretending that some manner of centralised policy,
doubtlessly run by a cabal of people of impeccable ethics, is either of
possible OR desirable.
Yeah, I saw the numbers.  Tor wins.  I suspect that exit services are not
I2P's main value proposition?
Good clarification.
I am _very_ glad that the IETFers who argued against ".onion" and said that
Tor somehow needed to become a "scheme" (eg: "onions://foo.onion/") were
My take on the whole matter is "just because Tor Onionspace is not based
upon DHS does not make the HTTPS protocol/scheme any different"
However, do be alert: some folk in the IETF are still not content with that
[on forking]
Oh, I can complain. :-)
Being independent from both parties I am free to characterise the
indieonion brigade as a bunch of pseudo-student-radicals bent on trolling
the community a-la Gamergate.
If eventually it turns into a wholly new privacy technology or a separate
and compatible Tor implementation that would be great.
But it should never be pretended that it started as anything other than a
tantrum by a handful of marginally-post-juvenile twerps, butthurt about
Tor's internal "drama" and threatening to split the Tor network.
It's that latter bit that I _really_ did not like. Make things better, but
don't fuck with the infra, and don't split the userbase.
We know the whitepapers tell us some of these systems have
The reason I wrote "that's bullshit" is because one moment someone is
calling for more anonymity - even being hardcoded by default into the
network - yet the next moment the same person is castigating (?) the
platform providers for not bothering to apply all the possible signals and
technologies at their disposal, to track, deanonymise, and even merge
multiple identities ("User 476 types in exactly the same way as User
9945!") - in pursuit of authentication.
A huge chunk of the people on this list, I aver, would be totally
freaked-out at the suggestion that what is needed is a _more_ comprehensive
approach to platform identity.
Almost.  It depends on the "perspective", and "intent".
Unfortunately, as I have seen first hand, you can build a tool "for great
good!" only for a bunch of privacy activists to say "ZOMG THIS IS CLEARLY
AN ATTEMPT TO DEANONON THE TOR NETWORK AND ADVERTISE TO THEM"
Privacy activists can be total assholes sometimes.  Me included.
[On User Service]
"Helpdesk", he says. Ho ho ho ho...
I'd suggest the
True, and - to reassure you - spamspotting is already often based on more
than just IP address/block/ASN.
It's a source of constant amazement to me that folk believe this stuff is
not already being tried - or got tried and then replaced by something
If the CIO/CSO/CTO, even on down into the techs, at any of these top N sites
Then you'd be firing some of the best netops and sysops people in the
world, merely because they believed the things that the media have told
them about "the dark web".
Frequently it's not "onboarding" - the spikes are largely people who -
faced with a sudden network block - fire up a tool (Tor Browser) that
bypasses the block and gets them to the site on which they are *already*
registered and which they *want to use*.
Then when the block is finished, they go back to Chrome or whatever their
preferred browser is - the one with session-cookie persistence, with Flash
support and great for playing music and porn.
The majority of users will use Tor "at need"; a relative minority use it
    -a

@_date: 2016-10-03 16:16:12
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Let me make an even more generalised statement:
"There are centralised databases of {many IPs or Subnets which appear to
emanate badness, aggregated across the experiences of many companies}"
Here is just one of many, and this one at least is open to participation:
- and there are many more, often closed.
Now my suspicion is that you will say: not the point. People will be
Given the assertion I make above, and the evidence that I provide for it, I
believe my responding to this is moot?
I daresay it checks _some_ of them, I don't know how often, plus - you know
- companies are at liberty to do their own tracking of badness and come to
their own conclusions.
I've occasionally found proxies that are 100% clean. Yet still I get asked
Yes.  I've been trying in the last few emails to dispel the notion that
there are any "hard and fast", absolute, 100% correct for all time, rules
about this sort of thing.  Evidently I have not yet succeeded.  :-)
TL;DR : it is pointless and verges upon stupidity to attempt to draw
conclusions about spamfighting behaviour on the basis of
small-to-even-medium-size amounts of experiential reporting.
Perhaps you were carrying around a tainted cookie from some previous
Perhaps one of the systems "burped"?
I cannot tell you why, and without a reproducable case the platform in
question probably will not be able to tell you why, either.
...and that's my point.
You/we are all speculating.
Why bother?  Speculating will not actually change anything.
If we want to have a better experience when using  over Tor, what
needs to happen is for  to:
a) learn to value the people who use  over Tor, and...
b) do some work on behalf of the people who use  over Tor
All that speculation does is stoke the whining and pseudoscience of "enable
this, disable that, stack a proxy atop/beneath the other, it must be
something to do with geolocation".
Perhaps it _is_ something to do with geolocation - today.   Tomorrow it
might be something else entirely.  On wednesday perhaps a gang of Ukrainian
scraper-noobs will burn your favourite SOCKS relay and it will go onto the
"naughty list" for a month, as a result - and then another company might
take a copy of that part of the IP reputation database and sell it to a
bunch of banks and newspapers as "fresh security data" for six months,
propagating the hassle.
So: do you want to waste time speculating about who told what, when, and
why, and reverse-engineering flaky databases of IP reputation and
Or would you prefer to work to get better access through engagement?
The latter strikes me as far more constructive.
    - alec

@_date: 2016-10-03 19:43:33
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
My perspective is that blocking in the context of Cloudflare is not an
"accident", nor is it exactly "intentional".
CF are acting in what they rationally believe - and I can follow and
understand their logic - that lots of scraping (etc) badness emanates from
Tor, and so they have taken to offering, "by default", some manner of
protection from that badness.
CF have many smaller customers, few amongst whom will be *aware* of Tor,
and few of whom will be substantially impacted by CF impeding access to the
tiny numbers of legit user(s) who might want to access them over Tor.
So that's why I argue that even CF blocking is borne out of the world of
spamming and scraping.
What will lead folk to switch that _off_ is awareness.
Absolutely.  Exactly that.
CF (and any other company) as a hosting-provider is somewhat divorced from
the signals that would allow them to separate "bad" scraper/spammy Tor
connections from "good" Tor connections.
This all is an unfortunate side-effect of sites choosing outsourced layer-3
network frontends and/or site hosting without bundled-in site management &
related layer-7 logic; traditionally hosting providers have added basic
blocking of "bad" connections by equating "badness" to "IP address" - but
since that's the one thing which Tor eliminated, the hosting providers are
lack a grip on the badness and so are blocking Tor wholesale.
Totally.  :-)
Correct.  One can enumerate the exit nodes from Onionoo quite easily, but
making that information dynamically available to your web fleet, and
integrating it into your business logic, is trickier.  :-)
    -a

@_date: 2016-10-03 19:49:28
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
But, for a practical non-future-speculative use-case: what if you're
What's interesting to me here is that you've phrased this as a "what if"
question - and, my answer is, "if I want to do this, I will fire up Tor, or
some other Proxy Network"
I _really_ hope that people are better than thinking this is an
all-or-nothing, zero-sum game; that for one network protocol to live, the
other network protocol must die.
We can (and do!) have _both_ anon, and non-anon connections.
And that's _okay_.
That would be coolness.
So: who knows someone at Github to approach about setting up an Onion site
for them?
I'll help.  :-)
    -a

@_date: 2016-10-03 19:59:34
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I can see that - I am very much in favour of not collecting (and therefore
not having to store or scrub) data that one does not actually need.
That said, I am also a big fan of comprehensive debugging information for
working out what the hell went wrong, and so I can see the applicability to
mobile comms of location information.
And then I am aware of some of the asinine requirements for geolocked DRM
which some media organisations (eg: the BBC) place upon their viewers &
listeners, often for legacy reasons.
And I really don't know how to square this quadridimensional oblate
spheroid of conflicting wants, desires and contractual and legal obligation.
So when it comes down to cases I just try to do my best if I am designing
the architecture, and (where I am the customer) I try to avoid services
where I consider the data collection to be egregious - and I tell my
friends to do the same.
I wouldn't want to concede that it's appropriate that all of that data
Yep.  This is the point that I have made in past - when lecturing about
security opsec - that the GPS that helps you drive home is also the one
which has a history feature that will tell your spouse where your you have
been meeting your lover, and will be produced in evidence at the divorce
It seems that people often discover later on that they wish they had
I think you'll find that's a standard in history textbooks. :-)
    -a

@_date: 2016-10-03 20:08:00
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Yes there is, so long as the result does not suck.
If the result sucks (significant dropped frames, spending minutes futzing
with NoScript, whatever) then the user experience will be a net negative to
Tor, because people will say "Tor can't do X" rather than that they were
not capable of doing it.
This is why (for example) I posted videos of myself streaming HD Video over
Tor, to help dispel the old myths about video over Tor:
    Irregardless of the political and privacy issues there are also technical
Totally.  So many people are fixated on "anonymity" and completely ignore
the end-to-end nature of Onion addressing, for instance.
It's a fantastic enabler of high-integrity communications.

@_date: 2016-10-03 23:41:44
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Here's a picture of me loading Google over Tor:
  That much works. A narcissistic self-search subsequently crashed, very
  And doing "New Identity" led to a CAPTCHA; but the subsequent "New
Identity" worked just fine.
Analysis: I agree, Google have some work to do. :-)
with the IPv4 address space becoming over saturated. Its not a Tor only
My memory is that that was no longer the case any more, at least for any
sizeable country.  Certainly it _used_ to be true.  Have you a recent
citation, please?
Yes. Indeed, the two may be in opposition.
Get people to change the logic by which they are assessing the situation?
It's a bit of a Kobayashi Maru cheat, but it's what needs to happen in a
no-win scenario.
    -a

@_date: 2016-10-04 08:01:09
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I like to look at Onions from the perspective of a network engineer:
- it's a lightweight near-equivalent (and in no way as powerful as, but
hey, it's an 80% solution which requires zero setup) to Layer-2 / IPsec
- this means it operates and is available at the "Link Layer" and is
inherited by any protocol which uses it, including plaintext HTTP,
plaintext Telnet, etc
- In IPsec AH means "Authentication Header", extra metadata that IPsec
sends, using certs and keys and shit, to guarantee that you are talking to
the machine that you asked for
- In Onion, if you can type in the address and get connected, you are
talking to the machine that you asked for
- In IPsec, ESP means "Encapsulating Security Payload", extra metadata on
the packet which stops people tampering with, or reading the packet
- In Onion, all that shit comes pre-packaged from Tor, with zero user setup.
- Onion also routes around blocks
So my position is that Onion routing is "cheap-ass IPsec, without all the
configuration BS, and *yay* with E2E/disintermediation".
That is _really_ cool; at a stroke you selectively pypass a bunch of
internet balkanization technologies and reconnect people like it's 1990 all
over again.
I'm old enough to remember when `finger username at host.subdomain.tld`
actually worked and was useful; there's a lot you can build with that kind
of connectivity.
That's also nice.
Could do that, but then you'd just be reinventing IPsec-like features at
layer 4, rather than at pseudo-layer-2.
I shall elide your other question, because - as should be obvious by now -
I rate Onions highly for qualities  other than the "anonymity" and
"location hiding" - which are obviously very important to other people.
    - alec

@_date: 2016-10-04 09:01:27
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Rolling together a couple of Joe's emails…
Not at all costs, but I believe I've done a fair job in previous mails of
explaining how they might consider it to be uneconomic to add "special
case" code to reduce friction for Tor users.
If you've not read those yet, let me know off-list and I will send you the
text again, rather than repeat it.
If you're trying to sell that Tor isn't blocked because it's Tor, that'll
I don't actually understand this sentence.
1) I have explained at length that Tor is one technology - a big one - from
which spamming and scraping emanate and attack platforms/their users.
2) To defend your platforms you block the spamming and scraping, therefore
you block Tor.
3) If you're at least vaguely sympathetic to legitimate people who want to
access your site over Tor, you try and give them a filter bypass so that
they can continue to use your site over Tor.
4) But there are very few of them, and you probably don't understand Tor
very well, so you probably make an inferior / poorly-integrated job of it.
Hence crappy CAPTCHAs.
5) Thus arises the current situation, today.
I don't believe that this is hard to understand?
If you're trying to defend Google and their colleagues' wonderful, law
Why do I get the feeling that you are trying to haze me, or chide me, or
warn me off-of having opinions other than whatever ones you currently hold?
Apart from anything else, that would be a recipe for groupthink - and
groupthink is a bad idea.
All companies do bad shit.  All companies do good shit.  Sometimes they
manage to achieve both simultaneously. But if you want to take this thread
and digress into an argument about which company did what and when, and if
you want to try coercing me into doing so, then "fuck you" because I am
here to talk about Onion sites and setting them up, and all the wonderful
technological things that they can do - things which y'all on the tor-talk
list have completely missed for the past decade whilst you've been focusing
monomaniacally on "spooky things".
Yes, that's because you are possibly paranoid, thinking that companies are
"Out to get Tor Users!" - it's entirely possible that CloudFlare have been
through a phase like that, but I feel that's understandable if/when parts
of the Tor community have been producing "Fuck Cloudflare" badges.
They are called "session cookies" and if you have a few dozen, a few
hundred, a few thousand machines which coordinate in order to send you
webpages in a timely fashion, the session cookie is the thing which
stitches this all together.
It's a bit like the ignition key for a car.  You could create a car which
would "just go" whenever someone is sitting in the driver seat, but then
the car would have no sense of ownership.
Most people want to stay in charge of the resources they own, so they
demand a sense of ownership, so they need a key.
And - like some vehicle manufacturers - if one complains "we can do so much
better than cookies/ignition-keys nowadays" - the answer is "yes, but
that's not how we got here, and a lot of folk are simply geared-up for
Could it be that certain tracking - not just on that domain - won't work
I love your habit of using interrogatives to convey conspiracy.  :-)
When cookies are enabled and resources are fetched from a site, then yes
they can become ad-tracking and so forth.
That is how that mechanism works.
A huge bunch of people consider it to be entirely legitimate, or if not
that then at least a minor nuisance / the cost of using the network at all.
One may scream "ZOMG CORPORATE MONETIZATION!!!" but a lot of sites are
getting more frank about saying "Look, we are funded by advertising, please
turn off AdBlock or we will just quit and give up" - and so I whitelist ads
on those sites, even if it means digital breadcrumbs about where I have
been are available to the NSA or whomever.
Seeing adverts is the quid-pro-quo of that's site's existence.
One may then say "YES BUT EVIL TRACKING... FACEBOOK... GOOGLE..." - and,
okay, if you are worried about tracking on random sites, install tools like
PrivacyBadger and *manage* your risk.
But cookies are not inherently evil, especially "first party" cookies which
enable "the site you are currently looking at" to function.
Third-party cookies are arguably more evil, I agree.
...and in fact, if everyone uses them, the site may eventually cease to
exist. :-)
See my explanation at the start of this post; you are trying to pull off an
astonishing feat of mathematics, suggesting that Tor users are a
substantial number of people who are blocked because Tor is classified as
an ad-blocker (hint: no it is not) and a lot of ad-blockers are blocked
(hint: mathematics does not work that way).
2) In my experience the "blocking" that companies do to Tor (and similar)
No, I am not saying that, and before I disassemble your argument with
examples, I am interested to know why you are asking this?
Oh - wait, I think it's because of your understanding of how commutation
works.  Let me see if I can simplify a bit:
1) About 100% of blocking that happens to Tor users, happens because Tor is
perceived or experienced to be amongst the sources of spamming and scraping.
2) This it not a claim that 100% of spamming and scraping comes through
Tor.  That's what "amongst" implies.
3) Phrased differently: the blocks and CAPTCHAs that Tor users experience
are "collateral damage" in the war on scraping and spamming.
Is that clear?
I promised examples; here's a couple:
Within Tor: TBB is not the only route for spamming and scraping which comes
through Tor; in fact quite a lot of the spamming and scraping which comes
through Tor is sourced from "curl"
Outside of Tor: there is this stuff called "Browser Malware" which is
implemented in (say) extensions for Chrome or plugins for Firefox, on the
clearnet.  Normal people get scammed into installing the malware because of
some value proposition ("It turns your webpages pink and animates happy
bunnies at the bottom of your screen" / whatever) - and then proceeds to do
low-impact scraping and spamming in the background, whilst the victim gets
on with using their browser. Essentially it's a parasitic browser extension
which turns the victim into a source of scraping, which then gets detected
by {Google, Facebook, whatever} and subjected to an block until the victim
cleans up their system / extensions / whatever.
That other browsers can't be or aren't adapted by skilled users for similar
I think I just gave you a concrete example of that, without any of the
insinuating, conspiratorial questioning.  :-)
I'm really getting the impression that what I have been explaining to-date
has not really been clear enough for you.
Perhaps this posting can/will help?
I understand what you're saying.
Your privacy is clearly a very important matter.
    - alec

@_date: 2016-10-05 09:15:48
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I'd be interested to see you continue / expand upon how you believe this
would be manifest / what this would do and how it would be achieved.
As it stands it's an suggestion (?) that's widely open to interpretation
and thus flaming arguments; I'll restrain myself from dropping a couple of
examples until hearing back... :-)
    -a

@_date: 2016-10-05 13:11:20
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Mirimir: Generally I like your suggestions, they are thoughtful, and I
think you're shooting in the right direction.
A few observations:
a) I like the idea of Google giving you "one free search" and from that
trying to determine whether you are an "asshole" after which it lightens up
with the oppression; the challenge here is that "one free search" is easily
exploitable by the "League of Assholes" who will create a vast army of
"apparently-noob-non-assholes" and aggregate across their free searches in
order to perform the scraping/searching/spamming that they desire.  (Yes,
even search results are interesting to scrapers, eg: using the Google cache
to mine e-mail addresses from some third-party website which provides open
access to GoogleBot but not to normal people.)
b) I am not familiar with Wilders (?) but it sounds quite intensively
moderated, something that perhaps Reddit also pulls off to some extent.
Communities can be self-policing (see also Wikipedia) but not all
communities offer a value proposition where self-policing would be a
complete solution, eg: I feel that Reddit is far less family-friendly than
FB.  Also, at FB-like scale, self-policing would be really challenging.
c) Further, "graduated access", where the tuple of {you + the means which
you use to access the site} gaining privilege by being a good community
member?  That's great, though it is open to "identity farming" and "what
happens if/when community members who validate the new members, themselves
go rogue?" - "quis custodiet?", and all that stuff.
d) Finally, to get technical, I like these ideas but I see the challenges
of convincing social networks to implement the code to support such
graduated access, and to factor in the use of Proxy Networks such as Tor,
will require greater awareness of how to address issues b) and c) above,
plus for Tor to be popular enough that folk consider it worthwhile to
address in this way.
This all strikes me as a massive bootstrap challenge.  Not impossible, but
hard and long-term.
    -a

@_date: 2016-10-07 13:00:57
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Amplifying just one little bit of this:
I would phrase that as "Why _should_ sites care about the _definitely_
small share of users who want pseudonymity or geolocation-neutral access?"
With the FB Onion the argument was simple: "there are a lot of such people,
they are at the mercy of sketchy exit-nodes, and we can make people happier
and give them a better service for a small expenditure."
For smaller organisations, especially ones with less-good stats and
less-good resources, to attempt to metaphorically beat them into submission
Yes, but to be fair, that information is wanted on the clearnet site also.
To FB the Onion is just another form of access: HTTP(defunct)
HTTPS(default) and Onion(new hotness). :-)
Oy veh.  Well, at least it sounds like civil discussion of the actual
     -a

@_date: 2016-10-14 13:32:39
@_author: Alec Muffett 
@_subject: [tor-talk] Tor DNS Deanonymization 
can be deanonymized through their DNS lookups. Is this something I should
be concerned about?
That is an excellent question! What are you doing, and who are you afraid
of?  :-P
    - alec

@_date: 2016-10-19 07:19:20
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I would not disagree; but - and pardon me but I thought we had already
discussed this aspect - I likewise would not impose the overhead of trying
to maintain anonymity upon all the people of the world "just in case they
need it".
We're just here to make more and various forms and levels
Yep.  And add into the mix that I am trying to do those, plus all the other
stuff that Onion networks are capable of but everyone else is too hung up
on anonymity to notice, e.g.: greater integrity and assurance.  :-)
    -a

@_date: 2016-09-17 21:50:17
@_author: Alec Muffett 
@_subject: [tor-talk] New Document: Building a "Proof of Concept" Onion Site 
One of the questions I get asked lots is "How [do I] set up a Onion site to
be an Onion equivalent to my [normal WWW website]?"
Some people call these "onion mirrors" or "onion copies" of [a website] -
but I feel that those are narrow, perjorative and incorrect descriptions.
Some websites you access over HTTP, some over HTTPS, and nowadays some of
them you will access over Onions.
So, for me these are "Websites accessed over Onions" - or simply "Onion
Sites" because I am old enough to remember "$PROTOCOL sites" for values of
$PROTOCOL including: https, http, gopher, ftp...
But how do you set them up?  It's pretty simple and I am documenting and
testing a process.
The first of (probably several) documents has been posted at:
...and has been written for people who are comfy setting up a simple Ubuntu
Server instance in a virtualised environment or on (say) AWS.
The goal of the document is to build an educational, non-production-quality
"playpen" onion site which uses "man in the middle" request-and-response
rewriting to make a normal website available as an onion site; this is not
a technique that I would typically recommend for production use* - but it's
great for messing around, learning, and starting to see how Onions are set
If you like the document and would like to see some prior discussion and
diagrams, read also:
    My next document will probably drill into the matter of how subdomains work
with onion sites, and how to modify the process from the first document to
experiment with supporting multiple subdomains.
    - alec
* although last time I checked I think, Ubuntu and perhaps ProPublica were
doing this, but rather more professionally than I describe here.

@_date: 2016-09-19 14:57:56
@_author: Alec Muffett 
@_subject: [tor-talk] is it me or did tor talk get really quiet? 
I think it's awesome, to the point where I've actually resubscribed.
It's nice to have a maillist which is about the topic of Tor, rather than
filled with conspiracy drama.
Now maybe I can contribute without fear of being swamped in ad-hominem
    -a

@_date: 2016-09-24 15:21:54
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
That's a good question; I don't know about Google specifically, but when I
was at Facebook the most common Tor-exit-node-related problem was called
Scraping was/is when people with bad intentions hid behind Tor in order to
disguise attempts to access and copy people's public pages, looking for
personal information (names, addresses, pet names, emails, anything...)
which could be correlated somehow and monetised, eg: via phone fraud or
Tor is useful to these people because if they were making such access
attempts from a single IP address, or a single subnet, it would be easy to
track and stop them.
So "scraping", along with other/similar reasons, is why tor exit nodes have
such shitty "IP Reputation" in the tech industry.  The Tor exit nodes hide
a bunch of people who are doing scraping.
Of all the big companies in tech, Facebook probably has some of the
theoretically easiest challenges of addressing scraping - because quite a
lot of content is only available when one is "logged in" to Facebook, so
instead of blocking IP addresses Facebook instead can block _accounts_ that
scrape; however that is not a panacea and fighting scraping at Facebook is
still a _massive_ task.
By comparison Google may have a even harder challenge to combat scraping
because much of Google content is meant to be available without logging-in,
therefore Google rely more heavily upon IP-address as an identifier.
Continuing the spectrum - Cloudflare have an enormously harder challenge
than Google, because they are mostly supplying only "network-level"
services to their customers, so lack knowledge of username, userids, and
(most?) cookies that actual platform-providers might be able to use when
fighting scraping.
If you correlate this spectrum with "corporate friendliness towards Tor", I
think you will see a causative pattern emerge; Tor does great work in
enabling access to these services and platforms for people in need, but it
also serves to hide/enable scrapers and other malfeasance. To not recognise
this and instead (for example) to violently beat-up Cloudflare for
"blocking tor" serves only to entrench anti-Tor sentiment.
This is why a few months ago I wrote a blogpost[1] explaining how best I
believe to get more companies to be friendly towards Tor.
Because any amount of denial, public raging and placard-waving is not going
to help.  It needs outreach.  It needs mutual understanding and
communication of benefits.
    - alec

@_date: 2016-09-25 19:14:42
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Scraping comes in many forms and with many motives and intentions - in the
previous email I managed to outline a couple, but that is no more than a
sketch of one aspect of the topic.
Scraping also raises interesting legal arguments, both pro-and-con - for
* ...and Weev:
...and of course, Aaron Swartz:
* ...so when I say "many forms and with many motives and intentions", I must
acknowledge "dual use" - that some forms of scraping are benign, or are
protest, or are sharing that which perhaps should be shared.
But here, primarily, I am discussing the forms of scraping which are
third-party-based and exploitative of user data with intent to defraud; or
BTW: are you the Alec Muffett name-checked in Kevin Mitnick's
Yeah, that was a long time ago. :-)
Indeed, that's possible; in fact I should amend my previous post to point
out that "scrapers" - people who scrape - do so through many different
proxy networks, not only Tor, and also that some forms of scraping utilise
(eg:) malicious browser plugins that are installed by otherwise entirely
blameless people: victims who don't realise that their web browser is now
helping a part of some scraping outfit's infrastructure.
You ask an interesting question about "badness" of IP addresses; long story
short what you are referring to are "IP reputation databases" - which are
used by many people, for instance:
…from Claudio Guarnieri ( is a list of IP-based Snort IOC
(Indicator of Compromise) rules for civil society organisations to use.
tldr: If your organisation sees network traffic matching the list of IOCs
on your network, bad shit may be happening to you.
Speaking generally about industry rather than specifically about FB or any
other company: there are only (worst-case) 4 billion IPv4 addresses in the
world (and a few more v6) and since the average hard drive is ~1Tb nowadays
it's pretty trivial to build & share databases of how much "badness" is
measured to be emanating from any given IP address.
So that's what tends to happen: it's not (necessarily) a matter of what
kind of software the computer is running (though that is helpful to know) -
nor would it completely matter what country the computer appears to be in
(though some countries _are_ more lax about quenching bad network
Instead it's more (though not exclusively) a matter of measuring actual
observed behaviour emanating from given IP addresses.
What happens *after* such information gets collected is more interesting;
some organisations call for network "shunning" a-la redlining (
 - others enforce CAPTCHAs on IP
addresses which are known to enable scrapers.  Yet more do rate-limiting or
temporary bans.
An organisation's response to scraping seems typically the product of:
1) the technical resources at its disposal
2) its ability to distinguish scraping from non-scraping traffic
3) the benefit to the organisation of sieving-out and handling the
non-scraping traffic, rather than ignoring it all
I would argue that Facebook was the first to launch a really large onion
site by scoring highly (HHH/HMH) in all three of these categories: big
brains, actual high-signal login credentials, and a million normal people
who want to use Facebook over Tor (especially "at need").
By comparison I would estimate Google as HMM (or HML) and Cloudflare as
HLL; both companies with great people (I know many of them) but with Medium
or Low abilities to sort scraping from non-scraping, and Medium or Low
impetus to do so.
This is why corporate outreach is so important for Tor: to build awareness
and raise perception so that that third statistic becomes more important
for other companies to address.
    - alec

@_date: 2016-09-26 09:14:36
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Lots, some commercial, some open-to-anyone-who-appears-legit.
    -a

@_date: 2016-09-27 08:39:43
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
You know, I would never phrase it that way, but in some respects I agree
with you.  I'll explain...
I mean, provide real data showing that it's intolerable and
It's not right to conflate:
    "their infrastructure can surely handle it!"
    "they cannot be bothered to sort the wheat from the chaff!"
...but the latter is a lot closer to the truth than the former, and I find
it regrettable.
Let's do some back-of-the envelope maths: I have no idea of Google's
statistics but if 1 million people use Facebook over Tor, and Facebook
serves 1.7 billion people, then the Tor-using population of Facebook is
  ( 1 million / 1.7 billion ) * 100 = 0.06% (rounded up)
...of the userbase.
To put this into context, imagine a vacuum cleaner, and a bag of dust it in
is about 1.5kg / 3.3lbs; then put a single grain of rice into the bag
(1/64g) -
  ( 1 / ( 64 * 1500 ) ) * 100 = 0.001%
So globally per capita, the overall percentage of people who use Facebook
over Tor would be about 60 grains of rice.
That's about a teaspoonful of rice in a vacuum cleaner.  Have you ever
vacuumed-up a teaspoonful of dropped rice and not bothered to pick it out
of the bag?
You have to really _care_ about that rice, care about those users in order
to want to do that.  It's not economical behaviour.
But the situation is actually _worse_ than this, because the vast majority
of "legitimate" traffic does not pass through Tor en-route to Facebook or
Google, most of it is via apps, or via direct browsing.
When you're dealing with the traffic which emanates from Tor's exit nodes
the relative percentage of dust (scraping & spam) to rice (legit people)
increases greatly.
I don't know the numbers - 10x, 100x ? - it will vary from platform to
platform, and (as stated before) FB will have a slightly easier time of it
because of the richer signals from login credentials.
It might be 6 grains of rice in a vacuum cleaner. or 1 grain. Or less,
depending on the platform.
So to convince people who work at companies of the value of hunting for and
recovering these grains of rice, you have got to make them _care_.
So sorry... when I search 'keyboard controllers' and get
I understand that perspective, but again that's looking at the "tail
wagging the dog".
In such circumstances they are not actually looking at you / what you are
searching for. They are looking at the behaviour of all traffic, of
everyone and everything else which emanates from that exit node.
They are mostly looking at a bag of dust, not at your rice-grain legitimate
And if you want to make them care about that, and if you would like them to
do better, my first tip is not to go around telling the (say: Google)
engineers that they are "full of shit".
It's a human thing.  It tends to make people upset and not listen.
I would love for Google and CloudFlare to do better in this space.  CF did
at least _try_ with a crazy proof-of-work scheme (which is a popular way of
identifying scrapers, btw) but that's a category error because Tor is a
network stack not a browser-access-solution.  But the Tor activist
community just totally savaged CF, with the entirely predictable result of
both sides hunkering down into a war of attrition.
Let's not repeat that?
    -a

@_date: 2016-09-27 10:45:07
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Exactly, especially since circuits rotate around exit nodes fairly rapidly.
And eventually someone has to write the code which says "This IP is
emanating bad stuff, but it is currently a Tor node, so just put it on the
naughty step for a few minutes until it calms down, rather than blocking it
for a longer period."
Once someone has done _that_, then the organisation is on the path to
caring about the real people who access the site over Tor, and finding
better solutions.
Exactly.  This manifests where folk on Twitter complain that "zomg i'm
using the onion site and it's blocked me!" - when in fact some perhaps code
is running - code that someone took the time to write - to learn/remember
that you are a person who logs-in over Tor, that you really are who you
claim to be, and that this is all "okay".
Otherwise the first time that someone logs-in from a Tor exit node might be
someone using Tor to experiment with your credentials, which they phished
off you via an e-mail, or something. (This is another popular misuse of Tor
from the perspective of the big platforms.)
It is definitely a _tough_ problem.
    -a

@_date: 2016-09-27 16:50:06
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
I feel that there's probably no silver bullet.
In some ways this is exactly what Mirimir posted about above - I think
there is much (much!) more to Tor than "Anonymity", but the architecture of
TorBrowser in particular revolts against long-lived session session cookies
and the other technologies which afford strong, trustable, long-term
concepts of authenticated communication between a browser and a site.
For more about this, the latter half of a video I did at a conference a
couple of years ago may be interesting:
  Summary: authentication is not just binary "I Have A Session Cookie!" any
Is it possible to use a different proxy way to access Gmail, FB, etc
If I understand you right (?) I think that was exactly the reason
we/Facebook set up the Onion site.  A Tor-sympathetic access mechanism,
more likely to be selected by human beings than folk pursuing the
scraperfriendly adequate location-anonymity which exit nodes provide.
Check the video - it's not just "location".  Remember, when working in a
London office, employees of non-UK companies often ip-geolocate to being in
(eg:) USA, FR, NL, or JP; this _really_ confuses organisations (eg: The
BBC) who fee (or are) obligated to take geolocation overly seriously.
You're welcome! It's nice to share!
     -a

@_date: 2016-09-27 22:33:10
@_author: Alec Muffett 
@_subject: [tor-talk] Tor and Google error / CAPTCHAs. 
Just to reinforce this a bit, it's not only the biggest/hugest names:
    Why does  not allow connections over Tor?
     (and thread)
I haven't actually tested this "block", nor do I have any special knowledge
of Airbnb, but I would expect them to suffer similarly from scraping & spam
sourced by people of bad intention who use Tor to hide their tracks.
I believe that I suggested "outreach", and perhaps "charm", as being
beneficial for turning companies from "victims of Tor" into "evangelists
for Tor"? :-)     - alec

@_date: 2017-04-21 23:35:45
@_author: Alec Muffett 
@_subject: [tor-talk] Shodan & Hidden Services 
So it turns out that Shodan - a kind of multi-protocol Google-alike search
engine for metadata and protocol headers - has indexed a bunch of Onion
sites which were configured to leak their (onion) hostnames into protocol
This is... tragic, perhaps, and avoidable to varying extents (eg: my
proposed setup process*) but the situation also possibly presents an
opportunity for anyone who has identified addresses of sibyl/other naughty
tor-infra-impacting activity, to maybe check some logs and see if any
badly-configured onions were also hosted on the same addresses/subnets, get
some concept of what hidden services were hosted there, and what they may
have been up to?
    - alec

@_date: 2017-04-24 09:33:26
@_author: Alec Muffett 
@_subject: [tor-talk] Shodan & Hidden Services 
Sometimes.  See sample results in my Twitter thread:

@_date: 2017-08-10 00:07:53
@_author: Alec Muffett 
@_subject: [tor-talk] Motivations for certificate issues for onion services 
(2) What reasons do people have for wanting certificates that cover
onion names?  I think I know of at least three or four reasons, but I'm
interested in creating a list that's as thorough as possible.
Six to start with:
- not having to rewrite CMS code which assumes HTTPS, eg for secure
cookies; the Onion acts as a straight deployment on a new domain name
- corollary: not having to lobby browser manufacturers to pollute their
code to understand that http under this magical "onion" TLD is somehow
almost but not entirely treatable like https.
- access to secure-locked protocols like WebRTC
- protection of traffic for the link between Tor daemon (basically a
reverse-proxy) and the site load-balancer fanout in enterprise deployment
- user expectation for padlocks, consistency rather than special-snowflake
creeping featurism
- EV: attestation.

@_date: 2017-08-10 11:12:50
@_author: Alec Muffett 
@_subject: [tor-talk] Motivations for certificate issues for onion services 
I can explain this.
I don't agree with it, please don't argue with me, it was a CA/B-forum
argument, I am not a member of CA/B-forum, please don't blame me, etc...
Also: the argument is gonna be redundant real soon, so there's no point in
kicking a dead whale along the beach.
Seth has not quite framed the issue properly.
The CA/B-forum argument against issuing DV SSL Certificates to 80-bit
onions, goes like this:
- SHA1 is bad, m'kay?
- And Onion addresses are truncated SHA1
- So maybe someone could brute-force a collision, using bad SHA1, to
generate their own "facebookcorewwwi" onion certificate?
- And the thing about DV certificates is that they can be validated via a
simple HTTPS request loopback, m'kay? (eg: LetsEncrypt)
- So someone generates their own Facebook Onion certificate, sets up an
onion site, and requests and receives a DV certificate via some automated
- And ZOMG this means that SSL will be no longer be perceived as the
snow-white, unimpeachable source of trust that it currently is
- Therefore: force Onions to use the EV process so that the SSL Issuer *IS
REALLY SURE* that it is Facebook who is asking for the certificate, not
some SHA1-hacker
- And please: nobody point out that equivalent problem in the DNS namespace
means that the entirety of SSL's trustworthiness is (in truth) slaved to
the ability to revoke a DNS record when someone sets up a fake site.
That's it.
All of it.
Put sarcastically but accurately.
There's no point in arguing about it, as geeks so often enjoy.
It's over, we can move on, and - as Seth rightly points out - with Prop224
the root of this argument (the SHA1 dependence) simply vanishes, taking the
entire rest of the house of cards with it.
Like I say: it's past, we should all move on and be grateful for having got
here at all.  I know I am, and that I never want to have to deal with the
above argument ever again.
    -a

@_date: 2017-08-12 20:28:16
@_author: Alec Muffett 
@_subject: [tor-talk] TBB Updates via Onion? 
When TBB checks & updates itself, does it use an Onion site?
If not, shouldn't it? Especially in these days where SingleOnion is
(rationale: more trustworthy networking, test tor more practically, reduce
exit-node load... )
    -a

@_date: 2017-08-30 12:15:43
@_author: Alec Muffett 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
Simply, it's as short-sighted as any other perspective that sees Onion
networking as an anonymity tool, rather than as a better-than-mere-TCP+SSL
mechanism for providing communications privacy, integrity, availability and
In case those terms need spelling out:
- onions provide circuit-level privacy on-par with the likes of VPNs, but
without the setup hassle.
- ditto, providing integrity at the circuit level, thereby inhibiting the
likes of (say) "sslstrip"
- availability of a service; I'm finding it interesting to consider that
the TCP/IP Internet requires the existence of companies (mentioning no
names) to provide DDoS mitigation, but sites which set up with Onion
addresses are getting comparable levels of DDoS mitigation for free*. Tor
blockproofing and (importantly) Onion DDoS-protection is pretty good.
- assurance: if you can type in the (static) Onion address, you know
immediately with whom you are communicating.
Proposals to undermine these qualities in the name of $GOAL are on-par with
Law Enforcement demands for "golden keys" to undermine the integrity of
end-to-end encrypted conversations**.
Practical example: the point of the Facebook onion site is to provide the
above-listed four benefits - plus a better quality of service - to people
who choose to access Facebook over Tor; the point is to free the
communications path from mediation of any form. To see this as a threat, or
to argue that "well maybe $THIS_SITE is okay, but $THAT_SITE should not be
afforded such protection" - is to call for censorship.
    - alec
*For a Twitter thread in this vein:
**For more on this thesis:

@_date: 2017-08-30 14:04:31
@_author: Alec Muffett 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
Hi Jon!
First is that the technical advantages of Tor are not in question, and
Did I do that? I don't think I did that.  If I did that, I didn't mean to.
What I meant to say, I suppose, after all that context, is that any
mechanism which denies or filters the availability of those "technical
advantages", to anyone who desires them, is tantamount to censorship.
I say that not as an ethical statement.  It simply is true.
Perhaps you can explain how it is not true?
It is a platform, and a corporation, and is bound by the laws of various
countries and geographies.
Should that privilege its access to good security and communications
technologies, above that of (say) an individual?
    -a

@_date: 2017-08-30 14:59:27
@_author: Alec Muffett 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
Hi Jon - in certain respects we have now hit the nub of the issue, repeated
twice / in two similar ways:
Version 1:
Ethical stuff gets murky awful fast, and is so full of
Version 2:
Yep. Very murky.  I've already sedimented my position on this a few years
ago - my Twitter bio and other bylines have read "Everybody Deserves Good
Security" for maybe a decade - but I'll be interested to see what you come
up with.
Yep; this is something I ascribe to the Tor Project acting to
counterbalance a prior few years of being mute on such topics.  As context
to the bigger debate of "the ethics of technology", I tend to ignore it as
window-dressing, in as much as I don't see IANA or IETF or W3C trotting out
denunciations of $GROUP for their $BAD_USE of DNS, TCP/IP or HTTP/S.
The "mea culpas" in that space have stopped with the various service
providers (Google, Cloudflare) rather than the technology providers.  Tor
in a sense has the rare distinction of being both.  Meh.
    - alec

@_date: 2017-08-30 15:12:27
@_author: Alec Muffett 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
In other words: you have to pay-to-play in order to have security; pay for
a DNS domain, be subject to takedown and
spoofing-between-the-onion-verifier-and-the-attribution-site, and
deanonymisation / doxxing / throttling / regulation / imprisonment via
blocking payments to your hosting or DNS provider.
To slave onionspace to the clearnet, in other words.
    - alec

@_date: 2017-08-30 15:31:31
@_author: Alec Muffett 
@_subject: [tor-talk] Neal Krawetz's abcission proposal, 
I entirely agree, but I feel that perhaps you missed one twist: that with
the churn which comes with "dark" markets changing their addresses (and
thus their reputational anchors) on a weekly basis, comes greater
opportunity for their inevitable customers to be fleeced by sites (say)
passing-off drugs cut with drain-cleaner as product, leading to a net of
greater misery (and probably death) by trying to drive the matter
This is "war on drugs"-type thinking.  Speaking as someone who to-date has
never even smoked pot, this seems like an intensely dumb idea.
    - alec

@_date: 2017-02-05 16:37:50
@_author: Alec Muffett 
@_subject: [tor-talk] Using EOTK to put clearnet websites onto onion addresses 
I released EOTK (Enterprise Onion Toolkit) a few days ago.
It's "alpha" code, very much still in development.
I posted a couple of videos of how to set it up:
* Introduction: * Rough Edges & "Gotchas":      - alec

@_date: 2017-02-20 10:19:23
@_author: Alec Muffett 
@_subject: [tor-talk] Finally a Cloudflare captchas workaround thanks to 
I believe they are referring to something which I have also heard from CA/B
Forum, regards SSL certificates.
There's a general perception in industry - with some justification - that
  SHA1 is bad.
  And current Onion addresses are based on SHA1.
  And they're only 80 bits, truncated SHA1.
  So current onion addresses are bad, too.
  Because a bad person could brute-force an 80 bit collision to hijack an
onion address.
  And that would be bad.
  Also, it would be way easier** than (say) social-engineering a CA to
issue a certificate to a fake or phishing site.
  Because that never** happens.
So: industry thinks that 80-bit cryptographic addresses are
brute-forceable, thus will not issue DV SSL certificates for them.  Instead
they will only permit EV certificates to be issued.
After all, having trivially** collided an 80-bit hash and set up your fake
Facebook Onion, you don't want some CA's automated
"URL-secret-cookie-reachability"-based certificate generator to blindly
issue an SSL certificate for the fake onion, thereby putting the SSL stamp
of approval on the site;  that would be bad.
Hence EV, which requires a more intimate relationship with the requester,
to mitigate this tremendous** security risk.
I suspect that the OP is pointing out that Prop224, with its 256-bit onion
addresses, will be much more resistant to brute force and therefore may be
more broadly acceptable to the trust/comms industry.
    -a
** your mileage may vary.

@_date: 2017-02-23 08:57:40
@_author: Alec Muffett 
@_subject: [tor-talk] Finally a Cloudflare captchas workaround thanks to 
To be fair, it's not Cloudflare's excuse, it's the entire CA/Browser Forum
The security community has been caught before by "merely hypothetical"
exploits suddenly appearing in the wild - TCP Sequence Number Prediction
springs to mind - so now the rule is "SHA-1 is bad", it's just been purged
from the certificate world in general, and they'll be damned if they're
gonna let it back in anywhere else.
    -a

@_date: 2017-02-23 17:53:28
@_author: Alec Muffett 
@_subject: [tor-talk] EOTK Video #3 - Deploying Onions At Scale with EOTK & 
New Demo:
How to set up 24 Tor + 120 NGINX daemons to mirror Wikipedia, without
working too hard... :-)
    -a

@_date: 2017-02-23 23:41:15
@_author: Alec Muffett 
@_subject: [tor-talk] EOTK Video #3 - Deploying Onions At Scale with EOTK 
Adjustment is to be made of course for the preferred single-onion
...and which is *default* in EOTK.  :-)

@_date: 2017-01-03 02:04:53
@_author: Alec Muffett 
@_subject: [tor-talk] Possibly Smart, Possibly Stupid, 
I will admit that I have not fully thought this through yet, so I am
writing this in the hope that other folk will follow up, share their
experiences and thoughts.
So: I have installed a bunch of Tor systems in the past few months -
CentOS, Ubuntu, Raspbian, Debian, OSX-via-Homebrew - and my abiding
impression of the process is one of "friction".
Before getting down to details, I hate to have to cite this but I have been
a coder and paid Unix sysadmin on/off since 1988, and I have worked on
machines with "five nines" SLAs, and occasionally on boxes with uptimes of
more than three years; have also built datacentres for Telcos, ISPs and
built/setup dynamic provisioning solutions for huge cluster computing. The
reason I mention this is not to brag, but to forestall
* "There is nothing hard about tar-xvf/configure/make/make-install", or...
* "All you need is {yum,apt-get} and to add $MAGIC_REPOSITORY (eg:
backports) to $WEIRD_FILE", or...
* "All you need is launch $NICHE_DISTRO_GUI_TOOL and tick $SOMEBOX under
* "Read the manual for 'dpkg'" / "what about reproducible builds?" /
"Install OpenBSD"
...responses.  I know that such tools exist, I know how to drive them;
however I am not "normal", and I suspect the same can be said of anyone who
would offer such advice to a new person who wants to use Tor.
If I have not messed this up, the current state of the Tor
"defaultinstalliverse" includes:
Debian Jessie: 0.2.5.12-4
Raspbian Jessie - tracks Debian (this is the Raspberry Pi platform)
Ubuntu Xenial LTS 0.2.7.6-1ubuntu1
Ubuntu Yakkety 0.2.8.8-1ubuntu1
Centos7(1611) - n/a, use Fedora
Fedora 0.2.8.12-1.fc26
OSX Homebrew 0.2.9.8
OSX Macports 0.2.9.8
There's quite a spread here; amazingly the OSX repos are on the cutting
edge, with Fedora and the latest Ubuntu is close behind.
Where I feel that issues arise are in the older Ubuntus and Debians.
Again, I understand that there are "backports" repos, but my experience of
encouraging new people to adopt Tor is one of trying to help them to jump
over the hurdles which we immediately place in their way.
The conversation usually goes a bit like this:
"Okay, you want to install Tor. First thing: you must ignore the version of
Tor that your operating system supplies. Oh, wait, you already installed
it? Did you use backports? You don't know what that is? Okay, if you type
'tor' what numbers does it give you? Type "which tor". Yes, that one. Okay,
that's an old version, we need to remove that and give you something
better. You don't know how to remove it because you were using the GUI?"
...which does not constitute a "positive user experience", nor "simple
advice", nor is it "fun".
It's gotten to the point where sometimes, if I want to ensure that the user
has a current Tor daemon on their machine to play with, I tell them to
install Tor Browser Bundle and use the SOCKS port to connect to Tor, a
solution which will go away once UNIX socketing is adopted for TBB
So this is kinda the problem statement:
- old versions of Tor are out there in the wild
- they pollute the software environment, representing "cognitive load" /
barriers to easy adoption and learning
- adoption and learning are critical to the growth in use of Tor
Further, as additional context, I am told that Tor "support" models will be
changing soon, and that only $SOME_NUMBER of recent releases will retain
support/bugfixes; presumably if one does not get on the train and track the
supported releases, one will be on one's own.  Given the (less than
corporate-sized) resources at Tor's disposal, I think this is fair and
agree with the decision.
I do not have a magic fix to address the problem statement, but I do have
some observations:
1) change always needs to be paid for; if we glibly say "someone at Tor
should build releases and push them to the repos!" then that person will
have to be paid for, and from where does the money come? This challenge
straddles growth, usability and outreach.
2) the repos won't always appreciate people throwing new code at them, and
even if that happens they may relegate it to a $MAGIC_REPOSITORY to a net
loss in usability as mentioned above.
3) There is a very big event impending, the freeze for "Debian Stretch";
from the above you can see that Debian is possibly the most significant
source of "install pollution" of Tor; it impacts all debian-derived
distros, and even Ubuntu "LTS" (the current Long Term Support release)
seems to treat Tor with some drag even though Ubuntu has "rolled its own".
I am told that Debian prioritises code stability - and as a former Solaris
engineer I wholeheartedly agree with that goal - but where Tor is
security-sensitive software with a bullseye target painted on its forehead,
perhaps it'd be wiser for the per-platform communities to plan to move
Observing the "installiverse" list, we can see that it is not considered a
fatal flaw that (say) Centos does not distribute Tor; documentation on the
Tor website says that Centos users should use the Fedora packages.
On that basis one possible step towards reducing install friction might be
to request that Debian wholly remove Tor from "Stretch", recommending
instead the binaries which Tor provides at
 - Tor possibly taking
on-board Raspbian builds as mentioned on that page. (but see point 1 re:
Indeed, reviewing  provides
quite a nice thumbnail sketch of the "if-and-or-but" decisions which new
users face in adoption:
---- begin ----
If you're using Debian, just run "apt-get install tor as root.
Note that this might not always give you the latest stable Tor version, but
To make sure that you're running the latest stable version of Tor, see
option two below.
Now Tor is installed and running. Move on to step two of the "Tor on
Linux/Unix" instructions.
Option two: Tor on Ubuntu or Debian
Do not use the packages in Ubuntu's universe.
In the past they have not reliably been updated.
That means you could be missing stability and security fixes.
Raspbian is not Debian.
These packages will be confusingly broken for Raspbian users,
since Raspbian called their architecture armhf but Debian already has an
See this post for details.
You'll need to set up our package repository before you can fetch Tor.
First, you need to figure out the name of your distribution.
A quick command to run is lsb_release -c or cat /etc/debian_version.
If in doubt about your Debian version, check the Debian website.
For Ubuntu, ask Wikipedia.
---- end ----
Please note: none of this is to criticise the (I am absolutely certain)
heroic work which has been required to get Tor into the hands of potential
users to date; all projects go through these growing pains, and in no way
am I trying to point fingers, apportion blame, nor attempting to suggest
that anything has been done wrongly.
But what I would like to do is see the above complexity ("ask Wikipedia?")
be simplified into coherent nonexistence, for all major platforms.
My (personal, subjective) ideal end-state is when someone asks me how to
set up a Tor daemon I can confidently say:
"Go to the Tor website, Follow the instructions, you will have to poke some
menus for whatever OS you are using, and then paste a couple of lines of
code, and that's it."
My _ideal_ end-game would further have "tor" bundling "torsocks" - a tool
which will become more critical for server-side adoption, real soon now -
as well as some CLI "tor setup wizards"
But I am happy to park those in favour of the next generation of Debian
users, still installing Stretch in the year 2019* to not still be stuck
with Tor 0.2.9 when, by that time, we should be on... 0.3.2 ?
    - alec
* which is when I am guessing "Debian Buster" will freeze?

@_date: 2017-01-03 10:04:05
@_author: Alec Muffett 
@_subject: [tor-talk] Possibly Smart, Possibly Stupid, 
Hello Grarpamp!
Oh yes - and I am not saying that there is anything wrong with having a
community of technical experts who bicker amongst themselves about fine
details; but I am pretty confident, experientially, that exposure to that
manner of thing is generally off-putting for people who have heard of Tor
and wish to "try it out".
Disclosure: I am mostly talking about journalists with a technical bent,
and lawyers, and people with "alternative" lifestyles; plus a few teachers.
Aw, bless; I said "corporate sized resources", not "corporatized".   I just
left Facebook, forgive me.
Oh, I am certain you are right - I help out at ORG, likewise a non-profit -
but such does not preclude that much can be done:
- to make Tor more accessible
- to more people
- and thereby more important.
That's a very fair point you are making - all of the code in the trusted
computing base is critical.
But I think you are actually making my point for me, though:
* If all of the code (that goes into a Debian installation) is safety
* then if Tor can avoid stagnant tor code from being shipped with Debian
for (guessing) 75+% of the lifecycle of "Debian Stretch"
* should it not take that opportunity to make an incremental improvement to
the world's installed base, by moving Debian software repos entirely
It looks to me to be easier to explain, easier and more consistent to
install, and more likely to be updated, if tor (the software) moves
entirely to being under Tor's aegis.
Overall, this would be (marginally) better security for the installed base
of the world, for modest outlay.
Linux distro model has imparted a brokenverse upon itself and its users.
Yes, I agree, and I see this may be one way to help mitigate against that.
Good question!
It's my opinion of course, but Tor is very geared-up for one thing - relays
and/or onion web/poty reverse-proxying - with the assumption that the other
end, the client end, will largely comprise human beings who are competent
to poke bits of "netcat" into their ssh-configs, etc.
A few months ago someone posted a cute VMS instance on a Telnet port
attached to an onion address; the amount of work that would have been
necessary to hack a connection to the service (messing around with telnet,
fetching and setting up socat as a port forwarder) would have rather killed
the fun; so I tried runsocks, found it was out of date, and had to go down
the "backports" rabbit hole.
I agree with what you say - it's always been useful; but (just as with Tor)
the versions are wildly out of whack in various distributions, and are not
always up to date with respect to the current "tor" build, even.
Having torsocks (as a "universal client") exist separate from the main
"tor" binary, rather than in lockstep, is a bit messy. If onion networking
is to expand - as I think it will - then it will be key to have easy access
to a tool which patches around the lack of AF_ONION in the kernel socket
In fact, both tools are critical for case of helping mass adoption
Yep, it sucks when that happens, but sometimes change is necessary.
     -a

@_date: 2017-01-04 11:24:54
@_author: Alec Muffett 
@_subject: [tor-talk] Possibly Smart, Possibly Stupid, 
Hi Sebastian!
Okay, I'll give it a go :-)
I install Debian stable on my servers precisely because they don't
Yes, I totally understand that.  Especially for a long-lived platform I can
totally see the utility, sanity and wisdom of doing that, especially where
the person administering the system is skilled and knowledgeable.
When I have
Yes totally.
Actually, I don't believe that you do disagree with the problem statement
I believe that you may concerns with one of my proposed solutions to the
problem, and that's okay because I do too.  :-)
Let me see if I can reframe the problem statement, and maybe pose another
== Problem Statement, Reframed ==
I believe that, at the moment, the Debian "installiverse" assumes that
people who want to use Tor -- on a server -- are skilled and knowledgeable.
Case in point:
* For setting up relays you want to stand on a solid foundation, so Debian
"stable" works really well.
* But I will bet that you are not running the relays by using the 0.2.5
codebase which it offers. :-)
** (Not least, I believe that the crypto in anything older than 0.2.7.*
will be a lot slower?)
* So I believe that you are probably not running 0.2.5 because you are
skilled enough to use backports or roll your own.
Now, for contrast, consider Tor Browser Bundle:
* TBB is designed for use by "normal people".
* It is not running 0.2.5.* either, because it would be foolish to push out
such "old code" to the clients.
* TBB runs code which we might call "Tor Stable", being probably a similar
version to the code which you yourself would deploy for a relay.
* Therefore: "Debian Stable" is not offering "Tor Stable", instead it is
offering "Tor Stale", or "Tor Stagnant".
== Summary ==
Debian Stable, by default, offers code that we would deprecate on the
servers, and would never ship on the clients.
How is this to anyone's benefit?
It's like the old chef's rule about "never cook with wine that you would
not drink by yourself"
== Challenge ==
Consider schools. Consider journalists, hobbyists, non-CS university
students. Consider "IoT tinkerers".
Consider people who don't know what "vim" does. :-)
These are people who can use Tor for good purposes, and I believe that Tor
should seek to "enable" them, and make Tor easier to use for them.
But the Tor code which comes with Debian, and thus with Raspbian, and
kinda-with-Ubuntu, is (by the metaphor) bad wine which you would never
offer to these people to actually drink nor cook with.
In my previous e-mail I suggested removing Tor from Debian precisely
because of this future-staleness problem.
I still believe that this is a decent idea, because stale code sucks.
Another possible solution would be creation of a "Tor Server Bundle" -
designed and maintained to run successfully on most major platforms, it
would be a bunch of scripts that find existing, stale versions of Tor, kill
and remove them, and migrate the platform to a more recent version, with
*optional* enabling of auto-updating because you will want to have a stable
environment. :-P
And it would (ideally) also ship with some portable, stupid but bombproof,
interactive and scriptable CLI wizards for setting up Onion services.
== Why? ==
Large chunks of the Tor community are focused on Tor's primary purpose as
an anonymising proxy, and that's very, very important.
It is Tor's primary and best-understood, to provide people with a secure
and generally anonymized means to connect to cleartext websites.
But (subjective opinion) I think the future of Tor is in disintermediated
networking.  I think the future is in onions.
Prop224 is coming soon.
Prop224 will provide a larger, more secure means of onion addressing,
possibly permitting even DV SSL certificates.
Having DV certificates available would permit (for instance,
hypothetically) LetsEncrypt to issue certificates to arbitrary onions,
unlocking SSL-only features like WebRTC to permit disintermediated,
onion-secured, user-hosted, location-anonymised video chat
Aside: this was part of why we drunken reprobates were doing this stuff at
33c3 --  --
leading to nice, fun stuff like this --
I want other people to have an easier time innovating with onions.
Prop224 is due to land in tor 0.3.1.x.
But the generation of students, hobbyists, tinkerers who will be using
(say) Raspbian in 2018, 2019, 2020+... they will all be using "Stretch" by
default, and they will all be locked on 0.2.9.* unless they:
- CLUNKY: learn that the first thing to do is ignore the code in front of
them and that instead they must roll their own / poke their system or
- CLEAN: install Tor directly from torproject.org because tor is not in
"Stretch", or
- INTERMEDIATE: install TorServerBundle which nukes the stale 0.2.9 stretch
binary and installs Tor directly from torproject.org
I believe that we are now at a crossroads.
We can choose to avoid, or address, stacking up code/wine which will be too
stale to use/drink by the time the next generation of innovators and
tinkerers are booting up their 2020-era Raspberry Pi Model 5.
Or we can leave everything at the status quo, and just muddle along, like
the technically capable (elitist?) geeks that we already are, and hope that
a few more people may survive the process and join our ranks, eventually.
But me, I want to get _everybody_ - teachers, journalists, kids, everyone.
I want people to be able to use stuff like this (piece of crap, but it's
just a proof of concept)  - using
onions to communicate directly with one-another.
I want everyone to have the opportunity to connect without an intermediary,
if they choose.  It would be nice to have the option. :-)
But it needs to be easy, not clunky.
    - alec
ps: major, yuge props to Filippo, Str4d and GTank for their video work, I
was largely off working on RTC when they did the above video stuff
footnote: The announcement of 0.2.5.10 was October 2014.

@_date: 2017-01-04 20:13:33
@_author: Alec Muffett 
@_subject: [tor-talk] Possibly Smart, Possibly Stupid, 
Answer: "incrementally".
We're are not, and I think should not, attempt to replace the internet.
Just augment it.  Add value.  Make it more interesting, diverse and fun.
    -alec

@_date: 2017-06-15 19:33:13
@_author: Alec Muffett 
@_subject: [tor-talk] Improved sharing of .onion links on Facebook! 
Doubtless haters gonna hate, but Will at Facebook just shipped
thumbnail-generation and protocol-mismatch interstitials for Onion
The normalisation of Onion networking continues apace!
    - alec :-)

@_date: 2017-06-15 20:16:08
@_author: Alec Muffett 
@_subject: [tor-talk] Improved sharing of .onion links on Facebook! 
I wrote this to explain why, ages ago; short version -- it's not about
Also, possibly you got "blocked" but I suspect that it's more likely that
you got "checkpointed"; for people who may be interested:
- if there's a box asking you to answer some questions because something
apparently weird is going on, IT'S NOT A BLOCK.
- A "block" is when the site tells you "go away and never come back"
without recourse.
- If it's just asking you questions / to jump through hoops, it's trying to
protect your account in case you got hacked.
- Eventually it will learn.
- The funniest story was the time that Runa Sandvik got checkpointed
because Runa tried logging in one day *without* using Tor / the FB Onion:
- More at     - alec

@_date: 2017-06-15 20:41:48
@_author: Alec Muffett 
@_subject: [tor-talk] Improved sharing of .onion links on Facebook! 
That happens sometimes; in fact, for proof of name rather than account
recovery, they kinds of document the will accept are quite large… but
that's getting off-topic.
Something else we tested on FB: They quite often ask people to "upload a
Clearly you're an expert in fake profiles.  I can't _imagine_ why FB would
be blocking your accounts.
Considering this and the FB reputation about privacy violations, I even
Well, in a sense, but only that accounts tend to be associated with
particular IP addresses, some of which may be exit nodes.  These are in
what are called "logfiles".
In short, better connect from a public place with a spoofed MAC address ;)
As you wish :-)
    -a

@_date: 2017-06-18 10:50:27
@_author: Alec Muffett 
@_subject: [tor-talk] Is the recent growth in Ukrainian users confusing 
For those who haven't been paying attention, we got a jump in some
Or, indeed, there are people whose self-declared Google-account country of
residence is Ukraine, and enough of them have been logging into Google from
the exit node IP addresses for Google's geolocation tier to start
I would think that this is very plausible, indeed.  No actual/serious
machine-learning required.
In other news, the FB Onion, for some time after it launched, geolocated to
London. I can't imagine why.
    -a

@_date: 2017-06-20 13:35:20
@_author: Alec Muffett 
@_subject: [tor-talk] Is the recent growth in Ukrainian users confusing 
You are assuming a hidden (double) onion service.
Facebook uses a single onion service
Or, y'know, it could just be guessing.

@_date: 2017-03-12 22:43:28
@_author: Alec Muffett 
@_subject: [tor-talk] Possible solution to next-gen onion services UX 
Having lived through a period where email addresses as we know them (
foo at example.com) were pre-emptively declared to be a usability disaster
zone, and seeing the resultant train-wreck of X.400 addressing being
foisted upon the UK academic community as a simple, clear, and intuitive
    "G=Harald;S=Alvestrand;O=Uninett;P=Uninett;A=;C=no"
    see: …having seen this (-^) to be declared as self-evidently better and then
watch it die as RFC822 took over the world, then I for one am inclined to
ask tor just to roll out 53(?)-character Base32 addresses and "see what
Trying to forward-guess the community seems often to land in disaster.
    -a
ps: if the above looks familiar, it's because the X.500 directory services
that were invented to support X.400 email were later resurrected from the
dead as LDAP, because you can't keep a bad idea down in the "identity"
pps: i was a university sysadmin in the early 1990s; the ISODE consortium
literally gave us a high-end server to deploy X.400 and X.500 code on, a
server which sat idle because nobody liked X.500.  So I used it to develop
password cracking software.

@_date: 2017-11-06 08:45:15
@_author: Alec Muffett 
@_subject: [tor-talk] Onion Service stock photo anyone? 
Screenshots of the new New York Times onion site, and other similar
non-dark onion sites?
Partial index at

@_date: 2017-10-01 15:42:19
@_author: Alec Muffett 
@_subject: [tor-talk] Forward to Onion 
Hi Jason!
You want to go read and sign up for this Tor ticket, where this matter is
being discussed: Everything you've mentioned, is there.
    - alec :-)

@_date: 2017-10-11 10:38:07
@_author: Alec Muffett 
@_subject: [tor-talk] Recent Tor Circuit Setup/Stability Issues? 
A friend asked me to explain the recent/ish spikes in the following
performance graphs:
Timeouts of 50Kb requests to Public Servers:
Timeouts of 50Kb requests to Onions:
Both graphs show significant issues in August, plus a major anomaly in the
past few weeks.
Do we know what might be behind this, please?
    - alec

@_date: 2017-09-11 12:56:20
@_author: Alec Muffett 
@_subject: [tor-talk] /etc/hosts for .onion 
Hi Jason!
This is not exactly what you are asking for, but I cover something similar,
using /etc/hosts and virtual network interfaces:
HTH. HAND.
- a
Hi all,
One of the things that I've been working on lately is getting salt-ssh
working over tor. The salt-minion, by default, looks for the salt-
master using the hostname, "salt". I know that I can manually change
that to xxxxxxxx.onion but I would like to know if anyone knows how of
a way in *nix to alias local hostnames to .onion names the same way
that /etc/hosts does that with local names and IP addresses.
Thanks for your time!
tor-talk mailing list - tor-talk at lists.torproject.org
To unsubscribe or change other settings go to

@_date: 2017-09-20 11:00:41
@_author: Alec Muffett 
@_subject: [tor-talk] Unusual Tor's spikes in Egypt and Turkey on 28th 
SPECULATION_AHEAD 1
I have developed some theories - mere theories, not based on hard analysis
- about why this happens.
I believe that Tor is most widely deployed and used "at need"; people keep
a copy of it lying around, especially in environments where blocking is
common, and they use it on special occasions to get past the barriers and
to reassert their "normal" computing capabilities as-soon-as-possible.
I think this makes sense: because of the "anonymity" (eg: JS-related)
aspects of the tool, TorBrowserBundle (TBB) can feel like a somewhat
"degraded" experience when compared to a "normal" web browser, and also I
know at least one occasion in past where a TBB upgrade killed all the
bookmarks which I'd built up.
Thereby I believe that TBB has evolved, for some people, to be "the browser
that you use to get past the barricades, until you're safe enough and/or
have acquired updated VPN software, and then you can go back to your old /
normal / familiar browser".
In certain respects this can be read as "TBB's threat model excessively
trades-off consistency and usability in favour of protections which
$SOME_MAJORITY of its userbase do not actually need" - but I'm okay with
the status quo.
I would rather that TBB "shoot for the stars" in terms of integrity,
privacy and anonymity-protection, though I exhort any/all efforts that work
to address this issue.
The one insight that I can bring to the table here is this: the number of
people who know how to (and *do*) use Tor at-need, vastly exceeds the
number of people who use it on a regular basis; this is also why the
autoupdate mechanism (and improvements to the security and speed thereof)
are fundamental to the TBB proposition.
I'd really love to see Tor, TBB and TBB-Plugin mechanisms upgraded to run
over single-onion, for instance.  It would make sense.
    -a

@_date: 2017-09-27 21:19:55
@_author: Alec Muffett 
@_subject: [tor-talk] hidden service - for dummies ? 
If you're able and willing to drive Ubuntu, I've tried to document a
reasonable means to set up an Onion-only server here:
- which sets you up with up-to 4x onion addresses, and with them you get 4x
distinct virtual network interfaces which you can bind apps (eg: Apache,
NGINX, SSH) to, in order to avoid localhost-loopback privilege issues, and
other security risks.
    - alec

@_date: 2018-07-11 10:04:34
@_author: Alec Muffett 
@_subject: [tor-talk] How do tor users get past the recapacha and it's 
I wrote this process of realisation up at some length:
...or here, if you prefer onion networking:
I should really put a copy of this essay somewhere that it will get more
    - alec

@_date: 2018-10-16 21:11:08
@_author: Alec Muffett 
@_subject: [tor-talk] Let's not keep rehashing the past, 
...vast amounts of deletia...
b) Key material
Um; I can only see this being a risk or threat if you imagine that
Cloudflare is assigning abd surfacing permanent "parallel" onion
addresses/names to their customers.
If you do believe that, then you've misapprehended how Alt-Svc works.
Neither clients nor website owners ever see onion addresses; all the onion
addresses are ephemeral and buried at/below the HTTP layer.
2) Security aganst keytheft breach and subsequent
Ditto; likewise not an issue with Alt-Svc onionification; the mechanism
never surfaces onion keys to the user, and the onions themselves are
short-lived / ephemeral.
- alec

@_date: 2018-09-16 17:34:14
@_author: Alec Muffett 
@_subject: [tor-talk] Two Degrees of Removal 
There appears to be some kind of bot which sends repeated porn/sex-related
emails to people who post to various Tor maillists.
It's a nuisance but I just mark them as spam, albeit they arrive from
disparate email addresses.
Re: your problem, I regret I tend to use simpler and easier to debug
configurations than you describe.

@_date: 2018-09-18 21:14:19
@_author: Alec Muffett 
@_subject: [tor-talk] alt-svc supported by TBB 
Well — speaking as the former guy from Facebook who built facebookcorewwwi
and helped standardise ".onion" as an official TLD, in order that the
Facebook Onion SSL Certificate could continue to exist, I feel uniquely
qualified to answer this one.
The answer is: I understand that historically the Tor community has had a
lot of hatred for [various companies] for making use of [the related
websites] harder, over Tor; a lot of the actions of the companies came out
of (a) a place of fear and misunderstanding, plus (b) a sense that "civil
society / tor only ever criticise and hate-on us, so why should we bother
doing anything to help them?"; combined with bits and pieces of political &
protest rhetoric.
I personally believe that a lot of this attitude can be laid at the feet of
former members of the core Tor team who are thankfully no longer so.
As I fought (and won) the argument within Facebook, the important thing to
do is ignore the anger and vitriol, and instead to focus on the people
whose lives will be improved by making better access over Tor:
...and I have been preaching this gospel, every single week, since I left
Facebook in 2016 due to burnout and other reasons.
So, in a nutshell, the reason for Tor to engage with Cloudflare and
Facebook is... because Cloudflare and Facebook want to engage with Tor.
Maybe some folk disagree on corporate value propositions, or the value of
services provided, but underneath it all: the more people who use Tor, the
better.  And these corporations — in defiance of popular opinion — really
do care about the security and safety of people who access their services.
Tor helps with that.
That's why.
    - alec
ps1: anyone who wants to argue this matter ("zomg evil corporations!!1!")
is invited to email me directly, I am not going to argue attempted
point-scoring within this thread.
ps2: my latest video may be of interest:

@_date: 2018-09-22 13:45:33
@_author: Alec Muffett 
@_subject: [tor-talk] Draft: Different Ways To Add Tor Onion Addresses To Your 
I've spent the morning pulling together a bunch of draft thoughts regards
the technical pros/cons of differing forms of site onionification;
thoughts, comments & feedback are warmly welcomed:
- alec (ps: apologies if you see 2+ copies of this, I am treating maillists

@_date: 2018-09-22 16:15:08
@_author: Alec Muffett 
@_subject: [tor-talk] Deploying Alt-Svc on your own website. Hello? 
I see your point, but there are a couple of additional perspectives to be
- especially regarding new functionality that will be locked to HTTPS
Respecting AltSvc on port 80 would be as dangerous, possibly more
dangerous, than cleartext HTTP already is; and regards the notion of making
"onion" into a widely respected secure source equivalent to a HTTPS site,
please see the above essay.

@_date: 2018-09-22 18:16:44
@_author: Alec Muffett 
@_subject: [tor-talk] Deploying Alt-Svc on your own website. Hello? 
Well, if you want to take that attitude, you can, but it's not terribly
Perhaps you should write the document that you want to read, and then share
it with us all.
    -a

@_date: 2018-09-23 14:36:33
@_author: Alec Muffett 
@_subject: [tor-talk] Let's not keep rehashing the past, 
I've seen lots of postings from Grarpamp and I feel sure that I'm never
going to change any opinions that Grarpamp holds; but what I do want to
raise with everyone is "the possibility of change":
To a good approximation, literally *zero* percent of the organisations
which will benefit from "Opportunistic Onions" have ever used Onion
Services until now
However literally 100% of the websites who can benefit from "Opportunistic
Onions" are Cloudflare customers by choice, who choose to trust Cloudflare
with their traffic, and I respect the choices of the website owners to
select different ways of scaling their services and of keeping their
systems safe from being DDoS'ed.
The people who *use* those websites can and should make their feelings
known to the website owners; but the opinions they feed back should be
balanced and considered and up-to-date and fair.
Yes, there is much to criticise of Cloudflare's past approach towards Tor
(including tweets by the CEO) but as I have also said so many times before:
it's amazing what a little engagement and mutual respect will achieve.
To go back through my own history at Facebook Engineering, the turning
point was this Reddit post from June 2013:
...where one of Facebook's IP reputation systems burped after eating some
new config software, and blocked a large number of Tor exit nodes.
The civil society & reddit communities started commenting at speed, flaming
Facebook for "censorship"; and I had to argue against my own management,
some of whom suggested "why not just block Tor totally?" - because it
apparently caused nothing but vitriol and bad headlines.
I said "Give me a chance" and pinged Runa Sandvik (who was then at Tor)
asking her on behalf of Tor to explain the situation to the world:
quote> A number of users have noticed that Facebook is blocking connections
from the Tor network. Facebook is not blocking Tor deliberately. However, a
high volume of malicious activity across Tor exit nodes triggered
Facebook's site integrity systems which are designed to protect people who
use the service. Tor and Facebook are working together to find a resolution.
...and the anger faded. People were nonplussed: Facebook had merely goofed.
Facebook was working with Tor to "fix things". As I think one commenter put
it: "What do I do with this pitchfork, now?"
The important thing is what happened next:
This single event - proving that it was possible to get constructive
assistance from Tor - was enough to provide me traction for the concept of
building a Facebook onion site; I started building it 1 year later (needed
to learn some stuff, first) and launched it 3 months after that.
It's no coincidence that Runa subsequently helped with testing & launching
facebookcorewwwi, nor that three years later the New York Times launched
its own onion site.
I am sure that there are lots of people here who hate Facebook too - and
that's okay; my point is that without constructive engagement we would
probably not be where we are today, with Onion SSL Certificates, with an
official ".onion" top-level domain, with a increasing number of
"respectable" onion websites which are putting the lie to the "Dark Web"
Tor, and Onion Networking, is just the "More Secure Web"; and you grow it
by giving people and companies the opportunities and space to engage with
it, so that they can offer value to others.
tl;dr - Tor will grow by engagement and reconciliation, not by rehashing
old debates and historical enmities.
    - alec

@_date: 2019-12-13 11:29:34
@_author: Alec Muffett 
@_subject: [tor-talk] facebookcorewwwi on brief hiatus 
tldr: new TLS certificate is stuck in the pipeline for a few days, because
onion certificates are special and weird:
(Includes links to sources)

@_date: 2019-02-08 10:32:06
@_author: Alec Muffett 
@_subject: [tor-talk] Tor private-key behaviour impacts: pycrypto, 
Hi All,
Last night I found Tor was/is generating v2 keys that are not
loadable/parsable by PyCrypto.
A fully-worked-example with test code and an example key is on trac:
    I haven't the familiarity with the codebase (nor the standards expertise)
to triage what causes the behaviour; whether recent Tor is generating
private v2 keys which are not to specification, or whether (as suggested in
old threads elsewhere[1]) perhaps PyCrypto is just too old/busted and needs
updating or replacing.
Assuming that the dependencies I've pulled in via my OnionBalance install
are up-to-date, though, this suggests that maybe STEM and therefore
presumably also several other tools would be afflicted, though I don't know
to what extent.
Has anyone else seen this, please?
    -a
[1]

@_date: 2019-01-24 19:44:45
@_author: Alec Muffett 
@_subject: [tor-talk] [Cryptography] Implementing full Internet IPv6 
There's an open project for anyone who wants it...
Hi Grarpamp,
I'm aware of this.  I've seen you mention it, several times recently.
I'm wondering: could you please expand upon how this compares in importance
to simply promoting the native adoption of Tor v3 Onion Networking, amongst
the community of tool-developers and tool-users whom you envision the above
solution (OnionCat/OnionVPN/IP-routing) benefitting?
    -alec

@_date: 2019-01-25 09:40:11
@_author: Alec Muffett 
@_subject: [tor-talk] [Cryptography] Implementing full Internet IPv6 
I've not heard of "Tor v3 Onion Networking". Does it exist? Or if not, are
Yes, I mean almost precisely that.
Explanatory video: All this talk about making Onions pretend to be TCP/IP is ... not
maximising the value proposition of Onion Networking, in pursuit of some
result where I cannot see a clear benefit. (Adoption of a substandard[*]
solution, for adoption's sake?)
Tor's "presentation layer" is SOCKS5, which is okay ; perhaps eventually we
will have AF_ONION in the same way that AF_X25 exists:
        ...and like I had to use for sending/receiving email at X.25-based UK
universities in the early 1990s.
But we don't need AF_ONION and a socket stack yet; what I think we need
right now is people making more services available on v3 onion addresses,
because it's faster and more secure.
Easing client connectivity by any means, does not provide benefit when
there are no servers/peers to talk to (see video).
[*]Simply: I am happier to see the end clients knowing that they are
talking directly to Tor rather than relying upon some per-operating-system
"shim" to make Tor available to them; aside from any other reason, shims
tend to get pushed upstream (NAT-boxes, anyone?) and further break the
end-to-end principle.
    - alec

@_date: 2019-01-25 11:32:27
@_author: Alec Muffett 
@_subject: [tor-talk] [Cryptography] Implementing full Internet IPv6 
I'll wait; most questions about "what do [I] mean?" are answered in that
Let's say that I have a bunch of VPS, running Tor and OnionCat. Each has
How are you going to inhibit leaks and connections to "promiscuous" service
listener-sockets over the LAN interface? Perhaps firewalls? Yet more /
additional server misconfiguration opportunities?
Safer, instead, for the client to be clear and explicit about what manner
of network address it wishes to connect to.
I'm sure that one could write code that did all the same stuff, using
I've done similar hacks using /etc/hosts:
... but that is mostly a server-side convenience, and not strictly
.What do you mean by "services"?
As above.
If all you have is SOCKS5, you're pretty
My experience suggests otherwise, and I am calling for expansion in this
you use shims like AF_X25. I never had to use that, but I'm sure that
How many systems do you have using it?

@_date: 2020-10-12 14:08:20
@_author: Alec Muffett 
@_subject: [tor-talk] How to test that Alt-Svc is working for my Onion 
Hi Francois!
I have done this, too.  It was not easy.  I set up an Alt-Svc for my
Wordpress on Apache2. The project turned out to have several prerequisites
before it would work.
- firstly I had to rework my CGI mechanism to permit use of HTTP2, because
Alt-Svc would not work for HTTP/1.x under Apache (even though nothing would
actually prevent it). I did make several experiments with HTTP/1.x for
AltSvc, but the code to support it simply did not (does not?) exist.
- secondly: you are correct, there is little or no diagnostic on the client
side, to show that the Alt-Svc is being used.  In the end, I used the hack
that I described on Github[1] so that I could run a Tor daemon locally, and
so requests from that daemon would apparently arrive from a well-known IP
address (eg: 169.254.255.253) - and then I grepped my logs for that IP
address, to confirm that the AltSvc was being used.  It was not used
consistently, there are (or: were?) issues where Firefox or TorBrowser
would sometimes decide not to use an Alt-Svc even if one was offered,
mostly due to connection latency.
Hope this helps.
  - alec
