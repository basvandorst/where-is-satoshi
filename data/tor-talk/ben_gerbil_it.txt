
@_date: 2015-07-05 08:02:55
@_author: Ben 
@_subject: [tor-talk] OnionBalance Hidden Service has over 1 million 
Much the same, but with some stats (some of which are likely irrelevant,
but I poached the line from another script I use)
torify curl -w
"%{http_code},\"$HOST\",\"$URL\",\"%{url_effective}\",%{time_total},%{time_namelookup},%{time_connect},%{time_redirect},%{time_starttransfer},%{size_download},%{size_request},%{num_redirects}\\n" -o /dev/null  >> metrics 2> /dev/null
Running in a while loop that triggers 4 requests at a time. Only got as
far as setting running on one host

@_date: 2015-07-08 14:39:34
@_author: Ben 
@_subject: [tor-talk] OnionBalance Hidden Service has over 1 million 
HS, though it's a very small percentage (around 1.5%).
Count Status code
    590 000
 408391 200
000 being the code curl returns when it couldn't connect. In terms of
time to serve, there's a fair range of variation in terms of the total
connection time. Count  Seconds
  45207 0
 149979 1
 103050 2
  55134 3
  27011 4
  13688 5
   7405 6
   4022 7
   2217 8
   1324 9
All connections were established in less than a second, and the time to
first byte was generally < 2 seconds
Count TTFB
    590 0
 408479 1
      1 3
      1 4
      1 5
      1 6
      1 7
      5 8
     10 9
Obviously some of the variation might be down to my client's connection
rather than the hidden service, it's running on a stable 100Mb/s
connection, though the traffic graphs show some fluctuation in the
bandwidth being used (attached - stats taken at the NIC so likely
includes other traffic though the test will be the primary use).
Happy to send the stats file in full if it's of any use to you.

@_date: 2015-07-08 21:10:43
@_author: Ben 
@_subject: [tor-talk] OnionBalance Hidden Service has over 1 million 
Seems plausible, though from what I have logged it's hard to tell. All
the time calculations are 0, which may mean that curl completely failed
to connect, or that it's counters don't work if the connection fails
(I'm not sure which is true).
I forgot to tell it to add a timestamp, so comparison against your logs
would be nigh on impossible - have set the same script running with
timestamps added, will keep an eye to see whether any failed connections
have been logged.
I do, however, have some entries in my tor client logs
Jul 08 09:03:55.000 [notice] Rend stream is 120 seconds late. Giving up
on address '[scrubbed].onion'.
(time is UTC) though there aren't enough to account for all the failed connections.

@_date: 2015-07-09 04:23:32
@_author: Ben 
@_subject: [tor-talk] OnionBalance Hidden Service has over 1 million 
OK, you may be able to match the following times with your logs - each
had multiple connection failures (looks like any pending connection
died). Times are in UTC
1436393744 - Wed, 08 Jul 2015 22:15:44
1436396224 - Wed, 08 Jul 2015 22:57:04
1436399231 - Wed, 08 Jul 2015 23:47:11
1436400042 - Thu, 09 Jul 2015 00:00:42
1436400868 - Thu, 09 Jul 2015 00:14:28
1436403696 - Thu, 09 Jul 2015 01:01:36
1436404315 - Thu, 09 Jul 2015 01:11:55
1436405498 - Thu, 09 Jul 2015 01:31:38
1436406217 - Thu, 09 Jul 2015 01:43:37
1436407074 - Thu, 09 Jul 2015 01:57:54
1436409062 - Thu, 09 Jul 2015 02:31:02
1436409898 - Thu, 09 Jul 2015 02:44:58
There's no correlation between those times and entries in my Tor
client's log.

@_date: 2015-07-09 07:17:07
@_author: Ben 
@_subject: [tor-talk] OnionBalance Hidden Service has over 1 million 
Modified), 404 (Not Found), and 403 (Forbidden) codes were caused by.
I'd assume the 304 was probably someone pressing refresh in a browser,
it'll cause a conditional get (either based on Etag or last-modified)
which would result in a 304. IIRC it's also possible to have wget/curl
do revalidations as well, though not sure why you'd want to in this
I wouldn't be surprised if the 404's were also a browser attempting to
get a favicon.

@_date: 2015-05-17 11:26:41
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and on 
Hi all,
I've got a (www) site that I'm debating making available as a Hidden
Service, and I was wondering what peoples thinking on doing this was
It's a publicly available site, so it's not privacy for me (as the
operator) that I'm looking for by setting up a HS - it's got a
reasonable readership and it's occurred to me that it'd be good to give
readers the option of accessing via a HS so that they can make their own
choices regarding anonymity.
Some of the content could be considered controversial in some of the
stranger jurisdictions of the world, it's not impossible that it might
even be blocked in some of the even-stranger ones.
The site in question can already be accessed via a tor exit node, but
that brings a third party into the mix (the exit operator). Although the
site uses HTTPS there's still always the outside chance. Using a HS cuts
that out (and frees up a little exit capacity to boot).
So I've been scrawling a few perceived challenges and wondered whether
anyone can think of anything I've missed (or has suggestions on those I
Site uses HTTPS
There are some off-the-shelf protections built into the site. Given they
were designed for the www, they can (and do) ban any IP that's seen as a
repeat offender. Either an exclusion needs to be made, or the HS will sometimes show
'nice' visitors a potentially rude message :)
Whether or not an exclusion is made, things that don't _need_ to be
available via the HS can be blocked off at the reverse proxy (for
example management back-ends).
There are other sections of the site that could, potentially, be blocked
off to lessen the attack surface available to an adversary - for example
are you likely to use the shop section (physical items) if you're using
a HS? You don't want me to see your IP, so it seems unlikely you'd give
me a shipping address and payment info?
This is probably the part I'm most undecided on - I don't want to
un-necessarily restrict access to certain areas, but I also want to make
sure that the existing protections aren't weakened too much (in case I
or someone else make a mistake).
Minor Tweaks might be needed
This is a pretty minor thing, it's more a point of principle than
By having a service available over both the www and a HS, my site
becomes a potential target for anyone looking to test a method of
identifying where a HS is hosted (as they can verify their findings
against the www service).
Now, obviously, there's no reason they couldn't set up their own service
to do the exact same thing, it's more the principal of not wanting to
help someone de-anonymise.
It goes without saying that the server won't be hosting any HS that are
intended only to be HS's. I'll probably look at setting a HS up with authentication required for
testing, but I'm trying to think it through first to avoid dropping a
bollock. What I definitely don't want to do is set up, announce and then find
I've missed something and have to disable the HS until I can fix the
issue :)
Anyone have any thoughts of what else there is to trip up on?

@_date: 2015-05-17 18:46:53
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Hi all,
Thanks for your input!
I'll confess to finding storify pages quite hard to follow at times, but
there's definitely some food for thought in there. Will try and give it
a fuller read a little later
I can think of some nastiness you could probably do if you could get
malware onto a target's machine, and they were connecting via a tor
router (or similar) rather than using SOCKS, but we're getting into a
few if's and but's there (plus, if you've got malware onto their
machine, there are better ways :) )
I agree, but I suppose that we (as an industry) have spent so long
telling people they can't trust a site if there's no 's' that we
probably can't complain too much about people asking for HTTPS.
Yup, basically the scenerio I was envisaging there was that Bob tries to
hammer the site with SQLi and get's "his" IP banned. That IP, of course,
being the IP of my tor client (so, localhost). Alice then comes along
for an innocent read and gets presented with a page telling her she's
been banned.
To be fair, the IP based bans don't offer any protection against skilled
(or very patient) attackers. If you've got an exploit that the protections won't pick up on, you only need the one request - whether that be after a ban has expired (or an IP change) or the very first request you place.
I hadn't actually thought about it in those terms, but you're right.
Maybe blocking off the shop section isn't quite as easy a decision as I
A thought that's occurred to me on this one - currently most static
resources are referenced from a subdomain so that the browser can
parallelise a bit more (the setup is much the same way you might push
static resources to a CDN). So something's going to need to change there, otherwise you'll be
hitting foo.onion and then requesting static resources from
That's handled by a plugin at the moment, so my initial thoughts on
working around that are:
Have the reverse proxy include an additional request header (e.g.
'X-ima-onion-user') and then hack on the plugin so it behaves differently if that header is present (either use a second .onion or make sure the references are relative).
Will also need to take that into account if I ever decide to have the
existing reverse proxy in front of the site cache responses.
Scalability is definitely my next challenge to think about. One thing
I've scratched down to come back and consider is whether it might be
setting up a (slightly weird) load balancing solution along the
following lines
You -> foo.onion (load balancer)
foo.onion -> 301 bar.onion
Where bar.onion is a mirror, so you might also end up being redirected
to jay.onion. One thing that concerns me about that is the number of
URLs you might end up with (if you hit bar.onion and then post the link
somewhere, what  if I take bar.onion out of service?).  I'm working on the assumption there that Tor is the bottleneck, of
course, if that can be discounted then it should simply be a reverse
proxy with multiple origins configured.
Will give it some more thought at some point
That I like as an idea. Does anyone know if Tor Browser warns when
redirecting from a HTTPS site to HTTP (which is how it'll view it). I
know IE historically has, but have just realised I don't know with
Firefox/TB. Might have to check later.
Yes, that definitely struck me as a benefit. Don't think anyone will
notice my traffic drop off the exits, but if enough people set something
similar up, it could potentially be a good saving
Looks like I've some building/testing to do over the next couple of days
then - should keep me out of trouble for a bit

@_date: 2015-05-19 07:40:36
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Both, I guess. I was thinking of where you have Tor running and then
routing to it (whether that's a gateway device or on your local machine)
rather than connecting via SOCKS. Essentially if there's a REDIRECT
iptables rule.
So Browser -> Port 53 -> Router/Tor rather than Browser -> SOCKS -> Tor.
A simplistic explanation would be anywhere that quietly dropping
127.0.0.1  foo.onion into /etc/hosts would break foo.onion for you. But,
as I say, if you're at the point you can changes on your target's
machine, there are better ways of getting what you want :)
Well, I can't be the only one, right? :)
Perhaps it's because I'm primarily an Ops guy, but I'd say it's more
complicated than that. It's about whether it can be done without breaking _anything_,
introducing un-necessary additional security risk (for example if the
existing Application level protections needed to be disabled, would I
want to continue?) and also of assessing what potential other risks
there are, and how best to mitigate them.
The site's live, carrying real traffic and may contain other people's
personal information - so for me, plan -> mitigate -> test -> action is
the only way to look at such a change :) Take the duplicate content penalty, for example, whilst it's not like I
lose my monthly income if I get it wrong, I'd still like to make sure I
avoid getting hammered in the indexes. Getting compromised because I failed to adequately think about the
potential security implications would be, at best, extremely
embarrassing and at worst could see customer data taken (though I don't
think that's too likely, it's still got to be considered a potential
We're not talking about a service that millions of people will notice a
few seconds of downtime, but we are talking about a service that's live
in the wild, so IMO the principal remains the same (with some
Perfect, thanks! That's a much cleaner way to mitigate, and as you say,
it means that legitimate users can be told to visit either the https or
the .onion directly
Nice, that'd give a good middle ground where the existing protections
can be left at their current level (so not weakening for the HTTPS site)
without meaning that someone tripping a protection makes the .onion
unavailable for everyone else for the length of the ban.
I'm assuming you're pushing an IP in that range into the X-Forwarded-For
Excellent, that was pretty much the lines of thinking I was following
for having the origin handle any absolute references (like the static
That's a nice idea, probably not going with HTTPS on the first outing,
but definitely making a note of that on my to-do list
Without wanting to start a thread-in-a-thread, I've definitely got mixed
feelings on that one. I think most sites should be using HTTPS, but I
think there are also cases where HTTPS genuinely may not be
needed/desirable. Whether or not that's correct, I don't think my _browser_ should be the
one to dictate that (though I recognise it's a good way to add inertia
to the issue). If a site I visit only uses HTTP, it's potentially me
(the powerless visitor) who's getting penalised by Mozilla there (the example they give is access to new hardware capabilities).
Anyway, that's a completely different topic so I'll cut that ramble off
before it really begins.
Thinking about it, I suppose you're right - especially if I do find
myself in a position of having to disable specific functionality because
it won't work with the .onion (but for some theoretical reason does with
an exit). I guess the analogy at that point would be being redirected
the dumbed-down mobile site which doesn't contain the functionality you
wanted to use.
For me, that seems a reasonable trade-off, and only slightly different
to the situation today on the www side. If a protection bans your IP,
you're only banned until you acquire a new IP (which might be as simple
as restarting your router if you were coming direct). Although it's unlikely, the another visitor _could_ have been allocated
the IP you've just released so may well not be able to access the site.
That's why bans should be configured to last just long enough to make
brute-force attempts painful, but short enough to mitigate the risk of
being enforced against the wrong end user.
I got heavily sidetracked yesterday, but hopefully I'll be putting some
time into this today (the initial set up will probably be quick enough,
it's testing that'll take some time)

@_date: 2015-05-19 09:49:57
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Are you doing anything the maximise the effect that (say) a ban based on
IP can have?
Say for example, I insert a 'unique' IP into a header on a per
connection basis - that'd ensure the ban is effective for the length of
the HTTP keep-alive session (i.e. until my attacker closes or times
out). They could, of course, create a new TCP connection as soon as they
realise they've been banned, which would circumvent that.
Have you made any changes lower down (similar to the patch str4d posted,
i guess) so that you can do it on a per-circuit basis (making things a
little harder)
I'm not overly worried about it, IP based bans aren't particularly hard
to circumvent on the www. anyway, but it's piqued my curiosity to think
about different ways you could go about doing it. Per client->reverse proxy connection is probably the best you could do
at the HTTP server level I guess, and per Proxy->Origin would risk
inconveniencing others (if the proxy re-uses a connection in the pool to
go upstream).
It depends, here's a massively oversimplified example
- Content served: Feature films
- Format: HLS (2 second segments)
- Max Requests per keepalive session - 300
Admittedly, that keepalive setting isn't ideal anyway, but it'll work on
port 80 (and keeps the maths easy for me) - the connection gets torn
down every 300 requests and then you re-establish.
Switch to HTTPS. Every 300 requests, the connection is still torn-down by the origin but
now you have to redo your SSL handshake etc. With VoD that's once every
600 seconds (as you only need to retrieve the manifest once). With linear video, it could be far, far more frequent, as you'll be
re-retrieving the manifest regularly.
If we switch from HLS to Smooth Streaming, it gets even worse - we've
now got double the number of requests per fragment because the audio
fragments are delivered separately.
Whether you notice the additional cost of re-establishing will depend
entirely on how many clients are connecting at once of course, but it's
quite a jump to take if the only reason you have SSL is because it's
being mandated by the user-agent.
There are, of course, tweaks that can be made (like upping that
keep-alive limit) to mitigate, but; the point I'm trying to make is that people tend to assume that the
traditional overhead of SSL is largely negated by the power of the
systems we use now, but there are definitely areas where that assumption
might be incorrect. So, to summarise, you're saying my original post was tl:dr?
There was an element of "is it a good idea" but the aim was more to
examine the complexities and checking that I was mitigating what could
be mitigated. It was definitely never intended as a 'Why would I want both', that one
I've already answered (and was included in the first post [0])
The first question asked was
So I fear you may have mis-understood my intentions when starting the
[0] -

@_date: 2015-05-19 11:04:16
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
The point, though, is that running both at once is definitely not the
same thing.
You're 100% on the mark that things that were set up for the www-front
need re-examining, to take into account things like re-writing urls (for
Simply setting up a www-front OR a hs-front does not introduce
challenges like how to handle/prevent the fact that there may be
consequences to it being accessible by completely disparate domains (the
duplicate content penalty being the example I gave).
I agree, setting it up is not hard - but making sure changes that are
made do not interfere with existing operations is something that should
not be overlooked lightly. Admittedly, I generally work on much larger
scale affairs than this, but the self enforced discipline is still there
- we do _not_ impact production - it's going to be the same back-end serving the content for both sites, so the risk needs to be assessed
Again, I disagree. Both fronts will be served by the same back-end (so I
don't see much value in putting effort into hiding the origin in this
case), but there's more to consider than that.
As an example, I'm going to be giving this a run-through on my personal
site first (the intention had been to do it the other way round, but if
we're going to break anything, let's do it on a lower traffic site eh?).
I touched on this earlier, but to give you a little more depth.
The origin for that site, runs a reverse caching proxy, so that the
back-end doesn't have to handle every request for a dynamically
generated page (content is in a CMS but doesn't change often).
Now, once we multihome, I'll be rewriting absolute URL's to go via a
.onion, which means I need to take the caching into consideration - if you visit via .onion and I visit that page a little later via www. then I get broken references to static content, and so a broken page.
It's easy enough to sort, I just need to make sure the two fronts are
different cacheable entities, but that's still a good step away from it
being quite as straightforward a consideration as you're painting it to
That may be what you've seen is this thread, but it's certainly not all
I've been doing. To be fair, I think you could argue the same about any thread asking for
other people's input on something, just like others I'm not looking for
people to tell me what to do, but for input to help me run through it in
my mind. There have been some good ideas mooted, and others have led me
to think deeper into it.
Sarcasm aside, yes that's the route I'd already settled on taking, even
if I revisit later. Although setting up HTTPS might bring a few benefits
in the future, I don't see much value into diving straight into getting
that working.
Just to jump back to something you said earlier by the way
Followed by
There you go, there's a reason against.
Similarly, if I was serving other people's content, they might require
it only be available to certain geo-graphic regions (which I can't
reasonably claim to enforce if you're hitting an .onion). Another reason
Neither are technical, sure, but there's definitely reasons against
making an existing www. available on a .onion
I'd be curious whether you can find a post where I asked for examples?
I've certainly been given examples (and I'm definitely not complaining
about that), but you seem determined to paint it as though I'm looking
to have someone give me the full set up for this - to re-iterate, I'm
not.

@_date: 2015-05-19 11:22:02
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Cool, that was pretty much my line of thinking - the protection offered
by an IP ban is minimal anyway, so it's not a major concern.
And if it's the only form of payment you accept (or are willing to
accept)? It's not automatically a show-stopper, sure, but it may still
be a reason against.
There might also be other non-technical reasons against (such as the
example I gave you), but this discussion is not about whether it's a
good idea or not.
I've been considering and replying to the examples, yes, but it does not
mean they're the only thing I'm considering. Given the length of some of
my posts already, would you prefer I started including things that I've
thought about but no-one else has yet mentioned (there are in fact a
And, yes, you're right, in my last post, I definitely did precisely that
Probably because I failed to state that the set up for the two sites is
nigh on identical (with the exception that my personal site has a
caching reverse proxy in front of it). There will be differences I'll
need to account for, but those will should all be minor changes - I'd
assumed that it should be reasonably obvious I'd have considered whether
testing against a different site would constitute a valid test or not.

@_date: 2015-05-19 12:00:42
@_author: Ben 
@_subject: [tor-talk] Making a Site Available as both a Hidden Service and 
Without going back to have a look, I don't know if it's out of context,
but my suspicion is I wasn't entirely clear :)
I'll be running a test, yes, but the changes (and subsequent testing)
will happen in the production environment. I've some issues with my dev
environment which I need to address, so whilst I could use it for basic
tests, I'm not sure they'd necessarily be valid. Although I see it as a big enough change that it needs some thought
first (most of which is done), it's not so big that I feel I have to
consider it gated by the dev-env issues, especially as I've a different
site to test it against.
So the test will be against a live site but one with a much lower
traffic footprint (and one that, in worst case downtime won't lead to my
inbox being spammed). Basically, all I'm trying to avoid at the moment
is the things that are either severe, or will take a while to correct
(such as getting canned in Google's indexes), everything else will be tested and measured as I go.
It's a pity we live in a world where that could be considered HS
specific, but yes you're right - some of the bits I've also been
thinking about relate to whether there's anything on the www-front that
I'd consider either un-necessarily risky or less acceptable on a .onion.
There shouldn't be any, but I don't subscribe to the idea that it's your
problem if you've got JS enabled - I need to be at least considering
whether there's anything you might not want via an .onion that's
considered acceptable on the www. (Analytics is a reasonable example -
if you're accessing .onions through a gateway and www. traffic egresses
normally, the analytics package can tie your .onion session to your real
Having set up more than a few HS with that aim (though it wouldn't be
liberty/life threatening if I made a mistake - I'm fortunate to be in
that position) I have to admit it'll be nice to not have to do too much
on that aspect.
Based on your counter example, I think the same is (or will be) very
much due to you - longer term, I probably will dig deeper into that side
of it, partly because we don't know what the future will hold, but also
it gives me something new that isn't work to tinker around with :)

@_date: 2016-06-18 22:36:40
@_author: Ben 
@_subject: [tor-talk] Tesco's mobile banking app refuses to run on handsets 
All in the name of security apparently.
Their approach to security on their site, on the other hand, is to
simply slap a cert on it and not bother with HSTS, HPKP or DANE. They've
not even bothered with DNSSEC, and from the comments on that article
don't seem to care if your connection transits the Tor network so long
as it's via something on the network rather than a locally installed
The app at least verifies the certificate it's presented, but relies on
the devices trust store, so if you can get a certificate from any of the
_many_ CAs a handset trusts MiTM is as simple as redirecting DNS to your server and telling Nginx to listen or port 443, proxy to localhost 80 and then to proxy upstream on 443
And with a quick TCPDump you can start extracting credentials and other
exciting things
GET /broker/api/users/ids/7654321duh HTTP/1.0
Host: mob.tescobank.com
Connection: close
X-ClientAppVersion: 1.7.0
X-AvlHeight: 1920
X-InternalIP: 10.0.0.9
X-DeviceID: [Redacted]
X-Timezone: Greenwich Mean Time
X-Language: English
X-Jailbroken: N
X-FullWidth: 1080
X-Mac: [redacted MAC address]
X-OSName: Android
X-Credential: MobWord
X-AvlWidth: 1080
X-FullHeight: 1920
X-OSVersion: 4.4.2
X-DeviceType: GT-I9505
User-Agent: Dalvik/1.6.0 (Linux; U; Android 4.4.2; GT-I9505
Accept-Encoding: gzip
Almost like have Orbot installed isn't their biggest
Also, the check for whether a device is rooted is obviously faulty - the
phone I tested from is very definitely rooted. DNS for mob.tescobank.com
resolving to an IP on the same subnet as my phone should probably be a
concern too.
Given they know who issues their certificates, perhaps they should focus
more on tightening their own security that dropping in checks for other
apps (seems it objects to a number of packet sniffer apps too)

@_date: 2016-06-18 22:21:37
@_author: Ben 
@_subject: [tor-talk] Tesco's mobile banking app refuses to run on handsets 
All in the name of security apparently.
Their approach to security on their site, on the other hand, is to
simply slap a cert on it and not bother with HSTS, HPKP or DANE. They've
not even bothered with DNSSEC, and from the comments on that article
don't seem to care if your connection transits the Tor network so long
as it's via something on the network rather than a locally installed
The app at least verifies the certificate it's presented, but relies on
the devices trust store, so if you can get a certificate from any of the
_many_ CAs a handset trusts MiTM is as simple as redirecting DNS to your server and telling Nginx to listen or port 443, proxy to localhost 80 and then to proxy upstream on 443
And with a quick TCPDump you can start extracting credentials and other
exciting things
GET /broker/api/users/ids/7654321duh HTTP/1.0
Host: mob.tescobank.com
Connection: close
X-ClientAppVersion: 1.7.0
X-AvlHeight: 1920
X-InternalIP: 10.0.0.9
X-DeviceID: [Redacted]
X-Timezone: Greenwich Mean Time
X-Language: English
X-Jailbroken: N
X-FullWidth: 1080
X-Mac: [redacted MAC address]
X-OSName: Android
X-Credential: MobWord
X-AvlWidth: 1080
X-FullHeight: 1920
X-OSVersion: 4.4.2
X-DeviceType: GT-I9505
User-Agent: Dalvik/1.6.0 (Linux; U; Android 4.4.2; GT-I9505
Accept-Encoding: gzip
Almost like have Orbot installed isn't their biggest
Also, the check for whether a device is rooted is obviously faulty - the
phone I tested from is very definitely rooted. DNS for mob.tescobank.com
resolving to an IP on the same subnet as my phone should probably be a
concern too.
Given they know who issues their certificates, perhaps they should focus
more on tightening their own security that dropping in checks for other
apps (seems it objects to a number of packet sniffer apps too)

@_date: 2016-06-18 22:44:58
@_author: Ben 
@_subject: [tor-talk] Tesco's mobile banking app refuses to run on 
Whoops, sorry for the double-post, Mailserver claimed sending had failed

@_date: 2016-06-23 16:14:02
@_author: Ben 
@_subject: [tor-talk] Reminder to stay on-topic 
As much as I (and presumably everyone else) was getting sick of updates
on the topic coming into my inbox, there's definitely an argument, given
the position Jacob held, that it is/was at the very least Tor Project
related - so tangentially on topic, And as noted, there are plenty of
other discussions in the past that have wandered far, far off course.
But, for my 2 cents, it's some of the bans I have an issue with. Even if
we agree that the stuff related to Jacob was off-topic, for me
personally, it still doesn't sit right.
ja.talk was spamming the list with reposts - sure, chuck them a ban.
But some of those responding (described as "ludicrous rape
apologetics".... nice, classy....) were responding to say they felt the
evidence doesn't weigh up. It's an emotive topic for many, so of course
people are going to respond to repeated posts that colour someone elses
character - especially early on when the evidence was best described as
anecdotal and flimsy, and particularly if they happen to know the guy
and believe (for whatever reason) that the allegations are false.
Put it another way, regardless of forum, if you saw someone you knew
being (to your mind) libelled and labelled a rapist, would you sit back
or would you respond? IMO a warning would have sufficed - assuming there
isn't other stuff that's happened in the background
At the end of the day though it's the Tor Project's call. I'm not going
to miss the JA related threads, though I may miss some of the input of
some of those who've been banned.
