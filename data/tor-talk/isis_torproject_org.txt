
@_date: 2013-10-01 01:02:59
@_author: isis 
@_subject: [tor-talk] Why the Web of Trust Sucks 
Mike Perry transcribed 6.8K bytes:
Actually, this only works, given that Edward has *ultimate* trust in
Bruce. Most people only give ultimate trust to keys which they control.
See the options for the 'auto-key-locate' settings, specifically 'pka',
'cert', 'keyserver-URL', and 'ldap'. Don't ask me how the LDAP one works, I've
never messed with it.
1. DNS PKA
   For the DNS PKA setting, you'd need to add a TXT record to your DNS. [1][2]
   To see mine, do "host -t TXT isis._pka.patternsinthevoid.net". It looks
   like in my zone file:
   isis._pka       14400   TXT "v=pka1;fpr=0A6A58A14B5946ABDE18E207A3ADB67A2CDB8B35;uri=
   It unfortunately doesn't work if the specified URI is HTTPS, but it does
   work with crappy old protocols like finger. Go figure. However, there is
   some of the functionality you want, if you also add "pka-lookups
   pka-trust-increase" to the "verify-options" setting in gpg.conf (doing so
   would automatically increase your trustdb calculation for that key, if it
   has a DNS PKA TXT record and that the fingerprint specified there matches
   the fingerprint on the key you already have).
   Also, a warning: gpg totally disrespects any proxy settings for DNS
   resolutions. The only way around this (safe in my tests so far, which have
   been going for almost a year now) is to resolve the domain of whatever
   keyserver you use, and then use it's IP address instead. For example, in my
   gpg.conf, it goes like this (for hkps://keys.indymedia.org)[3]:
   keyserver hkps://204.13.164.120
   keyserver-options verbose verbose verbose include-revoked include-disabled no-auto-key-retrieve no-honor-keyserver-url no-honor-pka-record include-subkeys no-include-attributes ca-cert-file=~/scripts/certs/keys.indymedia.org http-proxy=socks://127.0.0.1:59050
   Also, using HTTPS or HKPS generally doesn't work, or is a huge pain in the
   ass to get working. These people still use finger, for fucks sake. I don't
   think they've heard the word "privacy" before.
2. DNS CERT
   Essentially the same as DNS PKA, but you need to compile GnuPG, including
   the tools/ subdirectory to get the make-dns-cert script. This actually puts
   your GPG key in a DNS TYPE37 record, [4] and (I think?) it works the same as
   DNS PKA for the trust increase stuff.
3. Preferred Keyserver notation packets
   For the '--auto-key-locate keyserver-URL' setting, see also the
   'preferred-keyserver-url' setting, which adds a Preferred Key Server
   notation packet [0] to all signatures and certifications. For example,
   looking at the last commit I made on BridgeDB (I sign all my commits):
    * commit 44606980d4674548048e164106cf2d2e73cd6a08 (HEAD, tpo-isis/feature/9199-improved-logging-2-r2
 gpg: Signature made Mon 30 Sep 2013 02:04:07 UTC
    gpg:                using RSA key 5C17776E27F7E84D
    gpg: Good signature from "Isis! " [ultimate]
    gpg:                 aka "Isis " [ultimate]
    gpg:                 aka "Isis " [ultimate]
    gpg:                 aka "Isis! " [ultimate]
    gpg: Signature policy:     gpg: Preferred keyserver:     gpg: Signature notation: isis at patternsinthevoid.net=FC63AA5CD193869C3237145A5C17776E27F7E84D
    gpg: Signature expires Tue 30 Sep 2014 02:04:07 UTC
    Author:     Isis Lovecruft  AuthorDate: 81 minutes ago
 Commit:     Isis Lovecruft  CommitDate: 77 minutes ago
     Add bridgedb.utils package, and add bridgedb.utils.parsers module.
  lib/bridgedb/utils/__init__.py |  9 +++++++++
  lib/bridgedb/utils/parsers.py  | 32 ++++++++++++++++++++++++++++++++
  requirements.txt               |  1 +
  setup.py                       |  3 ++-
  4 files changed, 44 insertions(+), 1 deletion(-)
   A normal gpg client would use    as the keyserver for updating and re-fetching my key. (The push updates
   don't work so well, I just get a denied resource in my webserver logs,
   which get shredded like every three hours or something.)
   To see all the extra crap on my signature on this email...you'd need to do
   "gpg --verify-options show-policy-urls show-notations show-user-notations
   show-keyserver-urls pka-lookups pka-trust-increase --verify
   ".
This already exists, see the 'ca-cert-file' option under 'keyserver-options'
in the gpg man page. It seems like a bad idea to hardcode it, because then I'm
dependent upon Werner Koch to update the code when the cert for
keys.mayfirst.org or keys.indymedia.org changes (and then also dependent upon
the distro maintainer of gpg to backport the patches, if I were to use a
packaged version).
Also see my above note under "1. DNS PKA" above, on HKPS/HTTPS keyservers.
Alright, but your problems are GnuPG-centric, and there are other ways of
checking trust paths. Try wotsap, lysator, [5] or footsie. [6] Just because
GnuPG and the most of the keyservers are exceptionally outdated (and IMO
crappy to begin with), doesn't mean that the WOT model is broken. I mean,
sure, it's centralised, hard to understand, requires trust-on-first-use, is
prone to all kinds of timing correlations attacks and enables social network
graphing. But that doesn't mean it's broken, right?
You know what, Mike Perry? I think you're just jealous that all the other Tor
developers are in the Strong Set. You know, we could put you in the Strong Set
pretty fast, I could put that cronjob back... :D [7]
I'm conflicted. I think GnuPG is a piece of shit, and that it's actively
hindering security via the difficulty of its use. GnuPG means that I can't
safely talk to my mom; I hate the thing. And I hate the WOT, just as much as
you do, if not proportionally more given my familiarity with it.
Fuck it. It's broken.  Not for the reasons you've listed, though. I think for
the most part the problems you mention are GnuPG-centric and I've invalidated
a good chunk of them (not excusing the GnuPG implementation of the OpenPGP
spec, which could likely be re-implemented in a substantially more robust and
user-friendly way). I think it's broken only because my mom's been coding
since before I was born and she can't fucking use the damned thing.
Somebody please write a replacement. One, two, three, not it. Or give Pond a
decent peer review. [8]
[0]: "Preferred Key Server" [1]: "OpenPGP in DNS" [2]: "Publishing PGP keys in DNS" [3]: The simplest way to get the the cert:
     openssl s_client -connect keys.indymedia.org:11372 /dev/null | \
         openssl x509 -in /dev/stdin
     The port for keyservers is 11372 (usually) if it's encrypted, and 11371
     otherwise.
[4]: "RFC 4398: Storing Certificates in the Domain Name System (DNS)"
     [5]:  [6]: [7]:                     [8]:

@_date: 2014-08-04 20:49:04
@_author: isis 
@_subject: [tor-talk] using locally installed Tor in TBB 
Patrick Schleizer transcribed 2.4K bytes:
This should be fixed (for Linux) in an upcoming Tor Browser 4.0 release. I've
added these things to the `start-tor-browser` script. There are:
    - Instructions for use, including additional Firefox preferences that
      you'll need to set (to tell Tor Button where your ControlPort and
      SocksPort are, etc.)
    - An "easy" spot in the `start-tor-browser` script to put in your
      ControlPort password so that it is given to the Firefox process (but not
      exported, because then other programs might be able to steal it)
    - A line will print to your terminal telling you that you're using a
      system-installed Tor.
See  [0] and this commit [1] containing the changes. If you're running
Linux and would like to test these, nightly builds are available. [2]
Contributions to improve this, i.e. automatically setting the preferences for
you, are welcome.
[0]: [1]: [2]:

@_date: 2014-08-08 00:13:16
@_author: isis 
@_subject: [tor-talk] using locally installed Tor in TBB 
Patrick Schleizer transcribed 2.2K bytes:
Hey Patrick!
Thanks for the thanks, and for the code review!
tl;dr: I've never used that devscript before. Neat, thanks! You're actually
correct that `/bin/bash` doesn't need to be there, because my changes which
required it [0] were reverted for TB-4.0-alpha-1 [1] due to the concerns
(mostly the fault of HTTPSEverywhere logging URIs) raised by Robert Ransom on
ticket  [2] (although they are still in 3.6.x). To read more about why I
wrote this crazy code, read on. :)
I think I probably know most of the bashisms, like `[` being a command, you
can still `exec 3<>/dev/tcp/fyb.patternsinthevoid.net/80` to open a TCP
connection even if there isn't a `/dev/tcp`, [3] doing `echo '-e'` doesn't
echo anything because echo is a broken piece of crap... I can keep going. :)
Bash is messed up. Usually, I know how to convert bash syntax to shell, but
there are a couple builtins in particular which don't really have a shell
Normally, I would not want to switch to bash due to portability issues, as you
point out (and unstable syntax and behaviour between bash versions). But the
the particular bashism that I needed in this case, there isn't any equivalent
(to my knowledge) in shell: the `disown` builtin. It's used in the very last
stanzas that I added to the start-tor-browser script:
for second in `seq 1 15` ; do
    sleep 1
    if `kill -0 $pid 2>&1 >/dev/null ` ; then
        wait "$pid"
        exitcode="$?"
        printf "Tor Browser exited suddenly! Exit code: %s\n" "$exitcode"
        exit "$exitcode"
    else
        continue
    fi
if test -z "${exitcode}" ; then
    if test -z "$(kill -0 $pid 2>&1 >/dev/null)" ; then
        printf "Running Tor Browser process (PID %s) in background...\n" "$pid"
        disown "$pid"
        exitcode="0"
    else
        exitcode="66" # Something odd happened
    fi
exit "$exitcode"
For additional reasoning behind this crazy code, see my commit message. [0]
Hmm, like most static analysis tools, it seems like it's not really doing a
good job at taking overall context into account:
  1. There are plenty of cases where I *do* want ${ to glob and split words,
     that's *why* there's no quotes around it.
  2. It doesn't seem to know about `test -t` syntax to test if a file
     descriptor is opened on a terminal.
  3. And it seems to be confused on the difference between $(...) and `...`
     syntax. $(...) is evaluated at access time (if unquoted), `...` is
     evaluated immediately. There's a good Bash Hackers article on this [4]
     (the writer actually concludes on the $(...) side of things too, like the
     shellcheck tool), but I really think it's a style preference. I could
     easily argue that because:
         $( (ls) )
     and
         $((ls))
     are entirely different commands, that therefore $(...) syntax is
     confusing and tricky and should be considered deprecated. Also, while
     `...` doesn't handle nested backticks nicely, its *real* POSIX equivalent
     "$(...)"  (with quotes) doesn't handle nested double-quotes nicely. :)
Though `shellcheck` is actually pointing out some valid mistakes. I'll have to
ask Mike Perry and the rest of the Tor Browser Team what they want done with
that. (I think there were some major changes planned, and the
start-tor-browser script is going to deprecated/replaced?)
Either way, you seem really knowledgable about shell scripting, if you wanted
to contribute to Tor Browser, using your skills on all the bash/shell in the
the tor-browser-bundle repo [5] would be a great way to start.
In the case of the old (pre TB-3.6.2, before my crazy `wait` code above),
possibly. But doing so would obviously mean that the `firefox` process and its
environment would replace the shell. This means that when the `firefox`
process dies, the shell would disappear (along with any advice, errors, logs
which had been logged to it, since the old version doesn't log to file).
In the versions which do have my crazy code, that crazy code wouldn't get run,
because there wouldn't be a shell to run it any longer. I felt that the crazy
code was necessary, as it gives error messages for the potential cases where
Firefox couldn't start because it was already running, couldn't start period,
exited unexpectedly, or couldn't be disowned for some reason. In all cases, my
crazy code *should* handle dealing with hung, non-existent, errored,
etc. Firefox processes, and then it *should* always exit the shell, which (to
my understanding) is pretty much the reason one is supposed to `exec` a
wrapped process, so that an extra (sub)shell process isn't left lying around.
I could be entirely wrong, though, and perhaps there is some fancy `exec`
syntax which takes care of the processes errors. (If there is, I'd be stoked
to learn about it!)
That sounds great! Just be sure to explain weird shellisms and bashisms with
code comments or commit messages.
I also see you have an (somewhat) old tbb-scripts repo. [6] I've got a bunch
of random hacky tor-related scripts too, for setting up transproxies and
catching GFW garbage probes, [7] verifying gitian builder scripts, [8] doing
all your DNS on a remote Tor relay (through tor, with DNSSEC validation and
caching), [9] etc. I know Ximin's (infinity0) got some nice hacky scripts too,
and meejah, perhaps we should all join forces and create a tor-hacks repo?
[0]: [1]: [2]: [3]: [4]: [5]: [6]: [7]: [8]: [9]:

@_date: 2014-08-18 22:56:49
@_author: isis 
@_subject: [tor-talk] Scaling Tor 
Mike Fikuart transcribed 4.8K bytes:
This is an interesting idea. Variants using random walks through nodes which
only know a random subset of other nodes have been proposed before, e.g.
MorphMix. [0]
However, it should be impossible to verify that a given sequence is, in fact,
random, rather than being a sequence in seeded such a way that it is
predictable, or an encrypted sequence, etc. The biggest concern with improving
Tor's scalability via handing out random samples of nodes from the consensus
would then be that malicious Directories (whether Authorties or simply
mirrors) could collude to hand out predictable subsets of relays to some/all
Further, even if we could verify that a given sample was truly random, and we
checked the results for some subset of clients, this would not prohibit
certain clients from being lied to. I would argue that the security of the
group of all Tor clients is only as good as the worst case scenario, i.e. any
mechanism which would allow a single client to subjet to targeted attacks is
an attack against all.
Nicholas Hopper and Nikita Borisov are two of the more significant researchers
who explore scaling specifically for Tor and/or onion routing in general.
Perhaps some of the following may help give you an idea of the extant research
in this area:
For a more detailed explanation of why random subsets of nodes cannot be used
to securely pick an unbiased path (more specifically, why we won't use most
DHT algorithms, or the Salsa/Cashmere DHT-overlays), see "Hashing it out in
Public". [1]
For an interesting proposal for using some specific DHT algorithms which claim
to keep maintain the current levels of security while providing better
scalability, see the Torsk paper. [2]
And for a Private Information Retrieval (PIR) based approach (admittedly, I
haven't read it yet, but it's been on my reading list for a while!), which,
like other PIR systems would permit DHT-like queries albeit without the
Directory being able to know what is being looked up, see the PIR-Tor
paper. [3] However, I think I recall from my skimming that the lookups
produced *routes*, not nodes... which is worrisome for another set of reasons.
[0]: M. Rennhard and B. Plattner.
 "Introducing MorphMix: Peer-to-peer based anonymous internet usage with collusion detection."
 In ACM Workshop on Privacy in the Electronic Society (WPES 2002),
 pp. 91?102. ACM, 2002.
[1]: Tran, Andrew, Nicholas Hopper, and Yongdae Kim.
  "Hashing it out in public: common failure modes of DHT-based anonymity schemes."
  In Proceedings of the 8th ACM workshop on Privacy in the electronic society,
  pp. 71-80. ACM, 2009.
  [2]: McLachlan, Jon, Andrew Tran, Nicholas Hopper, and Yongdae Kim.
  "Scalable onion routing with Torsk."
  In Proceedings of the 16th ACM conference on Computer and communications security,
  pp. 590-599. ACM, 2009.
  [3]: Mittal, Prateek, Femi G. Olumofin, Carmela Troncoso, Nikita Borisov, and Ian Goldberg.
  "PIR-Tor: Scalable Anonymous Communication Using Private Information Retrieval."
  In USENIX Security Symposium. 2011.

@_date: 2014-08-19 00:07:10
@_author: isis 
@_subject: [tor-talk] [tor-consensus-health] Consensus issues 
Sebastian Hahn transcribed 3.9K bytes:
s/Consensus belonging to/Consensus as reported by/
Only somewhat tangential, I still believe we shouldn't be mincing terms: a
consensus is full agreement without any blockers, whereas what Tor uses is
merely a democratic vote. Consensus-based decision making and democracy are
such different beasts that I might fight for the prior, yet would nearly
always denounce the latter as majoritarianist fascism-in-disguise.
Sebastian (err, the "bastik" one :) ), you might consider joining the
 channel on OFTC, it'll give you updates on the myriad ways that the
DirAuths are constantly being painful for their poor maintainers. :)

@_date: 2014-08-19 00:10:03
@_author: isis 
@_subject: [tor-talk] Scaling Tor 
isis transcribed 6.8K bytes:
I've just realised that my brain must have been sourcing Andrew's post without
telling me, because I just cited all the same papers as Andrew did [0] over a
year ago. BTW, if anyone has found/written more recent, worthwhile papers on
this topic, we'd love to hear about them!
[0]:

@_date: 2014-08-19 22:22:37
@_author: isis 
@_subject: [tor-talk] XSS on blog.torproject.org - 8 month old ticket? 
Nusenu transcribed 1.3K bytes:
The best way to see a change done is to do it yourself. Also see
 which probably explains
why no one has fixed the XSS.

@_date: 2014-08-28 00:17:18
@_author: isis 
@_subject: [tor-talk] Tor Data Leak 
sureyourejoking at aim.com transcribed 1.6K bytes:
Have you ever subscribed to any live bookmarks or RSS/atom feeds using Tor

@_date: 2014-08-28 05:36:04
@_author: isis 
@_subject: [tor-talk] Tor Data Leak 
sureyourejoking at aim.com transcribed 2.1K bytes:
No problem.
You've got an Apple RSS/Atom feed reader program, called PubSub, installed.
From the Apple documentation, [0] the thing appears to register a mimetype
handlers, so that any feeds you click on open in PubSub, rather than Firefox
or TorBrowser.
I recommend that you uninstall PubSub. Or find a friend who knows more about
MacOS who can help you.
[0]:

@_date: 2014-07-04 21:36:23
@_author: isis 
@_subject: [tor-talk] messing with XKeyScore 
Hash: SHA512
Eugen Leitl transcribed 5.8K bytes:
Hi. I maintain and develop BridgeDB.
For what it's worth, the released XKS rules would not have worked against
BridgeDB for over a year now. I have no knowledge of what regexes are
currently in use in XKS deployments, nor if the apparent typos are errors in
the original documents, or rather typos in one of the various levels of
transcriptions which may have occurred in the editing process. If these typos
were at some point in the original rules running on XKS systems, then *no*
bridges would have been harvested due to various faults. None.
Ergo, as Jacob has pointed out to me, the regexes which are released should be
assumed to be several years out of date, and also shouldn't be assumed to be
representative of the entire ruleset of any deployed XKS system.
I am willing to implement tricks against specific problems with them, mostly
for the lulz, because fuck the NSA. But it should be assumed that the actual
regexes have perhaps been updated, and that highly specific tricks are not
likely to land.
The ticket for this, by the way, was created by Andrea this afternoon, it's
 Interesting. I'm glad someone else is paying that close of attention to these
regexes. I'd totally take a patch which implements the BridgeDB equivalent of
little Bobby'); DROP TABLE Students.  Granted, as I said above, it likely won't land. But for the lulz. :)

@_date: 2014-07-08 12:36:55
@_author: isis 
@_subject: [tor-talk] messing with XKeyScore 
Sebastian Hahn transcribed 2.5K bytes:
The format is best described in torspec.git/pt-spec.txt, [0] given as:
[["Bridge"] SP] [[METHOD] SP] IP:PORT [SP [FINGERPRINT]] [[[K=V] "," [[K=V] ","]] ?]
BridgeDB currently doesn't include the "Bridge" prefix (and hasn't for just
upwards of one year now) due a backwards-compatibility issue with Vidalia. [1]
Meaning that a correctly formed bridge line currently looks like this:
obfs4 1.2.3.4:11111 abcdef0123456789abcdef0123456789abcdef01 sekrit=fu,password=bar
TorLauncher is smart about this, and if a bridge line (such as this one)
doesn't start with "Bridge", then TorLauncher rewrites the line before adding
it to the user's torrc file:
Bridge obfs4 1.2.3.4:11111 abcdef0123456789abcdef0123456789abcdef01 sekrit=fu,password=bar
The same obviously happens when configuring bridges in Tails, because Tails
now uses TorLauncher. The biggest problem we've seen here is that users cannot
correctly/accurately type a bridge's fingerprint.
I agree with Sebastian and Matthew. I'm not willing to deploy something which
makes it more difficult for legitimate Tor users to obtain/utilise
bridges. It's already difficult enough to correctly type a fingerprint.
There are plans to move towards implementing rBridge [2], which would allow
legitimate users to receive new bridges automatically and anonymously. There
is, however, no funding for this work. Either way, due to implementation
difficulty (largely due to certain prerequisite cryptographic primitives of
the anonymous authentication scheme) and integration with Tor Browser, this
will take me and the other volunteers quite some time. Give or take, 2-3
[0]: [1]: [2]:

@_date: 2014-07-14 18:06:09
@_author: isis 
@_subject: [tor-talk] Questions about NSA monitoring of Tor users. 
simonsnake at openmailbox.org transcribed 2.2K bytes:
Couple points of clarity:
The QUANTUM program, which is actually a family of
attack vectors developed by the TAO division of the NSA, actually
mainly allows for packet injection, i.e. inserting a cloned and/or
modified TCP packets or HTTP request/responses with source spoofing
which beats the original packet to its final destination. [0]
There are several programs which allow for the possibility of traffic
analysis, one of which is XKEYSCORE (XKS). [1] Several of these
programs interface with programs such as those in QUANTUM. Traffic
analysis programs aren't "data collections things"; [2] instead, they
run pre-collected traffic through a complex series of rulesets in
order classify the traffic for further processing by other programs,
or for storage in a database.
An example flow for the way an email to bridges at torproject.org might
be processed would be:
 0. The outgoing email from your Gmail account is captured by PRISM,
     or a related/similar traffic surveillance program.
 1. The captured email is processed by XKEYSCORE.
    1a. The email matches the XKEYSCORE ruleset as being an email to
        bridges at torproject.org (published in the Das Erste article you
        linked to above).
        1a. i. Your outgoing email to bridges at torproject.org, possibly
               along with other associated information, is stored in a
      	       database.
        1a.ii. Other processing can happen at this point, if there are
               additional matching XKEYSCORE rules defined on the
               XKEYSCORE system processing your traffic.
    1b. The email doesn't matches any XKEYSCORE ruleset.
        1b. i. UNKNOWN. We don't know yet what is done with the
               captured traffic at this point.
Dynamism, to the extent that it prevents geolocation, in IPv4 address
assignment is mostly a thing of the past. I'm usually able to
accurately track an IPv4 address down to the city, and I'm sure they
can do much better.
What they achieve is the ability to accuse a person in the future
based on that person's browsing/usage history. Why is this dangerous?
For the US, the Congressional Research Service has stated that they do
not know the precise number of federal crimes in effect in a region at
a given time. Ergo, one could assume that if the number of these laws
is unknown, their contents are likewise unknown. And therefore, not
even a good lawyer knows off the top of her head if her client is
doing something illegal. And then take into account that laws in the
US are interpreted by historical precedence, and it now also matters
when that person is accused of doing something. You have NO IDEA if
anything you are doing is legal or illegal. There is an excellent
lecture by a Regent Law Professor explaining more. [3]
Anyone who has regularly contracted or actively volunteered with Tor
has likely had quite some experience with spooks, not only Roger;
though, Roger is probably a bit nicer when he talks to them than some
others of us.
I've contracted to the Tor Project for four years and volunteered some
before that. I've spoken to senators and representatives on Capitol
Hill, [4] as well as other agencies, regarding my work. The State
Dept. has mentioned work by OONI that I had contributed to during one
of their morning televised briefings. [5]
The behaviours of the various branches and departments of the US
federal goverment is, in my opinion (my views do not necessarily
express those my employer's), like that of a two-year-old with
Multiple Personality Disorder. They only rarely accurately comprehend
the scope and impact of a technology, e.g. I've been asked by
congressional aides if the tools I contribute to "are for other
countries, or for the US?"  They seem to think there are borders on
the internet. Many of its personalities are often in direct conflict
with one another. Some of its personalities are downright sociopathic
and strive mainly for selfish ends via means which harm the
overwhelming majority of people worldwide, both US persons and
otherwise. In my opinion, the NSA, the FBI, and the CIA are prime
examples of the US federal government's sociopathic personalities.
As someone else mentioned in this thread, the official task of the NSA
is to monitor communications: "collects, processes, and disseminates
intelligence information from foreign signals for intelligence and
counterintelligence purposes and to support military operations."  The
NSA is also tasked with "preventing foreign adversaries from gaining
access to sensitive or classified national security information". [6]
Weakening the security of systems, while simultaneously preventing
others from accessing them, would make it appear as if the NSA is
actually in direct conflict with itself.
Additionally, the NSA is in direct conflict with the missions of
several other departments, e.g. the State Dept.'s aims to protect
U.S. citizens living/travelling abroad and assist U.S. companies in
the international marketplace, and likely several other Department's
mission statements.
[0]: [1]: [2]: [3]: [4]: [5]: [6]:  (Oh, the synecdoche!
      nsa.gov has a valid SSL cert, only to downgrade you to plaintext!)

@_date: 2014-07-14 22:29:17
@_author: isis 
@_subject: [tor-talk] [liberationtech]  messing with XKeyScore 
Seth David Schoen transcribed 3.2K bytes:
Yep, the idea is pretty old. A variant of it was even written up into
RFC1751 in 1994. [0] That one is already in the pycrypto module, [1]
at `Crypto.Util.RFC1751`. It encoded a fingerprint into 30 words with
maximal length of 4 characters each:
'CANT FLUB GLOB BURN LEA OW PA COD GLEN NILE WRY HUNT CANT GUSH GLOW CENT LAM LOFT RAP HO GYP DENY CARL HUE CANT FIVE ANNA CREW LYE HUG'
FWIW, Leif's Bananaphone (SSH-over-Markov-Chains) tool [2] would also
work for this purpose, also more efficiently than Zimmermann's PGP
wordlist; I was able to get an encoded fingerprint easily down to 28
words, each word between 2-7 characters:
?!isis?wintermute:~ ? egrep -x '\w{2,7}' /usr/share/dict/words > fingerprint-corpus.txt
?!isis?wintermute:~ ? wc -l fingerprint-corpus.txt
31082 fingerprint-corpus.txt
?!isis?wintermute:~ ? du -sh fingerprint-corpus.txt
212K    fingerprint-corpus.txt
?!isis?wintermute:~ ? XZ_OPT=-9 tar cJf corpus.txt.tar.xz fingerprint-corpus.txt
?!isis?wintermute:~ ? du -sh corpus.txt.tar.xz
56K     corpus.txt.tar.xz
?!isis?wintermute:(master>)~/code/torproject/bananaphone ? \
  echo -n 'c0f30b8a1ab2b4c9c2242b1b6cda9688c01462f8' | \
  python bananaphone.py pipeline \
  'rh_encoder("words,sha1,12", "random", "../../../fingerprint-corpus.txt")' > test1
?!isis?wintermute:(master>)~/code/torproject/bananaphone ? cat test1
beach pint heard phases Gallo giblet remands hares hiccups enlarge hates Pound Eisner unblock leakage lowdown desists prickly kilns motion tikes saltier cosiest incrust rarity Ulster wallops
?!isis?wintermute:(master>)~/code/torproject/bananaphone ? cat test1 | \
  python ./banaphone.py pipeline 'rh_decoder("words,sha1,12")'
An additional benefit to using Bananaphone would be that it doesn't
require the client-side (the TorLauncher side) to have the corpus (the
`fingerprint-corpus.txt` file, in the above example). Also, because of
this, BridgeDB could easily use Spanish, Arabic, etc. corpuses for
users who are perhaps unfamiliar with English words.
This, however, would mean that we'd need Banaphone written in JS.
This is a great idea.
The un-translation of the encoded fingerprint would likely need to
happen in either little-t tor or in TorLauncher.
I'm willing to take a patch to BridgeDB's UI templates to optionally
encode the fingerprint as a list of words, though I would prefer the
default display to give the actual fingerprint.
And I spoke briefly to Mike Perry about the possibility of including
such a decoder in TorLauncher. Mike said he's willing to merge the
patches, but unwilling to force the Pearl Crescent hackers (who
wrote/maintain TorLauncher) to use their time implementing this.
There is the possibility that, in the future, one of Tor's regular
funders will be sponsoring a thing called "Tor Challenge", which is
where Tor Project people point at small projects which require less
expertise yet which they would really like to see done, then someone
wanting to help out comes along and does whatever that small project
is and gets to collect the funding allotted to that Challenge. I'd
gladly add this to my list of things I want done.
Also an interesting idea. But what if they type the checksum wrong?
[0]: [1]: [2]:

@_date: 2014-07-23 02:29:08
@_author: isis 
@_subject: [tor-talk] Fwd: Tor and tlk.io 
Scott Arciszewski transcribed 0.9K bytes:
Many sites use HTML5 canvas fingerprinting. Visiting either
 or should trigger that little dialogue about "accessing the canvas" in TorBrowser
Can I ask you a question? When this dialogue (the one) comes up, what do you usually do? Do you click the "Allow in the Future"
button? Or click the little "X" in the corner? Or something else?
TorBrowser is patched to block attempts by websites to access HTML5 canvases,
since there isn't much legitimate purpose for a site to do this, other than to
track you as that article you linked points out.
However, if you've already clicked the "Allow in the Future" button on the
little dialogue that comes down from the URL bar when a site attempts to do
this, there isn't currently an easy way to revoke the permission you gave. [0]
Additionally, there appears to be an issue in nsIPermissionManager (used by
TorButton when "New Identity" is clicked), because the permissions currently
aren't being cleared properly. [1]
For now, my best advice is to be very careful allowing any site to access
HTML5 canvases until we make it easier to revoke the permission. (In other
words, click the little "X" next time. :) )
[0]: [1]:

@_date: 2014-07-25 07:24:10
@_author: isis 
@_subject: [tor-talk] Android app: Torrific 
CJ transcribed 2.5K bytes:
I agree that this should be done outside Orbot, for several reasons that I'm
not going to get dragged into again. And FWIW, Mike's blog post on Android
security specifically recommends setting up DroidWall (a similar AOS
iptables-based firewall app) with some bash scripts to log and deny all leaky
traffic from Orbot.
My primary concern would be regarding whether Torrific's iptables rules are
applied ASAP after Orbot starts Tor, and I actually can't recommend anything
there (short of building a new initramfs which enforces starting the firewall
from there, early during the boot process).
DroidWall already has a mechanism for running user-specified scripts at
startup... Perhaps the most portable way to do what you're trying to do would
be to add a similar script-sourcing mechanism to AFWall? Then you could simply
maintain a repo of startup scripts which (hopefully) work for any Android
firewall app which supports this mechanism.

@_date: 2014-07-25 23:19:53
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
Mirimir transcribed 1.5K bytes:
In addition to requiring that an email provider enforce some base difficulty
level for obtaining new accounts, BridgeDB requires that a provider must have:
 1) TLS enabled for both their SMTP and webmail/IMAP/POP interfaces. Using TLS
    when sending and receiving to/from the provider from BridgeDB is
    required. [0]
 2) Verifiable DKIM signatures on the user's outgoing emails. I've long been in favour of removing Yahoo from the accepted providers. [1]
However, we've decided not to do that for the sake of people who have already
followed BridgeDB's instructions and obtained Yahoo email addresses, and we've
opted for a different solution instead. [2]
I'm also strongly in favour of adding Riseup! to the list of acceptable
providers, as I believe that their account security, commitment to their
users, unwillingness to hand over logs, and difficulty of account creation to
be orders of magnitude better than any other email provider out there. I'm
currently working with the Riseup! birds to get (2) enabled so that we can do
this. [3]
[0]: [1]: [2]: [3]:

@_date: 2014-07-26 00:38:27
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
isis transcribed 4.9K bytes:
And... obviously, five minutes after I sent that email, I realised that
Riseup!'s DKIM signature now checks out fine, meaning that you all should now
be able to email BridgeDB from a riseup.net email address to receive
bridges. [0]
Thank the Riseup! birds for fixing this (and for being all around a great
bunch of people with everything they do). <3
[0]:

@_date: 2014-07-27 19:38:06
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
grarpamp transcribed 1.2K bytes:
Thanks, I'm aware of the difficulties.
I don't think a SIM card is the epitome of Sybil-proof
authentication. Purchasing Yahoo emails accounts (phone verified!), as noted
on ticket  costs $0.005 a piece. A SIM card doesn't prove you're a
unique human any more than 0.001 Satoshi proves you're human or solving N
CAPTCHAs proves you're human. I am increasingly convinced that the only way to
determine if a human is a unique human is to ask said human's friends. As you
can imagine, doing this without retaining a social graph of users is quite a
non-trivial task. And FWIW, the system most of us want to see implemented for
bridge distribution doesn't exactly have the most implementable
cryptography. [0] :/
Though perhaps you missed what I mentioned earlier: You can use Riseup
now. They don't require a phone, and I consider their form of social
verification to be the most secure way to authenticate strangers in any
currently-deployed system. It's got what activist groups have required for
years to combat snitches and feds AFK: convince a few real people that you're
a trustworthy friend, or convince a live human that you're useful and not a
parasite of some sort. There's no other proof-of-work system which is
effective against the state-level adversaries we aim to fight. They've got
more money, more guns, more CPU, more RAM, more Bitcoin, and more everything
than us, except friends.
[0]:

@_date: 2014-07-27 19:40:24
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
Mirimir transcribed 1.1K bytes:
I actually like the rBridge design better. [0]
[0]:

@_date: 2014-07-27 23:07:43
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
Matthew Finkel transcribed 5.0K bytes:
Nor had I, but they look and feel like a rebranded Google, and I appear to
have caused them a series of server errors when I attempted to make an account
just now, so I'm also not very impressed with their rebrading/coding skills.
Interesting. I like this idea. The requirements that I listed earlier for an
email provider to be acceptable were just requirements, and obviously don't
take into account features which are better for users.
Do you have a suggestion for some point values to assign to certain desirable
Should we take off points if something is missing? I.e. if ProviderX doesn't
have DKIM, they get penalised -20 HP, and so pretty much no matter what they
have 0 bridges in their hashring until they fix DKIM.
I kind of don't want to do all the research for all this, nor check up on
ProviderX a year down the line when it appears that some feature/requirement
of theirs is borked. What if there was, on some sort of "Don't see an email provider that you think is appropriate?"
link, which goes to a wiki page where people can say, e.g. "I checked Zoho and
they appear to get a score of 17 out of 25 in this arbitrary point system, so
they should be supported."

@_date: 2014-07-27 23:25:01
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
krishna e bera transcribed 3.9K bytes:
Nope. Last I checked, 10.0.0.0/8 isn't publicly routable. I manually sanitised
those lines. :)

@_date: 2014-07-27 23:50:41
@_author: isis 
@_subject: [tor-talk] Android app: Torrific 
CJ transcribed 6.4K bytes:
This is awesome. It seems like you've covered all the previous init script
functionality, except with an added UI. Hopefully this makes safer Tor usage
on Android devices more accessible for a greater number of people.

@_date: 2014-07-28 00:13:37
@_author: isis 
@_subject: [tor-talk] Android app: Torrific 
isis transcribed 4.9K bytes:
The simplest, least-"ragetastic" [0] reason for these functionalities to be
separated into different apps is the security concern of privilege separation:
that the modifying a firewall requires root access, and as Orbot handles
controlling the underlying tor process and interacting with other apps such as
the browser, the QRcode scanner, etc. the attack surface is greatly increased
by giving root to Orbot when (to my knowledge) it's only needed to control the
firewall. The app which handles all those other actions doesn't need root, and
therefore shouldn't have it.
[0]:

@_date: 2014-07-28 01:59:46
@_author: isis 
@_subject: [tor-talk] Why does requesting for bridges by email require a 
Mirimir transcribed 2.8K bytes:
Hey, thanks very much for mentioning that paper! I've somehow not seen it
before. I'm putting it at the top of my reading list. :)
Post scriptum: I would really love it if there were more collaboration between
academics researching these systems and their maintainers/developers. I'm just
going to throw that vague wish out into the void.

@_date: 2014-07-29 05:53:26
@_author: isis 
@_subject: [tor-talk] Other bridge distribution methods [was: why does 
Griffin Boyce transcribed 0.7K bytes:
That is essentially *why* BridgeDB used to require either Gmail or Yahoo,
because both email providers make accounts difficult to obtain via requiring
SMS verification.
I am never going to add SMS verification to BridgeDB. See my replies on the
rest of this thread for why I refuse to continue coding up broken, half-assed,
and non-privacy-preserving "solutions" to the verification-authentication
problem which ultimately do no more than sweep the problem under someone else's

@_date: 2014-11-02 20:23:39
@_author: isis 
@_subject: [tor-talk] POODLE ATTACK - eugeni.torproject.org 
elrippo transcribed 6.0K bytes:
It's unclear what you're asking for, but Eungeni does send emails over
encrypted connections, see  [0]  To my knowledge, it is also not
vulnerable to the POODLE attack, and it does support TLS_FALLBACK_SCSV (at
least, for the webserver). [1]  If you have more detailed information, we'd
be glad to hear it.
[0]: [1]:

@_date: 2014-10-12 21:32:22
@_author: isis 
@_subject: [tor-talk] Tor Relay Smartphone App 
Casey Rodarmor transcribed 2.0K bytes:
Hello Casey,
We're currently discussing that the above page should be updated to about
250KB/s. [0]
The problem is this: All clients fetch information about all the relays in the
network from the Directory Authorities/Mirrors, and these fetches take up a
certain amount of bandwidth. If the relay is too slow, the bandwidth provided
by that relay does not compensate for the directory fetching bandwidth used to
tell people about the relay, and thus it is actively harming the network.
Additionally, since Tor processes are normally CPU-bound, most relays aren't
able to use all their available bandwidth with a single Tor process. Running a
relay on ARM (or likely any other mobile/low power) CPU will only further
limit how much traffic your relay is actually pushing.
Additionally, if you're attempting to do this with Orbot on an Android device,
you'll run into issues with Android's process management system and the Tor
process randomly dying unexpectedly. This means that you are providing an
unreliable, flapping relay which is actively messing up other people's
connections through the Tor network.
For the full thread where Mike Perry, myself, and other Tor developers are
discussing the details of how badly slow relays mess up the network, see [0].
[0]:

@_date: 2014-10-15 14:37:32
@_author: isis 
@_subject: [tor-talk] New SSLv3 attack: Turn off SSLv3 in your TorBrowser 
Matthew Finkel transcribed 2.7K bytes:
I agree that adding `security.tls.version.min = 1` is the best fix until a
safe downgrading protocol like TLS_FALLBACK_SCSV is available. Someone
somewhere (I think Mike Perry quoting AGL) mentioned today that we'd only be
breaking 0.3% of the internet if we do this.
GeKo said they are rebuilding now, so updates should be online within a couple
The only thing that Addon does is:
    const gTLSVersionPrefName = "security.tls.version.min";
    const gTLSVersionPrefValue = 1;
    const { PrefsTarget } = require("sdk/preferences/event-target");
    const gPrefService = require("sdk/preferences/service");
    const gSimplePrefs = require("sdk/simple-prefs");
    let gPrefsTarget = PrefsTarget();
    function syncMinimumVersion() {
      var configuredMin = gSimplePrefs.prefs[gTLSVersionPrefName];
      gPrefService.set(gTLSVersionPrefName, configuredMin);
    }
which is what Nick's suggestion to manually set `security.tls.version.min = 1`
does. So, luckily, no one reading this should need to install any crazy
extensions. :)
We could try to fix it ourselves now, since NSS 3.17.1 added
TLS_FALLBACK_SCSV, [0] but this is likely development effort that we could
better place elsewhere since Mozilla is already prioritising the issue.
[0]:

@_date: 2014-10-15 22:47:45
@_author: isis 
@_subject: [tor-talk] New SSLv3 attack: Turn off SSLv3 in your TorBrowser 
AntiTree transcribed 0.6K bytes:
Neat. Those stats on SSL/TLS deployment are interesting... thanks! :)

@_date: 2014-09-15 18:20:05
@_author: isis 
@_subject: [tor-talk] Torbutton "load external content" 
SecTech transcribed 1.2K bytes:
If you go to `about:config` and search for the
`extensions.torbutton.launch_warning` preference and set it to `true`, that
should reenable the external content handler warning.

@_date: 2014-09-16 05:13:16
@_author: isis 
@_subject: [tor-talk] more sites requiring captchas from Cloudfare (using 
Mike Perry transcribed 3.6K bytes:
+1 However, I don't know of a competitor to Cloudflare who privides free (as in
beer) (D)DoS-protection via reverse webproxies, not to mention all the other
bells and whistles which Cloudflare offers. It'll be hard to make the argument
to switch for user-privacy reasons, given the seeming lack of marketed
Can anyone recommend a comparable alternative to Cloudflare?
I have considered starting an outreach effort to speak to the maintainers of
some of these sites, with the idea that I might gather sympathy from certain
communities who use Cloudflare.
For example, as you mentioned, the Bitcoin community, which I have personally
noticed while having discussions with some of the core bitcoin developers, who
pointed me to various bits of Bitcoin documentation... which I was
frustratingly unable to access due to an infinite CAPTCHA loop from
Cloudflare. The core Bitcoin developers, from my experience, are all extremely
well-informed about Tor and related privacy and security issues. I would guess
that they are likely using Cloudflare primarily as a mechanism to decrease the
attack surface of their sites, and probably are already aware (or would be
upset to learn) that Cloudflare sometimes prevents Tor users from accessing
the content entirely.
Here's the beginnings of your list. Others should feel free to amend.
Possibly-Tor-sympathetic sites which use Cloudflare:
 * [The Bitcoin Wiki](
 * [Open Tech Fund](

@_date: 2014-09-16 05:23:54
@_author: isis 
@_subject: [tor-talk] more sites requiring captchas from Cloudfare (using 
Mike Perry transcribed 3.6K bytes:
Side note: ReCaptcha only sort of supports no-JS operation, it only works so
far as the site proxies requests and responses between the ReCaptcha API
server and the client, extracting the CAPTCHA image from the ReCaptcha server,
serving it to the client, gathering the CAPTCHA solution from the client,
sending it to the ReCaptcha server, and so forth. Otherwise, using their
default APIs, you get JS sourced directly from the ReCaptcha server, with a
 fallback to an iframe which sources the content from the ReCaptcha
API server. [0]
[0]:

@_date: 2015-01-08 20:53:56
@_author: isis 
@_subject: [tor-talk] are there privacy benefits of running a bridge node? 
Virgil Griffith transcribed 0.9K bytes:
The super high-bandwidth Bridges with really good uptime which are included in
TBB have many thousands of clients, and push terabytes of traffic.  So, if you
ran a Bridge like those, then you might see some additional privacy benefits
under some certain circumstances depending on how you sent your own traffic
through the bridge.
Unless you're not really going for maximal privacy, and just going for what I
like to call "throwing some chaos into it"???wherein you just do the maximal
amount of crazy at all times in a half-assed effort to deter/confuse/annoy
adversaries. :)

@_date: 2015-01-08 21:01:20
@_author: isis 
@_subject: [tor-talk] orc- Onion Router Control in Go 
sycamoreone transcribed 0.6K bytes:
This looks like a pretty good start so far!  I'm looking forward to seeing
further development on this.  For what it's worth, I think there were some
other people vaguely hacking on Tor ControlPort libraries in Go; perhaps they
will speak up and you all can work together. :)
Thanks for starting this effort!

@_date: 2015-01-29 21:20:15
@_author: isis 
@_subject: [tor-talk] WebRTC to uncover local IP 
AntiTree transcribed 0.3K bytes:
Neat. Although? that's kind of most of the purpose of the WebRTC and STUN/ICE
protocols, being directly P2P and all? so I'm not exactly sure this could be
considered a PoC or a bug.
That said, Tor Browser has always disabled the WebRTC sections of Firefox's
code, at compile time, [0] so if you're using Tor Browser you don't have to
worry about things like this.  However, if Mozilla were ever to remove that
--disable-webrtc option from the .mozconfig settings, we'd potentially need to
figure out some other way to disable it, e.g. setting the FF preference:
    media.peerconnection.enabled = false
which worked in my tests to disable WebRTC after it was compiled in.
Even better than disabling it, the Tor Browser Team really needs help from
someone with a really strong knowledge of WebRTC and its potential privacy
caveats to help us assess which parts of WebRTC (if any) that we might be able
to safely allow.  The reason it's entirely disabled is because we know some
parts are unsafe, and sadly we didn't have the time/resources to sort out
which parts are which. :/
[0]:

@_date: 2015-05-01 03:03:56
@_author: isis 
@_subject: [tor-talk] What happened to Tor's Bridges? 
michael ball transcribed 0.4K bytes:
Thanks to both of you for pointing this out.  There is a known issue with the
BridgeAuthority causing this bug, see   The
fix should be deployed soon.

@_date: 2015-10-19 00:05:49
@_author: isis 
@_subject: [tor-talk] I can't use Tor via "obfs3" or other methods. 
Cubed transcribed 6.4K bytes:
Hey three,
Nothing happening at the network interface level should be capable of
affecting the functionality of either obfs3 or obfs4.  It sounds to me like
you might be having some issue with your local router setup, like perhaps a
NAT table messing something up somewhere, based on you statements that using
ethernet had better luck that using wifi.  I can't really imagine any way that
a censor would be able to determine whether you're using ethernet/wifi and
censor your connections differently based upon said information.  (If they can
do this, then something is horribly, horribly wrong with your wifi router!)
Best of luck,

@_date: 2015-09-16 22:10:55
@_author: isis 
@_subject: [tor-talk] tor + twitter issues 
Tempest transcribed 2.4K bytes:
Hey All,
I was contacted by some nice folks at Twitter who have worked to resolve this
issue.  The problems with Tor users being locked out of their Twittter
accounts and asked for phone number *should* be fixed, but I wasn't given
many details as to what the fix entails, so?
Is anyone still having troubles which they believe to be caused by using Tor?

@_date: 2015-09-20 19:15:37
@_author: isis 
@_subject: [tor-talk] Private obfs4 bridge 
Farbod Ahmadian transcribed 0.5K bytes:
Hey Farbod,
Please try sending an email to bridges at torproject.org with the line:
  get obfs4 bridges
and nothing else in the email.

@_date: 2015-09-30 12:31:57
@_author: isis 
@_subject: [tor-talk] I can't use Tor via "obfs3" or other methods. 
Jason Long transcribed 3.4K bytes:
Hello Jason,
First, please try not to paste Bridge IP addresses and ports (i.e.
"148.251.156.199:443") or Bridge fingerprints (i.e.
"3BECEABD174AE41C5CCC17254A40DD24EC5372CD") into public communications channels.
It's dangerous for you, because now people know which Bridges you are going to
try to connect to when you start up Tor.  It's also potentially dangerous for
other people, since there may be other people using these Bridges.  Lastly, it's
bad for the Bridges themselves, since they will likely now be blocked by several
censors and will no longer work in those places.
To answer your question, it looks like your SSL connections are somehow dying.
This could mean many things.  It could simply be that the router at your
house/office/caf?/etc. is doing strange things.  Or, it might mean that someone
somewhere is tampering with your connections.  Or it could mean something else
I would recommend that you email BridgeDB at mailto:bridges at torproject.org and
request some new bridges.  Perhaps try using obfs4 instead, if you can?

@_date: 2015-09-30 16:03:33
@_author: isis 
@_subject: [tor-talk] I can't use Tor via "obfs3" or other methods. 
Jason Long transcribed 7.1K bytes:
No worries; it's not your fault at all.  I think we should be logging sensitive
info at those levels anyway (see  [0]
Governments (and some other parties, like your network admin, your ISP, etc.)
could certainly block Tor, including blocking Bridges.  There are many ways that
they could do this, some with various consequences (for that government/etc.).
A simple example would be if your government only allowed traffic to
 and then block anything that doesn't look like plaintext HTTP
of someone reading CNN articles.  Obviously, this would be ridiculous if a
government did this, as pretty much all commerce, banking, online education, and
a million other things would completely stop.
However, without knowing more details about your specific situation, I can't
really determine if/how Tor is blocked for you.
[0]:

@_date: 2016-05-06 12:01:56
@_author: isis 
@_subject: [tor-talk] FBI harassing Tor devs 
Christian Pietsch transcribed 2.3K bytes:
Hello Christian,
Thanks for the concern!  I'm fine.  My server has been under heavy DDoS since
my last blog post, so it has been intermittently unavailable.  It should be
back again now.
However, (as the warrant canary text states) "The OpenPGP signature on this
warrant canary has a pre-determined expiration date that coincides with the
date on which this canary will die."  The website itself being up or down has
nothing to do with the validity of the canary; the signature is the only part
which matters.  Additionally, the sources used to generate my blog are
available in a git repository:
In particular, you can find the canary here:

@_date: 2017-10-02 23:15:40
@_author: isis agora lovecruft 
@_subject: [tor-talk] New alpha tor release: tor-0.3.2.2-alpha 
Alex Fry, Nick Mathewson, and I have just released a new alpha:
   Tor 0.3.2.2-alpha is the second release in the 0.3.2 series. This
   release fixes several minor bugs in the new scheduler and next-
   generation onion services; both features were newly added in the 0.3.2
   series. Other fixes in this alpha include several fixes for non-fatal
   tracebacks which would appear in logs.
   With the aim to stabilise the 0.3.2 series by 15 December 2017, this
   alpha does not contain any substantial new features. Minor features
   include better testing and logging.
You may find the source tarball [0] and its signature [1] on our
distribution server, and a blog post announcing it. [2]
[0]: [1]: [2]: Best regards,
