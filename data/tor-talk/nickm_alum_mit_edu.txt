
@_date: 2009-05-11 14:54:56
@_author: Nick Mathewson 
@_subject: A tor error message prior to crash 
What?? The logs say this?
Olaf, I didn't know the logs said that.  Do they?  It would help to
know if they did.
Praedor -- do you have a stack trace here?  What version of Libevent
do you have, if you know?  Is this a server or a client?

@_date: 2011-08-30 21:58:13
@_author: Nick Mathewson 
@_subject: [tor-talk] tor-0.2.2.32 compilation failure 
Hm.  No lrint?
I think it should be okay.  [Honestly, if anything makes me nervous
here, it's that RELENG_4 -- if I understand correctly -- is not
getting security patches any more, and hasn't been supported for over
4 years. Make sure that you've patched all your services!]
I could swear that all the f `l' length character with `f' ones were
fixed by my second patch.
The MIN and MAX ones look okay.
The log_mutex one is okay if this is really a no-threads build?  I
guess it would have to be.

@_date: 2011-12-17 13:00:14
@_author: Nick Mathewson 
@_subject: [tor-talk] Is Taking Checksum of Packet Payloads a 
See also discussion of this topic on tor-dev, where the original
poster cross-posted.

@_date: 2011-12-20 14:06:19
@_author: Nick Mathewson 
@_subject: [tor-talk] Automatic vulnerability scanning of Tor Network? 
On Tue, Dec 20, 2011 at 1:35 PM, Fabio Pietrosanti (naif)
When I read Lee's above paragraph, I worry Lee might have gotten the
idea that Fabio is speaking for Tor in some official capacity.  So:
Please be aware that Fabio is speaking for himself, and does not speak
on behalf of the Tor Project.
For my own part, I am perfectly fine with the idea of working *with*
server operators to help them secure their systems, and with making
sure that only secure systems are on the network.  But efforts in this
area need to work with the foreknowledge and consent of node
operators, and not alienate our volunteer community.  Also, the
appropriate response to horribly insecure servers on the network would
be to inform the operators and de-list the servers if they didn't get
fixed--not to publicly post them but leave them on the network.  That
would be the worst of all worlds.

@_date: 2011-12-20 15:37:54
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.3.9-alpha is out 
I've added this as  at
 ; more thinking
is needed about the best solution.

@_date: 2011-12-20 15:38:31
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.3.9-alpha is out 
Oops; both of those numbers should be "4752".
sorry there,

@_date: 2011-07-24 15:28:05
@_author: Nick Mathewson 
@_subject: [tor-talk] specify exit for only one website? 
In Tor, this would be done with the MapAddress configuration option.
If you want to have all example.com traffic go to exampleexit, whose
fingerprint is 12345678901234567890 you could say:
MapAddress example.com example.com.exampleexit.exit
MapAddress example.com example.com.12345678901234567890.exit
If we can get ticket  merged during the 0.2.3.x timeframe, you'd
be able to do wildcards here too, something like:
MapAddress *.example.com *.example.com.12345678901234567890.exit

@_date: 2011-11-03 15:51:17
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.2.34 crashes 
On Thu, Nov 3, 2011 at 3:38 PM, thomas.hluchnik at netcologne.de
The SEGV in your trace suggests that it died in a way that could have
produced a core, if core dumps were enabled.  Is there something in
your chroot configuration or your rlimit that's disabling them for
Oh! Also, you should make sure that Tor can find your GeoIPFile, to
rule out some variant of bug
 .
Those are likely to have data that I should not see, like your private
keys or user IPs.  I don't want to look at those. :)
Also, the SEGV suggests that it's a null-pointer reference or
something that's going wrong here, so probably the earlier syscalls
aren't going to help.  It's a stack trace that would be useful here,
I'm afraid.

@_date: 2011-11-08 10:32:28
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor and AES-NI acceleration , and Tor profiling 
Looking at those profiles, I'm not seeing zlib dominating anything,
and the public key functions seem to score pretty low too.  What am I
missing there?  This isn't what I'm used to running into in other

@_date: 2011-11-19 18:24:37
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor and AES-NI acceleration , and Tor profiling 
Hm.  This is weird enough that I'm not sure how I trust these results.
 There's nothing in assign_onionskin_to_cpuworker that's
performance-intensive and likely to get inlined, as far as I can
tell.... oh, wait!  See bug  I should have a patch ready for
review in a few minutes.
The function to blame there would be "be_mul_mont" and other stuff
that does bignum operations for OpenSSL.
This is going to be a little tricker; it might require some data
structure fixes.  I'll open another ticket for that one (
Other interesting points on the second profile: inet_csk_get_port and
__ticket_spin_lock both show up high in the profile.  The first one
appears to be used to find a port for a socket, if I'm reading it
right.  (Not sure what we can do, if anything, to make that faster or
do that less.)  Does the second one mean we're seeing lock contention
on kernel locks?  If so, it would be interesting to know why; that
function is eating a fair chunk of time.

@_date: 2011-11-28 18:46:01
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor Bug? 
That's definitely a bug, and it doesn't look like any one that I know
about getting fixed in the upcoming 0.2.2.35.  The best place to
report bugs is trac.torproject.org.  The best way to diagnose a bug
like this is to use gdb to get a stack trace: if you still have the
core file and the binary that produced it, you can do that by running
"gdb path_to_tor path_to_core" where path_to_tor is replaced with the
path to the Tor binary, and path_to_core is replaced by the path to
the core file.  At that point, running "bt" should produce a useful
stack trace that shows where the error occurred.

@_date: 2011-11-30 15:05:09
@_author: Nick Mathewson 
@_subject: [tor-talk] segfault at 8 ip ... error 4 in tor 
============================== START ==============================
The bad news is that there's no useful info in that stack trace.  The
good news is that it explains why:
A newer gdb, or a version of Tor built without PIE turned on, should
produce a useful stack trace.

@_date: 2011-10-03 12:13:32
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor as a sort of "library/dependancy" for third 
On Sun, Oct 2, 2011 at 8:29 AM, Fabio Pietrosanti (naif)
Ah, you're looking to have *everything* happen in a single file.  No,
there's no support for that right now.
Hm.  PE resources don't look like they're designed to support
read-write access in a generic way after the file is created; they
seem more like a way to more read-only things as part of an .exe file.
 I guess you could try to create a resource that was just empty space,
and pretend that it was a quick-and-dirty filesystem, but honestly I'm
not sure it would be worth it.
Keep in mind that the cache files are *cache* files and the state
files are state files: Tor isn't writing stuff to disk just for
amusement value.  If you don't cache the stuff you're supposed to
cache, you're going to wind up getting slower startups in the future.
If you don't keep state, then you won't get long-term protection from
guard nodes.  If you don't save keys, you won't be able to be a bridge
or a relay.

@_date: 2011-10-03 19:44:19
@_author: Nick Mathewson 
@_subject: [tor-talk] Revoking your secret_id_key 
The authorities can block nodes from appearing in the directory if you
convince them to do so.  One way to do that with cryptography is:
  * Make sure that your Contact line includes a GPG key fingerprint
for a key that you control.
  * If you need your node taken out of the directories, send out an
announcement saying so, signed with that GPG key.
Though in practice, people have gotten their nodes de-listed by just
sending out an unsigned announcement to the effect and convincing
everybody that they were really them.  There is probably an attack
opportunity there.
It might be worthwhile to add a feature where each Tor server
generates a signed "permanent shutdown notice" at the same time it
generates its key, and to suggest to node operators that they keep a
copy of that notice someplace secure so that they can circulate it as
needed if they need to prove that they are saying this node has been
compromised.  It'd probably need a design proposal.  I'm not sure how
much of a win it is over the GPG solution above: it saves some steps,
but still requires you to make preparations in advance.

@_date: 2011-10-07 16:10:53
@_author: Nick Mathewson 
@_subject: [tor-talk] Best Tool to connect to my Private TOR Network 
These days I use chutney
( for templating a
bunch of Tor instances and launching them locally.  It's more suited
for testing Tor changes than for running a production network on a
bunch of machines, but you could probably adapt it to do what you need
if you know a little python and are willing to get your hands dirty.
It is still in an underdocumented and basically unsupported state: "if
it breaks, you can keep both pieces" as they say.
See also Sebastian's privnet scripts at
 ; they might be
better if you prefer sh. Same disclaimers apply.

@_date: 2011-10-23 22:44:48
@_author: Nick Mathewson 
@_subject: [tor-talk] Legal or not on monitoring traffic at a Tor exit? 
The FAQ in question from that link says:
 "Tor relay operators in the United States can possibly create civil
and even criminal liability for themselves under state or federal
wiretap laws if they monitor, log, or disclose Tor users'
communications, while non-U.S. operators may be subject to similar
laws. Do not examine the contents of anyone's communications without
first talking to a lawyer."
This isn't a legal list, and I'm not a lawyer.  If you want legal
advice, you ought to talk to a real lawyer.  Your university probably
has a bunch who would be glad to help you; your department should be
able to put you in touch with them.  There is also contact info for
the EFF on that legal FAQ; they wrote it, so they might be able to
explain any parts that are not clear.

@_date: 2011-10-29 09:18:03
@_author: Nick Mathewson 
@_subject: [tor-talk] bug found on the Tor v0.2.3.6-alpha 
These bugs should now be fixed in the git repository.  Looks like
we're chasing down a couple more, though, related to geoip and/or
limited windows users.
Sorry for the trouble; hope we can get fixed versions out soon,

@_date: 2011-09-01 16:18:03
@_author: Nick Mathewson 
@_subject: [tor-talk] RSA identity keys 
Hm.  I'm not too fond of the idea of using Tor keys for other stuff
too: there are historically a lot of attacks that have been opened up
when a key that used to be single-purpose started getting used for
other stuff as well, and the two uses opened up attack vectors.
What would be much safer here would be to bootstrap trust from your
Tor ID key to some other key -- for example, by including a gpg key in
your contactinfo.  If there's a good use for it, we could probably
come up with more well-specified ways to do that.
Sounds like a fine thing to me, especially if somebody wants to code
it.  It shouldn't be too hard to do, though doing it _right_ would
probably take a lot more effort.
Much easier and possibly safer IMO is to look into a delegation
mechanism, where identity keys are never actually loaded by Tor, and
can be stored offline.  You'd only use them to sign shorter-term
signing keys, which would be the ones that Tor loaded.  Authorities
already have this; there would be some migration issues involved in
doing it for routers, but it's definitely worth thinking about if
anybody can come up with a good design that doesn't break backward

@_date: 2011-09-12 14:03:21
@_author: Nick Mathewson 
@_subject: [tor-talk] Hardware accel by default 
[...]
The big reason that hardware acceleration isn't on by default is that
when we tried it, we found that some versions of some hardware
acceleration modules, on some versions of Tor, used with some versions
of OpenSSL, made Tor crash ... and we weren't able to do a good job of
tracking down which ones were safe.
Probably a better approach would be to try to investigate which
engines, when used with which versions of openssl, should be on by
default -- and add infrastructure to look for those particularly and
try to turn them on.  Generally I'd limit this to things that are
supported automatically on CPUs or in common chipsets: that is, to the
hardware crypto acceleration that people are reasonably likely to have
without knowing they've got it.  I'd also err on the side of only
enabling it with unusually recent versions of openssl, so as not to
have to deal with all of the known bugs in older versions.
I've added a bugtracker ticket for this at

@_date: 2011-09-12 14:06:07
@_author: Nick Mathewson 
@_subject: [tor-talk] GNU make required? 
It's certainly not intended that Tor require gnu make.  It might help
track down the problem if you can figure out which Tor release (or
ideally, which commit!) introduced the new, broken behavior.  Even if
it turns out that there _is_ a bug in netbsd's side, it'd still be
worthwhile IMO to see if we can work around it somehow.

@_date: 2011-09-28 17:53:44
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor as a sort of "library/dependancy" for third 
Actually, fwiw, when I looked at the silvertunnel code it seemed that
they'd fixed a bunch of the onioncoffee issues.  Your comments still
apply though: both codebases need a lot more auditing before I'd be
comfortable recommending them for the kind of use that Fabio has in
On the original question: we do not currently support having the Tor
client run in the same address space as another application, nor do we
plan to.  If you've absolutely got to have it be a single executable,
your best option is to link everything except tor_main.c, then have
your program fork and call tor_main().  Don't call any other function:
there is no guaranteed-stable in-process API.
It's an ugly hack, but less ugly than running other stuff in the same
process with Tor.

@_date: 2011-09-28 20:35:34
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.3.5-alpha is out 
And if you profile but you don't notice a difference, try cranking the
TokenBucketRefillInterval down to 10 msec or so, and seeing how the
profile looks then. :)

@_date: 2012-04-06 10:55:47
@_author: Nick Mathewson 
@_subject: [tor-talk] Ask for RSA key size upgrade 
When the design is finalized and the code is implemented.  I'd hope
for some time this year, but with software you never know.

@_date: 2012-08-01 10:40:17
@_author: Nick Mathewson 
@_subject: [tor-talk] Private Tor network on IPv6 only 
Sorry; I meant that I don't have a current plan for IPv6-only
*network* support.  I'd like IPv6-only clients to work, but I haven't
placed a very high priority in getting a network with no IPv4 at all
to work.
If somebody wants to work on that, great.
best wishes,

@_date: 2012-08-09 16:58:01
@_author: Nick Mathewson 
@_subject: [tor-talk] Bug Remains: OpenSSL library does not load unless 
Not sure I understand what you're talking about here. What is making
you conclude that the openssl library isn't loading?  I don't know if
Vidalia has any special rules, but with Tor, the openssl library is
always loaded on startup.  Tor may or may not log stuff about its
openssl version, depending on what (if anything) it wants to say about
(If Tor weren't linking against OpenSSL, it couldn't connect to the
network at all.)

@_date: 2012-08-15 18:10:12
@_author: Nick Mathewson 
@_subject: [tor-talk] http://torbrowser.sourceforge.net/ 
Really? I kind of doubt it. The site's look and feel is just like
ours; the disclaimer is in tiny letters at the bottom of the page, and
you are giving your program the same name to a program we release.
You are either trying to confuse your users or not.
If you are trying to confuse users, PLEASE STOP.  It is a wrong thing to do.
If you are not trying to confuse users, please don't pretend that
sticking a tiny disclaimer at the bottom of the page is actually going
to unconfuse people.  Please don't pretend that people are going to
think that a project called "TorBrowser" is different from the Tor
project called "TorBrowser".  Please don't pretend that giving your
site the same look and feel as ours is not going to confuse people.
You aren't silly enough to believe any of those things, and neither
are we.

@_date: 2012-08-15 22:44:55
@_author: Nick Mathewson 
@_subject: [tor-talk] http://torbrowser.sourceforge.net/ 
Absolutely not.
And unless there really is some other guy named "Mike Perry" working
on browser privacy stuff, this does kind of make it look more and more
like this guy is trying to deliberately confuse people.

@_date: 2012-08-16 15:06:07
@_author: Nick Mathewson 
@_subject: [tor-talk] http://torbrowser.sourceforge.net 
Alexis, I don't think this "antispam" guy is talking about Dooble the
browser.  He's talking about "Randolph"/"Perrymike"/whoever's
so-called "TorBrowser."  He's talking about "that package" and a
"Dooble Bundle" to distinguish it from the actual TorBrowser that we
distribute in TBB.  It's not TorBrowser.  It is a browser based on Tor
and Dooble, and I guess that's why he's calling it "Dooble Bundle."
But yeah, I can appreciate how this would upset you! It sure is
annoying when people conflate your project with an irresponsible one.
How would you like it if there _were_ a dubious project out there
calling itself "DoobleBrowser" or "DooblePackage", whose author was
working under the name MegaAlexis?
That is how I feel about Randolph ripping off my software's name, and
Mike Perry's name, and who-knows-whose-name, and actling like he's
doing it for my own good.
I wish he would rename his software, redesign his website, and
actually warn his users that his stuff is not our stuff in a way that
they would notice.  If he has a shred of decency or good sense, he
will do so.

@_date: 2012-08-16 15:59:33
@_author: Nick Mathewson 
@_subject: [tor-talk] http://torbrowser.sourceforge.net 
We release a piece of software called TorBrowser.  I don't know how
much more simple I can make that, Randolph.  We *already* use that
name for one of the components of the TBB.
When you install TBB, it puts an icon on your desktop called TorBrowser.
When you download it, the filename is
The windows launch program has the name "Start Tor Browser".
The URL for TBB is The design of the browser called Tor Browser is at
 .
The git repositories are at  .
I could go on, but is it necessary? Please don't pretend that we
haven't been using the name TorBrowser.  Please don't pretend that
your use of the name has nothing to do with Tor Browser Bundle, and
that nobody would confuse the two.
Please do the right thing.  Please rename.  Please get a site that
doesn't look like our site,  get a logo that isn't derived from our
logo, and put the disclaimer where people will actually see it, not
buried in the middle of tiny paragraphs.

@_date: 2012-08-28 10:05:25
@_author: Nick Mathewson 
@_subject: [tor-talk] please suggest a new project name for Anonymous 
Personally, I think it's not so important to have a name that tells
you what the product is.  Tor's doing okay, after all, without being
called "Anonymaster" or "Streamhider" or whatever.
I'd also suggest heading over to
and finding something with a very low search-engine hit count.

@_date: 2012-08-29 09:22:36
@_author: Nick Mathewson 
@_subject: [tor-talk] Please review Tails stream isolation plans 
I'd need an actual list of applications to think about
IsolateDestAddr.  Which ones did you have in mind?

@_date: 2012-08-30 11:10:52
@_author: Nick Mathewson 
@_subject: [tor-talk] [Tails-dev] Please review Tails stream isolation 
You're welcome!
Now here's the email where I show how little I actually know about
protocols not called "Tor".  ;)
Not  too scary.   A typical mail program will make connections to,
like, one SMTP server and a small handful of POP/IMAP servers, right?
So this isn't a lot of circuits; seems like a fine idea.  You could
probably get a little better by allowing the SMTP and POP stuff for
each email account to share a circuit, if you can figure out a way to
make that work.
Not too scary, I think.  You'd typically wind up with one destination
per chat, or one per chat protocol?
This one is a little scary.  Do I understand correctly that an RSS
reader will make a separate connection for every RSS feed that you
subscribe to?  If so that might make some pretty serious load.
This has one destination per open session?  Seems fine.
Oh wow.  Instead of shunting these applications' traffic through
Polipo or privoxy, have you considered relinking against torsocks to
*make* applications understand SOCKS, or using some kind of iptables
trickery?   When we stopped using those proxies, we weren't really
thrilled with their security or their performance.  It makes me
uncomfortable to see "and here goes an HTTP proxy" in any Tor design
these days.

@_date: 2012-12-10 01:02:39
@_author: Nick Mathewson 
@_subject: [tor-talk] Aggregate-type settings in torrc 
All of the ones that take exit-policy-like things work that way, I believe.
Right now it always uses TCP.  You could add a new protocol to
obfsproxy that used UDP, but to do so you'd need to make sure that it
provided reliable in-order delivery semantics.
Interesting; I'm not sure that's how it should work at all.  George,
any ideas about this one?

@_date: 2012-07-23 16:01:02
@_author: Nick Mathewson 
@_subject: [tor-talk] What is 127.0.69.0? 
Everything on 127.x.y.z. is your local host.
As a trick, Tor can be configured to assign 127.x.y.z  addresses to
hostnames that you ask it to resolve, and later re-map those addresses
to the hostnames you want.  This can be handy for stuff like hidden
services, where there is no actual IP address that you get when you
resolve abcdefghijklmnop.onion.   So when you do a request to resolve
abcdefghijklmnop.onion , Tor can make up an answer (say, 127.192.3.3),
and remember that 127.193.3.3 should mean abcdefghijklmnop.onion for
the rest of your session.
This feature is controlled by the options AutomapHostsOnResolve,
AutomapHostsSuffiixes, and VirtualAddrNetwork.  127.192.0.0/10 is the
default address range it uses here.
You can also create new mappings like this using MapAddress in your
configuration file; your controller can do this too.
The reason that we recommend using 127.x.y.z addresses here is that if
your programs screw up and try connecting to one of them directly (not
over Tor), the traffic is less likely to leave your computer.

@_date: 2012-03-05 21:24:35
@_author: Nick Mathewson 
@_subject: [tor-talk] Awareness for identity correlation through circuit 
[...]
If you're asking this question, you *really* want to check out all the
isolation flags in the documentation for SocksPort in the Tor 0.2.3.x
manpage.  For a more full discussion of how it was designed, see
proposal 171.  If there's anything missing in the manpage, please let
us know.
Stream isolation is one of the big features in Tor 0.2.3.x, but it's a
bit hard to figure out how to use it up most effectively.  This is
something I hope people can help come up with good ideas and
documentation for.

@_date: 2012-03-19 11:52:46
@_author: Nick Mathewson 
@_subject: [tor-talk] "EVIL bug" Linux Tor Browser Bundle (2.2.35-8) 
It seems that a fix was merged yesterday: see
 and
I bet there will be new TBBs out very soon.
In the meantime, Linux users should delete vidalia-debug-log and
symlink it to /dev/null.  (Haven't tested that, but it should work:
  % ln -sf /dev/null /path/to/vidalia-debug-log
  % ls -l /path/to/vidalia-debug-log
  lrwxr-xr-x  1 username  username  9 Mar 19 11:53 vidalia-debug-log
-> /dev/null
IMO, this is a really good reason for us to move to getting enough
automation done so we can have nightly TBB builds and catch this kind
of thing *before* actual releases come out.

@_date: 2012-05-27 17:52:58
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor's stream isolation features defaults 
This all needs better documentation -- I'd like to get a write-up from
an integrator's point of view.  (This isn't a feature that typical end
users should necessarily want to mess with.)
For a good place to start in the meantime, see proposal 171, which
first specified this feature, and the attendant discussion thread
about it on the list.
Most of the stuff you'd want to do with it is better done by having
separate ports--especially if you're using RESOLVE calls.  (This flag
makes it so that socks4 and socks5 sent on the same client port won't
get sent over the same circuit.)
These last two are disabled because they're the wrong choice for lots
applications.  In particular, isolating by port is a bad idea for
separating protocols, although people often think at first glance that
it should be a good idea. For example, if you think that
IsolateDestPort will keep SSH separate from HTTP, you're not right:
it's trivial for a hostile webpage to include an img tag whose URL
specifies port 22, which would make HTTP and SSH requests get over the
same port.
Isolating by destination address is also ridiculously expensive and
slow if you're doing typical HTTP usage, or some other protocol where
a single logical request turns into requests to many different hosts.
Mike's got a more sophisticated approach here that he's aiming to do
in TB; he can link to the best explanation of it better than I can
find out which one is best.

@_date: 2012-05-28 22:02:55
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.3 Alpha ready for redistributed projects? 
Well, the alphas encounter bugs much more frequently, since we merge
lots more stuff.  But right now, 0.2.3.x is in a pretty good state,
and I'm starting to like it a lot.
I'll like it a lot more in a week, and a lot more in a month.
There are no regression bugs I'm aware of that make 0.2.3.x a
significantly worse choice than 0.2.2.x.  What makes 0.2.3.x a riskier
choice is that I'm merging new code into it at a much faster pace,
whereas we only put stuff into 0.2.2.x when it fixes bugs and seems
very safe to merge and (ideally) has seen a fair bit of testing in
I don't think you'd be crazy to put something out with 0.2.3.x in it.
I'm hoping to have a beta or a  release candidate in June.  Early
June, if possible, but with software you can never say.
(That list of tickets is a good place to look, but shouldn't be taken
as a list of "Stuff that must absolutely get fixed to put out
0.2.3.x-rc.  Many of those tickets can be fixed later, or deferred to
a later release.)

@_date: 2012-05-30 10:58:33
@_author: Nick Mathewson 
@_subject: [tor-talk] Data storage in cached-descriptors 
Basically, there are some open unsolved questions on how to do it
securely and efficiently.  It'd be cool to solve them -- and it looks
like the research community could be making progress -- but I don't
think I'd want to consider it solved yet.
There's some research on this.  Here are a few papers to start with,
but anybody who's serious about this should chase through their
references, and then read other works by the same authors and by
authors of related systems and attacks.
I think the Salsa paper was particularly well written, and explains a
lot of the design decisions you need to make in a p2p anonymity
  There _are_ published attacks against the system, though.  You might
want to stop here and see whether you can think of them before you go
Then I'd read:
  It's a paper that describes attacks against a couple of other
previously existing distributed directory designs. Its "related work"
section references some more p2p anonymity network designs, and the
known attacks against them.
The approach of
  provides an alternate approach for the scalability issue, but leaves
the centralized trust issue alone.
Sure, but why would you ever do that?  dir-spec.txt explicitly
requires everything that handles router descriptors to allow
unrecognized fields and pass them unchanged.

@_date: 2012-11-02 14:01:36
@_author: Nick Mathewson 
@_subject: [tor-talk] torsocks is broken and unmaintained 
I'd like to see more discussion from more people here first, and see
whether somebody steps up to say, "Yeah, I can maintain that" here, or
whether somebody else who knows more than me about the issues has something
to say.  Otherwise I don't know whether to write a "looking for maintainer"
post, a "who wants to fork" post, a "don't use Torsocks, use XYZZY" post,
or what.

@_date: 2012-11-03 20:38:49
@_author: Nick Mathewson 
@_subject: [tor-talk] torsocks is broken and unmaintained 
Okay, sounds like we've got some enthusiasm.  Let's get started.  I
volunteer to review commits and if people ask me to, and suggest that
asking me to review stuff for a while might be a smart idea.  I just gave
myself commit access to the git at git-rw.torproject.org repo too, in case
that helps.  I am not planning to be a primary author here.
Given the amount of people asking us to apply and/or warning us that we
mustn't apply particular patches, I'm going to suggest the following
principles for a while:
  * LET'S START MINIMAL.  Let's stick to doing only the very major bugfixes
and obvious fixes for at least the next release or two, so that something
usable comes out.
  * NO ARCHITECTURAL ASTRONAUTICS. I'm always tempted when I come to a
codebase for the first time to refactor the heck out of it.  Let's avoid
doing that till we have a little experience with this codebase.  There
isn't all that much here: let's
  * LOVE MEANS GET TESTED. If at all possible, we should make this codebase
easier to test (right now it wants you to install before testing), and
improve the coverage of the tests so that (if as people suspect) we're
likely to break things on one platform when we fix them on another, we can
at least find out fast whether a patch works everywhere.

@_date: 2012-11-04 00:26:30
@_author: Nick Mathewson 
@_subject: [tor-talk] torsocks is broken and unmaintained 
[...]
Done.  At some point we should migrate issues from google code, but IMO
that's best done once we have something nontrivial to show for our efforts.
Yeah; sometimes I start a sentence, then I think of something to write
elsewhere and start another sentence, but then by the time I'm done with
that one I don't remember the first sentence any more, so it
That one should end with "There isn't all that much code here; let's make
sure we understand it pretty thoroughly before we complexify it in the name
of some half-glimpsed vision."
Hm. Supposedly, it's _supposed_ to work on OSX.  It has a lot of code for
OSX support.  I just tried it with curl on my osx laptop, and it seemed to
work okay.

@_date: 2012-11-04 20:17:52
@_author: Nick Mathewson 
@_subject: [tor-talk] Confusion about Tor log messages showing relay 
[...]
Grepping for the string "protocol that may leak information" in Vidalia, it
looks like you're *probably* seeing this one:
"One of your applications established a connection through Tor "
         "to \"%1\" using a protocol that may leak information about your "
         "destination. Please ensure you configure your applications to use
         "only SOCKS4a or SOCKS5 with remote hostname resolution"
That's the warning that you should I'd expect that your application is
connecting to Tor and giving it an IP address rather than a hostname, and
it's not an IP address that your application is getting from Tor.  So
here's what Tor thinks might be happening:
 1. Application does a direct DNS request for some-site.com.  Your local
DNS server learns that you want some-site.com, and tells the application
"the IP is 1.2.3.4".  That DNS request would be the information leak that
Vidalia is warning you about.
 2. Your application makes a request to Tor: "Connect to 1.2.3.4".
 3. Tor goes, "Hm. Okay... but hang on. I never told any application about
the IP 1.2.3.4! I bet they got it by a direct DNS request.  That would be
bad. I should warn them!"  Tor makes a connection to 1.2.3.4, and tells
Vidalia to warn you.
 4. Vidalia warns you.
So in this case, you wouldn't be seeing any connections to 1.2.3.4 on your
AV.  Instead, you'd see your application making DNS requests for some
hostname, and getting 1.2.3.4 as an answer.  It's external DNS requests
that you need to watch out for.
Tor is telling you "Please ensure that your configure your applications to
use only SOCKS4a or SOCKS5 with remote hostname resolution" since that's
what you usually have to do to an application to make it do the right thing
This could give false positives for two reasons:
  A. Maybe the application is finding out about IP addresses through some
safe means other than DNS lookups and other than learning about them from
  B. Maybe the application learned about an IP address a long time ago
through Tor, long enough ago that Tor forgot that it ever told that
application about that address.
Things to look at: Is there some application other than TorBrowser in use?
 Are all settings at their default values?

@_date: 2012-11-04 20:20:53
@_author: Nick Mathewson 
@_subject: [tor-talk] IsolateSOCKSUser vs IsolateSOCKSAuth bug in 
Right.  In general, a closed or finished proposal is there for historic
interest only.  The actual manual and specifications are supposed to be
where the real documentation of how stuff works is.
(In some cases, we haven't been diligent enough at updating specs and
documentation to match finished proposals though.  We should get on that
more aggressively.)

@_date: 2012-11-22 19:34:16
@_author: Nick Mathewson 
@_subject: [tor-talk] details about Tor's packaging window and delivery 
Try the specification document, "tor-spec.txt".  You can get it from
the torspec git repository, or read it on the web at
 .  I
think the flow-control stuff should be in section 7.

@_date: 2012-09-07 10:28:57
@_author: Nick Mathewson 
@_subject: [tor-talk] hidden services and stream isolation 
I hope it works!  If somebody wants to test it, that would be much appreciated.
Hidden services are already isolated from each other, and from non-hs
client traffic.  The only additional feature for stream isolation to
provide would be isolating some streams sent to a hidden service from
another -- for example, so that if you have SocksPort 9050 and
SocksPort 9051 defined, any streams to a given HS over port 9050 will
be sent on a different circuit than streams over 9051.
I believe that works, though there could well be efficiency issues there.

@_date: 2012-09-07 12:13:31
@_author: Nick Mathewson 
@_subject: [tor-talk] hidden services and stream isolation 
On Fri, Sep 7, 2012 at 11:02 AM, Fabio Pietrosanti (naif)
For certain values of two, sure.
I was with you to this part.  I thought you were trying to benchmark
performance, not pick the fastest one at the expense of hosing the

@_date: 2012-09-10 22:35:52
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.4.2-alpha is out 
The version stuff is thill there in the first line of the output.

@_date: 2012-09-10 22:40:04
@_author: Nick Mathewson 
@_subject: [tor-talk] SocksPort: Circuit isolation is not Exit isolation 
This doesn't make sense to me.  If you've got two requests open from
the same exit to the same site, using different accounts, then all the
site can tell is that two Tor users (or maybe one) are connecting to
it.  That's also the same conclusion it could reach if the two
requests were coming from the same exit
Now, the site might have rules that say "no IP can have two sessions
open at once!"  But if that's what the site says, they probably don't
*want* you using two accounts at the same time.
Is there a better use case here?

@_date: 2012-09-11 00:34:22
@_author: Nick Mathewson 
@_subject: [tor-talk] SocksPort: Circuit isolation is not Exit isolation 
Actually, it's stronger!
Let's say that there are 50 accounts that all log in to my site over Tor.
Let's say that there are N tor exits, and let's pretend that each exit
is chosen with probability 1/N.
If anonAccountA and anonAccountB are run by different users, I'd
expect them to use the same exit 1/N of the times that they both log
But if, over time, I see that anonAccountA and anonAccountB both
sometimes use some of the same exits, but they never use the same exit
at the same time, I can conclude that they are run by the same user,
and that user has enabled some kind of exit isolation option.

@_date: 2012-09-11 01:41:28
@_author: Nick Mathewson 
@_subject: [tor-talk] SocksPort: Circuit isolation is not Exit isolation 
But circuit isolation should solve that?  If the circuits for the two
accounts are isolated, then account alice will use the same exit as
account bob the same amount of the time as they would if they were run
by two different users, right?

@_date: 2012-09-14 09:29:54
@_author: Nick Mathewson 
@_subject: [tor-talk] tor-0.2.4.2-alpha compile errors 
Ah, I've seen something like this before.  Some compilers get shirty
about having cpp directives inside of macro arguments.

@_date: 2012-09-14 10:07:18
@_author: Nick Mathewson 
@_subject: [tor-talk] tor-0.2.4.2-alpha compile errors 
Should be fixed in e4ce8cd9691708d9bc0bcc9904d656fe35001946.

@_date: 2013-04-04 08:17:29
@_author: Nick Mathewson 
@_subject: [tor-talk] NSA supercomputer 
Because in 2003/2004, when we were designing Tor, 1024-bit keys seemed
like they would probably be good enough, AND we weren't confident of
our ability to support arbitrary key sizes without screwing it up.
But as of 0.2.4, the forward-secrecy[0] parts of Tor[*] now support
256-bit ECC keys, which are probably about as good as 3072-bit RSA/DH
keys, and a lot faster for most uses.  I'd like to make more of the
authentication parts of Tor support ECC over the next couple of
[0] [*] Specifically, the ephemeral-key part of the TLS handshake supports
P224 or P256 if both Tors were built with a recent OpenSSL version;
and the circuit handshake supports the "ntor" protocol with curve25519
if the client has UseNtorHandshake turned on. I want to make that
on-by-default before the release.[**]
[**]

@_date: 2013-04-13 18:51:59
@_author: Nick Mathewson 
@_subject: [tor-talk] ExcludeEntryNodes 
Thanks; I've marked  for 0.2.5 and closed  as a duplicate.
That's not a commitment that it will get done in 0.2.5, but I do hope
that somebody will give it a try.
The tricky parts to get right will be:
  * Making sure it applies to entry nodes on the codepath where we
pick guards at random from all guards, AND on the codepath where we
pick guards from configured EntryNodes, AND on the codepath where
directory guards are turned off entirely.
  * Deciding whether it applies to anonymous connections only, or to
direct directory requests as well.
  * Figuring out what should happen when a guard is marked as
Excluded, then no longer Excluded.  Should do the same as if the guard
is excluded with ExcludeNodes, then not?
  * Figuring out what should happen when the user has no consensus
networkstatus document, and they have excluded every authority and
directory source that they know about.
  * Figuring out what should happen if a specified bridge is excluded.
  * Figuring out what should happen if a bridge is excluded by
fingerprint, but we don't learn its fingerprint until after connecting
to it.
  * Refactoring the code and/or adding mocking stubs to the code
sufficient for us to have rigorous testing for all of the above.
Oh hey, that's a checklist.  I've copied it onto   There's
enough trickiness here that a design proposal might be in order.
Whoops! That wasn't a preparatory commit; that was a typo in the
documentation, where it should have referred to "ExcludeExitNodes."
I've fixed it now; thank you!
I think that's actually a false dichotomy, and an interesting one.  In
order to help users get security, an option needs to work in a way
that they they expect. Otherwise, when they try to avoid using nodes
in one way, and they wind up telling Tor to do something else
entirely, they are likely not to get the security properties they
thought they were getting by asking for what they thought they were
asking for.
I believe that should work, though it might be somewhat inconvenient.
Note that you should probably be using 0.2.4.x-alpha in that case,
since I think we fixed some EntryNodes bugs and improved some
EntryNodes behavior along the line in the 0.2.4.x series.
Assuming you're not disabling guard nodes, you might do as well to
pick a half dozen or so countries you like with a bunch of Tor nodes
in them, and just list those. I haven't done the math, though; it
might not be.
No trouble.  I *am* a programmer, and I figure the least I can do here
is generate the list for you.
I made it with
perl -ne 'if (/,([A-Z][A-Z])$/) {print "{\1},\n";}' src/config/geoip
though there are probably better ways.
{AD}, {AE}, {AF}, {AG}, {AI}, {AL}, {AM}, {AO}, {AP}, {AQ}, {AR},
{AS}, {AT}, {AU}, {AW}, {AX}, {AZ}, {BA}, {BB}, {BD}, {BE}, {BF},
{BG}, {BH}, {BI}, {BJ}, {BL}, {BM}, {BN}, {BO}, {BQ}, {BR}, {BS},
{BT}, {BW}, {BY}, {BZ}, {CA}, {CC}, {CD}, {CF}, {CG}, {CH}, {CI},
{CK}, {CL}, {CM}, {CN}, {CO}, {CR}, {CU}, {CV}, {CW}, {CX}, {CY},
{CZ}, {DE}, {DJ}, {DK}, {DM}, {DO}, {DZ}, {EC}, {EE}, {EG}, {EH},
{ER}, {ES}, {ET}, {EU}, {FI}, {FJ}, {FK}, {FM}, {FO}, {FR}, {GA},
{GB}, {GD}, {GE}, {GF}, {GG}, {GH}, {GI}, {GL}, {GM}, {GN}, {GP},
{GQ}, {GR}, {GS}, {GT}, {GU}, {GW}, {GY}, {HK}, {HN}, {HR}, {HT},
{HU}, {ID}, {IE}, {IL}, {IM}, {IN}, {IO}, {IQ}, {IR}, {IS}, {IT},
{JE}, {JM}, {JO}, {JP}, {KE}, {KG}, {KH}, {KI}, {KM}, {KN}, {KP},
{KR}, {KW}, {KY}, {KZ}, {LA}, {LB}, {LC}, {LI}, {LK}, {LR}, {LS},
{LT}, {LU}, {LV}, {LY}, {MA}, {MC}, {MD}, {ME}, {MF}, {MG}, {MH},
{MK}, {ML}, {MM}, {MN}, {MO}, {MP}, {MQ}, {MR}, {MS}, {MT}, {MU},
{MV}, {MW}, {MX}, {MY}, {MZ}, {NA}, {NC}, {NE}, {NF}, {NG}, {NI},
{NL}, {NO}, {NP}, {NR}, {NU}, {NZ}, {OM}, {PA}, {PE}, {PF}, {PG},
{PH}, {PK}, {PL}, {PM}, {PN}, {PR}, {PS}, {PT}, {PW}, {PY}, {QA},
{RE}, {RO}, {RS}, {RU}, {RW}, {SA}, {SB}, {SC}, {SD}, {SE}, {SG},
{SH}, {SI}, {SJ}, {SK}, {SL}, {SM}, {SN}, {SO}, {SR}, {SS}, {ST},
{SV}, {SX}, {SY}, {SZ}, {TC}, {TD}, {TF}, {TG}, {TH}, {TJ}, {TK},
{TL}, {TM}, {TN}, {TO}, {TR}, {TT}, {TV}, {TW}, {TZ}, {UA}, {UG},
{UM}, {US}, {UY}, {UZ}, {VA}, {VC}, {VE}, {VG}, {VI}, {VN}, {VU},
{WF}, {WS}, {YE}, {YT}, {ZA}, {ZM}, {ZW}
best wishes,

@_date: 2013-04-18 22:10:50
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.4.12-alpha is out 
Indeed it should.  With luck Roger will make the correction.
Heh. I think my reasoning for calling it a bugfix was that even though
_for all we know_, nothing was broken, that Tor's behavior here was
incorrect. (And for all we know, there's some weird platform somewhere
where one of our attempts to set CLOEXEC was failing for some odd
Thanks, Sebastian!

@_date: 2013-08-07 21:42:23
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor blacklist 
Huh?  Are you talking about the BadExit flag?  Or the ability that
authorities have to stop voting for particular nodes?  Something else?
I'm going to guess that you're talking about one of those features.
The way that the directory system works is, nodes publish information
about themselves to all the directory authorities.  A node is listed
in the consensus document if a majority of the voting authorities vote
for it (more or less, it's complicated, see dir-spec.txt for full
Additionally, there are some flags that affect how nodes are used
(like Running and Valid and BadExit).  A node gets a flag in the
consensus if the majority of the authorities who vote about that flag
vote for it (again, see dir-spec.txt for the full details).
An authority operator can configure their directory authority to vote
for a node as BadExit, never vote for a node as Valid, or never vote
for it at all.  (The code for this is in src/or/dirserv.c).
The directory authorities, and the authority operators are listed in
every directory consensus document.
Generally, the likeliest way for a node to get the BadExit flag would
be to get caught altering traffic. (Mostly, this isn't done
intentionally: The majority of nodes with the BadExit flag are ones
that have upstream ISPs that alter traffic somehow.)
The best way to get authorities to manually stop listing a node
entirely would be to fire up a whole bunch of nodes in an obvious
Sybil attack.  (Frequently, this isn't done intentionally: in at least
once case, IIRC, it was a sysadmin who got a "brilliant idea" to run
Tor on every system he had access to.)
Either of these statuses will last till enough authority operators
stop voting against those nodes.  If somebody thinks that the votes
have been made in error, they should contact the authority operators,
or the tor-assistants mailing list, or post here, or something.
(I'm not an authority operator myself; somebody who is can post here
to say more about how they make choices.)
There's some other stuff that will make authorities *automatically*
not vote for a node -- like if it isn't running when they try to
contact it, or if it can't build circuits when the authority tries it,
or if its bandwidth is way too low to be useful for the network, or if
it's running a way-too-old version of Tor.
I hope this answer your question!
best wishes,

@_date: 2013-08-16 21:42:38
@_author: Nick Mathewson 
@_subject: [tor-talk] log heartbeat update on demand? 
There isn't a mechanism to do that.  But I agree it would be cool to
have one.  I've opened a ticket as
(You can make the heartbeat message more frequent by altering the
HeartbeatPeriod configuration value, but that's not quite what you
asked for.)

@_date: 2013-12-09 11:28:16
@_author: Nick Mathewson 
@_subject: [tor-talk] Avg packaged cell fullness - notice in logs 
A non-exit relay should be packaging very few cells ; if it's not a
directory cache, what it packages will be more or less entirely
directory requests, which are pretty short. I wouldn't be too worried,
unless you happen to be a directory cache, in which case we should
look into some of the ideas we had for improving cell packaging.
This one, I'm not so sure about. Is this value typical for folks?
Best wishes,

@_date: 2013-12-09 13:30:36
@_author: Nick Mathewson 
@_subject: [tor-talk] Avg packaged cell fullness - notice in logs 
To anyone about to report these numbers: when you say "non-exit"
please remember to report whether you running as a client at all? A
relay? A directory cache?

@_date: 2013-12-18 12:03:58
@_author: Nick Mathewson 
@_subject: [tor-talk] Help testing patch on SandyBridge/IvyBridge? Force 
I'm fairly sure that patch doesn't actually do anything; see comments
on  (URL above) for my investigation.
Lessons I learned: Do not assume that you have really replaced an
undesirable function until you've investigated with a debugger.  Do
not assume you were using the undesirable function in the first place
until you've investigated with a debugger.  Above all, do not assume
that you understand how OpenSSL works until you have investigated with
a debugger, the source code, and a pot of coffee.
There is a probably fixed patch ready for testing at that URL that
should apply cleanly to 0.2.4. I've made a quick and dirty 0.2.5
version for people to use as well, if they like.
These could use review and testing, of course.  Comments at the above
URL if possible please.

@_date: 2013-12-29 13:38:01
@_author: Nick Mathewson 
@_subject: [tor-talk] Improved HS key management 
Here's the latest version:
And see also:
Comments solicited on the tor-dev list!  The best time to get all of
this design stuff right is soon, since I'm hoping to start building it
before too much longer.

@_date: 2013-02-20 03:17:31
@_author: Nick Mathewson 
@_subject: [tor-talk] please re-consider Tor Trademark policy 
Here's my opinions, speaking on my own behalf and not on behalf of my
employer (Tor).  I generally try not to get into legal stuff, since
IANAL, and since any time I spend legalling is time that I'm not
spending on software, so please forgive my legal ignorance.
First off here, I want to thank you for writing this. I don't agree
with a lot of what you said, but I agree it's important to talk about,
and hope I can respond as thoughtfully as your message deserves. (IOW,
please forgive me if I sound like an utter asshole at any point
below-- that's not what I want to do. I mostly try to write about
technical stuff, so I'm a little out of my usual waters.)
The next thing: I don't understand how your paragraph above is using
the word "Affiliated". (Did you mean another word?) After all, I
object to projects using "Tor" exactly when they are *not* affiliated
with the Tor Project.  It's projects that are trying to feign
affiliation that I object to the most.
Actually, no. There's something I object to most of all: Any project
that actually misleads users, intentionally or not.
Here's why:
Historically we started on this road when there were a couple of
commercial efforts selling software with "Tor" in their names, doing
their very best (as far as we could tell) to make people think that
Tor was backing their efforts.  One of them was just a one-hop web
proxy running Squid or something. There were a significant number of
users who thought that by using their products, they were getting Tor.
This, frankly, pissed me off.  I wouldn't personally care very much
about people using my software's name for whatever reason, if there
weren't a substantial number of users who believed that their software
was actually made or developed or endorsed by the same people working
on mind.
(IANAL, so I won't get into the topic of trademark dilution. IIUC,
trademarks are unlike copyrights in that we we don't legally have the
flexibility to let well-intentioned folks infringe without challenging
them, if we want to keep the trademark so that we can use it to
challenge obvious imposters. Wendy could say more here probably.)
Among free software projects with a trademark and a policy about use
of that trademark, who do you think is doing a better job?  Which free
software project's trademark policy would you suggest we adopt
I'd love to see ways we can be more permissive without abandoning the
trademark entirely. I'd been under the impression that we were doing
pretty good.  We seem to be at least as open as with our trademark as
Debian, for example.  Where specifically is our current policy shitty?
After reading it... what specifically should it allow IYO that it
Anybody implying that they're Tor when they aren't is IMO fake.
Anybody who accidentally implies that they're Tor is IMO accidentally
fake. (see below)
IMO also that's not the best paragraph about our trademark problems
that we could haver written.  I would personally prefer something like
"Here are a bunch of sites that have confused users into thinking that
we are producing or endorsing their software by using Tor in the
But please realize that there's a tension here between transparency
and diplomacy.  I want as much as possible of Tor's work to be done in
public.  That means that I want napkin-notes and random lists like
this one to be kept on the wiki, not in some text file on Andrew's HD.
(Also, it's a wiki.  It's easy to change the paragraph there to avoid
subjective pejoritives -- I just did, because I agree that it wasn't
the optimal way to say what it needed to. Now it says, "Current domain
names confusing users about the origin of their software".  I'll stand
by that: those project names are, as a matter of objective fact,
making a substantial number of users think that we wrote the software.
 Probably still not optimal though)
I understand what you're saying here, but what concerns me most is not
people's intentions, but the effect on users.  The possibility of
misattribution isn't theoretical -- it happens every day in support
requests.  People assume that projects with "Tor" in the name are made
by and endorsed by the Tor Project, all the time.
Probably because nobody's edited the list to put it on, I guess.  We
get tech support requests from people who think that we wrote and
endorse "TorChat" all the time.
We try to do that!
That's why we have a FAQ entry, after all.  Please consider this: you
think that not reading disclaimers or checking with the original is an
"obvious trap", and that anybody who messes up there is not following
"common sense".  But then what could we say about people who decide to
start new projects based on Tor without even reading the FAQs?  To me,
that seems like it should be at least as obvious.
Also, we try to contact projects with Tor in their names as early as
we can. (Subject to our insanely overworked and disorganized
schedules. :[ )
This isn't what we do.
I think we've tried to contact *all* the people listed there as soon
as we found out about them. I don't know if we've done a good job
contacting them or not: Andrew could say more.  Despite the impression
some people have, we're a really small organization, where everybody
is overworked.  Please don't attribute to malice what can be explained
by ... trying to do as much as we can with too few hours?
I *DO* know that trademark issues are something that we've talked
about again and again on the tor-talk list.  And think about it: if
these people aren't aware that we don't want other projects using our
name like this, they aren't even reading tor-talk.  They aren't
looking at our website and clicking on the trademark link at the
Well-intentioned people can make mistakes like that!  But I don't
think those mistakes are so very different from the kind of mistakes
that users make when they look at a website for TorWhatever and assume
that this is a Whatever made by Tor, and not merely a Whatever made
using Tor by some third party.  If I am going to treat intelligent,
thoughtful developers who overlook our trademark policy as not
necessarily evil, then I need to treat intelligent, thoughtful users
who overlook their disclaimers as not necessarily "too stupid for
[[Finally -- if I'm far offbase or wrong in some of my assumptions or
whatevers, please drop by some place where I hang out for chat and
chat with me, or something.  What I really want here is to do the best
thing, and I can only do that if people try to convince me when I'm
[[[Okay. Back to software. Am I still awake enough for software?  Wow,
I hope so.  Software is excellent.  Of course, writing paragraphs like
this one is a sign I am maybe too sleepy for software.  let's see what
happens when I start refactoring....]]]
my opinion for whatever it's worth,

@_date: 2013-02-22 12:47:34
@_author: Nick Mathewson 
@_subject: [tor-talk] torsocks 1.3 is tagged and released 
Hm. It looks like Jacob is might just be completely hosed right now,
so I've uploaded a tarball to
 and a signature to
  I hope we get
get these put somewhere more official soon.
As an extra wrinkle, this is my first time running "make dist" on
torsocks, so it's possible that "make dist" has bugs that don't appear
when using the tags.  Please open tickets if so; caveat haxxor. :/

@_date: 2013-01-18 11:44:48
@_author: Nick Mathewson 
@_subject: [tor-talk] Mosh safe with tor? 
FWIW, mosh has a decent security reputation, though I don't know how
secure they are personally.  According to their paper, they're using
AES128 in OCB mode, which is basically kosher.  (If they *are* using a
modified AES, that's pretty scary.)
But yeah, in any case: if mosh is UDP-only, then it won't run over Tor.

@_date: 2013-01-21 15:04:08
@_author: Nick Mathewson 
@_subject: [tor-talk] [tor-dev] Open streams on the fly 
Proposals go to tor-dev, not tor-talk.
Before you re-send, you should check out the discussion (what there is
of it) on ticket  at
 .  The major
concern at the time was the performance impact from a large number of
users all activating this option.  The discussion on the ticket has
stalled; it would be nice to reboot the discussion on tor-dev and try
to bring it to a conclusion.
In particular, if people think *this* is a good way to "maintain
separate identities" for something like web browsing, that's an
accidental DOS attack waiting to happen.

@_date: 2013-01-29 10:51:26
@_author: Nick Mathewson 
@_subject: [tor-talk] Errors in logfile, tor relay stuck @ 100% CPU 
Not offhand, but I would love to know where that CPU is going.  Can
you attach a debugger or use oprofile or perf or some tool like that
to see where the CPU is spending its time?

@_date: 2013-07-02 18:33:49
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor for upcoming FirefoxOS? 
I haven't looked at it much and can't comment on its status or QOI,
but I believe Aymeric has started a project that could be like you
describe:

@_date: 2013-07-05 09:44:43
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.4.15-rc is out 
If the binaries work, then the failing "-pie" option shouldn't be a
big deal.  It's just an attempt to enable a security feature which
apparently older versions of OSX and OSX gcc don't support.
best wishes,

@_date: 2013-07-12 17:16:45
@_author: Nick Mathewson 
@_subject: [tor-talk] [Question] How to use chutney to test private 
I'm afraid that right now the README is the only documentation there
is.  The bootstrap-network.sh script might also help there too, if you
can read shell scripts.
I'm hoping that some time in the next few months (or when we get a QA
engineer[1], or both) we can get chutney to a point where it's easier
to use for people who aren't prepared to read the source code to
figure it out.
[1] best wishes,

@_date: 2013-07-27 10:17:59
@_author: Nick Mathewson 
@_subject: [tor-talk] GoldBug.sf.net: Encrypted Messenger 
I would be pretty surprised if the EFF were behind the press release
you mention.  EFF press releases are always written in very good
English, and that press release is not; nor is the website.  It seems
very unlikely to me that the EFF would let a project like this go live
without making sure the press release was idiomatic and grammatical.
And as you note, it seems EXTREMELY unlikely that the EFF would be
involved in a project like this without linking to it from their blog,
website, or twitter account.
(Finally, remember: don't run binaries from people you don't know.)

@_date: 2013-06-16 12:34:52
@_author: Nick Mathewson 
@_subject: [tor-talk] Third-party Q&As 
Hi, Adrelanos!
You have hit the nail on the head with that part.
So because I'm not principally involved with the decision to move away
from askbot, or the attempt to get a working Q+A forum, I can't
comment on the actual deficiencies. (I hope that someone else can!)
But as I understand it, the main reasoning was that we didn't seem to
be getting any closer to having an adequate production system (for
whatever value of "adequate"), and it seemed like a better idea to
have something working today than something even better in two or
three years or however long it seemed likely to take to develop
something better.
I that we as a project have had a tendency to stretch ourselves thin
by trying to do the optimal version of everything at all times.  We
need to question this impulse, since it can lead to abandonware,
opportunity costs which distract from other efforts, and situations
where we defer the good indefinitely in the name of the perfect.
It would be cool if we get something that's better than StackExchange
some day.  But my understanding is that the askbot deployment wasn't
that, and wasn't likely to become that without a fair amount of
expertise that we don't actually have and haven't been able to get.
(I hope others will step up with details here -- I'm afraid I don't
have them, and don't actually know what stood in the way of askbot's
best wishes,

@_date: 2013-06-16 19:00:19
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.4.13-alpha is out 
We'll probably post in more detail once the fix is out.  See
 for a summary of
what we've got so far.
The issue is that the hostile website can't trace the client through
the middle and exit per se per se, but they *can* force the client's
circuit to close, thereby forcing them to make another circuit.  If
the attacker controls the website *and* a fraction of the nodes on the
network, eventually the client would use a middle node or exit node
which the attacker also controls.
The Guard system is there for a reason: See
 .  If you're
hoping to resist adversaries of even modest funding on the medium or
long term, you need some solution to the long-term sampling problem.
If it weren't for entry guards, this would probably be a deanonymizing
issue, since clients would eventually pick an attacker-controlled
entry point: guards are making clients safer here, not less so.
best wishes,

@_date: 2013-06-16 19:02:05
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.4.13-alpha is out 
Personally, I'd suggest that relays just stick with 0.2.4.12-alpha and
wait 0.2.4.14-alpha: we should have it out pretty darn soon.

@_date: 2013-06-17 15:32:50
@_author: Nick Mathewson 
@_subject: [tor-talk] "Torifier" for Windows 
Yeah, but caveat emptor there.  It seems to wrap rather less than the
full Winsock API, so I wouldn't assume that this will actually prevent
your program from going around Tor without actually doing a lot of
Also, its API hooking system is based on the ubiquitous  ZDisasm.c
file written by Z0MBiE, who to the best of my knowledge never actually
put a license on it.  So it might be legally problematic to
redistribute (unless I'm wrong about that, and Z0MBiE *did* explicitly
put it ZDisasm.c under some open-source license or release it under
the public domain.)
best wishes,

@_date: 2013-06-18 09:30:48
@_author: Nick Mathewson 
@_subject: [tor-talk] "Torifier" for Windows 
On Tue, Jun 18, 2013 at 2:50 AM, Fabio Pietrosanti (naif)
My guess is that everybody is assuming, with no actual evidence, that
the code is in the public domain.
(Maybe they think that because the author put the code up with no
license, that makes it public domain?  It doesn't.  Or maybe they
think that because "Z0MBiE" is a "hacker name", the author probably
doesn't intend to enforce copyright?  That might be so, but again, I
have seen no real evidence for that so far.)
best wishes,

@_date: 2013-06-18 15:27:33
@_author: Nick Mathewson 
@_subject: [tor-talk] Are non-official projects welcome to Stackexchange 
So, we chatted about it a little, and we're not 100% actually sure
what the right answer should be there. We don't mind general questions
about other Tor-related projects, but we'd like to avoid having that
stuff ramp up faster than the Tor questions.  Also we're also worried
that if we say 'yeah, sure, go ahead' then we'll get swamped with
support requests for stuff like "tormail," or the next Haystack, or
that it's going to turn into a promotional vehicle for somebody with a
forked Tor called tOr, or some drama like that.
On the other hand, I don't want to have people decline to answer
questions like "What are all my options for a Tor-based operating
system?" or "How can I transparently proxy traffic to Tor on Windows".
 So we need a good answer here, and I don't know what it is.
How about we this: Let's all try to make an honest effort to *focus*
on stuff that Tor releases for now, answer relevant questions for
other Tor-related stuff as it shows up, and revisit this in a little
while, once we see how it goes?
Everybody's still working out how we want to use this, and it's only
going to work out if it has a happy, engaged community.  That means
IMO we should avoid making and avoid looking for Big Policy
Pronouncements, and lean towards decisions of the form "let's try X
for now and see how it works out".
At least, that's how I'd go.  It's not actually my call, and I'm not
currently moderating it, but I don't think that's a crazy approach for
best wishes,

@_date: 2013-06-21 09:17:26
@_author: Nick Mathewson 
@_subject: [tor-talk] Are non-official projects welcome to Stackexchange 
as I2P.
Speaking only for myself: I'd sure like any Tor-related SE to be
Tor-related.  If other folks want to start a general anonymity one,
that would be fine by me.
Well, personally, I'd like any Tor venue to be about Tor, and any
anonymity venue to be about anonymity.  If people decide to create an
anonymity venue, and it works out better than the Tor one as a place
to talk about Tor, that could be grand -- but right now, as it stands,
we're trying to make a Tor one.
Personally, I am no more try to monopolize any possible anonymity
stack exchange by starting a Tor one than I am trying to monopolize
anonymity mailing lists by running tor-talk.
Please, exercise the principle of charity. Just as I'm assuming that
everybody who advocates something here really believes in it, and is
trying to make a better outcome for no ulterior purpose, please accept
that WE are human beings trying to do the best we can with imperfect
resources, limited time, and sometimes a poor ability to communicate
what's going on.
best wishes,

@_date: 2013-06-29 17:53:57
@_author: Nick Mathewson 
@_subject: [tor-talk] Fwd: [Full-disclosure] tor vulnerabilities? 
I think this guy is confused.  I tried to tell him as much when he
twittered at me last night; you can see more or less the full record
if you look at the  from last night.
tl;dr: relay onion keys are indeed authenticated by the consensus
document.  On discussion, it appears that the guy thinks we aren't
actually authenticating them, though.  He posted
 to try to explain what he has in mind.
The attack doesn't work, though, as far as I can tell.  Her's what I
started writing up about it.
Some preliminary notes to clear up:
  - Being in the microdescriptor cache (the one implemented in
    microdesc.c as microdesc_map) is not sufficient for a
    microdescriptor to actually get *used*. It has to be linked to a
    node_t. The function that does that is the one in nodelist.c.  More
    on this below.
  - Directory authorities and directory mirrors are different.
    Directory authorities are a closed set, whose public keys are
    distributed with the source.  Anybody can be a _directory mirror_
    simply with a configuration option and an IP.
  - There are indeed three paths to the microdescs_add_to_cache()
    function.  One of them (in directory.c, not dirserv.c), passes a
    list "which" of the microdescriptor digests we requested
    microdescriptors for.  The other two don't.  But those are the ones
    that are reading microdescriptors from disk, so those
    microdescriptors were already checked on a previous run of the
    program.  (Also, adding them to microdesc_map is harmless; see
    above.)
  - Note that a corrupt directory mirror could try to influence path
    selection, by simply not answering requests for some nodes'
    microdescriptors, and pretending not to have them. I'll call this
    the "response filtering" attack. (Note also that it has nothing to
    do with cryptographic verification.) To resist it:
        * When clients want a directory resource, and they don't receive
          it, they request it from other directory mirrors until they
          do get it.
        * Clients don't build client circuits until they have
          information for a sufficient fraction of the nodes in the
          network, as calculated in nodelist.c,
          update_router_have_minimum_dir_info().
    So unless the "response filtering" attacker controls all the
    directory mirrors that the client uses, they can't prevent the user
    from learning microdescriptors for all the nodes they want.  And if
    they temporarily prevent the user from learning a given discriptor,
    the extent to which they can distort the user's view of the network
    is limited by the minimum_dir_info check
Okay, so let's walk through the code.
    Here's what's *supposed* to happen.
    The client decides to make a request for microdescriptors.  This
    happens in update_microdesc_downloads, where they call
    microdesc_list_missing_digest256 to get a list of the
    microdescriptor digests listed in the microdesc-flavor consensus
    such that the client does not have and is not already trying to
    fetch a microdescriptor with that digest.  The client passes this
    list to launch_descriptor_downloads, which actually does the work of
    sending the requests to one or more directory mirrors.  The list of
    microdescriptor digests requested is encoded in the
    "requested_resource" field of the directory connection.
    The directory mirror responds with a buffer, which the client hopes
    will contain microdescriptors with those digests.  In directory.c,
    the client reconstructs the list of which digests it asked for (by
    calling dir_split_resource_into_fingerprints) and passes that list
    of requested digests, along with the directory's response, to
    microdescs_add_to_cache().
    In microdesc_add_to_cache, the client first calls
    microdescs_parse_from_string.  Now "descriptors" contains a list of
    the received microdescriptors.  For every microdescriptor,
    md->digest is a digest of all of its textual contents.
    Then, it makes sure that the directory did not tell it any
    microdescriptors it hadn't asked for.  It does this by using a
    temporary map, "requested".  It initializes requested as mapping D
    to 1 for every digest in requested_digests256.  It then iterates
    over the microdescriptors.  If a microdescriptor's digest is in
    "requested", it sets the value in "requested" for that digest to 2,
    indicating that the microdescriptor was found.  If the
    microdescriptor's digest is not in "requested", it frees the
    microdescriptor, removes it from the "descriptors" list, and logs a
    message.
    (The function then removes every digest corresponding to a received
    microdescriptor from the 'requested_digests256' list, so that the
    caller knows what it didn't receive.)
    Notice that at this point, it has not checked whether the
    microdescriptors' digests match the digests listed for particular
    nodes or not -- only that the client actually requested
    microdescriptors with those digests.  It hasn't even matched
    microdescriptors up with nodes!  That comes later.
    Now we move on to microdescs_add_list_to_cache.  Our job here is to
    store the newly received microdescriptors to disk; to insert them
    into microdesc_map, and finally pass them to
    nodelist_add_microdesc.
    Before we pass the nodes to nodelist_add_microdesc(), let's recap
    where we are.  The microdesc_map contains microdescriptors, indexed
    by their digests.  These are all microdescriptors that we read from
    disk cache from an earlier session, or ones we received in reply to
    a request that we made for directory mirror request.  They are not
    yet associated with nodes. We have not yet checked that they still
    match the consensus.
    Now we get to nodelist_add_microdesc.  This part is key.  It looks
    up, in the microdesc consensus, whether we have any routerstatus
    whose listed microdescriptor digest (stored in its descriptor_digest
    field) matches the digest of the microdescriptor we have received.
    If so, it finds the corresponding node_t object, and associates the
    microdescriptor with that node_t.
            [Aside: if we already have a microdescriptor when we get a
            new consensus, it gets associated with the node_t in the
            nodelist_set_consensus function, where we look it up using
            microdesc_cache_lookup_by_digest256 with the microdescriptor
            digest listed in the consensus.]
    Associating the microdescriptor with a node_t might seem like an
    afterthought, but it's actually the security-critical part here.
    Here's why:
    When do we use an onion-key from a microdescriptor?  When we extract
    it in extend_info_from_node().  But that only looks at the
    microdescriptor currently associated with a node by
    nodelist_add_microdesc().  If the microdesc wasn't associated with a
    node there, we wouldn't even find its onion key.
So, what could go wrong?
1. Suppose that we start up with a microdescriptor cache that contains
   some microdescriptors which aren't in the consensus.
   In this case, microdesc_add_list_to_cache will indeed add them to
   microdesc_map, indexed by their digests.  But they won't get
   associated with nodes, so they won't affect client behavior.
2. Suppose that the directory mirror sends the same (requested)
   microdescriptor more than once in a given response.
   In this case, the "md2 = HT_FIND(microdesc_map, &cache->map, md)"
   check in microdesc_add_list_to_cache will make only one copy get
   added.
   In any case, they will only get associated with nodes if they match
   the digest listed for that node in the consensus.
3. Suppose that the directory mirror sends some microdescriptors in the
   directory response that did not have their digests listed in the
   request.
   In that case, they'll not be found among the microdescs in
   requested_digests256, and they'll be dropped.
Okay, now let's try the real thought experiment.
   Suppose that according to the consensus, node N1 with identity ID1
   has a microdescriptor M1 with digest D1, and node N2 with identity
   ID2 has a microdescriptor M2 with digest D2, and so on.  Suppose that
   the client sees a consensus that lists D1 for ID1, D2 for ID2, and so
   on.  Suppose that the client requests D1...Dn.  Suppose that the
   directory mirror sends back ANYTHING OTHER THAN M1...Mn.  What could
   happen?
   First off, any members of the response that are duplicates will get
   dropped, and any whose digests don't appear in D1...Dn will get
   dropped.  It will be as if the directory mirror didn't send them at
   all.
   So the only stuff that will make it into the microdescriptor cache
   can will be microdescriptors whose digests match a subset of
   D1...Dn.  Assuming that SHA256 is collision-resistant, that means
   that a subset of M1..Mn will make it in.
   Can anything cause the client to associate M1 with a node other than
   N1?  No, since this association is done explicitly by the    mapping in the node's routerstatus in the signed consensus.
   So the directory mirror can, at worst, cause the client to have a
   subset of the answers it requested.  This reduces to the "request
   filtering" attack above, which has defenses.

@_date: 2013-06-29 17:58:11
@_author: Nick Mathewson 
@_subject: [tor-talk] Fwd: [Full-disclosure] tor vulnerabilities? 
If you think he's actually got a point and you don't want to navigate
twitter's horrible search to find it , he started out tweeting as:
then as and then as and then as and finally as Like I said, I'm pretty sure that the stuff he's describing is simply
wrong.  Please feel free to check my work on this one though.
best wishes,

@_date: 2013-06-29 18:18:22
@_author: Nick Mathewson 
@_subject: [tor-talk] Fwd: [Full-disclosure] tor vulnerabilities? 
And let me try to explain this a little more, and give some background
about how keys are supposed to be authenticated in the current
("microdescriptor") directory design.
There are a few directory authorities. (You *can't* be one in any
meaningful way just by starting up a server; you need to actually have
your key shipped with the Tor source, and have the other directory
authorities agree that you are a directory authority.)  Periodically,
they send each other signed statements of who they believe is in the
network, and they use this collection of signed statements to compute
a "consensus" vote outcome including the results of everybody's vote.
The then all sign the consensus document.
The consensus document includes, for each node that the vote
computation, a digest of that node's long-term "identity" signing key,
and a digest of a "microdescriptor" for that node.  The
microdescriptor is a small unsigned document that contains the node's
medium term "onion key".
Directory caches download these signed consensus documents from the
authorities, and clients download them from caches.  No cache or
client trusts a consensus unless it is signed by greater than half of
the authorities that that cache or client believes in.
So, the trust property is that an attacker cannot dictate the contents
of the consensus unless he controls more than half of the authorities.
Similarly, caches download microdescriptors from the authorities, and
clients download them from caches.  These microdescriptors are *not*
self-authenticating.  You need to make sure that you only use a
microdescriptor for given a node if, when you compute that
microdescriptor's digest, you find that its digest  matches the digest
listed in the consensus networkstatus document.
The person who talked to me on twitter last night seems to be sure
that we do not adequately check whether microdescriptors match their
hashes in the consensus when we download them.  I think he was
confused by looking only at the checking necessary for those
microdescriptors added to the microdesc_map data structure, when in
reality (as I tried to explain in my earlier long email) the critical
step that determines whether a microdesc (and the key it contains)
will be used is when it gets associated with a node_t.  That's the
point when Tor explicitly checks whether the microdescriptor's digest
matches the microdescriptor digest listed for the node of the given ID
in the networkstatus consensus.
(If you think I'm full of shit here, and you want to avoid the whole
microdescriptor system, you can turn it off with "UseMicrodescriptors
0".  I'm sure not perfect.  But on the whole, I'm pretty confident
that the code here actually works like how I'm saying it does.)
best wishes,

@_date: 2013-06-29 22:37:16
@_author: Nick Mathewson 
@_subject: [tor-talk] Binary patch downloads (for updating TBB)? 
Is there a good rundown somewhere on the security properties of the
firefox updating system?

@_date: 2013-03-14 15:24:46
@_author: Nick Mathewson 
@_subject: [tor-talk] What would Tor v1.0 look like? 
Personally, I've been thinking we should just drop the leading "0" for
the first release to become stable after this fall, in honor of the
tenth anniversary of our first public release.  (Assuming I'm counting
Once I thought there was such a thing as being "done" with all this
stuff, and that kind of "done" would be called "1.0".  Now I think
there's always more challenges and opportunities, and so on.

@_date: 2013-11-11 10:53:10
@_author: Nick Mathewson 
@_subject: [tor-talk] Are project change logs online? 
Wrong spot; you're looking at changelogs for the Tor program itself,
but there has never been a Tor release called "2.4.17b2" -- that looks
like a TBB version number.

@_date: 2013-11-25 11:53:45
@_author: Nick Mathewson 
@_subject: [tor-talk] Regarding #8244; 
That's a pretty clever idea!  It reminds me of the way that
unsanctioned[1] "numbers game"[2] lotteries used to run. To gain
gamblers' trust, the house wouldn't pick the numbers themselves;
instead, they'd use a number that was supposed to be outside of their
control -- typically, the final digits of the total bets placed that
day at a racetrack.[3]
The history of gambling sure does have interesting stuff for people
who are interested in computer security! [4]
I'm not sure that I want to incorporate a full bitcoin dependency in
our voting protocol, though: it seems like it would pull in a lot of
code if a Tor authority needs to also be running a full bitcoin
So, how hard is it to receive and authenticate an unambiguous version
of "The blockchain hash as it stood at midnight UTC today?" Can we get
that with a stripped-down subset of the bitcoin protocols?
How hard is it to try to get the last transaction before midnight?
What chance of success would an attacker have doing that?  I see that
transactions-per-second these days is still pretty close to 1. That
doesn't feel like an impossible target to me, but I'd like to hear
more from people who know the bitcoin protocols better than I do.
How hard is it to figure out --before doing a transaction-- what
effect it would have on the blockchain's hash afterwards?
Also, besides bitcoin, are there other public verifiable values we
could look at?
[1] (okay, illegal.)
[2] While researching this to refresh my memory, I found that my state
has a lottery called "The Numbers Game".  I have no words.
[3] Because there's obviously no way that the mob could have any
influence on _that_, right?
[4] And for computers in general. Check out George Julius's early 20th
century work on electromechanical computers for running parimutuel
betting at racetracks.  It's fascinating stuff!
So, for commit-and-reveal protocols, the issue is that a malicious
party can decide whether to fake a network failure or something when
it's time to reveal, and then not expose their temporary secret.  They
can wait until all honest parties have revealed before making this
choice. That gets the attacker one bit of control per corrupt party.
Call the total number of corrupt parties C.
I'm not sure that using a commit-and-reveal protocols *together* with
a bitcoin hash makes sense, though: If the attacker can have C bits of control on
the bitcoin hash, then we didn't gain any additional security by using
So I think that when we're looking at "global external secure secret"
versus "secure multiparty random number generation" designs, we should
consider it an either-or choice, unless I'm doing the analysis wrong.

@_date: 2013-11-25 13:10:44
@_author: Nick Mathewson 
@_subject: [tor-talk] Regarding #8244; 
Oh, and to clarify: I rather like this approach. It's just that the
only way I know to do good security analysis is start with the
assumption that the idea must be broken somehow, and try to figure out
how what the attack is.
The fact that bitcoin blocks contain many transactions help us out
here, but makes the analysis less trivial than I'd thought before I
started re-reading the protocol.  Also, duh, it's not about getting
the last transaction -- it's about being the miner who happens to
generate the block.
So unless I'm missing my guess, the cost of setting N bits of the hash
with probability P here is equal to the cost of generating a target
bitcoin block with probability P, times 2^N ?

@_date: 2013-09-06 22:26:55
@_author: Nick Mathewson 
@_subject: [tor-talk] NSA has cracked web encryption! 
I'd seriously recommend the primary sources rather than USA today.
Try the Propublica writeup, the Guardian writeup, or the Nytimes
writeup -- those are the ones with the original research.  I'd also
have a close look at Bruce Schneier's two essays on the topic.
All of these are linked to from the following Bruce Schneier blog post:
Basically -- I wouldn't suggest USA Today for summarizing information
about cryptography.
It seriously depends on what the NSA has broken.  If they've got a
strong AES break, or a cheap way to break ECDH-P256 or
ECDH-Curve25519, then we're pretty screwed.  But none of the good
reporting I'm seeing suggests that.  (FWICT, none of the good
reporting is actually being very specific at all, and the stuff that
*is* being specific is speculating or misunderstanding or
free-associating, for the most part.)  The stuff I'm seeing is pretty
vague, but if I had to speculate myself, I'd most suspect:
   * Dubious stuff in NIST standards. Everybody's pointing at that
Dual_EC RNG, but other stuff will be getting a lot of cryptographer
scrutiny.  What isn't broken may often be found to be deliberately
   * The commercial CA world is possibly a house of cards.
   * Operating system RNGs are a black hole of stupidity. On the one
hand, entropy collection really ought to be an OS function.  On the
other hand,
   * Paranoia time: I suspect deliberate obstruction of progress and
encouragement of complacency in relevant standards bodies.  Seriously,
it's 2013, and our options for TLS are mac-then-encrypt-with-CBC, CTR
CGM (which-will-be-usually-implemented-with-table-lookups), and RC4? I
suppose that human frailty alone might explain such a sorry state of
affairs, but everybody knows That One Guy who won't let a simple
standard get approved when a complex protocol already exists, and who
won't stand for fixing the mistakes of yesterday so long as a
half-assed workaround is conceivable.
    Then again, it's not like non-cryptograhpic standard move any
faster than cryptographic ones, so this could be my paranoia acting
Also, RSA1024 and DH1024 are *not* what folks ought to be using
nowadays.  (See that article where a guy who knows how to use   So
please, everybody upgrade to Tor 0.2.4.x once you can so that we can
start getting our forward secrecy with stronger keys.
Over the 0.2.5 series, I want to move even more things (including
hidden services) to curve25519 and its allies for public key crypto.
I also want to add more hard-to-implement-wrong protocols to our mix:
Salsa20 is looking like a much better choice to me than AES nowadays,
for instance.  I also want to support more backup entropy sources.
Then again, I'm not a cryptographer myself, so you might want to check
out what actual cryptographers are saying.
These are interesting times for crypto.

@_date: 2013-09-06 22:28:22
@_author: Nick Mathewson 
@_subject: [tor-talk] [Cryptography] 1024 bit DH still common in Tor 
Yup.  Please upgrade, people.  0.2.4 is looking pretty good right now,
and I'd recommend it strongly over 0.2.3 or a variety of reasons, not
limited to this.

@_date: 2013-09-07 13:02:04
@_author: Nick Mathewson 
@_subject: [tor-talk] NIST approved crypto in Tor? 
The TLS ECDH groups P-256 and P-224 are NIST-certified.  For circuit
extension, we use Dan Bernstein's non-NIST-certified curve25519 group.
NIST has certified tons of stuff, including AES and SHA1 and SHA256
and SHA3.  If you're jumping ship from NIST, you need to jump ship
from those as well.
Of all the NIST stuff above, my suspicion is not that they are
cryptographically broken, but that they are deliberately hard to
implement correctly: see
  * (on the P groups)
  *  (on AES)
Also, we're not using DSA, but DSA (as recommended by NIST) fits into
this pattern: DSA (as recommended by NIST) requires a strong random
number generator to be used when signing, and fails terribly in a way
that exposes the private key if the random number generator is the
least bit week or predictable. (see
To me, this suggests a trend of certifying strong cryptographic
algorithms while at the same time ensuring that most implementations
will be of poor quality.  That's just speculation, though.
(And I'm probably falling to the fallacy where you assume that
whatever results somebody gets are the ones they wanted.)
Of course, the "deliberately" in "deliberately hard to implement
correctly" is almost impossible to prove.  Is it nearly impossible to
write a fast side-channel-free AES implemenation in C because because
of a nefarious conspiracy, or simply because cryptographers in 2000
didn't appreciate how multiplication in GF(2^8) wasn't as
software-friendly a primitive?  (Looking at the other AES finalists, I
see a bunch of other hard-to-do-right-in-fast-software stuff like
GF(2^8) multiplication and table-based s-boxes.)   Are the ECC P
groups shaped that way for nefarious reasons, or simply because the
standards committee didn't have an adequate appreciation of the
software issues?
And it's not like NIST standards are the only ones that have problems.
 TLS is an IETF standard, but TLS implementations today have three
basic kinds of ciphersuirte: a fraught-with-peril CBC-based
pad-MAC-then-encrypt kind where somebody finds a new active attack
every year or so; a stream-cipher-based kind where the only supported
stream cipher is the ridiculously bad RC4, and an authenticated
encryption kind where the the AEAD mode uses GCM, which is also hard
to do in a side-channel-free way in software.
Conspiracy, or saboteurs in the (international) TLS working group, or
international bureaucratic intertia? Who can say?
And let's not mention X.509.  Let's just not, okay?  X.509 is
byzantine in a way that would make any reasonable implementor's head
spin, *and* the X.509 CA infrastructure is without a doubt one of the
very worst things in web security today.  And it's an international
Yes; see above.  Also, there were once NIST recommendations for using
TLS; I have no idea whether we're following them or not.  (There are
NIST recommendations for nearly )
I'm not sure that there *are* international-standards recommendations
for ECC groups or for ciphers that diverge from NIST's.  The IETF is
an international body, after all, and TLS standards have been happily
recommending SHA1, SHA256, AES, DSA, and the P groups for ages.  (See
also notes above about the not-much-betterness of international
With any luck, smart cryptographers will start to push non-NIST curves
and ciphers into prominence.  I've got some hopes for the EU here;
ECRYPT and ECRYPT II produced some exceptionally worthwhile results; I
hope that whoever makes funding decisions funds a nice targeted ECRYPT
III some time.
As I said on another mail, I've got a mind to move a lot of our crypto
for other reasons, as well.
The elephant in the room here is TLS itself.  Frankly, I'm starting to
think we should cut the Gordian Knot here and start a little
independent protocol group of our own if the TLS working group can't
get its act together and have one really good ciphersuite some time
Yeah, I know how it is.  I'm seeing conspiracies under every protocol
and in every patch these days.  Gotta stay focused, write the best
protocols and designs and software I can, and maintain.
(And with that in mind I should really start on my weekend soon.)

@_date: 2013-09-07 13:03:37
@_author: Nick Mathewson 
@_subject: [tor-talk] [Cryptography] 1024 bit DH still common in Tor 
I don't install Tor from RPM myself, since I'm always building it from
source.  But would the alpha/development instructions at
 help?

@_date: 2013-09-07 13:41:15
@_author: Nick Mathewson 
@_subject: [tor-talk] NSA has cracked web encryption! 
One note about that Schneier essay.  On his website[1], he says:
 "EDITED TO ADD: That was written before I could talk about this.[2]"
[1] [2] So if I'm interpreting him right, [2] is probably where operational
security folks should be focusing in the near term. Not that we
shouldn't be looking at [1] too.
 [...]
I'm as much in the dark here as you.
Public key:
Older Tors, like 0.2.3 and earlier use RSA-1024 and DH-1024
everywhere.  With Tor 0.2.4, forward secrecy uses 256-bit ECC, which
is certainly better[3] , but RSA-1024 is still used in some places for
I want to fix all that in 0.2.5 -- see proposal 220 [3], and George
Kadianakis's draft hidden service improvements, and so forth.  I'd
like to see a Tor that can run with no reliance 1024-bit Z_p crypto
inside the next three to six months, if at all possible.
As for "are these the right parameters" -- whether we should be using
a larger ECC group than curve25519 -- I wish I knew more than I did
today.  Bruce is smart, but he doesn't specialize in ECC as far as I
know.  The ECC people I know have told me that they're pretty sure
that curve25519 is okay.  That said, you can be sure that there will
be lots of discussions in the ECC sphere about appropriate group sizes
in the next months and years, and probably cryptographers will be
reconsidering other non-EC, non-Z_p stuff too.
(One issue here is that designing ECC groups is not an exercise for
the likes of me. Using a curve that we made up ourselves would pretty
much guarantee using cryptographic code we implemented ourselves,
which is not the wisest thing in the world.  Maybe in a few months DJB
or somebody will start pushing a "curve38331" or "curve511187"[4] or
something like that.  If that's so, you can bet we'll be jumping.)
Symmetric key:
We're using AES128.  I'm hoping to move to XSalsa20 or something like
it -- I'm mainly waiting for this one paper to get released about the
right wide-block construction and/or once TLS support exists and/or
once we can finally jettison TLS (which is not a sure thing).
The problem here in my mind isn't key length -- it's that 1) AES
software implementation quality is quite variable, which prevents me
from having confidence in compatible Tor implementations that haven't
spent so much time scrutinizing their underlying cryptographic
libraries, and 2) AES offers a pretty bad security/performance
tradeoff; we could IMO be getting more security per cycle out of our
symmetric cryptography.
[3] This only works once users and relays start upgrading to 0.2.4
though.  Please upgrade!
[4] These curve names are completely hypothetical.

@_date: 2013-09-11 10:56:59
@_author: Nick Mathewson 
@_subject: [tor-talk] Should Whonix ship Tor 0.2.3 or 0.2.4? 
I concur that there should be as many 0.2.4.x deployments as possible.
New 0.2.3 installations are not IMO a great idea.

@_date: 2014-04-14 07:33:47
@_author: Nick Mathewson 
@_subject: [tor-talk] Leaky pipe design in TOR 
Tor still supports the leaky-pipe topology (as far as I know), but we
haven't actually used it for anything intentionally.

@_date: 2014-04-18 13:48:46
@_author: Nick Mathewson 
@_subject: [tor-talk] Programming language for anonymity network 
[Replying to Stevens and tor-talk only. Crossposting runs against my
religious beliefs.]
Also consider
  5) Amenability to side-channel-free programming.  It is
next-to-impossible to write cryptographic code in some programming
languages without introducing timing side channels.
  6) Availability of, or access to, high-quality cryptographic and
networking backends.  You don't want to be doing your own from
  7) Runtime quality on target platforms.  If you want to deliver
packages to windows users, your options are more constrained than they
might be than if you only need to support Linux.  If you need to run
on smartphones, that's also a factor to consider.
  8) Testing strategies. Some languages and environments make it
easier to isolate components for testing than others do.
Personally, if I were starting over, I'd look into a multi-languge
approach: a memory-safe compiled language for most of the programming,
with cryptographic and lowest-level networking code in C, and a
scripting language for higher level tasks and for component testing.
I'd probably use a multi-process architecture rather than trying to
cram everything into one address space.  I'd use a terse performant
functional language for testing cryptographic components for
(As a charming coincidence, this _is_ the architecture that I'd like
to migrate Tor to, resources permitting.)
Also, as I'm sure you're finding out, language choice is a classic
"bikeshed problem" (see  you will get more
advice from more people about your choice of language than you will
about any other question you might ask, with the possible exception of
which license to use.
Best of luck and wishes for your anonymity network!

@_date: 2014-08-11 12:33:08
@_author: Nick Mathewson 
@_subject: [tor-talk] Rendezvous RSA Exponent 
Any RSA exponent that meets the regular security requirements for RSA
should work fine.  (This is not deliberate, but enough people have
generated vanity hostnames that we're allowing goofy exponents for
.onion domains.)

@_date: 2014-01-20 12:35:45
@_author: Nick Mathewson 
@_subject: [tor-talk] key generation on first boot with low entropy 
If you're worried about the particular scenario where you've got a
device --especially a diskless device -- that has never been booted
before, one option is to have the script you use to start Tor check
whether an identity key has been created.  If it hasn't, it should try
to read a byte from /dev/random before it starts Tor, and block until
it actually can read that byte.
This is an ugly hack, of course!  I'd much prefer if kernels
everywhere would have /dev/urandom block in the case when there has
never been enough entropy in /dev/urandom.
I've added a ticket
( for better
workarounds for this.

@_date: 2014-06-16 11:07:54
@_author: Nick Mathewson 
@_subject: [tor-talk] Strange problmes when building Tor private network 
It's a little complicated.  Simplified: you can control which
bandwidths an authority will vote that relays have by using the
"V3BandwidthsFile" to specify a list of bandwidths.    Ordinarily,
this file would be produced a bandwidth scanner; but for a test
network, you can generate your own.  The file's format is documented
in section 8 of proposal 161.  (Does anybody have better documentation
for this format?  Anybody want to turn it into a spec?)
You need to turn on VersioningAuthoritativeDirectory before an
authority will vote for versions.
Many of these can be controlled by the options listed in the manpage
under "DIRECTORY AUTHORITY SERVER OPTIONS."  Some others can be
controlled by the approved-routers file documented on the manpage.
hope this helps,

@_date: 2014-05-14 20:28:36
@_author: Nick Mathewson 
@_subject: [tor-talk] Upcoming stable release: 0.2.4.22. Please test? 
Note the first line of the changelog:
  "Changes in version 0.2.4.22 - 2014-05-1?"
That's a hint of what I currently expect: probably Monday, or earlier,
unless something comes up.  Of course, I'll be a lot more comfortable
tagging on that time frame if nice helpful people (including nice
helpful people like you) try it out, and let us know whether you find
any regressions.

@_date: 2014-11-07 09:01:54
@_author: Nick Mathewson 
@_subject: [tor-talk] Hidden Services v2 proposal (nickm) 
The proposal is right here -- anybody can read it:
    The attack you mention should be ameliorated by the directory system
changes of section 2.

@_date: 2014-10-07 14:34:26
@_author: Nick Mathewson 
@_subject: [tor-talk] Another Tor is Possible, Kane/Ksec 
What's saddest: You didn't explain why you think it's broken.  So
other people will have to read it too if they'd like to know whether
it's any good.

@_date: 2014-10-14 13:05:57
@_author: Nick Mathewson 
@_subject: [tor-talk] Reasoning behind 10 minute circuit switch? 
If I'm reading the source right:  Back before commit
d2400a5afd70b009b632b307205273fc25c8cd92 from 2005, the number was 30
seconds.  But that was too short and led to unacceptable performance.
So Roger picked 10 minutes more or less intuitively.

@_date: 2014-10-16 14:58:15
@_author: Nick Mathewson 
@_subject: [tor-talk] Wrong links on Tor Browser download page 
By the way, if you ever have any troubles reaching the downloads, you
can probably find what you need from

@_date: 2014-10-30 19:40:24
@_author: Nick Mathewson 
@_subject: [tor-talk] Questions about crypto used in TAP/Ntor 
Neither one; it's a fingerprint of the identity key. (That's the one
called "signing-key" in the descriptors.)  See section 1.1 of
tor-spec.txt for a list of keys.
These identity key fingerprints are used to authenticate link
encryption, to know you've done a TLS connection to the right node.
They're used to sign all the other keys.
ntor is probably as strong as curve25519; TAP is probably as strong as
dh1024. (So, ntor is probably far far stronger than TAP.)
I'd like to deprecate TAP.  Some time in the next 2-8 months, for
instance, I'd like to make authorities reject relays that don't
support ntor.  That should be sufficient to stop clients running 0.2.4
and later from having to use TAP.
I'm working on implementing proposal  right now, which migrates
relay identities to (unhashed) Ed25519 keys.
[tor-spec.txt] [proposal 220] [implementation in progress]

@_date: 2014-10-31 09:03:52
@_author: Nick Mathewson 
@_subject: [tor-talk] Facebook brute forcing hidden services 
Also, if you're feeling technical, you might want to jump in on
reviewing and improving proposal 224 [prop224] , which include a brand
new, even less usable, but far more secure, name format. :)
[prop224]

@_date: 2015-08-17 15:27:01
@_author: Nick Mathewson 
@_subject: [tor-talk] (no subject) 
Hi, Thomas, and congratulations!  You've asked a question I wasn't
prepared to answer.  Here's a thread we had about it today:
18:25 < nickm> So, I assume people have seen the tor-relays/tor-talk
thread about "Hey Tor folks, what would you want in exchange for
making tor parallelize better"
18:25 < nickm> Do we have a way of even answering that?
18:26 < nickm> If not, I think we should reply to say "This is a
generous offer and we need to apologize for taking so long, but it's
not been something we had a way of answering before. We'll try to come
up with such a way and see what it outputs RSN"
18:26 < nickm> thoughts?
18:44 < arma4> sounds plausible. i think the issue is a combination of
not enough developerpower and also not enough money
18:44 < arma4> a short small amount of money wouldn't be enough to
overcome the first issue,
18:45 < arma4> and we need to overcome both
18:47 < nickm> yeah.  I think that anything less than a year fulltime
of dev time, plus overhead and incidentals, can't work out here.
18:47 < nickm> plus, no timeline promised
18:48 < nickm> arma4: thoughts?
18:49 < arma4> are there any incremental steps that can be done, by
other people, in the mean time?
18:49 < nickm> in theory sure
18:49 < arma4> it seems like a wildly unpredictable amount of work
18:50 < nickm> in practice nobody who isn't a Solid Wizard is going to
get much done here
18:50 < arma4> and it's not even clear, to me, what architecture we
should use to parallelize cleanly
18:50 < arma4> all of this ipc stuff sounds great in theory until you
try to run the program on ios or something and then boy are you
18:51 < nickm> I have an architecture in mind for circuit crypto
18:51 < nickm> for tls, I have no bloody clue
18:52 < toml> but would we feel good about taking a shot if there was
one full-time equivalent devoted to the problem?
18:52 < arma4> maybe explaining very briefly why it isn't trivial, and
why it is going to be hard to do right, would be helpful for the folks
wondering why we don't just do it already
18:52 < toml> arma: I agree that we should take the opportunity to
explain the challenge
18:52 < arma4> toml: and if we had said full-time developer, would
this be the most important thing to have her work on?
18:53 < arma4> so far the answer has been "no, other things are more important"
18:53 < toml> well, it would be an answer to the question: what would it take?
18:53 < toml> so if they put cash on the barrel head, we could
dedicate. (I would bet there would be other associated benefits not
strictly related)
18:54 < toml> probably the cost would be too steep, but they would
know where we stand. (part of the education piece)
18:55  * nickm suggests that we just copy-and-paste this conversation
into the thread
18:56 < arma4> sounds good
18:58 < toml> arma: and let's always use the term "full-time
equivalent." There is an industry standard for a FTE amount, but we
reserve the right to apportion those funds among more than one person.
18:58 < nickm> any more to add ?
19:00 < nickm> I feel like we could safely say "More than 80k and less
than 500k" on this today, and if those numbers don't scare people
away, invest time into digging into getter numbers
19:01 < arma4> sounds good. it is basically a big architectural change
inside tor. our work on better testing and better modularity is
(slowly) moving us in the right direction as we wait.
19:01 < toml> I would say minimum $100K, as this would leapfrog
several other priorities.
19:02 < nickm> also overhead
19:02 < toml> si
19:02 < nickm> yeah, good point, toml
19:03 < nickm> OTOH, we can also mention the $0 price point: for no
money at all, we will _care_ about this, because we already do. And at
some point eventually, somebody will surely work on it in their free
time, one of these days
19:03 < toml> (and that is too low for a FT equivalent, but it is
enought to motivate us to explore
19:03 < arma4> heck, not only do we care, but we even wrote up a thing
on how it might be done
19:04 < toml> arma: should we share that? (or share it again?)
19:05 < arma4> nickm should point to it in his response i hope
19:06 < arma4> he wrote it so hopefully he knows what is the best
thing to point at :)
19:06 < nickm> well,it's quite old and maybe I should revise some
morning/afternoon when I am smarter
19:08 < toml> perhaps leave it as is ? show how long we have been
thinking on this. Then maybe a add brief bit on things we have learned
since, at your leisure.
19:08 < arma4> that way lies paralysis. which is almost like
parallelization, but not quite. :)
18:13 < nickm> ok.  So I am going to add this to topics for the
wednesday core tor dev meeting, and send it to the ml, unless somebody
I think the URL I was asked to add was
, but that's rather outdated.
tl;dr: more data really soon now. We are bad at doing cost estimates
of this kind, and hope to get better RSN. Thanks for the interest!

@_date: 2015-01-08 10:07:28
@_author: Nick Mathewson 
@_subject: [tor-talk] new paper on Tor and cryptography 
Interesting stuff!
I wonder, does anybody around here have the cryptographic background
to comment on the PQ part of their scheme?
Personally, if I were doing something like this, I'd aim closer to
Yawning's "Basket" protocol, which uses an established PQ construction
(ntru in Basket's case) rather than trying to invent a novel one.

@_date: 2015-10-20 20:38:56
@_author: Nick Mathewson 
@_subject: [tor-talk] How the NSA breaks Diffie-Hellmann 
For encryption, it already happened back in 0.2.4, with the
introduction of the ntor protocol.  (And with the use of ecdhe in tls
where available.)
The remaining use of RSA is for authentication, and should be mostly
phased out over the next 8 months.

@_date: 2015-09-02 11:40:00
@_author: Nick Mathewson 
@_subject: [tor-talk] Introducing KroTor 
Interesting stuff.  I'm hoping nobody tries to use this as a
TorBrowser substitute, but it's a neat piece of engineering.
Have you considered submitting your patches upstream?

@_date: 2015-09-06 15:37:00
@_author: Nick Mathewson 
@_subject: [tor-talk] New mailing list: tor-teachers 
This sounds awesome, Alison, and thanks for organizing it!  (And
thanks to Nima too!)
Could there be some mechanism for reporting highlights from this list
to the wider world, whether via this list, Tor Weekly News, or some
other means?
Also, please never hesitate to file usability/teachability bugs
against Tor.  Let's have the easiest software on earth.

@_date: 2016-10-25 04:21:51
@_author: Nick Mathewson 
@_subject: [tor-talk] Tor 0.2.9.4-alpha is released 
If I'm reading that right, that line is just a strchr() call?  Do all
the glibc strchr() calls have this problem with your gcc and
-Wlogical-op ?

@_date: 2017-05-04 12:14:19
@_author: Nick Mathewson 
@_subject: [tor-talk] 0.3.0.6 on fedora 24: systemd? 
That check isn't just for "are you running systemd" -- it's for "do
you have the right headers and libraries to build Tor with extra
systemd support".   If you want to do that, you probably need to
install systemd-devel.

@_date: 2018-08-03 07:47:18
@_author: Nick Mathewson 
@_subject: [tor-talk] The Onion Report at #hopeconf (video) 
It clicks through to a video at the URL:
 .
Youtube-dl works fine for me on that URL.
(I'd upload it to youtube for you, but I don't have a youtube account.)

@_date: 2018-11-08 10:14:14
@_author: Nick Mathewson 
@_subject: [tor-talk] Fixing Orchid (again), need help! 
What versions does Orchid send in its VERSIONS cell?

@_date: 2018-10-05 19:43:20
@_author: Nick Mathewson 
@_subject: [tor-talk] bug in tor 0.3.4.8? 
If this is easily reproducible, and you can build from source, using
"git bisect" to find the first version that caused it would be very
helpful.  Do you want more info on how to do that?
