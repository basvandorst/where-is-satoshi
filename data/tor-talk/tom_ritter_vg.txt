
@_date: 2012-04-19 12:01:22
@_author: Tom Ritter 
@_subject: [tor-talk] Another openssl advisory: Tor seems not to be 
To add another data point, Colin Percival has blogged about how he
terminates SSL connections in a jail to mitigate this risk.

@_date: 2012-12-02 23:14:28
@_author: Tom Ritter 
@_subject: [tor-talk] Synchro of database server over Tor 
I'd design as much or all of the db-parts of the site to load over AJAX as
possible, so you can put up a nice "Loading..." message.  Keep a persistent
connection to the database; don't connect for every client (pconnect in
PHP).  Maybe do a redundant design that aims for eventual consistency if
you have to fallback to database B because you can't reach database A.
Have your site have very robust failure handling for timeouts, data not
read etc both client and server side.

@_date: 2012-12-18 19:51:46
@_author: Tom Ritter 
@_subject: [tor-talk] Tor Browser protections 
Yes.  At least based on what I see here:
and in particular here:

@_date: 2012-12-19 10:26:15
@_author: Tom Ritter 
@_subject: [tor-talk] Status reports archive, blog and git? 
It'd require some custom development, but it'd be possible to load comments
via javascript - basically to build your own Disqus.  I do something like
this on my blog.  It grabs the page name/path from the URL, and uses that
as a key to fetch comments out of a database and display them on the page.
  Now the comments are stored elsewhere and the page in git has some
boilerplate javascript to load the comments.  You can even let uses
markdown-style their comments.

@_date: 2012-07-06 12:02:31
@_author: Tom Ritter 
@_subject: [tor-talk] How to pin the SSL certificate for torproject.org? 
In what application?
In Chrome, your best bet would be to compile Chromium and add the
project cert into their pinned list in the code before doing so.
In Firefox, you'd probably be best served by using Convergence or
CertPatrol to verify the certificate through external validators or
notify you if the certificate changes (respectively).
In other applications: IE, wget, curl, etc - there aren't any
'prebuilt' options - you'd have to write some sort of plugin or hook

@_date: 2012-07-20 20:48:01
@_author: Tom Ritter 
@_subject: [tor-talk] Torbirdy and gpg --throw-keyids 
Hash: SHA1
I agree with Jake.  Less information disclosed is better.
Under some circumstances I will encrypt a message to recipients not in the email.  For example, if I am emailing on behalf of a group, I will encrypt to the group, even if I do not CC/BCC them, because I consider it a 'trust' thing.  I never intended them to not be able to read that message, so I portray it.  (It's also super-handy if I need to forward the email from a phone w/o my key.)  Another situation could be encrypting emails to a backup key of my own.  Or even (whip me for suggesting it) encrypting to a message escrow service of some kind.
So throwing the keyids of everyone but the recipient and sender is very good, and should be done.  I argue strongly for that.
Under some strange circumstances, the receiver and/or sender may have a non-public key that the message would be encrypted to, that they would not like to disclose the existence of.  It could be used to segment working vs personal relationships, keep a high-security key under wraps for use with your spouse, be a project specific key, or perhaps be used to bypass a previously theorized key escrow service.  If I was performing reconnaissance on someone, and say 85% of their traffic went to a public key on a keyserver, and 15% went to an undisclosed key - that's strange.
But on the flip side, it's obvious the message is encrypted to the recipient(s) specified on the email and the sender saw it unencrypted... and in some cases those recipients may be greatly inconvenienced by throwing the keyids - as in your case.  So throwing the keyids of the recipient(s) is still arguably important, but less so than third-parties.  I could go either way on it.
It almost seems like it could be worth codifying a preference in the OpenPGP standard. Potentially interpreting  to also imply throw-keyid or adding a new option.
- -tom

@_date: 2012-07-29 16:48:39
@_author: Tom Ritter 
@_subject: [tor-talk] new tld question 
Other good news: no one registered for .onion, and it's going to be
several years until the next round of applications open.  Hopefully by
then, the process will be much smoother than this time around.
It's possible that next time around, Tor could apply for .onion, and
use it as a tor2web portal - but even if a lot of engineering effort
was put in[0] - a user visiting aabbccddee.onion in a normal web
browser would leak its DNS request, and an observer would know exactly
who they were trying to browse to.  That's not an issue with tor2web
mode, because it's only the HS, not the user, trying to be anonymous.
But trying to keep the user anonymous when visiting a .onion would be
extremely difficult, if not impossible.
But then again, on the flip side, if a user visits aabbccddee.onion
without using either a Tor DNs Proxy or TBB, that .onion DNS request
is still leaked.  So maybe the threat model becomes "We know we can't
protect users trying to visit a .onion without/with-misconfigured Tor,
so perhaps we want to at least enable the functionality, and hide what
the user is doing on the HS'.
Obviously there's a mess of holes with this, but I'm just thinking
aloud, and if the idea of exposing HS to the normal web through .onion
is desirable, we could start brainstorming in advance of the several
hundred pages of paperwork applying for a gTLD requires.
[0] If every DNS Request returned the IP of Entry Guard or similar
node, along with a DANE record, and a DPF policy of 'Always use SSL',
the client would connect to the IP hardcoded to use SSL with a
pre-arranged certificate.  They would then request the resource of the
hidden service (let's say '/').  That Entry Guard would hold all the
information: the client connecting, and the resource requested.  This
is obviously nowhere-near-ideal, but for a 'Let everyone use any
browser' situation, I'm not sure how to avoid it.  That Entry Guard
would then route the request through the Tor network, potentially
padding it.

@_date: 2012-06-24 20:49:08
@_author: Tom Ritter 
@_subject: [tor-talk] Roger's status report, May 2012 
The only reason I could think not to put them on the blog would be
because it may 'turn off' some users because it's too mathy/programmy.
 If you're not worried about that, put them on the blog.  If you are,
perhaps a separate 'Tor Engineering' blog?  If you do separate it into
a second blog, you could disable comments, simul-post to the tor-dev
list, and say all comments should go on-list....

@_date: 2012-05-09 09:37:21
@_author: Tom Ritter 
@_subject: [tor-talk] Webserver on 127.0.0.1 only? 
In addition to Ralf's advice (which is correct), you can/should
configure a firewall to prevent connections to port 80 and 443 (and
really everything except how you connect to the box which is probably
ssh) just to be double-safe.  You can use iptables for this, but if
iptables is really confusing to you, I personally use shorewall which
abstracts iptables to configuration files that make (more) sense.

@_date: 2012-05-27 10:26:44
@_author: Tom Ritter 
@_subject: [tor-talk] Torbutton-birdy version 0.0.2 
Wouldn't this (or some of the other header settings) allow the
recipient or general public (if a mailing list post) to learn that a
person was using TorBirdy?  I hate to say it, but "What's the threat
A passive attacker watching a user learns the user uses Tor, but
unless they do network analysis, nothing else.  The same passive
attacker watching the IMAP/POP endpoint learns someone is connecting
over Tor, and because subsequent SMTP-SMTP  connections are often
unencrypted (or unauthenticated)[1], they may be able to learn the
user's name some of the time; and email contents some of the time if
not encrypted with S/MIME or PGP.
The entire SMTP-server path is in email headers AFAIK - does that
include the connecting IP (e.g. the tor exit node?).  If it does, then
the next part doesn't matter - if it does not: a recipient wouldn't be
able to learn that the sender sent it using TorBirdy... unless
TorBirdy used some non-standard and distinguishing email header or
setting... like this one.
Is that important?  It seems like it would be.  As an example, go
through this thread, and see whose reply header is of the form "On X,
" and now you know who's not running the latest version.
[1]

@_date: 2012-11-12 08:58:28
@_author: Tom Ritter 
@_subject: [tor-talk] Hidden services home hosting 
I think different operational practices would solve the problem.  For
example, Host the blog on a HS on a VPS outside the country.  Or for
something that costs nothing, Host the blog on blogger.com or wordpress,
and connect to it over Tor, signing up using non personally identifiable

@_date: 2012-10-04 09:45:18
@_author: Tom Ritter 
@_subject: [tor-talk] Can we come up with a lighter, 
I don't know if I'd say that... I think TorBrowser could be improved
by integrating Tor+Vidalia+Firefox into a single app.  This may also
go towards fixing the user confusion with having several windows
running.  Or imagine a VirtualBox where all network traffic from the
VM was automatically sent over Tor.. because Tor was embedded in it's
network emulation.
Of course those are the huge, monolithic cases.  Take simpler apps
like gpg, ssh, putty, pidgin (god help us), git, svn.  While tracking
upstream would certainly be a problem, having a statically linked tor
and a modified binary that sent everything over Tor I think would go a
long way towards getting average users using Tor safely... without
ever having to say the words "Proxy"  or "Socks" to them.
I guess the problems to overcome would be to figure out a way to track
upstream easily and identifying use cases where people would really
benefit from specific tools, to focus on those first.  TorPidginOTR
seems like it'd be a likely candidate... unless there's a
non-libpurple OTR-enabled chat client.

@_date: 2012-10-05 11:52:44
@_author: Tom Ritter 
@_subject: [tor-talk] Request: would someone create a tutorial on how to 
There's some wiki articles, but I'm surprised there wasn't a simple one...
For Linux, I think the fastest/most naive way would be:
1) Set up a tor bridge on another host in your network/on the internet
2) Boot your client/testing machine from a LiveCD
3) Install Tor, have it point to the bridge
4) Install shorewall (or use iptables, but I don't know the correct
iptables commands)
5) /etc/shorewall/zones:
fw    firewall
net    ipv4
6) /etc/shorewall/interfaces
net    eth0    detect    dhcp
7) /etc/shorewall/policy
fw    net     DROP     warn
net    fw      DROP    info
8) /etc/shorewall/rules
ACCEPT    $FW      bridgeip     tcp    bridgeport
9) This should prevent anything from leaving your machine that doesn't
go to that single port on that single ip over TCP.  Tail your logs,
and let your machine quiet down.  Stop any ntp services, system
updates, etc.  Get it so your machine is sending no traffic.
10) Start up your application, use *every single feature* and make
sure it's all being correctly proxied through tor.  Anything that
shows up in your logs is a leak.  investigate it.
I purposely did not put this guide on the wiki, because this is stream
of consciousness, and it's probably missing something.  (For example,
you should confirm that it correct stops outgoing icmp and udp.)  Try
it out, flesh it out, work out the kinks, write it more verbosely with
samples and screenshots, and then put it on the wiki
Finally, the application may read your IP address or another
de-anonymizing information and send it over Tor.  This is another
class of leak, much harder to detect.
In addition to what I wrote, you really should read all of what is on
the wiki also:
 -  -  -  -  - It's long, but you're now doing work as a developer, not a user - you
owe it to the users of *your* work to make every effort to do it

@_date: 2013-08-11 10:05:43
@_author: Tom Ritter 
@_subject: [tor-talk] HS drop 
Hey grarpamp,
You may have explained this elsewhere, but if so I missed it
(potentially while on an internet moratorium for the past week) - how
are you observing these statistics?

@_date: 2013-08-13 07:25:23
@_author: Tom Ritter 
@_subject: [tor-talk] How to designate specific node as exit node 
The "ExitNodes" configuration parameter.
The standard caveat applies: messing with Path Selection in this way
is likely to decrease, not increase, your security and anonymity.

@_date: 2013-08-28 12:49:47
@_author: Tom Ritter 
@_subject: [tor-talk] Tor relay activity from Antarctica 
Is there any reason you suspect that Tor is being used maliciously,
and that it's not likely someone on the research station is interested
in protecting their privacy or anonymity? (Although if it's the
latter, they'd probably not be doing too good =P)  Seeing as it's
likely the research stations are supplied internet through government
connections, I'd be interested in using Tor if I were on such a

@_date: 2013-12-18 19:06:18
@_author: Tom Ritter 
@_subject: [tor-talk] Harvard student used Tor to send bomb threats, 
Which is exactly how we got a couple mixmaster nodes raided last year
by the FBI. (University of Philadelphia bomb threats)  =(

@_date: 2013-01-07 11:17:55
@_author: Tom Ritter 
@_subject: [tor-talk] On the Theory of Remailers 
I'm hoping this will be of interest to this list.  To encourage
interest in the waning art of remailers, I'm starting what I aim to be
a long series on how they work, design choices, technical limitations,
and attacks.  The first five are now live at

@_date: 2013-01-07 16:30:45
@_author: Tom Ritter 
@_subject: [tor-talk] On the Theory of Remailers 
I intend to, but I've never been the receiving end of remailer abuse,
so I've only got academic knowledge.  I had a few ideas I was
brainstorming about this:
1) a shared, hashed list of emails to provide a 'global' opt-out of
emails for all participating nodes
2) For plaintext mails, adding a spam filter on outgoing mails
3) some form of 'status' remailers could publish where a client could
see that their email was either delivered, flagged as spam by a
remailer's exit policy, or just never recieved.
And advantage of 3 is that it helps with the reliability quesiton: did
my message get delivered? A disadvantage is it requires a client to
remember a GUID of a message that would tie the user to the message
(very bad).
That's all assuming you mean abuse from the perspective of the
recipient, and not abuse form the perspective of the remailer

@_date: 2013-01-08 08:15:28
@_author: Tom Ritter 
@_subject: [tor-talk] On the Theory of Remailers 
I believe this is the concept behind Alpha Mixing [0].  When I talked
to Roger once, many moons ago, I recall he expressed the desire that
one day, in an ideal world, Alpha Mixing would indeed be the main
mixing of the network, to allow for transit of other types of things,
like email.
[0]

@_date: 2013-01-09 10:20:06
@_author: Tom Ritter 
@_subject: [tor-talk] On the Theory of Remailers 
On 9 January 2013 10:05, Alexandre Guillioud
Someone can probably explain it better by putting more time into it,
but the gist of it is how long a mix node will 'hold onto' a message,
before sending it on.  (Effectively 'mixing' it.)
As a mix node, if I accumulate 8 same-size messages, and then send
them all out at once to 8 recipients, you can't use traffic analysis
to see who I sent which message out to - because they're
indistinguishable.  That's high latency.  But if I had sent out each
message as soon as I got it, you could see which message went to each
recipient - that's low latency.

@_date: 2013-01-09 11:03:42
@_author: Tom Ritter 
@_subject: [tor-talk] On the Theory of Remailers 
On 9 January 2013 10:33, Alexandre Guillioud
Allowing a client to choose their 'delay' and combining low and high
latency networks is (as I understand it) the basis behind Alpha Mixing
As far as a 5 second window in Tor nodes - off the top of my head, I'm
not sure 5 seconds would really gain anything.  If the nodes you're
using (.e.g bridges) aren't used much, 5 seconds doesn't help you.  On
the other hand, adding 30 seconds (3 hops, 2 directions) to *each*
request, keeping in mind a page maybe have 20 requests quickly makes
web browsing near-unusuable.
The other elephant in the room is that *even with* high latency, given
*enough* traffic, you can always link it statistically.  Think of it
this way: If I'm sending a packet a day to a recipient, you can see a
packet a second leave my machine, and a packet a day received at the
other end.  Even if my message is mixed well, held for an hour and
mixed with other messages - it's not hard after a few days to realize
the correlation.  High latency makes this harder, harder still if you
don't have a regular pattern.  If statistically it's still easy, even
with a 5 second delay, what's the point in making the software harder
to use if you're not getting the defense you seek?
I'm not the authority on Tor's design decisions, but those are my thoughts.
[0]

@_date: 2013-07-12 10:22:04
@_author: Tom Ritter 
@_subject: [tor-talk] Will Tor affect Internet Explorer? (newbie question) 
Explorer. If I download Tor, will I still be able to use Internet Explorer
when I want to?
compromising anonymity.
website, or Tails at  Not only do either of these
solutions work out of the box (in most cases), they also preserve anonymity.
Actually, to use Tor with IE, you have to set the system-wide proxy, which
will affect the operating system and other applications.
I don't know anyone who's ever looked at what will be sent through that,
but I do know it will be a lot.  (Windows update traffic for example)
For this reason, and others, I would really recommend against using IE with

@_date: 2013-07-19 08:23:02
@_author: Tom Ritter 
@_subject: [tor-talk] NSA, Tempora, 
I don't think a 'series percentage' of relays are in EC2.  I would
politely ask you to research that and prove me wrong if you feel
strongly about it.  There might be a serious percentage of bridges,
but even that is questionable.  (Related: Runa is giving a talk at
Defcon on the Diversity of the Tor Network, so hopefully that will be
a canonical answer to these sorts of questions once her slides go up
in a couple weeks.)
Regarding their ability to monitor EC2 - well it depends on what
datacenter.  The bulk of EC2 is in the Virginia one - and yea the NSA
probably has a line on that one or it's upstream ;)  But what about
the one in Singapore?  /shrug

@_date: 2013-06-17 21:10:01
@_author: Tom Ritter 
@_subject: [tor-talk] Plans about Askbot? 
Not that, this:
It's torrent of all the data, made every 3 months, in a machine
readable format.  Frankly, the StackExchange folks are pretty good
about this, if they didn't make the data available in that sort of
dump, containing all the data up until the last day, after they
shuttered a site, I would consider them as having COMPLETELY violated
the principles the site was founded on, and would go over to their
office (they're in NYC) and see if I could plead with them into giving
it to me on a thumbdrive.

@_date: 2013-03-16 11:24:05
@_author: Tom Ritter 
@_subject: [tor-talk] Mail services: Hotmail / Live, Outlook 
I don't think this should be a recommended practice, because (while
you are in that country) it explicitly enables your government to
perform a traffic confirmation attack against you.
Unless you meant you did this while you were traveling, in which case
that's different.

@_date: 2013-03-16 14:48:30
@_author: Tom Ritter 
@_subject: [tor-talk] Mail services: Hotmail / Live, Outlook 
According to the guy at Google who has posted here before, they
require you to verify yourself (e.g. via your alternate email or
phone) on the first login from an anonymous proxy service, after that
they flag your account so they don't bother you again.
I haven't test the limits or implementation of this.

@_date: 2013-03-30 21:23:13
@_author: Tom Ritter 
@_subject: [tor-talk] FlashProxy and HTTPS 
I finally watched the recent FlashProxy talk, and the bit about "Not
working on HTTPS" intrigued me.  I looked into it, and had two initial
Mixed Content. This isn't great, but it's something that might work for now.
    Chrome and FF do not block an HTTP iframe on an HTTPS site.
    Chrome26 displays a different icon, and logs to console.
    Chrome Canary (28) did the same
    FF9.0.2 allows and has no indication
    IE9 blocks
So putting the badge on a page in an iframe could allow a webmaster to
deploy it on a HTTPS site.  That frame would be on a different domain, to
get protections via Same Origin Policy
Root Cert.  This one is more than a bit crazy, but I don't believe in
discounting crazy out of hand.
Basically, if you accept that the TLS connection provides *no security
whatsoever*, that is - it does not provide authenticity, and therefore
should not be assumed to provide confidentiality - but you want to use it
as an opportunistic layer (hey maybe this will help, it can't hurt), or to
enable it working on HTTPS sites, or as an anti-fingerprinting tool (now
they have to look at the handshake/certificate instead of te traffic) it
becomes acceptable.
Create a FlashProxy Root Cert, with a critical NameConstraint extension.
The Name Constraint would be something like ".entire-internet.flashproxy.com".
 Because it's Name Constrained, and critical, no client will accept a cert
for a domain like paypal.com chaining to your root. IIRC the only desktop
client that does not support NameConstraints is Safari - BUT because it's
critical, Safari will outright reject the certificate.  Mobile Clients
should behave the same way.  A group of CA's and Browser vendors are
working to document the veracity of those claims, but I'm pretty confident
in them because they recently, to great consternation of the IETF, said
"we're going to allow non-critical NameConstraint extensions, because if we
don't, we'd break Safari".
So you've got the root cert.  Folks who want to run FlashProxies install it
in their browser or OS.  (The NameConstraints give them confidence you're
not going to, nor can you, mess with them.)
Now when a client wants to have a FlashProxy connect to them, they talk to
the facilitator or another facilitator like system, and they receive a
Root-CA signed cert for 127.0.0.1.entire-internet.flashproxy.com
(substitute 127.0.0.1 for the client's actual IP) that's valid for a short
window, say 30 minutes.
Now, when the FlashProxy connects to the client, they do so using wss://
and receive the FlashProxy Root-signed certificate, and the browser lets
the SSL handshake succeed.
There's a lot of downsides here:
 - NameConstraints are not rock-solid in the sense that we've taken them
for long test drives, but no one's subjected them to 20 years of continual
use. When the value of the system attacked is greater than the cost, the
attack happens.  What's the cost for an attack on Name Constraints?  We
don't know.
 - It requires the FlashProxy user to install a root cert (e.g. do more
than just open a webpage)
 - The requirements for the client -> facilitator communication channel go
up: it must now be bi-directional and support up to 1K of data or so.
 - The signing of certificates would introduce a DOS channel. This can be
mitigated in some sense by rejecting requests for an IP if you've signed a
cert for that IP in the last validity_window / 2, and preventing the
IPfrom being spoofed (free if done over
TCP, difficult otherwise)

@_date: 2013-03-31 08:29:27
@_author: Tom Ritter 
@_subject: [tor-talk] FlashProxy and HTTPS 
TLS has many uses, beyond those employed in standard "Industry
Deployments". If you'd prefer to register your dissent on this being a
bad idea that's fine, but otherwise perhaps you can provide a little
more feedback, or elaborate on what areas you believe are factually

@_date: 2013-05-01 15:53:49
@_author: Tom Ritter 
@_subject: [tor-talk] HTML5 video and Tor anonymity. 
Well, when anyone from outside the Tor project talks about sandboxing
flash, they're talking about restricting the system calls it can make,
restricting it from touching files on disk, spawning processes - real
sandbox stuff.  That's what Mozilla is after with Shumway.  That's
what Chrome is/was after with their sandbox.
Tor is afraid of Flash for three reasons as I see it: it's buggy (see
my previous sentence), it can read your IP address, and (I believe) it
can or can be made to make requests that circumvent a configured proxy
that would leak your external IP to whatever you connect to (assumed
to be an attacker).  And when I say proxy, you can read "Tor".
If Flash is running on a machine with a RFC1918 IP (192.168.x.x,
10.x.x.x, etc) then knowing the IP doesn't help.  But it can still
make a proxy-circumventing request.  Putting Flash in a VM and
restricting the VM from making any request except through the proxy
(or routing all requests through the proxy) alleviates that concern.

@_date: 2013-05-01 23:11:12
@_author: Tom Ritter 
@_subject: [tor-talk] torslap! 
I used to be a big proponent of proof-of-work schemes, but I've scaled
back my preference significantly.  There's two problems with them:
1) An attacker can use economies of scale to get better results than
an ordinary user.  If a user takes 5 minutes, an attacker can use GPUs
or ASICs to take 20 seconds.
2) Attackers almost never, or would never, pay or use their own
computers to compute proof of work schemes - they use victims of a
botnet.  In which case the only person hampered by a proof of work
scheme are the legitimate users.[0]
Mike Hearn has given a good amount of thought to deposit-based
systems.  Pay a server $5 or 5 bitcoins, and if you're a legitimate
user (not dormant) for X months you can get the money back.  (Or you
never get the money back, and the payment is smaller).  In this
scenario, the cost of an account cannot be reduced via scaling; and
while you can use a botnet to mine bitcoins, now that GPU/ASIC/FPGA
bitcoin mining is the norm CPU-based botnets will be more expensive
that the income they generate.
I like this theory a lot, many many times I've tried to purchase
something, over Tor, but been rejected because I've come from an
anonymous proxy.  I don't even care about paying 25% more at that
point, I just want to do it anonymously.  These days, my opinion is
that I hope bitcoin matures to the point where a system like this is
both possible and widely implemented. [1]
[0] There's definitely some parallels to DRM there...
[1] I reserve the right, as always, to change it if swayed by a good argument ;)

@_date: 2013-05-07 21:41:27
@_author: Tom Ritter 
@_subject: [tor-talk] memory cached pages should reload instantly-but DON'T 
Hm, that's an tough question.  TBB doesn't modify the FF code very
much at all, and the patches are pretty lightweight - they're all
listed here: although some of them do deal with caching.
The about:config settings are all listed here (AFAIK):
so I wonder if there's anything in there you might recognize as
causing a problem?
I'm afraid I'm not quite sure that the issue could be, these types of
bugs are pretty tricky to track down.  I did want to point you in the
right direction for maybe finding the culprit though.

@_date: 2013-05-07 21:46:06
@_author: Tom Ritter 
@_subject: [tor-talk] Is using player like VLC safe alternative to Flash? 
VLC has a lot of stuff going on inside of it.  I would not be
surprised if there were proxy leaks that might be able to be forced by
someone doing something tricky.  Say you enter a url to a flash video
and the content is intercepted and replaced with an RTSP stream that
VLC somehow interprets, and due to a quirk of RTSP makes a request to
a third party domain that isn't proxied?  I have no idea if that's
possible, but I wanted to give some strange example of something VLC
supports that might have a proxy leak in some obscure component.
Likewise, when discussing security vulnerabilities... VLC doesn't have
the best track record.  (See  ).
I'm a big fan of VLC, but I put it in the same category as Pidgin when
it comes to "how far do I trust this program to not have bugs?"
I would love to see someone do an objective test of VLC as opposed to
my subjective hand-waving, but I'm not aware of one.

@_date: 2013-05-23 07:29:30
@_author: Tom Ritter 
@_subject: [tor-talk] You could use ModX to create .onion sites, 
I think what he means is that most websites are built with a
tremendous number of referenced images, css, and javascript.  Web devs
use techniques like spriting to reduce the number of includes and
prefixdomains (cache1.exmaple.com, cache2.example.com) to speed up
load times.  .onion sites incur a significant latency hit, so every
additional include is painful, especially when the site doesn't expect
there to be this latency.
The end result is when a user users .onion sites, they can wait for an
extremely long time for the site to "paint" (actually render) because
the browser has to fetch resources that it's blocked painting on
(javascript and css can cause this).  In contrast, it's best to design
an onion site to
a) minimize includes, even at the cost of inflating the page size
b) make good use of caching where possible
c) sprite or better yet base64 images
d) make sure a browser doesn't block painting based on necessary
includes (very tricky)
e) background-load resources and content (or even entire pages) that
the user is likely to want while they are browsing the page
f) potentially, I haven't tried this, using the new prefetch
directives for bleeding-edge browsers
Those are just off the top of my head, I'm sure there's more.  It'd be
great if someone wrote a guide on this.

@_date: 2013-05-23 17:14:14
@_author: Tom Ritter 
@_subject: [tor-talk] You could use ModX to create .onion sites, 
Can hidden services talk SPDY?  The resource push features of SPDY
might be a hugely tremendous boone, without requiring re-architecture
web apps.

@_date: 2013-05-24 07:22:28
@_author: Tom Ritter 
@_subject: [tor-talk] You could use ModX to create .onion sites, 
I guess I should have phrased this as "Can TBB talk to a SPDY enabled
HS?" or "Can users take advtange of a HS running SPDY?"  I think TBB
would need to make special provisions.  SPDY requires SSL, if you use
the weird "Use SPDY over plaintext" option[0] it breaks HTTP.  So if
someone without a SPDY client visited x.onion, it'd break.  A HS can
redirect to a SSL version it itself, but the certificate won't
validate, at least according to normal PKIX validation rules, because
no one can issue a cert for a .onion.
... Actually that's not true.  I could have bought a certificate for a
.onion address, any .onion address, from any CA until the end of 2015.
 They're starting to phase them out now so "any CA" is probably not
correct some "some CAs" would be true.  That's a mildly creepy
thought, although the HS architecture should protect against that.
(Unless you've broken RSA1024)
I suppose it would be possible for TBB to talk to a HS over SSL, and
attempt to negotiate an anonymous, non-confidential ciphersuite (to
reduce the CPU needed) or make other provisions to accommodate it,
like ignore PKIX validation and showing no security indicators.

@_date: 2013-05-24 09:44:31
@_author: Tom Ritter 
@_subject: [tor-talk] You could use ModX to create .onion sites, 
.onion is not a real TLD.  CAs can (or could, they're phasing them out
now in anticipation of the new gTLDs) issue publicly trusted certs for
domains that are not publicly routable.  Usually this is for stuff
like mailserver.corp or redmine.internal or sharepoint.dell  for use
inside corporate networks.  But there's no reason they couldn't do
It wouldn't serve a security purpose, as far as I can devise.  It
would just be for not annoying the user with a "Invalid Cert" warning,
when in fact HS are secure regardless of SSL cert presented.
Sure, but at that point you're talking about altering the SSL stacks
of the client and possibly server.  Mike and I were brainstorming
ideas that would require fewer engineering changes.

@_date: 2013-05-24 15:48:27
@_author: Tom Ritter 
@_subject: [tor-talk] Tragedy of the commons. 
In my opinion the tragedy here is not that people abuse Tor, because
everything will be abused.  It's that providers will allow themselves to be
bullied, and then bully others, when behavior falls into the bucket of
"legal, but annoying".  If they SWIPed the IP, this could be averted
easily, and remove Linode from having to deal with the complaints at all,
but they won't do that either.
-tom (who also uses Linode for his server, but runs it as a middleman)

@_date: 2013-05-27 20:41:38
@_author: Tom Ritter 
@_subject: [tor-talk] Anonymity of Leaking Servers (Was Re: [tor-dev] 
Switching to tor-talk.
Is that important for Strongbox?  I don't think Strongbox's threat model
needs the document upload server to *be* anonymous.  Strongbox is run by
the New Yorker.  If you want to find their upload server, just look at all
the IP ranges the New Yorker leases.  Or subpoena them, or serve them with
a warrant.
If you were talking about Wikileaks, I might agree - it might be important
for them for their servers to be anonymous.  But then again, it apparently
*wasn't* because IIRC they never ran a document upload service soley on a
HS.  (They may have run one, but everything was also available on the
general 'net, again, IIRC).
I think for all (or most?) of the document leaking services we've seen so
far, the anonymity of the server isn't terribly important, it's the
security & anonymity of the sender that must be preserved at all costs.  In
that regard, HS are still good, because as you said "sources are forced to
use Tor, [with] end-to-end crypto without relying on CAs".

@_date: 2013-05-28 07:15:45
@_author: Tom Ritter 
@_subject: [tor-talk] What are some good VPS providers for Tor? 
Most lower end VPS (which tend to go up to $20/mo) do not allow exit nodes.
 Linode does not.  Sometimes you can get away with running one on them for
a few months, but they usually shut you down from the abuse emails.  I got
shut down after 10 months on a VPS that explicitly did allow exit nodes
when I asked them about it before I purchased.
Generally speaking, the more Tor nodes are in a single location (single VPS
provider or single datacenter), the worse it is for the network, as it
decreases diversity and increases single points of failure.  If you cannot
find a VPS you can afford to run an exit node on, consider running a bridge
or a relay node on a VPS that (you think) other people aren't also using.
Sorry there's no easy answer,

@_date: 2013-05-28 15:06:31
@_author: Tom Ritter 
@_subject: [tor-talk] What are some good VPS providers for Tor? 
It's a supported mechanism: The Torcloud images run Obfs Bridges, so it's better to run one of those
than roll your own.

@_date: 2014-11-03 19:07:03
@_author: Tom Ritter 
@_subject: [tor-talk] Krypton Anonymous: A Chromium Tor Browser 
Same idea, but I use a full linux machine as a router rather than an
OpenWRT router.
You can also set up a linux box as a VPN server (not a router), and
connect your phone to that VPN, and do the logging there.  Both
require you to configure tor to use a bridge though.

@_date: 2014-11-05 10:15:48
@_author: Tom Ritter 
@_subject: [tor-talk] Platform diversity in Tor network [was: OpenBSD 
I tried installing OpenBSD once... it was tough, heh.
Coming from a Windows background, I like the idea of running more
nodes on (up-to-date, maintained) Windows servers.
I'll also throw out the obvious that if we're talking about diversity
for the purposes of security, the network-accessible parts of tor rely
on OpenSSL, which would probably be difficult to swap out, but might
be worth it as an experiment.  Even if it's to LibreSSL.  Maybe the
zlib library also, but that one's had a lot fewer problems than

@_date: 2015-05-04 11:11:49
@_author: Tom Ritter 
@_subject: [tor-talk] 100-Foot Overview on Tor 
Hi all,
I've put together a slide deck that aims to provide a 100-foot
overview on little-t tor and Tor Browser. 100 foot, meaning I go into
a lot of technical detail, but not 10 or 1 foot which means some
things are definitely glossed over or handwaved a little. My
consistency with the 'foot level' throughout the deck varies a bit,
but I think it's decent.
Before I post it on twitter or a blog, I wanted to sent it around
semi-publicly to collect any feedback people think is useful. In
 - Upcoming Improvements worth mentioning (I'm a little light on the
Hidden Services 2.0, but that's proposal is big)
 - Interesting 'hidden depths' worth shedding a little light on
 - Particularly good resources for a specific topic (I'm trying to
avoid linking too much, but some is good)
 - Anything factually wrong of course
Slides are at:   Yes - it is long.
There's a lot to tor these days :)

@_date: 2015-05-05 18:49:39
@_author: Tom Ritter 
@_subject: [tor-talk] 100-Foot Overview on Tor 
Thanks! I made the changes and put up a 1.4
No but mostly yes. It's more a surprise factor: when I tell people tor
uses HTTP to upload and download things, they're not surprised - when
I tell them it has its own HTTP server implementation that does all
the parsing of the requests, they're much more surprised.  I'm not
saying tor's code is insecure (I put up a $bounty inside my company
with my own money to anyone who finds a bug in it actually) - but
implementing your own HTTP server is not a recommended action. :)
It's (now) As far as the sources.... well, I made it in keynote. Yes, I know I'm
a bad person. I can export it as powerpoint, html, images, or pdf and
send you any one of those five. (Or all of them.)

@_date: 2015-05-09 22:14:36
@_author: Tom Ritter 
@_subject: [tor-talk] 100-Foot Overview on Tor 
I believe that most places actually use the below function however:
When I was talking with many of the DirAuths in Valencia, they said
that a 24-hour outage was necessary for the consensus to be well and
truly out of date.  I pointed out that wasn't what the consensus said,
but they insisted, and then I found this code.
Yea, the extension quirk I think is a bit much, but I fixed the number
of hops - now that I think about it closer, 3 makes more sense.
Fixed, and redirects updated.
