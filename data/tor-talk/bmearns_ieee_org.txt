
@_date: 2009-12-04 12:24:37
@_author: Brian Mearns 
@_subject: I exclude all bloxortsipt nodes in my tor use 
2009/12/4  :
Did you try emailing support at truxton.com, find out what the deal is?

@_date: 2009-11-24 18:05:44
@_author: Brian Mearns 
@_subject: livejournal ban tor-nodes 
I appreciate your passion for this issue, but this isn't a mailing
list for political issues. Thank you for the update on Tor, please
keep the political content to a minimum.

@_date: 2009-11-10 11:19:47
@_author: Brian Mearns 
@_subject: Tor WIN in germany :) 
On Sat, Nov 7, 2009 at 1:24 PM, Juliusz Chroboczek
Yes, I'd really like to see an English version if possible.

@_date: 2009-11-10 12:29:26
@_author: Brian Mearns 
@_subject: Kaspersky wants to make Tor illegal and supports a globalized  
Agreed. You would think a man at the head of an Internet Security firm
would have a better understand of Internet vs. internet. His comment
about the Internet being "designed" illustrates to me that he doesn't
actually know much about the history of networking, and apparently
doesn't even have a good understanding of how ad-hoc these things
really are.
Anyway, like I said, I totally agree with your point. If The Internet
is restricted in such ridiculous ways as Kaspersky suggests, then
other internets will just spring up to replace it. Maybe to this end
we should all make an effort to establish de-centralized networks in
our own worlds: connect a few neighbors together with CAT5, or hell,
even RS232, and you've got a network. Connect one of these to the
neighbors on the next block, and you've got an internet. How about
Sneakernets? Onion routing by snail-mail and courier? Packet
transmission by encrypted email? The Internet grew out of nothing,
once, and that when network theory was only in its infancy. There's no
reason we couldn't go it again.

@_date: 2009-11-12 09:23:16
@_author: Brian Mearns 
@_subject: Related topic (Privacy): Brittain wants to track all telecom usage 
I thought quite a few people on this list might be interested in this
story, regarding privacy on networks. Maybe it will lead to more
people using Tor, or maybe it will lead to increased legal pressure on
Tor users and relay operators.
The second paragraph gives the long and short of it.
"The British government has decided to go ahead with its plans under
what it calls the Intercept Modernisation Programme to force every
telecommunication company and Internet service provider to keep a
record of all of its customers' personal communications, showing who
they have contacted, when and where, as well as the web sites they
have visited, according to the London Telegraph and various other
British papers."

@_date: 2009-11-17 11:15:10
@_author: Brian Mearns 
@_subject: Reduce hops when privacy level allows to save Tor network  
Isn't an underloaded network a security concern anyway, since it makes
it theoretically easier to track if there's not as much going on in
the network?

@_date: 2009-11-19 12:33:55
@_author: Brian Mearns 
@_subject: Reduce hops when privacy level allows to save Tor network  
My question is: do you really think it would help? If people are using
Tor inappropriately (meaning they could get what they want with a
simple anonymous proxy), what are the chances they're going to have it
configured appropriately to reduce the bandwidth they use?
Also, is the number of relay's really the limiting factor? It seems to
me that the number of exit-nodes would be a bigger bottle neck, and
cutting down hop counts wouldn't help in this regard.

@_date: 2009-10-02 09:05:09
@_author: Brian Mearns 
@_subject: Is it desirable to prevent users from choosing their own  
Ok, good to know. Maybe I'll peruse some more of the literature and
find out why this is a bad thing. Thanks for the response! (more
I'm no expert, but I'm pretty confident the hash is not weakened by
the limited address space, or at least not nearly as much as you seem
to think. One simple way you could use it (maybe not the most
practical) would be as follows: compile a list of all available nodes,
sorted by IP address or nickname or something. Take the output of the
hash as a value, and evaluate it modulus the size of the list: the
residue can be used to index into the list.
For instance, if the hash is 512 bits, but there are only 1024 nodes
in the list, is this as weak as a 10 bit hash? I don't think so. You
seem to be suggesting that an attacker (someone who wants to force a
certain circuit) could simply enumerate 1024 different input values
for the hash function, until they find the one that gives them the
node they want using the prescribed method of selecting. If the hash
is strong, I don't believe (again, not an expert on this) that there
is any guarantee that any particular set of 1024 different input
values will necessarily correspond to 1024 unique nodes. Every
different input will almost certainly have a different hash, but
taking these hashes mod 1024 may very well yield a number of
collisions. In other words, just because they tried 1024 different
input values, doesn't necessarily mean they will get 1024 different
output values (from the selection function, not the hash function
Of course, modulus probably isn't the best way to do this, since it's
slightly biased towards the beginning of the list in most cases. But
that's just a simple example of how you might choose a node from a
hash. I actually decided to run some tests just now to check on this.
Using a 512 bit hash (SHA-512) and a 1024 nodes, I enumerated through
some possible input values, starting with 0 and just going up by 1
each time. Just taking the hash mod 1024 to choose the nodes, I
enumerated the first 1024 input values and only selected 640 unique
nodes. Enumerating the first 2048 nodes got me up to 72 unique nodes,
and enumerating 4096 got very close to all of them at 1008 unique
selections. Obviously, that did significantly weaken the hash, but
still not as weak as a 10 bit hash. Also, I was able to strengthen it
slightly more just by breaking the hash into upper and lower 256-bit
values, adding them together, and then taking the sum modulus the
number of nodes. It was only a slight improvement, but it was also a
simple and not very well thought through modification. I'm guessing
that a smarter person that I could figure out a better selection
I guess this is starting to get off topic, but I would very much love
it if anyone with more expertise on hashes can argue either for or
against my assertion that the reduced selection space does not
significantly weaken the hash.

@_date: 2009-10-02 12:50:06
@_author: Brian Mearns 
@_subject: Is it desirable to prevent users from choosing their own  
Thank you very much for the additional feedback. I hadn't really
considered that this was a criteria of a hashing function, but I guess
it makes sense: if it's biased when fairly mapped to a smaller domain,
it would be biased in the full domain as well. For what it's worth, I
was using SHA-512.
Interestingly, "Applied Cryptography" (by Bruce Schneier) briefly
discusses a distributed timestamping protocol that uses a hash of the
content to be stamped in order to select which nodes will provide the
stamp, the idea being that the requester can't simply choose to use
nodes he controls in order to forge the timestamp. The details are not
given, but it is mentioned that the hash is used to seed a PRNG, the
output of which is used to pick the nodes. I suppose this would suffer
from the same weakening effects of mapping the output into a smaller
domain. If there are only 2^N nodes to choose from, an attacker should
be able to get the one he wants by enumerating through about that many
different inputs. Of course, if he needs to choose 2 nodes, then I
guess he would need to enumerate through 2^(2N) input values to be
almost-guaranteed a hit (or maybe 2^(2N-1), since order doesn't
matter?) I suppose that's where the security comes from in that case.

@_date: 2009-10-04 08:30:01
@_author: Brian Mearns 
@_subject: Node received the reload signal (hup)...what does that mean? 
I discovered my relay had shut it's self off this morning, and I found
this log message: Received reload signal (hup). Reloading config and
resetting internal state.
This was followed by a bunch of errors about my config file that
apparently prevented it from restarting (but I think I can probably
figure these out myself).
So what's this reload signal? It happened at 4 o'clock this morning,
and I know nobody issued any commands locally, so what gives?
Any ideas?

@_date: 2009-10-04 10:17:54
@_author: Brian Mearns 
@_subject: Node received the reload signal (hup)...what does that mean? 
Ah, I see. It's Fedora, actually, but that makes sense. So I just need
to address the other errors so that it can restart afterwards, and it
should be fine.
Thanks for the help.

@_date: 2009-10-01 13:18:39
@_author: Brian Mearns 
@_subject: Is it desirable to prevent users from choosing their own circuits? 
My understanding is that Tor user's are responsible (via their client)
for creating their own circuit, and that this is typically done at
random. However, are there any safeguards in place to ensure that it
is random, and would this be desirable? I would imagine that attackers
might try to choose specific circuits in order to learn more about
particular nodes, and the network in general. Would preventing this
behavior be helpful, and if so, would it be helpful enough to offset
any disadvantage it causes for legit users?
My idea is pretty simple. Instead of creating the circuit through
black-box means (relying on their local RNGs, for instance), the user
would create some seed value S, and then a list of random adjustment
values, R0, R1, R2,..., one for each relay in the circuit. The S value
would be used to enforce randomness in the circuit, but the R values
would be used to hide their circuit from relays as usual.
Creating the onion, the user would put a different R value into each
layer, encrypted for that relay, of course. To create the circuit,
they would take a hash of S+R0 to get the address of the first relay:
A1 = H(S+R0), and then hash this plus R1 to get the second relay: A2 =
H(H(S+R0)+R1)), and so on.
The user would now encrypt S for the first relay, and send that along
side the onion. This relay would get R0 from the onion, and then hash
S+R0 and confirm that the result points to their node, offering strong
evidence that the user did not choose them deliberately (since that
would require reversing the hash, which we'll assume is strong). Now
they can unwrap the onion as usual, and forward it to the next relay.
They would also encrypt H(S+R0) for this next relay, and send that
along side the side, so that relay can also confirm that they were not
chosen deliberately (by hashing H(S+R0) + R1), and so on until it
reaches the exit node. Each each stage, the relay can't get to the
R-value for the next node, so they only have part of the value that
will be hashed there, and they can't figure out where it will go from
there, ensuring forward privacy as usual.
Ok, just a little idea I had. Someone can go ahead and tell me now
that this is already being done, or why it's so obviously flawed that
it wouldn't work, or why it's so obviously flawed that it would be
devastating to legit users.

@_date: 2009-09-16 10:02:50
@_author: Brian Mearns 
@_subject: Vidalia exit-country and Hulu 
You seem to understand the burden such activities place on the Tor
network, in which case I'm curious what reason one might have for
accessing Hulu anonymously? (Genuine question, not a snide comment)
Feel free to contact me using PGP Encryption:
Key Id: 0x3AA70848
Available from:

@_date: 2009-09-17 10:58:05
@_author: Brian Mearns 
@_subject: "I Write Mass Surveillance Software" 
Well, I'm not entirely convinced that this guy is legit, or if he is
that his equipment is really as powerful as he implies. On the other
hand, I've only been casually studying cryptology for a few years, and
in that short time I've encountered more mind-blowing "you can do
that!?" moments than I can count on one hand (in binary).
Everyone knows that there are side channels in any system if not
properly and carefully implemented/operated. DNS lookups, search bar
suggestions, software update checks, etc., all have the potential for
subverting your privacy with Tor by not using the configured proxy
settings. Based on a bunch of the comments, I'm going to guess this
sort of thing (and probably many other equally simple but largely
non-obvious channels) are a big part of what he does (assuming he does
I think he (or someone else) also implied that traffic analysis is a
big part of it. This has been another one of those "holy crap!" fields
for me; the idea that an intelligent and diligent person can uncover a
significant amount of information from encrypted communications
without even breaking any of the encryption, is surprising but
apparently very realistic.
Lastly, I can't help but recall the early years of modern crypto, when
the public/academic sector was impossibly far behind the more
clandestine government/military sector. We like to think that this has
changed, but we can't really be sure, can we? I feel fairly
comfortable putting a good amount of stock in modern publicly
available cryptography, but then again I'm not doing too much that
could get me in trouble if I'm wrong, so it's not a high wager. My
point is that I personally wouldn't put it completely outside the
realm of possibility that a government agency has the capacity to just
straight up break modern public cryptography. I think the poster
pretty explicitly denied this, but then again, he would, wouldn't he?

@_date: 2009-09-17 15:59:17
@_author: Brian Mearns 
@_subject: good troll, intelligence psyops, or the genuine article? you  
Could you explain what psyops refers to? Psychological operations? If
I understand correctly, you're suggesting that perhaps he/his
organization doesn't really have all the capabilities he implies, but
that they're trying to scare people away from using technologies they
can't beat?

@_date: 2009-09-23 10:01:07
@_author: Brian Mearns 
@_subject: Random chaff [was: more work for Grobbages] 
So, if I understand this correctly, a correlation attack works (on a
very basic level) by noticing that Alice sent a message to Bob (a
known Tor node) at time X, and Dave (another known Tor node) sent a
message to Wally (a web server) at time X+e, where e is about how long
we would expect it to take for the onion to be routed. Is that more or
less the idea?
It seems like determining e (time to route the packet) with any degree
of precision would be pretty difficult, so is this really a big
problem? (or is that still being debated?) On the other hand, if an
attacker could monitor a good number of nodes, wouldn't it be fairly
easy to determine each three-node circuit segment (like Alice, to Bob,
to Charlie) and trace the whole thing end-to-end? It seems like this
could be defeated with a more intelligent type of "chaff", where the
receiving relay generates N random dummy onions (with an appreciable
circuit length) for each onion it receives, and then sends all N+1
into the network in a random order.
Then again, I may have completely missed the boat on this whole
correlation attack thing.

@_date: 2009-09-10 20:58:13
@_author: Brian Mearns 
@_subject: Reliable relay status check 
Is there a way to test that my relay is working? My logs indicate that
ORPort and DirPort are both "reachable from the outside", but several
different websites (such as  can't find
my relay (nicknamed "shallot").

@_date: 2009-09-10 20:57:33
@_author: Brian Mearns 
@_subject: Faking a local connection to services running on exit node 
I have a relay running on the same system as several other services.
Some of these services only accept connections from the localhost (or
otherwise give special privileges to localhost) . If I allow my relay
to be an exit node, someone attempting to connect to these services
through Tor will appear to be coming from localhost, right?
Is there anyway to prevent this while still allowing my relay to exit
to these services?
