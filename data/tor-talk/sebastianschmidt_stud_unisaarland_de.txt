
@_date: 2008-10-17 13:51:02
@_author: Sebastian Schmidt 
@_subject: about the legal consequences of the data-retention in germany for 
I'm a law student at the saarland university. I got noticed in the newest blog entry of the tor-blog that you wanna gather some information about what the owner of tor-servers in germany have to do on 01.01.2009. And what are the legal consequences for them in germany of the data retention law.
Early this year I gave a talk at the third Merziger medienrechtlichem Seminar about Tor in Germany and the data retention law and these upcoming issues. It was planned that I release a paper about it but sadly I never found the time to finish it. Nevertheless I wanna share all information I gathered so far. In the link at the end of the email you will find an impress presentation I used at the talk and a little mind map of it. At last you will find the !!NOT FINISHED!! paper I started. Please keep in mind while reading it that it's not finished but you will get my point if you read it. It's all in german. Presentation and paper need OpenOffice, the mindemap needs vym. Oh and yes the paper uses this template ( and it could be that you need it to view it but I'm not sure about this.
If some other german jurist is interested to join in and work with me together on finishing this paper, you're pretty welcome to contact me!!
 *  *

@_date: 2008-10-17 14:43:59
@_author: Sebastian Schmidt 
@_subject: about the legal consequences of the data-retention in germany 
Thanks for telling me! I'm already subscribed to that list but I'm subscribed to many lists and can't follow all. So I didn't knew about that discussion but now joined in :)

@_date: 2009-01-14 21:41:20
@_author: Sebastian Schmidt 
@_subject: Questions about gathering information and statistics about the 
I'm writing a tool right now to gather some longtime statistics about the tor-network. I want to plot these hourly taken information (e.g. with gnuplot) to offer plots on a daily/weekly/monthly/yearly base about the tor-network.
I think this is usefull (for the tor-development and the interested users) to observe the development of the tor-network over the time like: is the number of nodes growing/shrinking, are routers positions spreading more around the world over time or starting to even more concentrate on some countrys like the US, Germany,.. , number of and relation of exit-to entry-/middle-nodes, average uptime of the nodes, development of which ports are being blocked by the nodes, is the average bandwith of the network growing or shrinking and so on...
There are some informations which can be easily collected by the single server-descriptors by simply asking the control-port like: the number of nodes, with geoiplookup and their IP's also their country, the uptime and the blocked ports and stuff like this.
But there are some informations which are interesting too which aren't as easily to gather:
1.) the number of users: this would be a cool information but I don't know if there's at the moment any way also even just to roughly estimate the number of users. There are in my opinion just two places where such informations could a bit reliable be gathered but both are out of the game because of the current implementation to offer a good security. And one way (place) to get a rough estimation not of the number of users but if this number is growing or shrinking.
a.) the entry-nodes: every entry-node knows (or can know) how many individual users ( at least individual IPs ) are connecting to it right now. But because we don't know how many different circuits a user has open at one moment, we can't say how many users we have in total even if all entry-nodes would report the number of currently individual connections it has. Only workaround would be throwing all the information of all entry-nodes with all IPs of all users in one pot. But this would be a very very bad idea. So gathering the number of users based on entry-nodes is not going to work (at least not if we want the network to be as safe as it is at the moment).
b.) the directory-servers: if all clients would ask the directory-servers in a constant intervall for new information we could gather the number of requests per dir-server per 24h hours and divide it with the interval lenghts. But this has two problems: one is that not every client is on 24h per day so the information would be pretty unreliable even if we would guess an average time a client is online within 24h. The other is that the implementation ( under 5.1) isn't a static interval for all clients but more randomly choosen. So also this is no option by a matter of fact that we don't know how long each client is up and the random interval.
c.) the number of downloads of a new released tor version: the number of downloads of a new stable release of the tor-client could give an hint if the number of users is growing or shrinking. Of course this could just be collected on the tor-project page and thus would just be a snippet of all downloads/users because there are e.g. many users of modern operationsystems ( yes some small bang against MS/MacOS/Sun ;) ) which offer a packagemanagment-system and don't compile by hand. Those downloads and updates can't be count but even this snippet of downloads of a new stable-version (maybe within one week after it has been released) could give some impression if we compair this number to prior releases if the average number of users is growing or shrinking. 2.) the network health: network health can be understood in many different ways. One aspect I thought of would be the comparison of the bandwith all nodes are offering compaired to the bandwidth which is acutally used under the premise that we have enough users to consume all the bandwidth the nodes do offer (and I think we can safely make this premise). A good network health would mean under this condition that the bandwith which is acutally used is nearly the same as the bandwith the nodes offer. This gives an estimation of how good is tor on building circuits. If there are some nodes which aren't used all the bandwith they have to offer and other nodes which are nearly breaking under the bandwith they are asked for it means tor isn't doing well on assembling circuits. Also interesting would be here the number of connections each node has compaired with the bandwith it offers but the number of connections isn't exported at all. At least I couldn't find it in the service-descriptor. I came to think about this by simple tests. Building a circuit with three really fast nodes gives you more bandwidth than building a circuit with three really slow nodes. But on a healt network you would have the same bandwidth in any case because the number of connections through the slow ones would be lowered and on the fast nodes increased until they offer the same bandwidth to their users.
But also with simple checking the bandwith we have some limitations (at least as I understand the specs:  under 2.1). We have bandwidth-avg and bandwidth-observed (burst is kind of useless here for us as I think). I don't know how these values are gathered, the specs are a bit unprecisly here but they are pretty different if I take a look at them. Sometimes the observed value is less than 10% of the avg so I don't know if this value is usefull/accurate? It would be cool if a router tells us how much it is willing to share and how much it is acutally sharing but afaik we don't have the bandwidth a router is willing to share but just how much it is sharing which is bandwidth-avg or? Am I interpreting this correct?
I wanted to ask what you think about the idea to create such statistics at all? And have you some better ideas or thoughts about the number of users and the network-health?

@_date: 2009-01-17 21:23:11
@_author: Sebastian Schmidt 
@_subject: Questions about gathering information and statistics about the 
Hi Karsten,
sorry for my late reply but I was really busy these days.
On Wed, 14 Jan 2009 23:10:10 +0100
You're right with this! It also wasn't my intention at all to set up such special logging facility on my node but was just an idea how all entry-nodes could do together to get some overall network stats. But you correctly reminded me that such data could be used to deanomyize people so it looks like getting information about the current number of users of the tor-network without risking their anonymity is not possible.
This sounds interesting. Can those informations be questioned somehow from the dir-servers or are they non-public?
Thanks for the hint, I found it online (
It won't ;)
Those stats you gathered about the bridges here are really interesting! Since I read it I'm thinking how to interprete them. It looks like we have already "many" bridges for the short time they are supported in stable tor but just a small number of overall traffic (based on the bandwith consumption) on them. This could be intresting for people who want to support the network because they don't need to setup a 4TB-root for running a bridge. Also most users seem to be germans/americans and not people of countrys one would think who would be the number one. I'm thinking why? Afaik no provider in germany restricts the access to tor. Do people use bridges because they think this "extra hop" increases their anonymity instead of letting the bandwith for the people who really needs them?  Well there are many interesing information which could be gathered without touching users anonymity at all. In contrast there are information which needs to be collected to protect their anonymity. Stats like the sudden increase of nodes in a fascist country like e.g. burma,china and so on shouldn't happen without people noticing it. For the beginning I want to get all the interesting information out of the service-descriptors and make them visible.
Have you already thought about a good way to present the data? I think best would be a dynamic solution so one gatheres all the information and users can throw exactly the information they want in one pot which they want to see joined in one graph for a timeline they can choose. But I don't know any good framework which offers this. At the moment I just found cewolf( and I don't know if it fits the needs of which I'm thinking off. So I think gathering the already available information is more easily than finding a good way to make them easily public for the average user.
I'll be really busy the next month but as soon as I have something to show I'll let you know!
          Sebastian

@_date: 2009-01-20 18:55:09
@_author: Sebastian Schmidt 
@_subject: Questions about gathering information and statistics about the 
On Sun, 18 Jan 2009 13:17:17 +0100
I already created my own one :)
Yes but there are so many links you can make between different informations, you can't show them all on one page. I think a dynamicaly solution like giving one the possibilty to say: show me one single graph with the development of all exitnodes at all and all exitnodes in britain between this dates, would be pretty cool. If stator's ready does everything on the console and with gnuplot which I want I'm going to look further into this.
Thanks ;)
Before I leave you for my books I wanna show you what my tool (called stator, written in ruby) does so far.
It's divided in two main parts:
 - an api for connecting and talking with the controlport
 - an api for requesting and evaluating server-descriptors
As soon as it can be used by others I will make a public release of it. At the moment both apis need way more coding work to be used by others than me and aren't documented at all.
I spent most of the work so far on catching the data and creating useable data-structures to be able to make all links between different informations one wants to have. The server-descriptor api at the moment is able (next to creating the datastructure: an array of hashes of the single routers) to take a single-property like 'entrynodes' or an array ['Germany','France'] and returns the number of routers which have those properties. Next when I have time this will be extended to accept an array of arrays of properties like [['Germany','entrynodes'],['France','entrynodes'],['USA','exitnodes']]..to show the number of routers which have all properties of one array at the same time. In the attachment you see some output of a programm using these apis at the moment. It's a request for all property values which different routers have and the number of routers which have this value. It's structured like this:
$NUMBER_OF_ROUTERS - $PROPERTY
while $PROPERTY is in this order
- typ
- OS
- Country
- Tor-Version
- uptime
- bandwidth_avg of a router
blocked_ports of exitnodes is still work in progress because I haven't found the time to handle ranges in a good way...
          Sebastian

@_date: 2009-01-07 19:03:03
@_author: Sebastian Schmidt 
@_subject: tor controlport wants authentication even if authentication is 
I wanted to play a bit with the tor controlport. To check some basic functionality of it I just switched the authentication in vidalia off and restarted tor to make the settings take effect.
ps aux shows this now about the process:
DataDirectory /home/MYUSERNAME/.tor ControlPort 9051
CookieAuthentication 0 HashedControlPassword But if I now try a simple connect to it with telnet and type some random command I get:
$ telnet localhost 9051
Trying 127.0.0.1...
Connected to localhost.localdomain.
Escape character is '^]'.
meaningless command
514 Authentication required.
Connection closed by foreign host.
Why does TC tell me authentication is required even if it's switched off? Or is this the default reply if a not supported command was given to it?
Tor version: 0.2.0.32
Vidalia version: 0.1.10
OS: linux

@_date: 2009-01-07 23:59:41
@_author: Sebastian Schmidt 
@_subject: tor controlport wants authentication even if authentication is 
Thanks for your reply now I understand :) !
But this isn't explained in control-spec.txt. At least I hadn't understood it in that way. Maybe the specs could be updated to make this more clear? Of course 3.5 says "Before the client has authenticated..". But it feels more natural that when authentication is switched off that no authentication at all is needed, you know. I hadn't understood while reading the specs that even if authentication is switched off this AUTHENTICATE command is needed. Maybe after the syntax explaination could be written "The AUTHENTICATE command has to been sent in every case. Even if all authentication-methods are switched off it has to be sent to prevent some neat attacks against the TC.". Because as I read 3.5 an authentication failure is just reported if the cookie is incorrect and not that it is needed in any way.
Also the first sentence in 5.1 could maybe be more accurate with saying: "By default, the current Tor implementation trusts all local users." because it trusts just those users by default who authenticate.
3.5. AUTHENTICATE
  Sent from the client to the server.  The syntax is:
     "AUTHENTICATE" [ SP 1*HEXDIG / QuotedString ] CRLF
  The server responds with "250 OK" on success or "515 Bad authentication" if
  the authentication cookie is incorrect.  Tor closes the connection on an
  authentication failure.
  The format of the 'cookie' is implementation-dependent; see 5.1 below for
  information on how the standard Tor implementation handles it.
  Before the client has authenticated, no command other than PROTOCOLINFO,
  AUTHENTICATE, or QUIT is valid.  If the controller sends any other command,
  or sends a malformed command, or sends an unsuccessful AUTHENTICATE
  command, or sends PROTOCOLINFO more than once, Tor sends an error reply and
  closes the connection.
  (Versions of Tor before 0.1.2.16 and 0.2.0.4-alpha did not close the
  connection after an authentication failure.)
5.1. Authentication
  By default, the current Tor implementation trusts all local users.    If the 'CookieAuthentication' option is true, Tor writes a "magic cookie"
  file named "control_auth_cookie" into its data directory.  To authenticate,
  the controller must send the contents of this file, encoded in hexadecimal.
  If the 'HashedControlPassword' option is set, it must contain the salted
  hash of a secret password.  The salted hash is computed according to the
  S2K algorithm in RFC 2440 (OpenPGP), and prefixed with the s2k specifier.
  This is then encoded in hexadecimal, prefixed by the indicator sequence
  "16:".  Thus, for example, the password 'foo' could encode to:
     16:660537E3E1CD49996044A3BF558097A981F539FEA2F9DA662B4626C1C2
        ++++++++++++++++**^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           salt                       hashed value
                       indicator
  You can generate the salt of a password by calling
           'tor --hash-password '
  or by using the example code in the Python and Java controller libraries.
  To authenticate under this scheme, the controller sends Tor the original
  secret that was used to generate the password, either as a quoted string
  or encoded in hexadecimal.
kind regards

@_date: 2009-05-12 17:58:32
@_author: Sebastian Schmidt 
@_subject: A tor error message prior to crash 
Maybe it's not related to this error at all but maybe it is because since I upgraded to 0.2.1.14-rc version my entry-guard does also have some problems related to the libevent. My node is not crashing but my logfile gets spammed with these messages:
May 12 14:32:45.027 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 14:44:33.837 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 15:30:59.486 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:06:27.265 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:12:04.715 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:30:29.316 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:36:23.985 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:50:51.429 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 16:59:06.841 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 17:09:32.178 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 17:17:11.892 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 17:24:02.176 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
May 12 17:31:00.699 [warn] Error from libevent setting read event state for 1023 to watched: No such file or directory
This happens on lenny with libevent 1.3e-3. I haven't seen these messages with 0.2.0.34 but my node has just been running 4 days before I upgraded to rc so it could be just coincidence..
