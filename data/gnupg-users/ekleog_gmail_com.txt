
@_date: 2013-08-12 15:21:50
@_author: Leo Gaspard 
@_subject: understanding GnuPG "--clearsign" option 
According to  the OpenPGP standard is RFC 4880.
So, as your question is quite technical, you should be able to find your answer
here : Sorry for not being able to help you more!

@_date: 2013-12-05 00:14:10
@_author: Leo Gaspard 
@_subject: Renewing expiring key - done correctly? 
AFAIK by default ntpd dismisses changes to the RTC when NTP time is off more than
15 min of the RTC. One would need a special flag to force it to update the clock
in this case. (at least the ntpd I used)
So you could only delay the expiration date by 15 min... So useful ?

@_date: 2013-12-05 21:50:47
@_author: Leo Gaspard 
@_subject: Renewing expiring key - done correctly? 
I only have in mind moving the clock 15min by 15min, but thought this method
could not be used. Indeed, the clock can only be reset 1000s by 1000s, and by
default I thought ntpd queries the server 1024s by 1024s, thus allowing the
attacker only to slow down time on the machine. However, it seems that by
sending times not exactly 1000s apart from the machine clock but alternatively
1000s - clock jitter and 1000s, you could force the query time to be back to
64s, thus making this attack possible.
However, a basic security measure (setting panic threshold to 50s, as there is
no reason the local time should be off by more than 50s, being re-updated every
1024s at most) would be enough to counter this fact.
BTW, reading more about ntpd [1], I learned that the query time could not be
back more than 300s. So, even in an unconfigured system, the attacker would
have to be in control of the victim's network for at least half the time since
the expiration date. Granted, this is not a lot (especially knowing the attacker
could have been attacking since before the expiration date), but is better than
mere immediate clock setting. And this 300s delay could be arbitrarily
increased, thus allowing to counter the attack by a second mean : setting it to
more than 2000s, for example.
Finally, I believe the user would notice, if their computer clock was suddenly
off one day / month / year / more...
One last comments : not everyone uses NTP (e.g. I rely on RTC, once in a while
syncing it with my watch -- did not have to for a year, and I'm 2min off)
Just wondering... What are your other two solutions ?
[1]

@_date: 2013-12-14 21:14:07
@_author: Leo Gaspard 
@_subject: Sharing/Storing a private key 
AFAIK, ssss *is* an implementation of SSS. So, why would you write a new
I must say I didn't look at the source, as I do not see the point at first.
So, this is a warning about security issues : something you made yourself is
likely to be unsafe. A tested implementation exists.
Maybe is there really a point in writing it, but I can't see which. Maybe if you
explained what the limitations of ssss are...?

@_date: 2013-06-13 11:58:09
@_author: Leo Gaspard 
@_subject: Clarifying the GnuPG License 
IANAL either, but wonder whether hard-coding the GPG program name and arguments
in your binary would not be sufficient to consider your program as linked to the
GPG executable.
This would mean the program would be bound by the GPL terms.
But, again, this is only a supposition, and you should get proper legal advice.

@_date: 2013-03-27 22:15:04
@_author: Leo Gaspard 
@_subject: gpg for anonymous users - Alternative to the web of trust? 
Well... IMHO you did all what you had to/could do, if you want to keep
confidentiality : claiming your public key in association with your name on
several websites. Now, just hope no covert agency will try to impersonate you
until a lot of people verify and sign your public key.

@_date: 2013-11-01 00:23:13
@_author: Leo Gaspard 
@_subject: The symmetric ciphers 
Disclaimer : I am not a mathematician, only a student in mathematics. I did not
learn mathematics in the English language, but have tried to check on wikipedia
the vocabulary I am about to use is the correct in english, but please pardon me
if I make a mistake. Anything I have not checked should be redefined. And this
text is not intended for people with no insight on basic group theory.
Definitions :
 * [1;n] will be the set of all integers from 1 to n, ends included.
 * M is the set of possible messages.
 * C is the set of possible ciphertexts.
 * F(M, C) is the set of encryption functions (key included), that take a
   message in M as input, and yields a ciphertext in C as output. IOW, it is the
   set of bijections from M to C.
Assumption : F(M, C) is a group for \circ, the composition, as any encrypted
message ought to be decipherable. (Well, not really a group, as the inverse
bijection would be in F(C, M), but I will write it is a group for ease of
expression. Correcting this would only be adding useless text, so feel free to
do it in you mind if you prefer.)
First, I'll assume that, when you say "ROT is a group", you mean that
(n -> ROTn) is a group morphism between (F(M, C), \circ) and (Z/26Z, +).
Let n be a positive integer.
So, now, let's assume K = [1; 2^n] is a group for some law *. Let's assume that
AES-n is a group morphism between (F(M, C), \circ) and (K, *).
In my opinion (and a bit more than that), it changes nothing to the question.
Indeed, composing two (or more) AES-n with independantly random-chosen keys is
at least as strong as one AES-n with a random-chosen key, which, IIRC, was the
heart of thhe matter.
As a proof, let's take k1 and k2 two independantly random-chosen keys in K.
Then, AES-n(k1) \circ AES-n(k2) = AES-n(k1 * k2).
Now, let's prove k1 * k2 is a randomly-chosen key in K. First, (x -> k1 * x)
is a bijection. So, if x is chosen randomly, then so is k1 * x. And k2 is chosen
randomly (independantly from k1, which is quite important here), so k1 * k2 is a
randomly-chosen key in K.
Proof of the "first" statement :
Let a, b two keys in K. Then k1 * a = k1 * b implies a = b by mere
multiplication by k1^{-1}.
So (x -> k1 * x) is an injection from K to K, and K is a finite set, so
(x -> k1 * x) is a bijection on K.
Another way of seeing this would be by exposing the inverse : (x -> k1^{-1} * x)
I know this is a well-known result, but preferred to redemonstrate it, just in
So, whether AES-n is a group morphism or not does not matter for the question,
which was trying to find a resulting algorithm at least as strong as the
strongest of all.
And DES was checked for a group-like behavior because the objective was not to
create an algorithm at least as strong as the strongest component, but to create
an algorithm as strong as the sum of all components, which is substantially
BTW, the example about ROT still fits the proof : remember ROT0 could be
selected by a random key with probability 1/26. You can check ROTn \circ ROTm
(ie. ROT(n + m)) yields ROT0 with probability 1/26, when n and m are both chosen
uniformly. (Well, it's just 26 ways of making 26 from the sum of two numbers
from 0 to 25, this divided by the total possibilities of 26^2.)
As a conclusion, IMHO (and without proof here, just gut feeling, even though a
start of proof was given by Philipp earlier), stacking two algorithms with
unrelated randomly-chosen keys makes an algorithm at least as strong as the
strongest of the two algorithms, to the cost of transimitting a longer key and
spending more time on enc/decryption, which, admittedly, might be quite an
Hoping I did not make too much mistakes, both in mathematics and in the English

@_date: 2013-11-03 03:34:25
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
(Sorry, I once again sent the message only to you and not to the list -- I
really need to get used to mailing lists, sorry !)
 1) Checked by the other key's message. Because signed (K1) message from Alice,
    saying she has access to K2, means any UID on K2 named Alice is as right as
    the equivalent UID on K1. So the UIDs are correct.
 2) Checked by the presence of the UID. Because, to add a UID, one must have
    control of the secret key, and thus be able to decrypt / sign messages with
    it. And, as stated in (1), the UIDs are valid. So Alice, who added the UIDs,
    must have access to the secret key.
The only case I could find of (2) invalid would be if Alice herself tried to
trick you into signing a key with her name but used by Bob. Except it turns
out that she could just as well have the key for the time of the key exchange,
and then pass it to Bob.
Where am I wrong ?

@_date: 2013-11-04 23:20:12
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
Well... I still do not understand why decryption is sufficient to demonstrate
control of the private key and not adding a UID (note I'm talking about signed
UID's, not unsigned ones, of course).

@_date: 2013-11-05 18:26:07
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
OK, I think I understood your point. (That is, assertion is not as strong as
However, I think in this case (assuming there are no more UID on key 2 than on
key 1), assertions are sufficient, *because* there are two assertions, one in
both ways.
I mean :
 * Owner of Key 1 says (s)he is owner of Key 2 (through signed message saying
   you so)
 * Owner of Key 2 says (s)he is owner of Key 1 (through signed UID on Key 2)
So, except in case of collusion between owners of Keys 1 and 2, I believe there
is no way one can be wrong in signing Key 2 (of course, if Key 1 is signed).
IIUC, your point is that verification would enable one to avoid collusion, as it
is the only flaw I can see in this verification scheme.
Except collusion can not be avoided in any way, AFAIK.
If that is not your point, could you exhibit a scenario in which there is a
signed UID on Key 2, a signed statement from Key 1 owner saying he owns Key 2,
and Key 2 not being usable by Key 1 owner ? (Of course, excepting collusion,
which as stated above can not be avoided.)

@_date: 2013-11-06 23:28:35
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
(Sorry, failed again to reply to the list, so you probably have this message
twice again.)
Well, thus my reasoning (last message) allows me to prove that I can have the
same level of confidence in Key 2 than in Key 1, even though I have not done
again all the steps of verification.
Thus, signing being an attestation of the validity of the key (I assume you
meant of the confidence in the validity of the key), why should one sign Key 1
and not Key 2 ?
For the same reason, signing (and exporting signatures) based on people I
blindly trust is not an issue to me. (I know, I just released the troll.)
Because if I blindly trust these persons, I believe with absolute certainty that
the person is who (s)he says (s)he is. And so I can announce this certainty by
signing the key. (I use the term blindly to mean even more than the technical
"ultimately", as this one could be expressed using trust signatures. Just really
blindly trust, as when you would let them to decide your fate, knowing they
could be better off by sending you to hell.) Of course, if I sign the key only
because it is validated through technical means, not by hand-checking for a
signature from a blindly trusted owner, I would never sign that other key.
The fact that others could get just the same effect by twisting their WoT
parameters is not an issue to me. Firstly, because there are few trust
signatures (according to best practices I read, that said trust signatures are
mainly made for closed-system environments), so WoT rarely expands outwards of
one signature by someone you know. But mostly because signing is an attestion of
your belief someone is who (s)he is. Thus, if you believe someone is who the UID
states (s)he is as much as if you met him/her in person and followed the whole
verification process, I would not mind your exporting signatures of the key.
And saying that it allows the blindly trusted person to force you to see a key
as validated through three persons you marginally trust is meaning nothing to
me. Indeed, these three persons are all asserting they believe with certainty
that the key owner is who (s)he says (s)he is. That all used the same
information source is just commonly done.
Indeed, how do you check an identity ?
 * Name : Passport. Any government could make a passport as wanted, not even
          speaking about forgery. Thus everyone you know who signed some UID
          probably based their verification work on a single passport.
 * Comment : Depends of the comment. For "CEO company X", it is probably based
             on public archives. Them referring to a person by his/her name, any
             forged passport also means forged name.
 * Email : Probably a mere exchange of emails. Thus, anyone doing MitM could
           intercept the exchange and reply so as to make you validate the key,
           and even without MitM, the email provider could do as well.
Every time, the certainty of the UID element is heavily dependent on other's
work. Thus, why should we refuse to base our work on other's signatures ?
(*assuming* you believe in the UID validity as much as you would have done using
full verification)
I just found a "counter-example" : in case the message (signed by Key 1) telling
owner of Key 1 is owner of Key 2 is signed by a subkey, which might have been
compromised. However, I assumed such a message would only be sent signed using
the master key, as it must be totally relied upon. Thus, anyone able to forge
such a message would be able to forge any message using the master key, and
especially to add new encryption subkeys... Thus, such a scenario is not a
threat IMHO.

@_date: 2013-11-07 17:09:30
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
Well... The answer to your previous message was in my first two paragraphs. The
rest of my answer, to which you answered, was mostly thinking over some debate
that aroused earlier, and whose authors I do not remember. Anyway, I think you
answered the most important part of my last message.
Except they do not have to know X, nor that he makes perfectly reasonable
decisions in signing keys.
And I believe it's not noise. Let's make an example in the real world :
 * I would entrust X with my life
 * X would entrust Y with his life, without my knowing it
 * Thus, if I actually entrusted X with my life, why should I be frightened if X
   asked Y to take care of me ? Provided, of course, X told me he was letting Y
   take care of me. After all, I would entrust X with my life, so I should just
   agree to any act he believes is good for me.
(That's what I called blind trust. Somewhat more than full trust, I believe.)
So, is a signature a matter of belief in the validity of the key or of actual
work to verify the key ?
If I understood correctly, the depth parameter you are talking about is useless,
except in case there are trust signature. And you agreed with me for them to be
taken out of the equation.
If you do not know their key signing policy, and assign them any ownertrust,
then are you working with such sensitive stuff ?
At least, a key signing policy such as mine would be clear enough : I sign a key
when I believe it is valid as much as if I had met its owner in person.
Which is, as pointed above about trust signatures, quite irrelevant. (sorry for
being so blunt, I found no other wording)
Wrong. More verification has been done for B's identity than you would have
thought. Because you believe A is marginally reliable, while your web of trust
believes A is fully reliable : C, D and E did enough work to check A is
trustworthy, which apparently you did not do.
If you believe they were wrong in this checking of A's trustworthiness, just
don't assign them ownertrust. Sure, this would weaken the WoT, but as you
conflict on whether A is trustworthy, why would you not conflict on whether B is
who (s)he is ?
In fact, they did. Because you assigned ownertrust to C, D, and E, which you
should not have done.
BTW, if I understood the WoT correctly, if C, D and E trust-signed A with full
ownertrust (after all, you're talking about max depth, so why not?), then your
WoT would have validated B any way, as you marginally trusted C, D and E.
We do totally agree.
So, finally your meaning of signatures is no longer about key validity, but
rather about key verification ?
I still do not sort this out, sorry.
BTW, I do not know anyone I would trust enough to assign full ownertrust, let
alone re-signing keys signed by (s)he.
I did not mean to raise a topic on identity check, only to raise the issue that,
in fact, you are already relying on a single assertion for UID assessment,
whether it is the government or whatever.
However, if the government started to sign keys, would you assign it full
ownertrust ? I think that, due to NSA scandals, most would not. But they would
just be fooled into thinking they are out of the reach of the government, as
most identity checks would be based on government assertion. But you would
expect people to continue checking information based on passports, right ? So
you would implicitly condone this re-signing the key.
Now, change the word "government" with the word "person A", and you are back
with your example.

@_date: 2013-11-07 20:10:11
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
Indeed, I never thought someone would assign ownertrust without verifying the
key. Please accept my apologies.
However, I still believe that, under the condition any ownertrusted key has been
verified (which, I assumed, was commonplace, but I was apparently wrong), the
depth parameter is useless.
I'm sorry, I think I gave too much importance to your earlier statement
("Signing is to be an attestation to the validity of the key."), incorrectly
deducing from it that signatures indicates that you should sign whenever you
believe a key is correct as much as if you met in person
I think this time, you gave too much importance to some of my sentences. Or
maybe was I too bad at making myself understood.
Anyway, I meant I should sign a key whenever I believe a key to be valid as much
as if I met with the keyowner. Which, of course, does not equates with merely
believing a key is valid. Indeed, on the WoT, one is rarely sure of the quality
of signatures. (Indeed, I believe(d) full ownertrust must be quite rare., for
that same reason ; but I am probably wrong.)
And, now I know assigning ownertrust to not-personnally-checked keys is
relatively common, I know I should not sign keys based on other people's
However, to come back to the initial problem, I still believe the key change
problem (ie. owner of K1 switchs to K2) does not require re-verifying ownership
etc. (BTW, isn't this also why transition statements, like
 were written ?)
But I still wonder how one should deal with key duplication (ie. owner of K1 now
has a second key K2)...
Anyway, thanks for you detailed explanations about the WoT !

@_date: 2013-11-07 20:20:54
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
Sorry again, just noticed it actually wasn't you statement, but Paul's !
So, double mistake...

@_date: 2013-11-07 20:19:31
@_author: Leo Gaspard 
@_subject: trust your corporation for keyowner identification? 
Indeed, I thought of this case after having sent my email. Anyway, by "blind
trust", I did mean a superset of all trusts related to keysigning.
Indeed. I just backed off in my answer to Peter, by understanding why it was not
needed. However, I believe that for the initial problem (ie. key change),
information provided by a signed message accompanied from a UID on the other key
is significant enough, and moreover definite, so I would not be bothered signing
such a new key (of course, also revoking the signature on the old key).
Indeed. Thanks for your answer, clarifying once again what signatures mean ! (I
know, I'm slow to understand, but I think I'm OK no.)

@_date: 2013-11-08 00:23:29
@_author: Leo Gaspard 
@_subject: Signing keys on a low-entropy system 
(Failed again to answer to list. I really ought to replace this shortcut...)
I heard haveged is quite good at gathering entropy from anywhere it can
(processor cycles, etc.)
In theory, if /dev/random is configured to allow only random enough data to
pass, it should just mean operations would just take longer. However, I am not
absolutely sure of this -- but I know in theory /dev/random ensures some minimum
entropy, thus sometimes blocking reads.
Cheers & HTH,

@_date: 2013-11-19 23:19:29
@_author: Leo Gaspard 
@_subject: article about Air Gapped OpenPGP Key 
Especially knowing in most EU countries judges are not allowed to force you to
hand over your secret key, only to decrypt specific messages for them. (Don't
remember where I read that.)

@_date: 2013-11-20 01:21:55
@_author: Leo Gaspard 
@_subject: article about Air Gapped OpenPGP Key 
Actually, I answered the "encrypted mails" part. Thanks anyway.
Well... I can see one : the user used a plaintext storage device without
thinking about it, and then understands he needs an encrypted device and scrubs
his hard drive when the encrypted drive is set up with the necessary
Another one would be (paranoid) fear about the long long term : who knows some
three-letter agency would not steal your computer, and store its hard drive
content until decryption is available (say, 10 years from now, being quite
optimistic?). So scrubbing the already-encrypted data would help ensure data is
never recovered.
Maybe scrubbing a specific file, without need to reset files on full blocks,
block-based encryption being AFAICT the most frequent way of encrypting complete
hard drives.
That's all I can figure out.

@_date: 2013-09-08 23:00:16
@_author: Leo Gaspard 
@_subject: Recommended key size for life long key 
I have to agree on this point.
The issue is that I disagree with him on his stance : in my opinion, having a
schedule stating when the keylengths will become insecure is useless : we just
have to be able to predict that longer keys will most likely be at least as hard
to crack.
And this means that, as long as the drawbacks associated with the use of the key
are assumed by the key owner only (as the tables state, encrypt and verify
operations being almost unchanged in time), preconizing 10kbit RSA keys is no
issue, and can only improve overall security, to the key owner's usability's
detriment at most.
And each key owner might choose whatever keylength they feel best suits them,
according to their usability / security needs ; as long as these choices do not
impede other keyholders' usability or security.
BTW, the statement "[Dan Boneh] proved that breaking RSA is not equivalent to
factoring" is wrong : he did not prove that breaking RSA is easier than
factoring numbers ; only that a whole ways of proving that breaking RSA is as
hard as factoring numbers are ineffective ; thereby reducing the search space
for potential valid ways of proving the conjecture. Hence the title of the
article : "Breaking RSA *may* not be equivalent to factoring".
Please pardon me if I misunderstood the english used in the abstract.
Oh, and... Please correct me if I am mistaken, but I believe the best we can do
at the moment, even with a quantum computer is Shor's algorithm, taking a time
of O(log^3 n). Thus, going from 2k keys to 10k keys makes if approx. 125 times
harder to break. Sure, not so wonderful as what it is today, but if the constant
is large enough (which, AFAIK, we do not know yet), it might be a practical
attack infeasible for a few more years.
So, back to the initial question, I do not understand why the article should be
judged so poor. No, to me the article is not about predicting 87 years in front
of us. To me, the article is about stating that using 10k RSA keys is not as bad
as one might think at first. The gist of the article is, to me, precisely this
set of tables.
And the first part is, still in my opinion, only here to explain why 10k RSA
keys were chosen for the experiment. Explaining that, according to our current
estimates, they might potentially resist until 2100, assuming no major
breakthrough is made until then in the cryptanalysis field. You might notice
that such precautions are taken in the article too.
So... I find the article interesting. I would not have thought "everyday" use of
a 10k key would have so little drawbacks. And, finally, I want to recall that
signing keys need not be the same as certifying key, thus allowing us to lose
the signature time only for certifications, and use "normal" keys the rest of
the time ; thus getting the best of both worlds, by being able to upgrade
signing keys to stronger ones without losing one's WoT. The only remaining
drawback being when others certify our master key.

@_date: 2013-09-09 00:54:28
@_author: Leo Gaspard 
@_subject: Recommended key size for life long key 
Well... If factoring takes a month, with the factor of 125, it takes ten years.
Seems not that irrelevant to me.
Of course, this is made using completely made up numbers, as I do not at all
know how fast QC will be able to do the factoring.
Strangely enough, I would have thought 4k qubits would be quite a huge need,
thus meaning we would have overcome the major problems with decoherence.
But, again, not being a quantum physicist, I cannot be relied upon on that
You are right, we do not know whether RSA-2048 will hold or not. The only point
I was saying was that RSA-10k will quite likely (even more likely than NIST's
predictions, IMHO) resist at least as long as RSA-2048.
That said, everything is a matter of preference. The only reason I would see for
using a 10k key would be in order to encrypt or sign documents that should be
valid for years -- more years than what we are able to forecast. Because,
unfortunately (or not, depending on the viewpoint), cryptanalysis is
That said, cheers !

@_date: 2014-04-02 21:14:48
@_author: Leo Gaspard 
@_subject: Using an RSA GnuPG key for RSA ? 
If you are not to use the key in gnupg, why make gnupg generate it in the first
place? Why not use the program with which you'll use the key to generate it? Or,
if the program does not offer this functionality, why not use openssl, which
provides this capability on purpose?
Were you to use the key both for gnupg and other systems, I would understand,
but doing things this way...?

@_date: 2014-04-04 18:17:15
@_author: Leo Gaspard 
@_subject: Using an RSA GnuPG key for RSA ? 
(As you didn't answer to list, I'm not cutting. Hope you didn't mean it to be a
private message, but it clearly didn't seem like one.)
Well... I inferred it from "use it (not in GnuPG, but in other systems using RSA
keys)", from your first message.
Anyway, as Sam puts it, you'd be better not putting your RSA key everywhere.
And... You say you do not trust closed source programs for key generation, but
does that mean you trust them for key usage? Otherwise, you could just as well
throw your key to the dustbin.
What I could propose would be to :
 * Make a gpg key, master key, airgapped, etc.
 * On each system on which you mean to use cryptography, generate a keypair
   using the program with which you are going to use it (or possible openssl, if
   the program does not generate keys).
 * Sign the public key of each keypair with your gpg key. As it is not a stricto
   sensu pgp key, sign the armored key as a plaintext message, if possible with
   a preceding comment explaining what it is to be used for.
 * Publish these signatures somewhere easily found.
 * If you want so, encrypt the private key with your mainkey and store it
   somewhere safe enough (it's encrypted, after all).
This way, each keypair gets the maximum security it can have : the security of
the application using the private keypart. (Actually, if you choose to keep an
encrypted backup, you also need to keep the mainkey safe, but that's supposed as
being the most protected part of the whole setup, so...)
What do you think about it?

@_date: 2014-04-04 21:13:23
@_author: Leo Gaspard 
@_subject: Using an RSA GnuPG key for RSA ? 
Well... As this seems not documented (otherwise I guess someone else would have
answered you), I'm going to assume there is no such function available in gnupg.
So, this (and the reasons explained by Sam) explains the reason why I'm trying
to figure out what you actually want to do, in order to perhaps propose you
another solution, instead of merely answering you to write your own extractor.
So, if you forgive my bluntness... With what closed program are you trying to
interface? Why would you want to use your pgp keypair for this program, and not
a key generated for this use?

@_date: 2014-04-10 01:57:38
@_author: Leo Gaspard 
@_subject: PGP/GPG does not work easily with web-mail. 
Well... I started to write a firefox addon, but never had enough time to finish
it. Perhaps later. If anyone wishes to get what I've done (that is, a js-ctype
binding of gpgme, along with tests AFAICR), I can try to locate the source code!
However, a major issue remains the encryption of HTML documents, which is,
AFAICT, not possible today (well, not automatically at least, as of course gpg
can be used to sign html files); and besides not obviously secure: what about
white-on-white text and such? I don't doubt there are fixes for such, and most
isn't even an issue; I just remember enigmail forbids it, so I guess there are
Sorry for not helping you more,

@_date: 2014-01-03 14:12:43
@_author: Leo Gaspard 
@_subject: sign encrypted emails 
I think the need for such a fix could also be highlighted in the following
I sign the message "Got to talk tomorrow at dawn", then send it to Alice,
thinking about the cake for the birthday party, not important so not encrypting
it. Bob grabs the message, and sends it encrypted to Alice's highest security
key. Alice then thinks it is a really important message, and the matters to
discuss are really important. She takes with her the top secret files we are
working together on.  Bob, knowing the place and date of the meeting, then comes
and steals the top secret files.
So changing the encryption could break an opsec.
I'm not saying it would be useful everyday. But some use cases seem to require
it. However, I'm not saying this feature should be included by default, as a fix
would be easy (call gpg twice), and I can think of few use cases.
BTW, is a timestamp included in the signature? If not, it could lead to similar

@_date: 2014-01-04 00:56:36
@_author: Leo Gaspard 
@_subject: sign encrypted emails 
Well... So, where is the flow in my example? This example was designed so that,
depending on the level of encryption (and so the "importance" of the safety of
the message according to the sender), the message had different meanings.
Sorry, I can't see yet where I went wrong.

@_date: 2014-01-04 16:09:51
@_author: Leo Gaspard 
@_subject: sign encrypted emails 
Which is exactly the reason for which Hauke proposed to sign the encrypted
message in addition to signing the cleartext message, is it not?
Sure, there might be other ways: add a message stating to which key the message
is encrypted, etc. But this one has the advantage of requiring AFAICT no
alteration to the standard, and of being easily automated, for humans are quite
poor at remembering to always state to which key they encrypt.
Anyway, wouldn't you react differently depending on whether a message was
encrypted to your offline key or unencrypted?

@_date: 2014-01-05 03:10:48
@_author: Leo Gaspard 
@_subject: sign encrypted emails 
Well, the idea would be that the receiving program would check there *is* an
additional signature, and refuse it if not.
Nevertheless, adding a second layer of encryption would help, both in avoiding
this threat with less requirements on the receiving program, and in avoiding the
metadata-analysis and irrevocability threat. Less requirements, as the receiving
program merely has to run decrypt-and-check twice, not having to check it
actually has two levels of signature, as any absence of the second level would
be detected by a failed second check. Avoiding metadata analysis, as encrypting
the second signature forbids an attacker to grab a message and have an
undeniable proof that Alice sent an encrypted message to Bob, even without Bob's
Well... I, personally, would attach more importance (no more validity, just
importance, like in "listen to me very well" or whatever english people say to
others to get them to listen carefully) to a message signed to an offline main
key that might wait for a month than to a message sent in cleartext. For I would
assume the sender designed his message to be important enough to make me move to
my safe deposit box so as to read it.
Of course, without encryption-checking, this assumption is wrong, and this is
emphasized in one of my previous messages on this thread, with the "We got to
talk tomorrow" taking importance for the receiver that is unexpected to the
sender, thus leading to a security flaw.

@_date: 2014-01-23 21:25:26
@_author: Leo Gaspard 
@_subject: Revocation certificates [was: time delay unlock private key.] 
Actually, this is something I never understood. Why should people create a
revocation certificate and store it in a safe place, instead of backing up the
main key?
So long as the primary key is encrypted and the passphrase is strong, this
should not lead to any security danger. (Anyway, it's stored in a "safe" place.
And a revocation certificate misused is dangerous too, as it ruins a web of
And the advantages of doing so are that in case or accidental erasing of the
private key (who never accidentally deleted an important file?), it also allows
the main key to be retrieved.
The primary key allows one to create a revocation certificate, not the other way
around. So, why store a safe revocation certificate?
PS: Please, do not tell me one might have forgotten his passphrase. In this case
there is no harm in shredding the secret key and waiting for the expiration
date, answering persons emailing you encrypted files that you lost your
passphrase. Anyway, in this case, you're screwed, and a revocation certificate
would be no better -- unless it was stored unencrypted, at the risk of having it
used when you do not want it to be.

@_date: 2014-01-23 22:54:55
@_author: Leo Gaspard 
@_subject: Revocation certificates [was: time delay unlock private key.] 
Well, why not give them a copy of the encrypted key? So long as your passphrase
is strong enough (e.g. diceware for 128-bit passphrase), it should not require
to have absolute trust in the backup holder.
And if you want to account for risks of mental injury serious enough to make you
forget your passphrase, well... our threat models are not the same: in mine, my
key will expire soon enough in this case, and noone will ever be able to
compromise my secret key as noone on earth remembers the cryptographically
strong passphrase.
As you put it, this is essentially indistinguishable from a man-in-the-middle
attack, so anyone who resend the encrypted message unencrypted is not respecting
the protocol (or believe it is not important enough to be encrypted -- let's
remember a lot of people just send christmas cards without envelopes).
If anything important has to be transmitted (and the sender refuses to send it
in cleartext), the sender will most likely send the message to someone else with
physical contact with the recipient.
One might argue this protocol is non-perfect, yet it is the best one the sender
could achieve, revocation certificate or not.
Indeed. Yet absence of answer should be clear enough to let the sender
understand his message did not come to the recipient. Sure, a MitM could block
the outgoing message, but anyway the sender has no better option than to find
another way of sending his message: the recipient clearly does not receive the
In case the fact of knowing a key has been revoked is critical in time, well...
Knowledge of head traumas tend to expand quickly enough into the circles of
acquaintances. (Sure, forgetfulness is a risk. Yet, forgetting a passphrase you
type so often must be quite hard past the first few weeks.)
And, what's more, such a time-critical scenarios happen only with special needs,
which are AFAICT not usual.
Well, you lose the dead-man-switch. BTW, once your key is revoked, will it some
day be cleared out of the keyservers, or will it stay there forever?
AFAIC guess, keys with an expiration date must be purged a little while after
they expired, as there is no point in keeping them, while revoked keys must be
kept, as anyone might need to update them to retrieve the revocation
Of course, I'm only discussing the case of "normal people". There must be plenty
of cases for which a revocation certificate is really useful, yet the number of
tutorials in which I read "Store a backup of your private key and a revocation
certificate" just looks like nonsense to me: if one stores both, it should at
least be precised it should be in two different locations, as storing the two
together is completely pointless, one being able to make the second.
Of course, YMMV!

@_date: 2014-01-23 23:06:14
@_author: Leo Gaspard 
@_subject: Revocation certificates [was: time delay unlock private key.] 
Well... Diceware generates 128-bit passphrases of ten words, which is not *that*
much. Yet is can be regarded as far too much. Well... seven-word passphrase
provides 90-bit of security, and should not be so hard to remember. And
bruteforcing it should be quite long...
Sure, you would need to use really good random number generator, yet you could
use /dev/random just as well as you would have for your randomly-generated
Yet, I agree I would not send my encrypted private key. But having your divorced
spouse bruteforce 90 bit of passphrase just to annoy you... seems quite an
unreasonable threat to me. And AFAICT even well-funded-organizations are not yet
powerful enough to bruteforce a 90-bit passphrase with enough s2k iterations.

@_date: 2014-01-23 23:15:16
@_author: Leo Gaspard 
@_subject: Revocation certificates 
Oh? I thought the most common reason was test keys, and tutorials which explain
step-by-step how to make a keypair and push it on a keyserver, without telling
to put an expiration date. I, unfortunately, have myself put a few test keys on
the keyservers (whose passphrase I no longer have) without expiration date
before knowing they would never (?) be deleted, and am still remorseful about
And keys with an expiration date are someday deleted, while keys, even revoked,
without are never, are they?
BTW, revocation certificates are not produced by default either. So, why not
advise people to put an expiration date, instead of counselling them to generate
a revocation certificate?
Well, my question is then: Why not restore the key immediately (having stored it
at the place you would have stored the revocation certificate), and revoke it
These corner-case scenarios are ones I did not mean to discuss, sorry for not
having made them clear.
I'm also feeling I may have failed to make myself understood: I am not denying
the usefulness of revocation certificate, just the advice always popping out to
generate a revocation certificate in any case, without thinking of whether it
would be useful.
Cheers !

@_date: 2014-01-24 00:58:00
@_author: Leo Gaspard 
@_subject: Revocation certificates [was: time delay unlock private key.] 
Well... I don't know how you type, but someone looking at me who sees me type my
passphrase would really have to try hard to guess what passphrase I am using.
And even more to remember a seven-word sentence seen once.
BTW, I once had a fun experiment: just type an eight random chars password with
no protection at all, and asking people behind me to remember it. The password
was displayed as I typed it, and left approx. two seconds more. No one managed
to see it and remember it. A few days later, I conducted the same experiment
with the same people and the same password, and no password was successfully
guessed. Sure, the information gathered would be enough to bootstrap a
successful "bruteforce", but the experiment was a lot more easy to complete than
peeping at and remembering a seven-word password.
So, if the spouse is doing it, then marital bliss has already come to an end,
and one should have noticed it.
Yet, being unmarried, I cannot say anything about such things.
So, within that threat model, revocation certificates are useful for sure.
Assuming one's spouse would first grab the secret key and remember the
passphrase before divorce.
Thanks for making that point!

@_date: 2014-01-24 18:23:35
@_author: Leo Gaspard 
@_subject: Revocation certificates [was: time delay unlock private key.] 
Great laugh!
(of course, I meant how fast)
Well... You are right, of course. Yet this does not answer my second point: if
the spouse is spying on you to get your passphrase and remember it, then love is
already gone, and you are being subject to the usual hooker attack.
Yet I do see your point for revocation certificates here, I think.
Oh, just found another one in favor of revocation certificates: they can be
easily sent to keyservers from cybercafes without any special software
Thanks and cheers,

@_date: 2014-01-24 18:44:20
@_author: Leo Gaspard 
@_subject: Revocation certificates 
OK, thanks! (for the remainder of the message as well, just have nothing to
Guess I got my answer, with every message combined.
Thanks all!

@_date: 2014-01-28 20:13:30
@_author: Leo Gaspard 
@_subject: Non email addresses in UID 
Wouldn't it be better to publish unencrypted (and unsigned) a challenge received
encrypted? As signing unknown data should be avoided, as noone knows whether
this data won't ever have a real meaning one does not intend to mean.
Hope this message is not syntactically flawed to the point of being meaningless,

@_date: 2014-01-30 22:28:27
@_author: Leo Gaspard 
@_subject: MUA "automatically signs keys"? 
Well... To this at least I can answer. Sure, it links a key to an email address.
Yet, more often than not one knows the email address of the intended recipient
(otherwise, how would he/she send the email?). So knowing an email address is
associated to a key can be useful.
About emails reused by different persons... AFAICT most major email services
never re-issue the same email address twice. Which could be considered good
practice. If one worries about an email agency stealing the email addresses,
well... A signature on an email UID means "Yes, this key is used by the same
person as the email address". So signing it "automatically" would not conflict
with the meaning of the signature. Yet if the UID also includes a name, then it
should be signed only after appropriate verification of the owner.
Just my two cents,

@_date: 2014-07-07 23:52:25
@_author: Leo Gaspard 
@_subject: GPG's vulnerability to quantum cryptography 
Wasn't there an experiment running, one or two years ago, about trying to make
anti-electrons anti-gravitate? I don't remember of having read any result,
Weren't you the one who preached to assume the worst? It seems rather reasonable
to assume that somewhere in the future quantum cryptography (or any other kind
or huge advance in science) will break whatever cipher we are currently using...
after all, vigenere-like ciphers are almost ridiculous nowadays, while they were
once state-of-the-art.

@_date: 2014-06-08 20:12:02
@_author: Leo Gaspard 
@_subject: Trust and distrust [was: Re: Google releases beta OpenPGP code] 
Sorry to hijack this topic, but... Why would you trust the OpenPGP.js
At least, you can hold google as accountable for their actions. You cannot for
them: perhaps they do not even physically exist, and are just nameholders for a
three-letter-agency project, willingly introducing backdoors in this project.
Maybe they just fixed the bugs you reported because it made them look less
Maybe will bring us all very far away.
What's great about open source is that you do not at all have to trust the
maintainer of a project. You only have to trust the project -- and by this I
mean the fact that at least a developer will have noticed the flaw. I may even
distrust Werner, and yet use gpg -- if e.g. I trust another gnupg developer.
And even this trust is not strictly required: you can always inspect the source
code all by yourself.
Sure, this model of "trust the community" is far from perfect, heartbleed being
the latest proof of that. But it is better than "trust the maintainer", who is
always part of the community.
And what's great about google's project is that they are quite likely to be
highly audited: if anyone found a willingly placed security flaw in google's
end-to-end library, it would mean a lot of prestige.
So, even if I trusted google less than OpenPGP.js developers [and who tells us
these developers are not disguised google agents?], I would likely, after a
period during which security experts will have had their time with this new
library, trust it more than OpenPGP.js.
Despite the fact that it might have a backdoor while the other does not. Because
the opposite is even more likely.

@_date: 2014-03-13 19:48:53
@_author: Leo Gaspard 
@_subject: Multiple Subkey Pairs 
Well... If you want to have messages sent to all machines by default, you can do
this way (signing subkeys as usual) :
 * Generate high-security encryption subkey to be used only on secure machines
 * Generate low-security encryption subkey to be shared amongst all machines
(Tinkering with timestamps could avoid the need to generate subkeys in this
By default (IIRC, not sure it's part of the standard though), all messages will
be sent to the latest enc subkey, thus to all machines. Someone who wants to
send secure messages can willingly encrypt to the other enc subkey.
In case of compromise, revoke the low-sec enc subkey and generate another, and
distribute it to the uncompromised machines.
Does that fit your needs?
Cheers & HTH,

@_date: 2014-05-14 21:22:26
@_author: Leo Gaspard 
@_subject: GPG's vulnerability to brute force [WAS: Re: GPG's vulnerability to 
I unfortunately have to object to this FAQ article. (Please note I'm not using
any information beyond what Wikipedia provides -- and I may be wrong in my
undertanding of it.)
First, the Margolus-Levitin limit: "6.10^33 ops.J^{-1}.s^{-1} maximum"
So, dividing the 2^128 by 6.10^33 gives me a bit less than 57000 J.s (assuming
testing an AES key is a single operation). So, that's less than 1min for 1kJ.
Pretty affordable, I believe.
Then, Landauer's principle: "energy k T ln 2".
Again, assuming testing an AES key is a single bit flip, as k is approx.
10^{-23}, this gives an overall energy (per kelvin) of
2^128 . 10^{-23} . ln 2 J.K^{-1}, which is approx. equal to 10^16 J.K^{-1}
(overestimated, as k was underestimated).
According to Wikipedia still, the lowest temperature recorded on Earth is
10^{-10} K.
This makes for a total of 10^6 J, if the computation is done at that
According to  ; the human body
uses approx. 6MJ (ie. 6 . 10^6 J) per day.
As a consequence, the process would consume less than a day of a human body.
Granted, this is still far from possible : Here I assumed testing an AES key was
a single bit flip, and that the computation was entirely done at the coldest
temperature ever recorded in a laboratory. Anyway, the former is a not-so-huge
constant (ie. less than 10^5, I'm almost sure of that), and multiplying the
results by this constant still yields an "imaginably possible" lower bound. And
the latter already has been recorded, despite my believing no computation has
been done at that temperature, it is still possible in a foreseeable future.
So, despite bruteforcing being obviously impossible in this day and age, and
most likely impossible in the near future, it seems to me that the following
statement is exaggerated: "The results are profoundly silly: it?s enough to boil
the oceans and leave the planet as a charred, smoking ruin."
The impossibility of bruteforce, to me, lies with current physical computation
capabilities, more than with theoretical lower bounds, that are far below
current prowesses.
Hoping I didn't miscompute,

@_date: 2014-05-15 00:11:12
@_author: Leo Gaspard 
@_subject: GPG's vulnerability to brute force [WAS: Re: GPG's vulnerability 
Well... Apart from the assumption I stated just below (ie. single bit flip for
AES), I cannot begin to think about an error I might have done with this one,
apart from misunderstanding Wikipedia's statement that "The processing rate
cannot be higher than 6.10^33 operations per second per joule of energy".
(I must say I was more confident about the Margolus-Levitin limit than the
Landauer bound.)
You may have noticed that, lower in my message, I stated the factor could be
10^5 and it would not matter much. You're calling it a million, why not?
I must admit I thought I overestimated the 10^5 bit flip cost, since I thought
AES-128 was like 7 rounds of something like 5 128-bit changeovers, for approx.
5000 ops in total. But I may have mixed up my recollection of AES with another
Looking up on Wikipedia : Apparently, my evaluation of rounds and changes was
little off, but I think your weighty "rekey" operation might be the optimization
described, that involved a 4kB table -- but it might be more energy-efficient
(or even time-efficient) not to precompute the table if running through lots of
different keys.
Sorry for my ignorance, but... if you have enough time to explain me, how do you
derive this?
BTW: AFAICT, a nuclear warhead (depending on the warhead, ofc.) does not release
so much energy, it just releases it in a deadly way.
So, I stated one could bruteforce at 10^6J (overestimated). You're adding 10^16,
for a total of 10^22J.
A nuclear reactor generates approx. 1000MW [1], ie. 10^9W. This makes 10^22J
every 10^13s. So, 300000 nuclear reactors generate enough energy to bruteforce a
key in a year. There are approx. 500 nuclear reactors active on earth (assuming
those under construction in 2012 are now built).
Actually, counting total energy production (13000 Mtoe [3], so
13000.10^6.11.10^6 Wh, ie. 14.10^16 Wh, that is 5.10^20 J per year), that makes
a total of 20 years.
Counting only 5000 bit operations per rekey (see upper for details), this "only"
makes 1 year.
OK, you're right. Sounds like nothing to fear (for the time being, at least --
energy production is steadily increasing).
But I have a few objections of my own about your use of Landauer's principle:
 * You state the energy would be released (or did I misunderstand?). Wikipedia
   states it is a
   "minimum possible amount of energy required to change one bit of information"
   So no ecological catastrophe (not counting nuclear waste, CO2, etc)
 * You state it is a lower bound on the energy consumed/generated by
   bruteforcing. Having a closer look at the Wikipedia page, I just found this
   sentence: "If no information is erased, computation may in principle be
   achieved which is thermodynamically reversible, and require no release of
   heat."
I do not know anything about reversible computing. But it seems to me that an
AES-performing algorithm does not necessarily have to erase a full bit of
information on each flipped bit. Actually, IIUC, flipping a bit is a
reversible operation, and so the landauer principle does not apply. And AES
being a 1-to-1 mapping (as the data to be decrypted is fixed, it is a 128-bit
keyblock -- to -- 128-bit cleartext mapping), it seems to me that AES is
theoretically a reversible program (even though our current implementations
might not be, as they are 2-to-1 mappings, not knowing the cleartext).
So it might be that Landauer's principle just does not apply to AES-128
Please note I'm not saying bruteforce is, or will ever be, technically feasible.
I'm just saying your estimate might be a bit too much. (Not saying the FAQ
article should be updated either, it makes a wonderful argument to bash people
relentlessly talking about bruteforcing.)
And, message to everyone reading: Please stop saying "Anyway I'll be dead by
then". According to Ray K. (I'm not going to say what I think about him), we are
all going to be immortal by 2050 (IIRC). So this is also your problem! (Or you'd
better die quick.)
Hoping it wasn't such a boring message, as it was already long enough,
[1] [2] [3]

@_date: 2014-05-17 01:12:09
@_author: Leo Gaspard 
@_subject: GPG's vulnerability to brute force [WAS: Re: GPG's vulnerability 
First: I agree with everything skipped in the quotes.
Well... A nuclear reactor produces 1GW, and thus produces 1PJ in 10^6 s, that is
approx. 11 days 14 hrs. Sure, you may be very interested in Health & Safety
compliance of nuclear reactors, but...
Well... Currently, at a French equivalent of undergrad level (CPGE), we're
learning entropy is a theoretical quantity, that has no real-world meaning --
thus not creating heat. Actually, its unit (J.K^{-1}) does seem to validate this
interpretation: contrarily to e.g. enthalpy, it's not an energy. Perhaps are we
oversimplifying, or perhaps did I completely misunderstand the teachers, but if
this is true there is no heat release. OTOH there would be heat absorption
through the need to move the entropy out of the system -- provided AES is not
reversible (see below for my case against that point).
Well... If the operation the bit just underwent was a bitflip (and, knowing the
bruteforcing circuit, it's possible to know that), the bit was a '0'.
I believe I must have misunderstood your challenge! (Or, just coming to my mind:
maybe was I unclear: when saying bitflip I did not mean setting a bit, but
rather setting its value as 1 - old value.)
I do not state that physically our processors are reversible. I do not even
state any processors might ever be, or adiabatic computers might ever exist.
I just state the theoretical application going from the set of 128-bit keys to
the set of 128-bit cleartexts (with the 128-bit ciphertext fixed) is a bijection
(or so I hope -- unless many keys produce the same ciphertext from the same
cleartext, which would be an attack on AES and ease bruteforce naturally).
As a consequence, I cannot see where a bit of information was lost, and thus
where Landauer's bound is supposed to apply. But maybe am I the one lost here!
Thanks for your previous and hopefully future answers,

@_date: 2014-05-25 12:56:33
@_author: Leo Gaspard 
@_subject: GPG's vulnerability to brute force 
Just for the record: I do not feel like I ever objected to a scientific theory
on the basis I did not study it properly. I merely object*ed* to Robert's
interpretation of them, stating that my objections might be invalid due to my
incomplete study of the underlying theories (which turned out to be the case).
Thanks for the discussion,
