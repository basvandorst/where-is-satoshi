
@_date: 2018-05-22 13:58:16
@_author: Konstantin Boyandin 
@_subject: Relocating pubring.kbx in gpg.conf 
GnuPG: 2.2.7 (built from sources), OS: Ubuntu 16.04.4 (64-bit).
Problem: file pubring.kbx is by default created in GnuPG default config directory. If some other files I can efficiently relocate in gpg.conf, i.e. by using something like
primary-keyring ~/mounted/gnupg/pubring.gpg
secret-keyring ~/mounted/gnupg/secring.gpg
trustdb-name ~/mounted/gnupg/trustdb.gpg
keyring ~/mounted/gnupg/pubring.gpg
but I see no obvious directives to relocate pubring.kbx - my only option, so it seems, remains relocating the entire configuration Is there a less total way to relocate pubring.kbx ?

@_date: 2018-05-29 10:39:29
@_author: Konstantin Boyandin 
@_subject: Installing a new version of GnuPG 
This (doing 'make' and 'make install') is what I did from time to time
on Ubuntu (currently 16.04). I used default path/prefixes, and after
having installed it I rename GnuPG executable /usr/local/bin/gpg to
Since /usr/local/bin precedes other default PATH components, gpg2
invokes the manually built GnuPG 2.2.*. That allows me to keep the
gnupg2 package (it installs /usr/bin/gpg2; several other packages I use
depend on gnupg2), while actually using the manually built one. That
also prevents apt and certain other programs using /usr/bin/gpg from
breaking (when they import a key, for example).
So far, no problems with the above approach.

@_date: 2019-07-03 10:28:33
@_author: Konstantin Boyandin 
@_subject: SKS and GnuPG related issues and possible workarounds 
Hello All,
After having read the recent multitude of messages related to SKS keyservers related issue, I figured out that
a. The entire SKS keyservers design and interaction has a fundamental design flaw named "unlimited resources assumption". I.e., it is assumed every server, every client has unlimited resources (to store signatures; to retrieve and process them - unlimited RAM/storage, unlimited network b. It is only a matter of time when other certificates are under attack. When the ones used by, say, Linux distributions to sign packages are affected, that will cause another wave of chaos. So the only valid option is to stop using current (the one written in OCaml) keyserver implementation and return to stone-age practice of manually sending c. More or less valid approach would be to
- when exchanging with keyserver, only load/transmit signatures for certificates in the user's keyring. To withstand traffic and other resources waste, client should pass known certificates footprints first, to only get from a keyserver the relevant signature
- implement local black/white lists feature - to able to filter out the signatures while processing them
d. The above, or any other approach is hard to implement in foreseeable future, even harder to make a de facto standard.
Personally, I am mostly concerned with b. at the moment. And if approach "data came, data stay" remains in effect for keyservers, they will merrily be flooded with junk certificates/signatures. I can see no easy means to prevent that, without wasting resources of human users.
Am I wrong - perhaps there are brighter alternatives?
Konstantin Boyandin

@_date: 2019-07-05 20:45:59
@_author: Konstantin Boyandin 
@_subject: SKS and GnuPG related issues and possible workarounds 
Thanks to everyone who expressed their opinions (I read the thread, even
if I don't reply often). I didn't expect the discussion would become so
red-hot, however.
There's a Russian saying with closest English translation "two movings
are as devastating as one fire". I assume that transition to the
announced GnuPG pre-release, as well as gradual switching to the new
keyserver(s) may reduce the number of movings.
As for data staying forever in keyservers records - I assume no amount
of GDPRs may force the Internet to completely forget a piece of
information (while it still can be wiped from most public places).
Whereas archive.org can remove any site/page from their history (they
explain the procedure upon request), Archive Team and many others
definitely won't, and there's no magic to remove the piece of
information from everywhere. So it's just a fact, anyone can post
anything that will stay for long.
ATM, none of systems I use GnuPG in has been hit with the signature
flood disaster. If I might miss that point - is it possible to get,
somehow, the list of flooded keys IDs (if anyone keeps the stats)?
Konstantin Boyandin

@_date: 2019-07-06 23:35:01
@_author: Konstantin Boyandin 
@_subject: SKS and GnuPG related issues and possible workarounds 
Since the list archives can be read by anyone, it's obvious the possible wrongdoers are monitoring the list closely, and can take any and all suggestions into account, to adapt further goals.
Konstantin Boyandin
Ryan McGinnis via Gnupg-users wrote 2019-07-06 18:50:

@_date: 2019-07-07 07:22:02
@_author: Konstantin Boyandin 
@_subject: robots.txt and archiveteam.org... 
I believe this subject is way off the mailing list, but just my 5 cents.
1. GDPR, as any other bloated, convoluted, written in inhuman juridical language law, mostly benefits two kinds of people: lawyers and government-related officials. It incurs a lot of ado and expenses, gives vast grounds for power abuse and so on and so forth.
As a side effect, it somewhat helps ordinary people to control the usage of their personal data. Since data lifespan on the Net is hardly controllable in whole, the abuse potential of GDPR is limitless. Cheer the politicians for this excellent masterpiece of legislation.
As many such laws (the closest example of similarly inadequate law is Russian Federal Law  "On personal data") are introduced worldwide, they will strike a lethal blow to majority of small and medium businesses, and cripple the base of normal human communication.
Let just watch the process and enjoy the show.
2. The "Robot Exclusion Protocol", as it's defined in its text, is advisory only. It is not mandatory for any kind of data transmission. Thus any claims or demands about following its statements are void. You may ask, not to demand.
Any entity trying to transmit data over Net can't be reliably *and* efficiently identified as human being (or a bot). Thus, it's quite easy to imitate bot/human being, which makes the robots.txt a lame excuse for lack of efficient control over which data should be taken by which Simply stating, if you don't want your digital crap being available to anyone, don't make it publicly available.
robots.txt usage was weird and strange in many cases. I remember several WordPress versions which silently changed, when installed, robots.txt to disable all page indexing. Also, you cannot magically demand to remove downloaded and stored locally data just by altering your robots.txt at will. That's pure nonsense.
Although I do not like, in many cases, the wording Archive Teams uses, in this given case I think they are, generally, right.
Konstantin Boyandin
Listo Factor via Gnupg-users wrote 2019-07-06 19:06:

@_date: 2019-03-10 01:25:41
@_author: Konstantin Boyandin 
@_subject: Several GnuPG instances, with their corresponding agents 
I am currently using Ubuntu 18.04; the "built-in", provided from
standard repository, GnuPG version is 2.2.4 (it can't be simply
removed/replaced by manually built version).
I would like to use, whenever I like, manually builds (such as current
Question: how do I keep several GnuPG versions installed, every version
with its own gpg-agent?

@_date: 2019-03-11 11:55:12
@_author: Konstantin Boyandin 
@_subject: Several GnuPG instances, with their corresponding agents 
Hi Damien,
Thanks for the pieces of advice, I'll try that shortly.
I'd  also like to share the same keys among all 2.2.* versions - I'll
check how to do that with as little ado as possible.

@_date: 2019-03-12 04:57:36
@_author: Konstantin Boyandin (lists 
@_subject: Several GnuPG instances, with their corresponding agents 
Thanks for the pieces of advice. I conclude that the only safe way to share same keys is to re-import all the keys manually into every corresponding GnuPG version's key ring.
To me, there's nothing wrong in using different versions of GnuPG under the same account: system-wide applications using the OS-provided version, and in separated environment I can run newer version, if I need its specific features. As soon as they have everything separated, agents sockets included, I see no possible problems.
