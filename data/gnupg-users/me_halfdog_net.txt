
@_date: 2019-12-16 15:14:36
@_author: halfdog 
@_subject: Usability of OpenSSL vs GNUPG 
I cannot speak for anyone else but from my experience the GnuPG
community ecosystem's way of dealing with issues is way more
conflict prone than how OpenSSL handles this.
One side of that conflict is how GnuPG treats the command line
"API". While my written OpenSSL instructions, automation scripts
around key generation, signing, de/encryption required (minor)
modification due to OpenSSL software changes in the last years,
GnuPG updates from Debian quite often require quite intruding
changes to written manual instructions or automated scripts.
To nail it down, here is the list of changes as an example and
the time to fix (estimate from memory):
* Generate dh-params during deployment and always use them on
  connection (15 minutes)
* Force SSL to drop older TLS versions on internal connections
  (5 minutes)
* Initrd restructuring due to gnupg-agent use (4h)
* Secure forced termination of gnupg-agent for single-use
  private key decryption (4h)
* Update, testing of numerous procedures after introduction
  of "pin-entry-mode" (4h)
* Decoding of PGP private key format using RFC to resurrect
  old private keys after suddenly not being useable any more
  after a minor update (1 day)
* Workaround for secure GnuPG passphrase handling after gnupg
  started silently ignoring the command line parameters to have
  a stronger passphrase bruteforce security by integrating
  GnuPG with argon2. (4h)
As I have the feeling I use OpenSSL and GnuPG in a similar number
of procedures, the much huger amount of time required to keep
GnuPG operational and secure does not make me cheerful about
command line/operating system API stability.
As mentioned above, this is only one side of that conflict. The
other one is, that when such problems occur, which cannot be
avoided even with best software, then documentation or online
resources are usually not very helpful. Thus the community contact
is sought, but to my opinion replies are often not acknowledging
the problem (not accepting that someone might have used GnuPG
for such procedures before the change and requires to find a
working solution to keep production running), empathic (a reply
like: "sorry, we understand this is annoying for you, but this
change was technically required ...), not increasing community
knowlege to build a better GnuPG community (the change was needed
for use-case [reference], thus requirements [reference] contratict
your use case. See [doc-url] for considerations, workarounds ...").
So for example following following sequence or events represents
quite well the usual experience I perceive dealing with GnuPG
software issues:
* Discovering accidentially that GnuPG private key passphrase
  bruteforce somehow was reduced to 1/100 strength with new keys.
* Searching the documentation, internet, not finding anything
  of relevance to this issue.
* Asking the GnuPG security contact finding out a) the parameter
  for better bruteforce protection is silently ignored in GnuPG2
  passphrase symmetric key generation b) there is no problem
  GnuPG downgrading security on its own without any warning
  by ignoring the parameter, c) the documention is outdated/ambiguous
  and does not mention the change, c) why do you want to have
  a strong bruteforce protection, the reduced value is way better
  for good user experience with gnupg-agent? and d) gnupg-agent
  will manage (calibrate) that parameter for optimized user
  experience automatically to 1/100, no need to mingle with that.
* Me not being happy to reduce bruteforce protection to something
  used 10 years ago, thus integrating GnuPG with argon2 to have
  appropriate protection again.
Having such experiences more than once reduced trust and sympathy
for GnuPG, thus also willingness to contribute to testing or
development. But maybe just my expectations of GnuPG as open
source software are wrong and my limited communication skills
do not allow me to sort it out in a more positive manner.

@_date: 2019-12-18 12:12:15
@_author: halfdog 
@_subject: Usability of OpenSSL vs GNUPG 
Yes, I fell this is exactly the cause. But what adds to that
frustration is, that e.g. offering to participate in defining
use cases, requirements, scope of various components seemed not
the way the project would like to go forward, maybe as this would
surface the conflicts in a short run (while avoiding them in
long run).
I can second nearly everything you wrote, but here I have a different
Changing the value might be good for gnupg as end-user tool and
therefore I completely understand, that a change in the default
is positive to most users and hence the project itself. It also
increases the security of the average user I guess, as most likely
they did not configure stronger KDF settings on their own.
I also understand, that such changes being fruitful for many
users (the main use cases) might require others with different
usecases to adopt to the new standard.
What I do not get is, why such design changes are applied in
a non-defensive manner, thus making the end users search for
the changes instead of clearly defining the new API, e.g. by
failing with exit(1). I would hope for that to be a standard
in security-aware software instead silently ignoring a parameter
that once before contributed major boost in protection.
By saying that, I hope I could make clear, that I am not against
having an agile approach to moving open source software forward,
when it is also causing some breakage sometimes. At least gnupg
should not be come the new MS with eternal backward compatibility
in some domains :-)
What I would wish for (and would offer to participate in creating
it) is some formalized, written down quality optimizing procedures
to drive development with such a life-cycle-model in mind for
API, features and deprecation. Maybe some of these already exists,
otherwise it might make sense to define it?
* Write down current usecases, requirements we are aiming to
  fulfill at the moment. This makes it more transparent what
  gnupg is at the moment and was it wants to be in the future.
  By formalizing this process, different opinions are easier
  to integreate at design phase thus avoiding troubles after
  implementtion. I believe this could also reduce frustration
  for gnupg-devs as changes are less disputed (usually only
  those opposing are the ones being loud, thus reducing the
  feel-good-factor of their great work).
* Rules for deprecation of features settings some rules for
  backward compatibility but also removal of features. One thing
  I could envision is something like a "--quality" mode switch
  with GnuPG. Using this, the software will hard-fail if any
  function marked for deprecation (--s2k-count, weak ciphers,
  storage formats, ...) is used. While this would still allow
  great flexibility for the average user (not depending on
  special API or cryptographic properties) while the others
  may also run with "--quality" in their continuous integration
  environment, thus catching update frictions before those reach
  production.
* Define a user feedback process to get a better insight and
  real numbers on their usecases (could be as something like
  having a lime survey open plus a defined feature request/
  positive criticism process) to have real numbers, hard evidence
  where end users are moving to. This can then be combined
  with the strategy, where we want to move them to, e.g. to
  get mail-encryption more common.

@_date: 2019-10-08 18:02:07
@_author: halfdog 
@_subject: We have GOT TO make things simpler 
In my opinion this argument has some similarity to arguments brought
up years ago when safety belt use for car driving was made mandatory
by law. Before that the individual driver deemed the safety belt
just an unneccessary obstacle when getting in and out of the car.
Also using it has no benefits for him as he believed to be a low-risk,
careful driver not crashing anyway.
On the other side on whole-society level a noticable loss of workforce,
tragedies was statistically measured, that could be prevented by
belt use.  As with encryption software, even "fool-proof" and easy-to-use
safety belts did not change behaviour, there had to be incentives
in place to trigger adoption ... The main "incentive" introduced
in the end was to be able to use the whole road network without
being annoyed by police asking you for money when you use it.
Therefore the belt-use rate increased quickly ...
So to put that to mail encryption, maybe use this tech-fiction
mind experiment: let's assume, there would be an SMTP response
code to "RCPT: " saying something like
"550 Address rejected, unencrypted message storage not safe, use key [id]".
The only thing the sending SMTP would then need to do is to check,
if the message was already encrypted, if not encrypt it with
the given key, then continue with the secure recipient
call "SRCPT: ". The receiving SMTP would
not even need to check if the transmitted message is then really
encrypted, just a well-behaved sender would not maliciously
declare unencrypted data as encrypted.
Why would that be an incentive to get own keys? Because e.g.
your bank, your tax administration, your doctor, your lawer would
refuse to accept unencrypted messages (or to respond to them)
when they deem associated risks of data leakage too high, e.g.
by violating GDPR. So if you as client want to use mail transport
also for these purposes instead of showing up in the office or
installing tons of specialized apps for specifically communicating
with one partner, users would start registering keys, because
they get a benefit from it. As the average dude does not operate
his own SMTP servers, the major mail providers are somehow forced
to provide this functionality with server-stored keys. Still anyone
having motivation to take things further can do local decryption,
even use hardware security modules to avoid key theft.
So in the end safety belt for every one, super-high-quality safety
belts for those, who deem their risks for crashes above average.
I hope I managed to make my point clear. Please do not be picky
if the hypothetical SMTP extension would be the best lever to
provide that incentive for encryption adoption, maybe there are
better ones (or none).
Still I would be interested if my argument seems correct or if
someone can point out serious flaws in it.

@_date: 2019-09-17 06:51:11
@_author: halfdog 
@_subject: Regenerate Openpgp Public Key from Private Key 
Hello list,
Regenerating private keys is mathematically trivial but tool-wise
a little tricky. It seems that quite some people were troubled
by this problem due to different reasons (I not attempted to
confirm all of these):
* Using (old) backups of keys for decrypting with only private
  key available.
* Smartcards with only private key on them
* Forensic scenarios
Since a Gnupg update the tool will refuse to perform decryption
with private keys only. As gpg seems to provide no easy option
to regenerate/calculate the public key from the private keys
or ignore the missing keys, I used the method described in [0]
as a hacky way to regenerate 4096 bit public keys from private
keys using peculiarities of the Openpgp storage format and minimal
binary editing.
I needed this to decrypt some old data, maybe someone else might
find it useful too.
[0]

@_date: 2019-09-17 11:09:58
@_author: halfdog 
@_subject: Regenerate Openpgp Public Key from Private Key 
That it won't work in some circumstances, e.g. those cited the
line below those you have quoted (fixing the wrong private/public
you got obviously right anyway drafting your reply):
"""Regenerating public keys is mathematically trivial but tool-wise
a little tricky. It seems that quite some people were troubled
by this problem due to different reasons (I not attempted to
confirm all of these):
* Using (old) backups of keys for decrypting with only private
  key available.
* Smartcards with only private key on them
* Forensic scenarios
Therefore some exports (or copies of old secring.gpg) just do
no include the public key, otherwise import would be trivial.
Usually problem reports of other users look like [0] and do not
contain any direct solution, only workarounds e.g. "get your
missing public key from somewhere else".
As the key causing me problems was very old, I do not have the
software at hand that was used to create it, nor it is clear
if I only stored away the secring or an explicit private key
export, therefore I cannot find out what exactly caused the
situation, just that for me as for many others import or decrypt
does not work any more.
I believe that decryption worked with older gpg1 versions and
this kind of key data but I do not remember when and the gpg1
software version used back then.
[0]

@_date: 2019-09-17 12:31:14
@_author: halfdog 
@_subject: Regenerate Openpgp Public Key from Private Key 
There must have been some easy/likely pathes to reach such a
state regarding the number people searching for such a solution,
e.g. checking only 'gpg "secret key without public key"', which
is only one possible error message in such an incomplete-private-key
One cause seems loss of pubring.gpg (or zeroing out, not replaying
it from backup, ...) as documented in [0].
Others might be related only to the missing user_id or sig packets,
maybe because these expired during the whole timeline.
At least my problematic keys were already v4 ones, I cannot say
for sure for similar problem reports on the net, they might have
used v3s.
As the problem might be related to long-time compatibility of
gpg, most of the reports and possible solutions are quite old
and may affect outdated software.
As it seems quite important for some users to decrypt old data
also years after creating it and that seems to fail sometimes,
the probability to have an outdated version in the whole scenario
is nearly 100%. Hence I did not spend much time to figure out
how the problem happened over the years, just took it as granted
and tried to find an easy fix - not recreating all the frames
with some Python opengpg framework as one poster suggested.
[0]

@_date: 2020-06-01 12:34:00
@_author: halfdog 
@_subject: Certified OpenPGP-encryption after release of Thunderbird 78 
Just out of curiosity, but knowing that this is not relevant
to standard users.
As encrypted mails cannot easily be malware scanned and even
if they were might contain really hard-to-detect social engineering
attacks, therefore systems running mail software are at a higher.
Hence to avoid full system compromise, running mail software
in virtual machines. With Enigmail I used some simple tool [0]
to act instead of gnupg, intercept all calls to forward them
over network and then filter all requests via whitelists before
passing the real requests to gnupg. Thus no private keys were
available on the risky desktop system (same as with smartcards), the
desktop system had never full access to the private key (each
whitelisted sign/encrypt operation had also to be reviewed and
confirmed outside the virtual machine) and thus even full system
compromise on root level would not compromise the keys the same
way as a directly attached smart-card could be (pin stolen on
desktop system or card used by Mallory while being unlocked).
With smartcard support fully built into TB, which method for
external filtering would you deem most appropriate? Have a custom
virtual-smartcard library, that forwards the requests over network?
Have a virtual-smartcard reader device attached to the virtual
machine, that intercepts requests and forwards them to a real
smartcard reader?
[0]  (outdated!)

@_date: 2020-05-27 20:42:45
@_author: halfdog 
@_subject: gpgsplit/pgpdump replacement 
Hello list,
I just noticed that gpgv2 packaged for Debian does not include
the "gpgsplit" and "pgpdump" tools any more.
Is there any replacement available for them, e.g. by option to
main "gpg" binary?

@_date: 2020-05-28 04:20:28
@_author: halfdog 
@_subject: gpgsplit/pgpdump replacement 
Hello Stefan,
Thanks for your helpful reply!
Oh, interesting. This might have been an inconsistency in my
standard procedures as "pgpdump" still seems to be available
as a package on Debian Bullseye, but was not installed by the
install automation system on the target device in question.
I updated the configuration to have it installed during initial
Oh, seems very nice, but Debian Bullseye is packaging only the
verifier at the moment. Packaging is quite useful when wanting
it preinstalled and updated automatically on multiple devices.
Do you know if there are plans to get it into Debian Bullseye
or at least run it via own deb repositories?
As you seem to know about both gnupg and Sequoia, may I ask your
opinion if it is possible to implement following use case with
one of both?
There are multiple devices in the field, which cannot efficiently
be protected against physical access. These devices contain a
number of quite large (MBs to GBs) encrypted files with historic
data, which are usually not needed during normal operation. Therefore
the decryption key is not available on the device.
When something fails, this data might need to be reprocessed.
Due to the expensive/slow mobile link it would be painful to
transfer the encrypted file to the centralized server to decrypt
them here and send back the decrypted data over the slow link.
On the other side agent forwarding would keep your private key
available to a low-security remote device for a long time until
all the files are decrypted and does not give you any realistic
control, what is done with your private key when operating on
a larger batch of files (some thousand key operations which would
be error prone/slow to review and acknowledge all one by one).
The old procedure therefore was:
1) Use pgpsplit remotely to split the first KB of each encrypted
file to get the encryption header packet.
2) Transfer all those packets over the slow link.
3) Perform some packet check with pgpdump and then extract the
session keys from all those packets locally.
4) Send back a text file with all session keys.
5) Slowly decrypt/decompress/process all files remotely using
the session keys.
6) Destroy the session keys on the remote tmpfs by overwriting
the memory pages.
As you can see the procedure did not require any additional
software on local/remote while the functions were still available
with gpg and allows you to make sure, that you only decrypted
exactly the number of session keys you expected from the remote
side. It prevented misuse of you key for signing. With remote
device compromised it did not protect against decrypting other
data from that device (e.g. historic files already deleted and
being reinjected by an attacker) or files from another device
sharing the same public key for encryption. Later could be easily
prevented by using a per-device encryption key. As the two remote
attack scenarios were deemed very unlikely and low-impact, there
is no protection in place.
 Could it be a nice feature for future of PGP (if
cryptographically unproblematic) to have some kind of manipulation
proof "tag" in the encrypted session-key packet, that can be set,
e.g. via
--tag-encrypted-data "mytag-$(echo "some-salt $(hostname -f)" | sha256)"
to identify encrypted data source and avoid decrypting injected
session key packets. Using a signing key per source seems to be
impractical here too as it would also require to transfer the
whole file beforehand for signature verification.

@_date: 2020-05-28 21:39:50
@_author: halfdog 
@_subject: gpgsplit/pgpdump replacement 
You are right. The reason for the gpgsplit/transfer/decrypt scheme
was mainly because increase of data volume made the initial design
with full data transfer problematic. I should have moved to intermediate
key design back then already.
I think I will change the encryption of new data according to
your suggestings and keep some old gpgv1 instance while there
is still some old data around using the old encryption scheme.
It would be even possible to "upgrade" the old data by extracting
the session keys and reencrypting them but I do not think this
would be worth it.
PS: good point thinking about the randomness needed. That should
be considered in general but in this specific use case (due to
the IO activity for the data to be encrypted) the software RNG
should gather sufficient entropy between invocations once per
