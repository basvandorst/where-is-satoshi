
@_date: 2011-08-12 19:27:50
@_author: brian m. carlson 
@_subject: Implementation question: validating left two of signatures 
I have a quality-of-implementation question (more in general than
specifically about GnuPG).  I am writing an implementation of OpenPGP
that verifies signatures, among other things.
Signatures contain the left two bytes of the hash as a quick check.
I've noticed that a small number of signatures are in fact valid even
though this quick check does not match the hash.  Is it considered
acceptable to fix up this value if it is wrong?  If not, is it
acceptable to treat two signatures as the same signature if they are
identical but for the left two?  Does GnuPG (or any other
implementation) actually give any credence to the left two whatsoever?
If there's an OpenPGP implementers' list or another, more appropriate
forum, please feel free to point me in that direction.  I couldn't find
one, so I posted here.

@_date: 2011-08-26 22:46:36
@_author: brian m. carlson 
@_subject: Multiple Keyrings WAS Signing multiple keys 
There is a lot of infrastructure in Debian that depends on the ability
to have read-only keyrings using a command-line option.  If that
functionality were to disappear, somebody would patch it in because the
breakage would be too great (and needless).  If an additional option
were required to use multiple keyrings, I would submit a patch to make
it the default because otherwise it would break existing functionality.
Besides the several different programs that handle key signing parties,
dpkg-source would lose the ability to verify packages before unpacking
them.  apt's archive verification would break.  That doesn't include
dak, the Debian Archive Kit, which also uses GnuPG and would also break.
I expect that most GNU/Linux distributions would also use those patches
for the same reasons.  Removing the capability from GnuPG would not have
the effect of removing the functionality, but only on shifting the
maintenance burden.
Creating a separate directory and populating it seems silly and
wasteful, plus it prevents the storage of multiple, separate keyrings in
one directory (like /usr/share/keyrings).  If you would like to use the
--homedir method, nothing is preventing you from doing that.  But
breaking existing infrastructure will go over like a lead balloon.

@_date: 2011-08-30 02:39:04
@_author: brian m. carlson 
@_subject: Manually compute key fingerprint 
The fingerprint is a hash of certain data in the public key packet, not
the entire file itself.  This makes sense if you think about it, because
the file containing the public key also contains user IDs, signatures,
and potentially subkeys.  If you were to just hash the file, then the
fingerprint would change every time you added a new ID or signature,
which would not be hhelpful.
If you need to be able to compute the fingerprint independently, you'll
need to parse the public key packet and follow the formula specified in
RFC 4880.  It's not terribly difficult.

@_date: 2011-12-17 22:54:41
@_author: brian m. carlson 
@_subject: Bad Signatures when using check-sigs 
It means that one of the following things is true:
* The key alleged to have made the signature did not make the signature.
* The data on which the signature was made is different than the
  original data.
* Someone made an error in the OpenPGP implementation.

@_date: 2011-12-28 01:36:56
@_author: brian m. carlson 
@_subject: maximum passphrase for symmetric encryption ? 
Not to my knowledge.  OpenPGP does not specify a maximum string length
for a passphrase, and it's limited only to the amount of memory you
have, or in some cases 2^61 bytes (which is essentially unlimited).
Yes.  When you use a passphrase to encrypt, the entire passphrase is
hashed, so if the input is at all different, the passphrase will be
rejected.  Most modern OpenPGP implementations repeatedly hash the
passphrase and use salt (8 bytes of random data stored with the
passphrase to make the hash unique even if you reuse the passphrase).
This makes brute-force attempts slower since more computation is

@_date: 2011-10-01 16:51:25
@_author: brian m. carlson 
@_subject: kernel.org status: establishing a PGP web of trust 
I have an RSA key with RSA subkeys, but now that larger DSA keys are
generally available, I'd be okay with revolving DSA signing subkeys.  As
you've pointed out, DSA has the disadvantage that k must always be
different, but it also has advantages, one of them being that p, q, and
g can be shared among a group of people such that p and q can be
*proven* to be prime and generated in a reproducible way.  Another one
is that DSA signatures are smaller: there are two MPIs stored for each
signature, but those MPIs are at most 256 bits long each, while for an
RSA signature that was only 512 bits long, the security would be
woefully inadequate.
Point being, both DSA and RSA have their good and bad points, and if
you're fairly confident that you have a good PRNG, such as /dev/urandom,
then there's not really much concern about k.  After all, you also need
a good PRNG for CFB IVs as well, although the consequences aren't as

@_date: 2012-04-13 00:13:58
@_author: brian m. carlson 
@_subject: [new-user] question 
The sender can sign the message to verify that it came from him or her.
If someone just sends you an unsigned encrypted message, there is no way
to verify that I came from who you think it did.

@_date: 2012-04-29 00:28:28
@_author: brian m. carlson 
@_subject: fingerprint 
That's the way the key ID is derived for v4 keys.  v4 keys use the low
64 bits (or 32 bits for short key IDs) as the key ID.  v3 keys used the
low 64 bits (respectively 32 bits) of the RSA modulus.  However, this
posed two problems.  One is that the low bit is always one (multiplying
two large primes together does that).  The other is that originally v4
keys were all DSA or Elgamal.  Those algorithms don't have a modulus in
the same way[0], so a different technique had to be used to derive a
unique fingerprint.
[0] Basically, the one (for Elgamal) or two (for DSA) primes that are
use as moduli can be shared securely among many keys, so using them as
the sole basis for a key ID means arbitrarily many keys can have the
same key ID, which kinda defeats the purpose.

@_date: 2012-02-01 01:20:44
@_author: brian m. carlson 
@_subject: [META] please start To: with gnupg-users@gnupg.org, i.e.: 
Because that means that instead of using one procmail rule to autosort
all mailing lists I have to write one for every list I might subscribe
to.  This is error-prone and defeats the purpose of using a generic tool
to do repetitive tasks easily.  Most mailing lists have a List-ID header
for this purpose.  Majordomo lists use a different convention which is
also easily sorted on.
Also, when I'm subscribed to a mailing list, I expect people to post
their replies to the list unless there's a personal reply that is not
appropriate for the list.  For lists that require subscriptions, that
means that it's guaranteed that everybody will get a copy, which is the
point of a mailing list.  Why intentionally send me an extra?  Who wants
two copies of an email?

@_date: 2012-02-02 21:53:00
@_author: brian m. carlson 
@_subject: Using the not-dash-escaped option 
I think what Werner is saying is to use quoted-printable encoding; then,
the space will be represented as =20 (when encoded) and it will be less
likely to get eaten by hungry mail-handling tools.

@_date: 2012-01-22 18:54:22
@_author: brian m. carlson 
@_subject: RSA padding scheme 
To use them correctly and securely, yes.
GnuPG uses PKCS  v1.5.  This is specified in RFC 4880.
You cannot choose a different padding scheme and remain in compliance
with the OpenPGP standard.
If the standard allowed different padding schemes, then all
implementations would have to support multiple padding schemes, which
would be burdensome without providing significantly more security.

@_date: 2012-01-23 00:47:03
@_author: brian m. carlson 
@_subject: RSA padding scheme 
This depends on a small message.  All secure padding schemes avoid this
problem because the pad the message so it is not small.
All secure padding schemes provide this, as well.
This is not a problem with OpenPGP because the attacker never gets to
see the value encrypted with RSA because it's the symmetric key.
The existence of PGP predates the invention of OAEP by at least three
years.  So it really wasn't an option, and PKCS  v1.5 is not insecure,
so there's no reason to break backwards compatibility.
Basically.  The issue is that if the padding is incorrect, the message
is rejected.  So the attacker can't manipulate the message without
risking corrupting the structure of the method.

@_date: 2012-01-23 16:52:17
@_author: brian m. carlson 
@_subject: 1024 key with 2048 subkey: how affected? 
Because it's also used to sign other people's keys.  Using a very large
key (for 256-bit equivalence, ~15kbits) makes verification so slow as to
be unusable.  You have to not only verify signatures on other keys but
also the signatures on the subkeys.  This is less of a problem with
implementations that verify signatures only once and then cache the
results, but most implementations do not do that.
Also, there's nothing preventing people from actually signing data with
the primary key, so someone who is unfamiliar with your strategy might
accidentally use a single, very large key.

@_date: 2012-01-24 19:26:15
@_author: brian m. carlson 
@_subject: Using root CAs as a trusted 3rd party 
This is why OpenPGP implementations have trust settings.  If Bob trusts
Trent's assertions, then he can give Trent full trust and Bob's
implementation will believe that Alice's key belongs to Alice.  There's
no need to sign the key.
If I truly believe that a key belongs to someone that I have seen use it
for several years and that is trusted by numerous other people, but I
have not verified the connection between that person's identity and key
myself, I use a local signature.  That way I don't have other people
rely on my assertion if I haven't done the amount of checking that I
would like to before making a public statement.

@_date: 2012-01-26 18:30:43
@_author: brian m. carlson 
@_subject: RSA padding scheme 
Yes, it is.  However, decrypting a message does not automatically
provide the session key to the user (outside of the internal
functionality of the OpenPGP implementation).  So what I'm saying is
that even if you have an oracle that will decrypt messages on demand and
provide them to the attacker, that doesn't mean that the oracle is going
to provide the session key used to decrypt that message, which you need
to conduct the attack.
Also, please, please, please don't ever CC me.  This resulted in a major
delay as I deleted the message which I am now replying to and had to
cobble it together based on the archive.  Please respect my
Mail-Followup-To and post replies only to the list.

@_date: 2012-01-28 17:38:47
@_author: brian m. carlson 
@_subject: Why hashed User IDs is not the solution to User ID enumeration 
I'm working on an OpenPGP library which may sprout a keyserver daemon
supporting this, but there's no guarantee that that will happen anytime
soon, if ever.  Don't hold your breath.

@_date: 2012-01-30 19:06:43
@_author: brian m. carlson 
@_subject: [META] please start To: with gnupg-users@gnupg.org, i.e.: 
The problem is that unlike regular list messages, the dupes don't come
with the list headers, which makes sorting them based on the list
headers problematic.

@_date: 2012-07-10 23:59:45
@_author: brian m. carlson 
@_subject: why is SHA1 used? How do I get SHA256 to be used? 
SHA-1 is considered cryptographically broken.  It does not provide the
level of security it claims.  Practically, collisions can be generated
for 75 of the 80 rounds[0].  I hardly consider an algorithm this close
to a collision "just fine".  There's no need to run screaming to the
exits, but a quick and orderly transition has been appropriate for some
time.  The time to move to something else is ending soon.
I don't generate signatures with algorithms I consider insecure because
that leads to people being able to forge signatures in my name.  If I
use MD5, even for one message, that allows a moderately determined
attacker to replay that signature on what is likely to become a fairly
large set of messages.  I'd rather avoid that, thank you.
The question is, will GnuPG fall back to SHA-1 if it's not in my digest
preferences?  I'd much rather fail to generate a signature than generate
one using an algorithm which is very weak.
[0]

@_date: 2012-07-12 01:23:22
@_author: brian m. carlson 
@_subject: why is SHA1 used? How do I get SHA256 to be used? 
I'm not saying it has collapsed.  I'm saying that it has weaknesses, and
that the number and magnitude of the weaknesses continue to grow, and
that I think it is imprudent to use SHA-1.  I would much rather people
make the move to something better now, because otherwise we'll all be
stuck with SHA-1 long after it's insecure, just like it's been with MD5.
It's an indication of weakness.  I've seen lots of people that work with
crypto claim that we don't need larger margins of security.  The cost of
computation is so small that I'd rather overdo it than regret my
decision later.
Really?  I'm pretty sure that I'm not generating SHA-1 signatures.  This
is signed using SHA-512, SHA-384, or SHA-256.  When I sign another key,
I use SHA-512.  At least that's what I've configured GnuPG to do, and
I'd be very surprised if it did not, in fact, do that.  If it is using
SHA-1, please report it to the list: it's a bug.
SHA-1, for my current key, is being used to generate my fingerprint.
It's being used in MDCs when I encrypt a message.  And it's being used
instead of the default checksum for my private key.  That's it.
Since my private key remains solely in my possession and is not subject
to tampering, what checksum is used is really irrelevant.  Since I sign
my messages when I encrypt them, the MDC is essentially redundant, since
it would be apparent that they'd been tampered with.  It is extremely
unlikely that an attacker would be able to tamper with the encrypted
message such that they could produce a valid, signed unencrypted
And I'm personally not happy with the use of SHA-1 for the fingerprint,
but it'll have to do for a while.  I wish we had chosen RIPEMD-160
instead.  I feel it's a better, more conservative design.
Really?  Can you show an example?
Is my statement not true for MD5?

@_date: 2012-06-22 22:39:19
@_author: brian m. carlson 
@_subject: ideal.dll 
There are people using v3 keys that are not using MD5 (other than the
fingerprint, obviously).  I am one of them.  My v3 key (0x560553e7) has
v4 self-signatures on it, none of which recommend MD5.  All of the
preferences are for algorithms presently considered strong (except
SHA-1, but removing that isn't possible, unfortunately).  Obviously, I'm
not using PGP 2.6, since it won't read my key.
I have moved to using a v4 key for everyday usage, but my v3 key still
has more signatures on it than my v4 key, and I am not planning on
revoking it by any means.  I still accept signatures on it and data
encrypted to it, just like I do with my v4 key.

@_date: 2012-03-03 00:25:10
@_author: brian m. carlson 
@_subject: small security glitches 
It is not true that encryption amounts to XORing the message text
against the secret key.  That type of encryption is not secure because
it is trivial for someone to XOR two blocks (of the key size) of
ciphertext together in order to get the XOR of the plaintexts.  This
allows trivial analysis of the plaintext.
Stream ciphers usually create a key*stream* and XOR the plaintext
against that.  OpenPGP implementations do not use stream ciphers proper;
instead, they use a block cipher in CFB mode.  So by flipping bits what
you get here is not only flipped bits in the data, but a corrupted next
block.  Also, CFB mode, what is XORed is the output of a block cipher
encryption of the previous ciphertext.
This doesn't work, because all you get is the output of the block
cipher.  That doesn't tell you the key if the block cipher is secure.

@_date: 2012-03-14 04:44:39
@_author: brian m. carlson 
@_subject: compilation information ? 
From looking source, I don't believe so.  Note that the only case
in which you have more than one option is Windows/DOS.  For other
platforms, the binary is always compiled in the ordinary way.  I expect
exposing this information was not considered to be terribly important
since most platforms don't have this issue.

@_date: 2012-03-14 23:54:13
@_author: brian m. carlson 
@_subject: compilation information ? 
I presumed from the original post that what the poster was looking for
was Cygwin v. Mingw32, since he found the Version string in ASCII armor
acceptable but not the --version output and they differ only in this
Obviously there are many different ways one can compile a piece of
software, but GnuPG has never exposed that information at all.  In
general, determining the build environment given only the executable is
difficult and embedding that information requires a lot of work for
little gain.  Most people don't need that information because they know
which compiler and options they (or their distributor) used.

@_date: 2012-03-17 18:51:37
@_author: brian m. carlson 
@_subject: comments on uid 
When you compute a signature over a UID, part of the data you hash is
the UID.  If the UID is different, then any signatures aren't valid
anymore because the hash result will be different.  The facility isn't
implemented since it breaks all existing signatures and is essentially
equivalent to deleting an old UID (which really can't be done if the UID
has been published) and adding a new UID.  If you want to do those two
steps, you have to do them manually.
