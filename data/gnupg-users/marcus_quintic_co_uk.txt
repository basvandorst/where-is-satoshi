
@_date: 2004-01-15 12:17:36
@_author: Marcus Williams 
@_subject: Corruption of archive files with afio and gpg 
Hi -
I saw an earlier message on this list about getting corrupted files
within an afio archive when using gpg to "compress" the afio files. I
have the same intermittent problem. I'm using a slightly different cmd
find *|afio -b 10240 -c 1000 -o -Z -U -P gpg -Q --symmetric \
            -Q --passphrase-fd=3 -Q --no-verbose \
            -Q --batch -Q --no-options -3 3 \
            /dev/nst0 \
            3<my_passphrasefile
I get errors about various files failing to compress like "afio:
archive". Again, without gpg there are no errors. In an archive of
about 700000 files I'm seeing probably 50-100 files corrupt.
Has anyone any ideas?

@_date: 2004-01-16 10:35:02
@_author: Marcus Williams 
@_subject: Corruption of archive files with afio and gpg 
[I'm not subscribed to the list but I've tried my best to keep this
threaded properly...]
Turns out this is a feature of afio (or possibly a bug depending on
how you look at it). The problem is that if you switch compression on
with gpg, you cant guarantee the encrypted file will have the same
filesize on different runs (presumably different ciphertext is
generated on each run and so the internal compression will be
better/worse each time).
When afio is run it tries to keep the "compressed" file in memory (the
max size of which can be adjusted using -M), but if the compressed
file if bigger than this max memory size it ends up zipping it twice.
Once to get the length, the second time to get the actual file.
Later on in the afio source the size of the original run is compared
to the number of bytes read in from the second run and if they
mismatch the error I was getting occurs.
I'm not sure if the size differences end up causing corruption of the
file later on in the afio process (I've contacted the author about
It looks like the solution is to use a simple script that gzips the
source plaintext file before piping to gpg (with compression off). You
then use this script in place of gpg in the afio command line. For
unencrypting it you pipe the output of gpg through gzip in a different
script and use that script for restoring the backup set.
Hope that helps someone else,

@_date: 2004-01-28 11:25:16
@_author: Marcus Williams 
@_subject: Multiple runs of gpg and differing filesizes 
Hi -
If I pick a reasonable sized file (>1mb say) and run something like:
for i in `seq 50`; do cat file.tar | /usr/bin/gpg--symmetric --passphrase-fd=3 > file.$i 3<pp ; done
I get 50 encrypted copies of the file, but they are not all the same
size. This leads me to three questions :)
1) Is this because of the internal compression?
2) If so, can I guarantee the same filesize if I switch off internal compression?
3) If so, can I pipe the file through gzip _and then_ gpg with no compression
can I guarantee the same filesize?
(I'm using gpg 1.2.4)
