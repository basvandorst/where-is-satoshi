
@_date: 2013-03-25 17:29:20
@_author: =?koi8-r?B?8NLRzcHRINDSz8TB1sEg2sXNzMkhIGNvdW50ZXJtYW5kc3c5QHJlcGxh?=.=?koi8-r?B?Y2VtZW50c2VydmljZXMuY29tPg==?=@jfet.org 
@_subject: =?koi8-r?B?4sXaIM7Bw8XOy8kg0NLPxMHAINXewdPUz8sgMzAg08/Uz8sh?= 
Недорого!! Продаю свой участок 30 соток, в живописном  коттеджном поселке  по Киевскому ш. в 89 км от МКАД  Собственник! Позвоните  расскажу подробнее: 8(926) 031 75 13

@_date: 2013-03-25 18:32:27
@_author: =?koi8-r?B?8NLRzcHRINDSz8TB1sEg2sXNzMkhIHN0YXJrc3A4NkByZmkuY29tPg==?=@jfet.org 
@_subject: =?koi8-r?B?4sXaIM7Bw8XOy8kg0NLPxMHAINXewdPUz8sgMzAg08/Uz8sh?= 
Недорого!! Продаю свой участок 30 соток, в живописном  коттеджном поселке  по Киевскому ш. в 89 км от МКАД  Собственник! Позвоните  расскажу подробнее: 8(926) 031 75 13

@_date: 2013-03-25 19:27:42
@_author: =?koi8-r?B?8NLRzcHRINDSz8TB1sEg2sXNzMkhIGhvZ2dlZHQ4MkByb3QtaW5jZW5k?=.=?koi8-r?B?aWUuY29tPg==?=@jfet.org 
@_subject: =?koi8-r?B?4sXaIM7Bw8XOy8kg0NLPxMHAINXewdPUz8sgMzAg08/Uz8sh?= 
Недорого!! Продаю свой участок 30 соток, в живописном  коттеджном поселке  по Киевскому ш. в 89 км от МКАД  Собственник! Позвоните  расскажу подробнее: 8(926) 031 75 13

@_date: 2013-03-28 20:40:27
@_author: =?koi8-r?B?8NLRzcHRINDSz8TB1sEg2sXNzMkhIGNvd2FyZGxpbmVzc3dvNkByaHJs?=.=?koi8-r?B?YXcuY29tPg==?=@jfet.org 
@_subject: =?koi8-r?B?IPDSz8TBzSDV3sHT1M/LICDQzyDrycXX08vPzSDbLiDrwdPLwcQg0NLV?= 
Участок с коммуникациями на возвышенности с документами и соснами. Охрана и каскад прудов, коммуникации и лес, звоните 8(916)017 00 87 я Вам его покажу.

@_date: unknown_date
@_author: i 
@_subject: unknown_subject 
programmers who had the solutions to the same set of
9 problems. We extracted only the features that had positive
information gain in 2014â€™s dataset that was used as
Where V j (i) = 1 if the jth tree voted for class i and
0 otherwise, and |T | f denotes the total number of trees
in forest f . Note that by construction, âˆ‘i P(Ci ) = 1 and
P(Ci ) â‰¥ 0 âˆ€ i, allowing us to treat P(Ci ) as a probability
There was one correct classification made with 13.7%
confidence. This suggests that we can use a threshold between 13.7%
and 15% confidence level for verification,
and manually analyze the classifications that did not pass
the confidence threshold or exclude them from results.
We picked an aggressive threshold of 15% and to validate it, we
trained a random forest classifier on the same
set of 30 programmers 270 code samples. We tested on
150 different files from the programmers in the training
set. There were 6 classifications below the 15% threshold
and two of them were misclassified. We took another set
of 420 test files from 30 programmers that were not in the
training set. All the files from the 30 programmers were
attributed to one of the 30 programmers in the training
set since this is a closed world classification task, however, the
highest confidence level in these classifications
was 14.7%. The 15% threshold catches all the instances
that do not belong to the programmers in the suspect set,
gets rid of 2 misclassifications and 4 correct classifications.
Consequently, when we see a classification with
less than a threshold value, we can reject the classification and
attribute the test instance to an unknown suspect.
the main dataset to implement the approach. The classification
accuracy was 96.83%, which is higher than the
95.07% accuracy obtained in 2014â€™s dataset.
The high accuracy of validation results in Table 5 show
that we identified the important features of code stylometry and found
a stable feature set. This feature set does
not necessarily represent the exact features for all possible
datasets. For a given dataset that has ground truth
information on authorship, following the same approach
should generate the most important features that represent coding
style in that particular dataset.
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 250 from 2014
A = 250 from 2012
A = 250 all years
F = 9 from 2014
F = 9 from 2014
F â‰¥ 9 all years
Average accuracy after 10 iterations with IG-CSFS features
Table 5: Validation Experiments
Mutliclass Open World Task
The experiments in this section can be used in software
forensics to find out the programmer of a piece of malware. In
software forensics, the analyst does not know if
source code belongs to one of the programmers in the
candidate set of programmers. In such cases, we can
classify the anonymous source code, and if the majority
number of votes of trees in the random forest is below a
certain threshold, we can reject the classification considering the
possibility that it might not belong to any of the
classes in the training data. By doing so, we can scale
our approach to an open world scenario, where we might
not have encountered the suspect before. As long as we
determine a confidence threshold based on training data
[30], we can calculate the probability that an instance
belongs to one of the programmers in the set and accordingly accept or
reject the classification.
We performed 270 classifications in a 30-class problem using all the
features to determine the confidence
threshold based on the training data. The accuracy was
96.67%. There were 9 misclassifications and all of them
were classified with less than 15% confidence by the
classifier. The class probability or classification confidence that
source code fragment C is of class i is calculated by taking the
percentage of trees in the random
forest that voted for that particular class, as follows2:
P(Ci ) =
âˆ‘ j V j (i)
Two-class Closed World Task
Source code author identification could automatically
deal with source code copyright disputes without requiring manual
analysis by an objective code investigator.
A copyright dispute on code ownership can be resolved
by comparing the styles of both parties claiming to have
generated the code. The style of the disputed code can
be compared to both partiesâ€™ other source code to aid in
the investigation. To imitate such a scenario, we took
60 different pairs of programmers, each with 9 solution
files. We used a random forest and 9-fold cross validation
to classify two programmersâ€™ source code. The average
classification accuracy using CSFS set is 100.00% and
100.00% with the information gain features.
Two-class/One-class Open World Task
Another two-class machine learning task can be formulated for
authorship verification. We suspect Mallory of
plagiarizing, so we mix in some code of hers with a large
sample of other people, test, and see if the disputed code
gets classified as hers or someone elseâ€™s. If it gets classified as
hers, then it was with high probability really
written by her. If it is classified as someone elseâ€™s, it
really was someone elseâ€™s code. This could be an open
world problem and the person that originally wrote the
code could be a previously unknown programmer.
This is a two-class problem with classes Mallory and
others. We train on Malloryâ€™s solutions to problems a,
b, c, d, e, f, g, h. We also train on programmer Aâ€™s solution to
problem a, programmer Bâ€™s solution to problem b,
programmer Câ€™s solution to problem c, programmer Dâ€™s
solution to problem d, programmer Eâ€™s solution to problem e,
programmer Fâ€™s solution to problem f, programmer Gâ€™s solution to
problem g, programmer Hâ€™s solution
to problem h and put them in one class called ABCDEFGH. We train a
random forest classifier with 300 trees
on classes Mallory and ABCDEFGH. We have 6 test instances from Mallory
and 6 test instances from another
programmer ZZZZZZ, who is not in the training set.
These experiments have been repeated in the exact same setting with 80
different sets of programmers
ABCDEFGH, ZZZZZZ and Mallorys. The average classification accuracy for
Mallory using the CSFS set is
100.00%. ZZZZZZâ€™s test instances are classified as programmer
ABCDEFGH 82.04% of the time, and classified as Mallory for the rest of
the time while using the
CSFS. Depending on the amount of false positives we
are willing to accept, we can change the operating point
on the ROC curve.
These results are also promising for use in cases where
a piece of code is suspected to be plagiarized. Following
the same approach, if the classification result of the piece
of code is someone other than Mallory, that piece of code
was with very high probability not written by Mallory.
Additional Insights
scales. We are able to de-anonymize 1,600 programmers
using 32GB memory within one hour. Alternately, we
can use 40 trees and get nearly the same accuracy (within
0.5%) in a few minutes.
Figure 3: Large Scale De-anonymization
Training Data and Features
We selected different sets of 62 programmers that had F
solution files, from 2 up to 14. Each dataset has the solutions to the
same set of F problems by different sets
of programmers. Each dataset consisted of programmers
that were able to solve exactly F problems. Such an experimental setup
makes it possible to investigate the effect of programmer skill set on
coding style. The size of
the datasets were limited to 62, because there were only
62 contestants with 14 files. There were a few contestants with up to
19 files but we had to exclude them since
there were not enough programmers to compare them.
The same set of F problems were used to ensure that
the coding style of the programmer is being classified
and not the properties of possible solutions of the problem itself. We
were able to capture personal programming style since all the
programmers are coding the same
functionality in their own ways.
Stratified F-fold cross validation was used by training
on everyoneâ€™s (F âˆ’ 1) solutions and testing on the F th
problem that did not appear in the training set. As a result, the
problems in the test files were encountered for
the first time by the classifier.
We used a random forest with 300 trees and (logM)+1
features with F-fold stratified cross validation, first with
the Code Stylometry Feature Set (CSFS) and then with
the CSFSâ€™s features that had information gain.
Figure 4 shows the accuracy from 13 different sets of
62 programmers with 2 to 14 solution files, and consequently 1 to 13
training files. The CSFS reaches an optimal training set size at 9
solution files, where the classifier trains on 8 (F âˆ’ 1) solutions.
In the datasets we constructed, as the number of files
increase and problems from more advanced rounds are
included, the average line of code (LOC) per file also
increases. The average lines of code per source code
in the dataset is 70. Increased number of lines of code
might have a positive effect on the accuracy but at the
same time it reveals programmerâ€™s choice of program
We collected a larger dataset of 1,600 programmers from
various years. Each of the programmers had 9 source
code samples. We created 7 subsets of this large dataset
in differing sizes, with 250, 500, 750, 1,000, 1,250,
1,500, and 1,600 programmers. These subsets are useful to understand
how well our approach scales. We extracted the specific features that
had information gain in
the main 250 programmer dataset from this large dataset.
In theory, we need to use more trees in the random forest as the
number of classes increase to decrease variance, but we used fewer
trees compared to smaller experiments. We used 300 trees in the random
forest to
run the experiments in a reasonable amount of time with
a reasonable amount of memory. The accuracy did not
decrease too much when increasing the number of programmers. This
result shows that information gain features are robust against changes
in class and are important properties of programmersâ€™ coding styles.
following Figure 3 demonstrates how well our method
detectable change in the performance of the classifier for
this sample. The results are summarized in Table 6.
We took the maximum number of programmers, 20,
that had solutions to 9 problems in C and obfuscated
the code (see example in Appendix B) using a much
more sophisticated open source obfuscator called Tigress
[1]. In particular, Tigress implements function virtualization, an
obfuscation technique that turns functions into
interpreters and converts the original program into corresponding
bytecode. After applying function virtualization, we were less able to
effectively de-anonymize
programmers, so it has potential as a countermeasure to
programmer de-anonymization. However, this obfuscation comes at a
cost. First of all, the obfuscated code is
neither readable nor maintainable, and is thus unsuitable
for an open source project. Second, the obfuscation adds
significant overhead (9 times slower) to the runtime of
the program, which is another disadvantage.
The accuracy with the information gain feature set on
the obfuscated dataset is reduced to 67.22%. When we
limit the feature set to AST node bigrams, we get 18.89%
accuracy, which demonstrates the need for all feature
types in certain scenarios. The accuracy on the same
dataset when the code is not obfuscated is 95.91%.
Figure 4: Training Data
length in implementing the same functionality. On the
other hand, the average line of code of the 7 easier (76
LOC) or difficult problems (83 LOC) taken from contestants that were
able to complete 14 problems, is higher
than the average line of code (68) of contestants that
were able to solve only 7 problems. This shows that
programmers with better skills tend to write longer code
to solve Google Code Jam problems. The mainstream
idea is that better programmers write shorter and cleaner
code which contradicts with line of code statistics in our
datasets. Google Code Jam contestants are supposed to
optimize their code to process large inputs with faster
performance. This implementation strategy might be
leading to advanced programmers implementing longer
solutions for the sake of optimization.
We took the dataset with 62 programmers each with
9 solutions. We get 97.67% accuracy with all the features and 99.28%
accuracy with the information gain features. We excluded all the
syntactic features and the accuracy dropped to 88.89% with all the
non-syntactic features and 88.35% with the information gain features
the non-syntactic feature set. We ran another experiment
using only the syntactic features and obtained 96.06%
with all the syntactic features and 96.96% with the information gain
features of the syntactic feature set. Most
of the classification power is preserved with the syntactic features,
and using non-syntactic features leads to a
significant decline in accuracy.
Results w/o
*Information gain features
Table 6: Effect of Obfuscation on De-anonymization
Relaxed Classification
The goal here is to determine whether it is possible to reduce the
number of suspects using code stylometry. Reducing the set of suspects
in challenging cases, such as
having too many suspects, would reduce the effort required to manually
find the actual programmer of the
In this section, we performed classification on the
main 250 programmer dataset from 2014 using the information gain
features. The classification was relaxed
to a set of top R suspects instead of exact classification
of the programmer. The relaxed factor R varied from 1
to 10. Instead of taking the highest majority vote of the
decisions trees in the random forest, the highest R majority vote
decisions were taken and the classification result
was considered correct if the programmer was in the set
of top R highest voted classes. The accuracy does not
improve much after the relaxed factor is larger than 5.
We took a dataset with 9 solution files and 20 programmers and
obfuscated the code using an off-the-shelf C++
obfuscator called stunnix [3]. The accuracy with the information gain
code stylometry feature set on the obfuscated dataset is 98.89%. The
accuracy on the same
dataset when the code is not obfuscated is 100.00%. The
obfuscator refactored function and variable names, as
well as comments, and stripped all the spaces, preserving the
functionality of code without changing the structure of the program.
Obfuscating the data produced little
programming languages by implementing the layout and
lexical features as well as using a fuzzy parser.
Classification IG
Top-5 IG
Figure 5: Relaxed Classification with 250 Programmers
Table 7: Generalizing to Other Programming Languages
Generalizing the Method
Features derived from ASTs can represent coding styles
in various languages. These features are applicable in
cases when lexical and layout features may be less discriminating due
to formatting standards and reliance on
whitespace and other â€˜lexicalâ€™ features as syntax, such
as Pythonâ€™s PEP8 formatting. To show that our method
generalizes, we collected source code of 229 Python programmers from
GCJâ€™s 2014 competition. 229 programmers had exactly 9 solutions.
Using only the Python
equivalents of syntactic features listed in Table 4 and
9-fold cross-validation, the average accuracy is 53.91%
for top-1 classification, 75.69% for top-5 relaxed attribution. The
largest set of programmers to all work on
the same set of 9 problems was 23 programmers. The
average accuracy in identifying these 23 programmers is
87.93% for top-1 and 99.52% for top-5 relaxed attribution. The same
classification tasks using the information
gain features are also listed in Table 7. The overall accuracy in
datasets composed of Python code are lower
than C++ datasets. In Python datasets, we only used
syntactic features from ASTs that were generated by a
parser that was not fuzzy. The lack of quantity and specificity of
features accounts for the decreased accuracy.
The Python datasetâ€™s information gain features are significantly
fewer in quantity, compared to C++ datasetâ€™s
information gain features. Information gain only keeps
features that have discriminative value all on their own.
If two features only provide discriminative value when
used together, then information gain will discard them.
So if a lot of the features for the Python set are only
jointly discriminative (and not individually discriminative), then the
information gain criteria may be removing
features that in combination could effectively discriminate between
authors. This might account for the decrease when using information
gain features. While in
the context of other results in this paper the results in Table 7
appear lackluster, it is worth noting that even this
preliminary test using only syntactic features has comparable
performance to other prior work at a similar scale
(see Section 6 and Table 9), demonstrating the utility
of syntactic features and the relative ease of generating
them for novel programming languages. Nevertheless, a
CSFS equivalent feature set can be generated for other
Software Engineering Insights
We wanted to investigate if programming style is consistent throughout
years. We found the contestants that had
the same username and country information both in 2012
and 2014. We assumed that these are the same people but
there is a chance that they might be different people. In
2014, someone else might have picked up the same username from the
same country and started using it. We are
going to ignore such a ground truth problem for now and
assume that they are the same people.
We took a set of 25 programmers from 2012 that were
also contestants in 2014â€™s competition. We took 8 files
from their submissions in 2012 and trained a random forest classifier
with 300 trees using CSFS. We had one instance from each one of the
contestants from 2014. The
correct classification of these test instances from 2014
is 96.00%. The accuracy dropped to 92.00% when using
only information gain features, which might be due to the
aggressive elimination of pairs of features that are jointly
discriminative. These 25 programmersâ€™ 9 files from 2014
had a correct classification accuracy of 98.04%. These
results indicate that coding style is preserved up to some
degree throughout years.
To investigate problem difficultyâ€™s effect on coding
style, we created two datasets from 62 programmers that
had exactly 14 solution files. Table 8 summarizes the
following results. A dataset with 7 of the easier problems out of 14
resulted in 95.62% accuracy. A dataset
with 7 of the more difficult problems out of 14 resulted
in 99.31% accuracy. This might imply that more difficult
coding tasks have a more prevalent reflection of coding
style. On the other hand, the dataset that had 62 programmers with
exactly 7 of the easier problems resulted
in 91.24% accuracy, which is a lot lower than the accuracy obtained
from the dataset whose programmers were
able to advance to solve 14 problems. This might indicate that,
programmers who are advanced enough to answer 14 problems likely have
more unique coding styles
compared to contestants that were only able to solve the
first 7 problems.
To investigate the possibility that contestants who are
able to advance further in the rounds have more unique
coding styles, we performed a second round of experiments on
comparable datasets. We took the dataset with
12 solution files and 62 programmers. A dataset with 6
of the easier problems out of 12 resulted in 91.39% accuracy. A
dataset with 6 of the more difficult problems
out of 12 resulted in 94.35% accuracy. These results are
higher than the dataset whose programmers were only
able to solve the easier 6 problems. The dataset that had
62 programmers with exactly 6 of the easier problems
resulted in 90.05% accuracy.
that coding style is reflected more through difficult programming
tasks. This might indicate that programmers
come up with unique solutions and preserve their coding style more
when problems get harder. On the other
hand, programmers with a better skill set have a prevalent
coding style which can be identified more easily compared to
contestants who were not able to advance as
far in the competition. This might indicate that as programmers become
more advanced, they build a stronger
coding style compared to novices. There is another possibility that
maybe better programmers start out with a
more unique coding style.
Effects of Obfuscation. A malware author or plagiarizing programmer
might deliberately try to hide his
source code by obfuscation. Our experiments indicate
that our method is resistant to simple off-the-shelf obfuscators such
as stunnix, that make code look cryptic while
preserving functionality. The reason for this success is
that the changes stunnix makes to the code have no effect
on syntactic features, e.g., removal of comments, changing of names,
and stripping of whitespace.
In contrast, sophisticated obfuscation techniques such
as function virtualization hinder de-anonymization to
some degree, however, at the cost of making code
unreadable and introducing a significant performance
penalty. Unfortunately, unreadability of code is not acceptable for
open-source projects, while it is no problem
for attackers interested in covering their tracks. Developing methods
to automatically remove stylometric information from source code
without sacrificing readability
is therefore a promising direction for future research.
Limitations. We have not considered the case where
a source file might be written by a different author than
the stated contestant, which is a ground truth problem
that we cannot control. Moreover, it is often the case that
code fragments are the work of multiple authors. We
plan to extend this work to study such datasets. To shed
light on the feasibility of classifying such code, we are
currently working with a dataset of git commits to open
source projects. Our parser works on code fragments
rather than complete code, consequently we believe this
analysis will be possible.
Another fundamental problem for machine learning
classifiers are mimicry attacks. For example, our classifier may be
evaded by an adversary by adding extra
dummy code to a file that closely resembles that of another
programmer, albeit without affecting the programâ€™s
behavior. This evasion is possible, but trivial to resolve
when an analysts verifies the decision.
Finally, we cannot be sure whether the original author is actually a
Google Code Jam contestant. In this
case, we can detect those by a classify and then verify
approach as explained in Stolerman et al.â€™s work [30].
Each classification could go through a verification step
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 62
F = 14
F = 12
Average accuracy after 10 iterations while using CSFS
Average accuracy after 10 iterations while using IG CSFS
1 Drop in accuracy due to programmer skill set.
2 Coding style is more distinct in more difficult tasks.
Table 8: Effect of Problem Difficulty on Coding Style
In this section, we discuss the conclusions we draw from
the experiments outlined in the previous section, limitations, as well
as questions raised by our results. In particular, we discuss the
difficulty of the different settings
considered, the effects of obfuscation, and limitations of
our current approach.
Problem Difficulty. The experiment with random
problems from random authors among seven years most
closely resembles a real world scenario. In such an experimental
setting, there is a chance that instead of only
identifying authors we are also identifying the properties
of a specific problemâ€™s solution, which results in a boost
in accuracy.
In contrast, our main experimental setting where all
authors have only answered the nine easiest problems is
possibly the hardest scenario, since we are training on the
same set of eight problems that all the authors have algorithmically
solved and try to identify the authors from
the test instances that are all solutions of the 9th problem. On the
upside, these test instances help us precisely
capture the differences between individual coding style
that represent the same functionality. We also see that
such a scenario is harder since the randomized dataset
has higher accuracy.
Classifying authors that have implemented the solution to a set of
difficult problems is easier than identifying authors with a set of
easier problems. This shows
to eliminate instances where the classifierâ€™s confidence is
below a threshold. After the verification step, instances
that do not belong to the set of known authors can be
separated from the dataset to be excluded or for further
manual analysis.
tural features to achieve higher accuracies at larger scales
and the first to study how code obfuscation affects code
There has also been some code stylometry work that
focused on manual analysis and case studies. Spafford
and Weeber [29] suggest that use of lexical features such
as variable names, formatting and comments, as well as
some syntactic features such as usage of keywords, scoping and
presence of bugs could aid in source code attribution but they do not
present results or a case study
experiment with a formal approach. Gray et al. [15]
identify three categories in code stylometry: the layout
of the code, variable and function naming conventions,
types of data structures being used and also the cyclomatic complexity
of the code obtained from the control
flow graph. They do not mention anything about the syntactic
characteristics of code, which could potentially be
a great marker of coding style that reveals the usage of
programming languageâ€™s grammar. Their case study is
based on a manual analysis of three worms, rather than
a statistical learning approach. Hayes and Offutt [16]
examine coding style in source code by their consistent
programmer hypothesis. They focused on lexical and
layout features, such as the occurrence of semicolons,
operators and constants. Their dataset consisted of 20
programmers and the analysis was not automated. They
concluded that coding style exists through some of their
features and professional programmers have a stronger
programming style compared to students. In our results
in Section 4.3.6, we also show that more advanced programmers have a
more identifying coding style.
There is also a great deal of research on plagiarism
detection which is carried out by identifying the similarities between
different programs. For example, there is a
widely used tool called Moss that originated from Stanford University
for detecting software plagiarism. Moss
[6] is able to analyze the similarities of code written by
different programmers. Rosenblum et al. [27] present a
novel program representation and techniques that automatically detect
the stylistic features of binary code.
Related Work
Our work is inspired by the research done on authorship
attribution of unstructured or semi-structured text [5, 22].
In this section, we discuss prior work on source code
authorship attribution. In general, such work (Table 9)
looks at smaller scale problems, does not use structural
features, and achieves lower accuracies than our work.
The highest accuracies in the related work are
achieved by Frantzeskou et al. [12, 14]. They used 1,500
7-grams to reach 97% accuracy with 30 programmers.
They investigated the high-level features that contribute
to source code authorship attribution in Java and Common Lisp. They
determined the importance of each feature by iteratively excluding one
of the features from the
feature set. They showed that comments, layout features
and naming patterns have a strong influence on the author
classification accuracy. They used more training
data (172 line of code on average) than us (70 lines of
code). We replicated their experiments on a 30 programmer subset of
our C++ data set, with eleven files containing 70 lines of code on
average and no comments. We
reach 76.67% accuracy with 6-grams, and 76.06% accuracy with 7-grams.
When we used a 6 and 7-gram feature set on 250 programmers with 9
files, we got 63.42%
accuracy. With our original feature set, we get 98% accuracy on 250 programmers.
The largest number of programmers studied in the related work was 46
programmers with 67.2% accuracy.
Ding and Samadzadeh [10] use statistical methods for
authorship attribution in Java. They show that among
lexical, keyword and layout properties, layout metrics
have a more important role than others which is not the
case in our analysis.
There are also a number of smaller scale, lower accuracy approaches in
the literature [9, 11, 18â€“21, 28],
shown in Table 9, all of which we significantly outperform. These
approaches use a combination of layout and
lexical features.
The only other work to explore structural features is
by Pellin [23], who used manually parsed abstract syntax
trees with an SVM that has a tree based kernel to classify
functions of two programmers. He obtains an average of
73% accuracy in a two class classification task. His approach
explained in the white paper can be extended to
our approach, so it is the closest to our work in the literature. This
work demonstrates that it is non-trivial to
use ASTs effectively. Our work is the first to use struc-
Related Work
Pellin [23]
MacDonell et al.[21]
Frantzeskou et al.[14]
Burrows et al. [9]
Elenbogen and Seliya [11]
Kothari et al. [18]
Lange and Mancoridis [20]
Krsul and Spafford [19]
Frantzeskou et al. [14]
Ding and Samadzadeh [10]
This work
This work
This work
This work
# of Programmers
Table 9: Comparison to Previous Results
7
Conclusion and Future Work
[1] The tigress diversifying c virtualizer, Source code stylometry has direct applications for privacy, security,
software forensics, plagiarism, copyright infringement disputes, and
authorship verification.
Source code stylometry is an immediate concern for programmers who
want to contribute code anonymously because de-anonymization is quite
possible. We introduce
the first principled use of syntactic features along with
lexical and layout features to investigate style in source
code. We can reach 94% accuracy in classifying 1,600
authors and 98% accuracy in classifying 250 authors
with eight training files per class. This is a significant
increase in accuracy and scale in source code authorship
attribution. In particular, it shows that source code authorship
attribution with the Code Stylometry Feature Set
scales even better than regular stylometric authorship attribution, as
these methods can only identify individuals
in sets of 50 authors with slightly over 90% accuracy [see
4]. Furthermore, this performance is achieved by training
on only 550 lines of code or eight solution files, whereas
classical stylometric analysis requires 5,000 words.
Additionally, our results raise a number of questions
that motivate future research. First, as malicious code
is often only available in binary format, it would be interesting to
investigate whether syntactic features can be
partially preserved in binaries. This may require our feature set to
be improved in order to incorporate information obtained from control
flow graphs.
Second, we would also like to see if classification accuracy can be
further increased. For example, we would
like to explore whether using features that have joint information
gain alongside features that have information
gain by themselves improve performance. Moreover, designing features
that capture larger fragments of the abstract syntax tree could
provide improvements. These
changes (along with adding lexical and layout features)
may provide significant improvements to the Python results and help
generalize the approach further.
Finally, we would like to investigate whether code can
be automatically normalized to remove stylistic information while
preserving functionality and readability.
[2] Google code jam,  2014.
[3] Stunnix,  November 2014.
[4] A BBASI , A., AND C HEN , H. Writeprints: A stylometric approach
to identity-level identification and similarity detection in
cyberspace. ACM Trans. Inf. Syst. 26, 2 (2008), 1â€“29.
[5] A FROZ , S., B RENNAN , M., AND G REENSTADT, R. Detecting
hoaxes, frauds, and deception in writing style online. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE,
pp. 461â€“475.
[6] A IKEN , A., ET AL . Moss: A system for detecting software
plagiarism. University of Californiaâ€“Berkeley. See www. cs.
berkeley. edu/aiken/moss. html 9 (2005).
[7] B REIMAN , L. Random forests. Machine Learning 45, 1 (2001),
[8] B URROWS , S., AND TAHAGHOGHI , S. M. Source code authorship
attribution using n-grams. In Proc. of the Australasian Document
Computing Symposium (2007).
[9] B URROWS , S., U ITDENBOGERD , A. L., AND T URPIN , A. Application
of information retrieval techniques for source code authorship
attribution. In Database Systems for Advanced Applications (2009),
Springer, pp. 699â€“713.
[10] D ING , H., AND S AMADZADEH , M. H. Extraction of java program
fingerprints for software authorship identification. Journal
of Systems and Software 72, 1 (2004), 49â€“57.
[11] E LENBOGEN , B. S., AND S ELIYA , N. Detecting outsourced student
programming assignments. Journal of Computing Sciences
in Colleges 23, 3 (2008), 50â€“57.
[12] F RANTZESKOU , G., M AC D ONELL , S., S TAMATATOS , E., AND
G RITZALIS , S. Examining the significance of high-level programming
features in source code author classification. Journal
of Systems and Software 81, 3 (2008), 447â€“460.
[13] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S.,
C HASKI , C. E., AND H OWALD , B. S. Identifying authorship
by byte-level n-grams: The source code author profile (scap)
method. International Journal of Digital Evidence 6, 1 (2007),
[14] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S., AND
K ATSIKAS , S. Effective identification of source code authors
using byte-level information. In Proceedings of the 28th International
Conference on Software Engineering (2006), ACM,
pp. 893â€“896.
This material is based on work supported by the ARO
(U.S. Army Research Office) Grant W911NF-14-10444, the DFG (German
Research Foundation) under the
project DEVIL (RI 2469/1-1), and AWS in Education
Research Grant award. Any opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not necessarily reflect
those of the ARO, DFG, and AWS.
[15] G RAY, A., S ALLIS , P., AND M AC D ONELL , S. Software
forensics: Extending authorship analysis techniques to computer
[16] H AYES , J. H., AND O FFUTT, J. Recognizing authors: an
examination of the consistent programmer hypothesis. Software Testing,
Verification and Reliability 20, 4 (2010), 329â€“356.
[17] I NOCENCIO , R. U.s. programmer outsources own job to china,
surfs cat videos, January 2013.
A
[18] KOTHARI , J., S HEVERTALOV, M., S TEHLE , E., AND M AN CORIDIS ,
S. A probabilistic approach to source code authorship
identification. In Information Technology, 2007. ITNGâ€™07. Fourth
International Conference on (2007), IEEE, pp. 243â€“248.
Appendix: Keywords and Node Types
[19] K RSUL , I., AND S PAFFORD , E. H. Authorship analysis:
Identifying the author of a program. Computers & Security 16, 3
(1997), 233â€“257.
[20] L ANGE , R. C., AND M ANCORIDIS , S. Using code metric histograms
and genetic algorithms to perform author identification
for software forensics. In Proceedings of the 9th Annual Conference on
Genetic and Evolutionary Computation (2007), ACM,
pp. 2082â€“2089.
[21] M AC D ONELL , S. G., G RAY, A. R., M AC L ENNAN , G., AND
S ALLIS , P. J. Software forensics for discriminating between
program authors using case-based reasoning, feedforward neural
networks and multiple discriminant analysis. In Neural Information
Processing, 1999. Proceedings. ICONIPâ€™99. 6th International
Conference on (1999), vol. 1, IEEE, pp. 66â€“71.
[22] NARAYANAN , A., PASKOV, H., G ONG , N. Z., B ETHENCOURT,
J., S TEFANOV, E., S HIN , E. C. R., AND S ONG , D. On the
feasibility of internet-scale author identification. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE, pp. 300â€“
[23] P ELLIN , B. N. Using classification techniques to determine
source code authorship. White Paper: Department of Computer
Science, University of Wisconsin (2000).
[24] P IKE , R. The sherlock plagiarism detector, 2011.
Table 10: Abstract syntax tree node types
[25] P RECHELT, L., M ALPOHL , G., AND P HILIPPSEN , M. Finding
plagiarisms among a set of programs with jplag. J. UCS 8, 11
(2002), 1016.
Table 10 lists the AST node types generated by Joern
that were incorporated to the feature set. Table 11 shows
the C++ keywords used in the feature set.
[26] Q UINLAN , J. Induction of decision trees. Machine learning 1, 1
(1986), 81â€“106.
[27] ROSENBLUM , N., Z HU , X., AND M ILLER , B. Who wrote this
code? identifying the authors of program binaries. Computer
Securityâ€“ESORICS 2011 (2011), 172â€“189.
[31] W IKIPEDIA. Saeed Malekpour, 2014. [Online; accessed 04November-2014].
[32] YAMAGUCHI , F., G OLDE , N., A RP, D., AND R IECK , K. Modeling
and discovering vulnerabilities with code property graphs. In
Proc of IEEE Symposium on Security and Privacy (S&P) (2014).
[33] YAMAGUCHI , F., W RESSNEGGER , C., G ASCON , H., AND
R IECK , K. Chucky: Exposing missing checks in source code
for vulnerability discovery. In Proceedings of the 2013 ACM
SIGSAC Conference on Computer & Communications Security
(2013), ACM, pp. 499â€“510.
[28] S HEVERTALOV, M., KOTHARI , J., S TEHLE , E., AND M AN CORIDIS ,
S. On the use of discretized source code metrics for author
identification. In Search Based Software Engineering, 2009
1st International Symposium on (2009), IEEE, pp. 69â€“78.
[29] S PAFFORD , E. H., AND W EEBER , S. A. Software forensics:
Can we track code to its authors? Computers & Security 12, 6
(1993), 585â€“595.
[30] S TOLERMAN , A., OVERDORF, R., A FROZ , S., AND G REEN STADT, R.
Classify, but verify: Breaking the closed-world assumption in
stylometric authorship attribution. In IFIP Working
Group 11.9 on Digital Forensics (2014), IFIP.
Table 11: C++ keywords
B
Appendix: Original vs Obfuscated Code
Figure 6: A code sample X
Figure 6 shows a source code sample X from our
dataset that is 21 lines long. After obfuscation with Tigress, sample
X became 537 lines long. Figure 7 shows
the first 13 lines of the obfuscated sample X.
Figure 7: Code sample X after obfuscation

@_date: unknown_date
@_author: i 
@_subject: unknown_subject 
programmers who had the solutions to the same set of
9 problems. We extracted only the features that had positive
information gain in 2014â€™s dataset that was used as
Where V j (i) = 1 if the jth tree voted for class i and
0 otherwise, and |T | f denotes the total number of trees
in forest f . Note that by construction, âˆ‘i P(Ci ) = 1 and
P(Ci ) â‰¥ 0 âˆ€ i, allowing us to treat P(Ci ) as a probability
There was one correct classification made with 13.7%
confidence. This suggests that we can use a threshold between 13.7%
and 15% confidence level for verification,
and manually analyze the classifications that did not pass
the confidence threshold or exclude them from results.
We picked an aggressive threshold of 15% and to validate it, we
trained a random forest classifier on the same
set of 30 programmers 270 code samples. We tested on
150 different files from the programmers in the training
set. There were 6 classifications below the 15% threshold
and two of them were misclassified. We took another set
of 420 test files from 30 programmers that were not in the
training set. All the files from the 30 programmers were
attributed to one of the 30 programmers in the training
set since this is a closed world classification task, however, the
highest confidence level in these classifications
was 14.7%. The 15% threshold catches all the instances
that do not belong to the programmers in the suspect set,
gets rid of 2 misclassifications and 4 correct classifications.
Consequently, when we see a classification with
less than a threshold value, we can reject the classification and
attribute the test instance to an unknown suspect.
the main dataset to implement the approach. The classification
accuracy was 96.83%, which is higher than the
95.07% accuracy obtained in 2014â€™s dataset.
The high accuracy of validation results in Table 5 show
that we identified the important features of code stylometry and found
a stable feature set. This feature set does
not necessarily represent the exact features for all possible
datasets. For a given dataset that has ground truth
information on authorship, following the same approach
should generate the most important features that represent coding
style in that particular dataset.
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 250 from 2014
A = 250 from 2012
A = 250 all years
F = 9 from 2014
F = 9 from 2014
F â‰¥ 9 all years
Average accuracy after 10 iterations with IG-CSFS features
Table 5: Validation Experiments
Mutliclass Open World Task
The experiments in this section can be used in software
forensics to find out the programmer of a piece of malware. In
software forensics, the analyst does not know if
source code belongs to one of the programmers in the
candidate set of programmers. In such cases, we can
classify the anonymous source code, and if the majority
number of votes of trees in the random forest is below a
certain threshold, we can reject the classification considering the
possibility that it might not belong to any of the
classes in the training data. By doing so, we can scale
our approach to an open world scenario, where we might
not have encountered the suspect before. As long as we
determine a confidence threshold based on training data
[30], we can calculate the probability that an instance
belongs to one of the programmers in the set and accordingly accept or
reject the classification.
We performed 270 classifications in a 30-class problem using all the
features to determine the confidence
threshold based on the training data. The accuracy was
96.67%. There were 9 misclassifications and all of them
were classified with less than 15% confidence by the
classifier. The class probability or classification confidence that
source code fragment C is of class i is calculated by taking the
percentage of trees in the random
forest that voted for that particular class, as follows2:
P(Ci ) =
âˆ‘ j V j (i)
Two-class Closed World Task
Source code author identification could automatically
deal with source code copyright disputes without requiring manual
analysis by an objective code investigator.
A copyright dispute on code ownership can be resolved
by comparing the styles of both parties claiming to have
generated the code. The style of the disputed code can
be compared to both partiesâ€™ other source code to aid in
the investigation. To imitate such a scenario, we took
60 different pairs of programmers, each with 9 solution
files. We used a random forest and 9-fold cross validation
to classify two programmersâ€™ source code. The average
classification accuracy using CSFS set is 100.00% and
100.00% with the information gain features.
Two-class/One-class Open World Task
Another two-class machine learning task can be formulated for
authorship verification. We suspect Mallory of
plagiarizing, so we mix in some code of hers with a large
sample of other people, test, and see if the disputed code
gets classified as hers or someone elseâ€™s. If it gets classified as
hers, then it was with high probability really
written by her. If it is classified as someone elseâ€™s, it
really was someone elseâ€™s code. This could be an open
world problem and the person that originally wrote the
code could be a previously unknown programmer.
This is a two-class problem with classes Mallory and
others. We train on Malloryâ€™s solutions to problems a,
b, c, d, e, f, g, h. We also train on programmer Aâ€™s solution to
problem a, programmer Bâ€™s solution to problem b,
programmer Câ€™s solution to problem c, programmer Dâ€™s
solution to problem d, programmer Eâ€™s solution to problem e,
programmer Fâ€™s solution to problem f, programmer Gâ€™s solution to
problem g, programmer Hâ€™s solution
to problem h and put them in one class called ABCDEFGH. We train a
random forest classifier with 300 trees
on classes Mallory and ABCDEFGH. We have 6 test instances from Mallory
and 6 test instances from another
programmer ZZZZZZ, who is not in the training set.
These experiments have been repeated in the exact same setting with 80
different sets of programmers
ABCDEFGH, ZZZZZZ and Mallorys. The average classification accuracy for
Mallory using the CSFS set is
100.00%. ZZZZZZâ€™s test instances are classified as programmer
ABCDEFGH 82.04% of the time, and classified as Mallory for the rest of
the time while using the
CSFS. Depending on the amount of false positives we
are willing to accept, we can change the operating point
on the ROC curve.
These results are also promising for use in cases where
a piece of code is suspected to be plagiarized. Following
the same approach, if the classification result of the piece
of code is someone other than Mallory, that piece of code
was with very high probability not written by Mallory.
Additional Insights
scales. We are able to de-anonymize 1,600 programmers
using 32GB memory within one hour. Alternately, we
can use 40 trees and get nearly the same accuracy (within
0.5%) in a few minutes.
Figure 3: Large Scale De-anonymization
Training Data and Features
We selected different sets of 62 programmers that had F
solution files, from 2 up to 14. Each dataset has the solutions to the
same set of F problems by different sets
of programmers. Each dataset consisted of programmers
that were able to solve exactly F problems. Such an experimental setup
makes it possible to investigate the effect of programmer skill set on
coding style. The size of
the datasets were limited to 62, because there were only
62 contestants with 14 files. There were a few contestants with up to
19 files but we had to exclude them since
there were not enough programmers to compare them.
The same set of F problems were used to ensure that
the coding style of the programmer is being classified
and not the properties of possible solutions of the problem itself. We
were able to capture personal programming style since all the
programmers are coding the same
functionality in their own ways.
Stratified F-fold cross validation was used by training
on everyoneâ€™s (F âˆ’ 1) solutions and testing on the F th
problem that did not appear in the training set. As a result, the
problems in the test files were encountered for
the first time by the classifier.
We used a random forest with 300 trees and (logM)+1
features with F-fold stratified cross validation, first with
the Code Stylometry Feature Set (CSFS) and then with
the CSFSâ€™s features that had information gain.
Figure 4 shows the accuracy from 13 different sets of
62 programmers with 2 to 14 solution files, and consequently 1 to 13
training files. The CSFS reaches an optimal training set size at 9
solution files, where the classifier trains on 8 (F âˆ’ 1) solutions.
In the datasets we constructed, as the number of files
increase and problems from more advanced rounds are
included, the average line of code (LOC) per file also
increases. The average lines of code per source code
in the dataset is 70. Increased number of lines of code
might have a positive effect on the accuracy but at the
same time it reveals programmerâ€™s choice of program
We collected a larger dataset of 1,600 programmers from
various years. Each of the programmers had 9 source
code samples. We created 7 subsets of this large dataset
in differing sizes, with 250, 500, 750, 1,000, 1,250,
1,500, and 1,600 programmers. These subsets are useful to understand
how well our approach scales. We extracted the specific features that
had information gain in
the main 250 programmer dataset from this large dataset.
In theory, we need to use more trees in the random forest as the
number of classes increase to decrease variance, but we used fewer
trees compared to smaller experiments. We used 300 trees in the random
forest to
run the experiments in a reasonable amount of time with
a reasonable amount of memory. The accuracy did not
decrease too much when increasing the number of programmers. This
result shows that information gain features are robust against changes
in class and are important properties of programmersâ€™ coding styles.
following Figure 3 demonstrates how well our method
detectable change in the performance of the classifier for
this sample. The results are summarized in Table 6.
We took the maximum number of programmers, 20,
that had solutions to 9 problems in C and obfuscated
the code (see example in Appendix B) using a much
more sophisticated open source obfuscator called Tigress
[1]. In particular, Tigress implements function virtualization, an
obfuscation technique that turns functions into
interpreters and converts the original program into corresponding
bytecode. After applying function virtualization, we were less able to
effectively de-anonymize
programmers, so it has potential as a countermeasure to
programmer de-anonymization. However, this obfuscation comes at a
cost. First of all, the obfuscated code is
neither readable nor maintainable, and is thus unsuitable
for an open source project. Second, the obfuscation adds
significant overhead (9 times slower) to the runtime of
the program, which is another disadvantage.
The accuracy with the information gain feature set on
the obfuscated dataset is reduced to 67.22%. When we
limit the feature set to AST node bigrams, we get 18.89%
accuracy, which demonstrates the need for all feature
types in certain scenarios. The accuracy on the same
dataset when the code is not obfuscated is 95.91%.
Figure 4: Training Data
length in implementing the same functionality. On the
other hand, the average line of code of the 7 easier (76
LOC) or difficult problems (83 LOC) taken from contestants that were
able to complete 14 problems, is higher
than the average line of code (68) of contestants that
were able to solve only 7 problems. This shows that
programmers with better skills tend to write longer code
to solve Google Code Jam problems. The mainstream
idea is that better programmers write shorter and cleaner
code which contradicts with line of code statistics in our
datasets. Google Code Jam contestants are supposed to
optimize their code to process large inputs with faster
performance. This implementation strategy might be
leading to advanced programmers implementing longer
solutions for the sake of optimization.
We took the dataset with 62 programmers each with
9 solutions. We get 97.67% accuracy with all the features and 99.28%
accuracy with the information gain features. We excluded all the
syntactic features and the accuracy dropped to 88.89% with all the
non-syntactic features and 88.35% with the information gain features
the non-syntactic feature set. We ran another experiment
using only the syntactic features and obtained 96.06%
with all the syntactic features and 96.96% with the information gain
features of the syntactic feature set. Most
of the classification power is preserved with the syntactic features,
and using non-syntactic features leads to a
significant decline in accuracy.
Results w/o
*Information gain features
Table 6: Effect of Obfuscation on De-anonymization
Relaxed Classification
The goal here is to determine whether it is possible to reduce the
number of suspects using code stylometry. Reducing the set of suspects
in challenging cases, such as
having too many suspects, would reduce the effort required to manually
find the actual programmer of the
In this section, we performed classification on the
main 250 programmer dataset from 2014 using the information gain
features. The classification was relaxed
to a set of top R suspects instead of exact classification
of the programmer. The relaxed factor R varied from 1
to 10. Instead of taking the highest majority vote of the
decisions trees in the random forest, the highest R majority vote
decisions were taken and the classification result
was considered correct if the programmer was in the set
of top R highest voted classes. The accuracy does not
improve much after the relaxed factor is larger than 5.
We took a dataset with 9 solution files and 20 programmers and
obfuscated the code using an off-the-shelf C++
obfuscator called stunnix [3]. The accuracy with the information gain
code stylometry feature set on the obfuscated dataset is 98.89%. The
accuracy on the same
dataset when the code is not obfuscated is 100.00%. The
obfuscator refactored function and variable names, as
well as comments, and stripped all the spaces, preserving the
functionality of code without changing the structure of the program.
Obfuscating the data produced little
programming languages by implementing the layout and
lexical features as well as using a fuzzy parser.
Classification IG
Top-5 IG
Figure 5: Relaxed Classification with 250 Programmers
Table 7: Generalizing to Other Programming Languages
Generalizing the Method
Features derived from ASTs can represent coding styles
in various languages. These features are applicable in
cases when lexical and layout features may be less discriminating due
to formatting standards and reliance on
whitespace and other â€˜lexicalâ€™ features as syntax, such
as Pythonâ€™s PEP8 formatting. To show that our method
generalizes, we collected source code of 229 Python programmers from
GCJâ€™s 2014 competition. 229 programmers had exactly 9 solutions.
Using only the Python
equivalents of syntactic features listed in Table 4 and
9-fold cross-validation, the average accuracy is 53.91%
for top-1 classification, 75.69% for top-5 relaxed attribution. The
largest set of programmers to all work on
the same set of 9 problems was 23 programmers. The
average accuracy in identifying these 23 programmers is
87.93% for top-1 and 99.52% for top-5 relaxed attribution. The same
classification tasks using the information
gain features are also listed in Table 7. The overall accuracy in
datasets composed of Python code are lower
than C++ datasets. In Python datasets, we only used
syntactic features from ASTs that were generated by a
parser that was not fuzzy. The lack of quantity and specificity of
features accounts for the decreased accuracy.
The Python datasetâ€™s information gain features are significantly
fewer in quantity, compared to C++ datasetâ€™s
information gain features. Information gain only keeps
features that have discriminative value all on their own.
If two features only provide discriminative value when
used together, then information gain will discard them.
So if a lot of the features for the Python set are only
jointly discriminative (and not individually discriminative), then the
information gain criteria may be removing
features that in combination could effectively discriminate between
authors. This might account for the decrease when using information
gain features. While in
the context of other results in this paper the results in Table 7
appear lackluster, it is worth noting that even this
preliminary test using only syntactic features has comparable
performance to other prior work at a similar scale
(see Section 6 and Table 9), demonstrating the utility
of syntactic features and the relative ease of generating
them for novel programming languages. Nevertheless, a
CSFS equivalent feature set can be generated for other
Software Engineering Insights
We wanted to investigate if programming style is consistent throughout
years. We found the contestants that had
the same username and country information both in 2012
and 2014. We assumed that these are the same people but
there is a chance that they might be different people. In
2014, someone else might have picked up the same username from the
same country and started using it. We are
going to ignore such a ground truth problem for now and
assume that they are the same people.
We took a set of 25 programmers from 2012 that were
also contestants in 2014â€™s competition. We took 8 files
from their submissions in 2012 and trained a random forest classifier
with 300 trees using CSFS. We had one instance from each one of the
contestants from 2014. The
correct classification of these test instances from 2014
is 96.00%. The accuracy dropped to 92.00% when using
only information gain features, which might be due to the
aggressive elimination of pairs of features that are jointly
discriminative. These 25 programmersâ€™ 9 files from 2014
had a correct classification accuracy of 98.04%. These
results indicate that coding style is preserved up to some
degree throughout years.
To investigate problem difficultyâ€™s effect on coding
style, we created two datasets from 62 programmers that
had exactly 14 solution files. Table 8 summarizes the
following results. A dataset with 7 of the easier problems out of 14
resulted in 95.62% accuracy. A dataset
with 7 of the more difficult problems out of 14 resulted
in 99.31% accuracy. This might imply that more difficult
coding tasks have a more prevalent reflection of coding
style. On the other hand, the dataset that had 62 programmers with
exactly 7 of the easier problems resulted
in 91.24% accuracy, which is a lot lower than the accuracy obtained
from the dataset whose programmers were
able to advance to solve 14 problems. This might indicate that,
programmers who are advanced enough to answer 14 problems likely have
more unique coding styles
compared to contestants that were only able to solve the
first 7 problems.
To investigate the possibility that contestants who are
able to advance further in the rounds have more unique
coding styles, we performed a second round of experiments on
comparable datasets. We took the dataset with
12 solution files and 62 programmers. A dataset with 6
of the easier problems out of 12 resulted in 91.39% accuracy. A
dataset with 6 of the more difficult problems
out of 12 resulted in 94.35% accuracy. These results are
higher than the dataset whose programmers were only
able to solve the easier 6 problems. The dataset that had
62 programmers with exactly 6 of the easier problems
resulted in 90.05% accuracy.
that coding style is reflected more through difficult programming
tasks. This might indicate that programmers
come up with unique solutions and preserve their coding style more
when problems get harder. On the other
hand, programmers with a better skill set have a prevalent
coding style which can be identified more easily compared to
contestants who were not able to advance as
far in the competition. This might indicate that as programmers become
more advanced, they build a stronger
coding style compared to novices. There is another possibility that
maybe better programmers start out with a
more unique coding style.
Effects of Obfuscation. A malware author or plagiarizing programmer
might deliberately try to hide his
source code by obfuscation. Our experiments indicate
that our method is resistant to simple off-the-shelf obfuscators such
as stunnix, that make code look cryptic while
preserving functionality. The reason for this success is
that the changes stunnix makes to the code have no effect
on syntactic features, e.g., removal of comments, changing of names,
and stripping of whitespace.
In contrast, sophisticated obfuscation techniques such
as function virtualization hinder de-anonymization to
some degree, however, at the cost of making code
unreadable and introducing a significant performance
penalty. Unfortunately, unreadability of code is not acceptable for
open-source projects, while it is no problem
for attackers interested in covering their tracks. Developing methods
to automatically remove stylometric information from source code
without sacrificing readability
is therefore a promising direction for future research.
Limitations. We have not considered the case where
a source file might be written by a different author than
the stated contestant, which is a ground truth problem
that we cannot control. Moreover, it is often the case that
code fragments are the work of multiple authors. We
plan to extend this work to study such datasets. To shed
light on the feasibility of classifying such code, we are
currently working with a dataset of git commits to open
source projects. Our parser works on code fragments
rather than complete code, consequently we believe this
analysis will be possible.
Another fundamental problem for machine learning
classifiers are mimicry attacks. For example, our classifier may be
evaded by an adversary by adding extra
dummy code to a file that closely resembles that of another
programmer, albeit without affecting the programâ€™s
behavior. This evasion is possible, but trivial to resolve
when an analysts verifies the decision.
Finally, we cannot be sure whether the original author is actually a
Google Code Jam contestant. In this
case, we can detect those by a classify and then verify
approach as explained in Stolerman et al.â€™s work [30].
Each classification could go through a verification step
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 62
F = 14
F = 12
Average accuracy after 10 iterations while using CSFS
Average accuracy after 10 iterations while using IG CSFS
1 Drop in accuracy due to programmer skill set.
2 Coding style is more distinct in more difficult tasks.
Table 8: Effect of Problem Difficulty on Coding Style
In this section, we discuss the conclusions we draw from
the experiments outlined in the previous section, limitations, as well
as questions raised by our results. In particular, we discuss the
difficulty of the different settings
considered, the effects of obfuscation, and limitations of
our current approach.
Problem Difficulty. The experiment with random
problems from random authors among seven years most
closely resembles a real world scenario. In such an experimental
setting, there is a chance that instead of only
identifying authors we are also identifying the properties
of a specific problemâ€™s solution, which results in a boost
in accuracy.
In contrast, our main experimental setting where all
authors have only answered the nine easiest problems is
possibly the hardest scenario, since we are training on the
same set of eight problems that all the authors have algorithmically
solved and try to identify the authors from
the test instances that are all solutions of the 9th problem. On the
upside, these test instances help us precisely
capture the differences between individual coding style
that represent the same functionality. We also see that
such a scenario is harder since the randomized dataset
has higher accuracy.
Classifying authors that have implemented the solution to a set of
difficult problems is easier than identifying authors with a set of
easier problems. This shows
to eliminate instances where the classifierâ€™s confidence is
below a threshold. After the verification step, instances
that do not belong to the set of known authors can be
separated from the dataset to be excluded or for further
manual analysis.
tural features to achieve higher accuracies at larger scales
and the first to study how code obfuscation affects code
There has also been some code stylometry work that
focused on manual analysis and case studies. Spafford
and Weeber [29] suggest that use of lexical features such
as variable names, formatting and comments, as well as
some syntactic features such as usage of keywords, scoping and
presence of bugs could aid in source code attribution but they do not
present results or a case study
experiment with a formal approach. Gray et al. [15]
identify three categories in code stylometry: the layout
of the code, variable and function naming conventions,
types of data structures being used and also the cyclomatic complexity
of the code obtained from the control
flow graph. They do not mention anything about the syntactic
characteristics of code, which could potentially be
a great marker of coding style that reveals the usage of
programming languageâ€™s grammar. Their case study is
based on a manual analysis of three worms, rather than
a statistical learning approach. Hayes and Offutt [16]
examine coding style in source code by their consistent
programmer hypothesis. They focused on lexical and
layout features, such as the occurrence of semicolons,
operators and constants. Their dataset consisted of 20
programmers and the analysis was not automated. They
concluded that coding style exists through some of their
features and professional programmers have a stronger
programming style compared to students. In our results
in Section 4.3.6, we also show that more advanced programmers have a
more identifying coding style.
There is also a great deal of research on plagiarism
detection which is carried out by identifying the similarities between
different programs. For example, there is a
widely used tool called Moss that originated from Stanford University
for detecting software plagiarism. Moss
[6] is able to analyze the similarities of code written by
different programmers. Rosenblum et al. [27] present a
novel program representation and techniques that automatically detect
the stylistic features of binary code.
Related Work
Our work is inspired by the research done on authorship
attribution of unstructured or semi-structured text [5, 22].
In this section, we discuss prior work on source code
authorship attribution. In general, such work (Table 9)
looks at smaller scale problems, does not use structural
features, and achieves lower accuracies than our work.
The highest accuracies in the related work are
achieved by Frantzeskou et al. [12, 14]. They used 1,500
7-grams to reach 97% accuracy with 30 programmers.
They investigated the high-level features that contribute
to source code authorship attribution in Java and Common Lisp. They
determined the importance of each feature by iteratively excluding one
of the features from the
feature set. They showed that comments, layout features
and naming patterns have a strong influence on the author
classification accuracy. They used more training
data (172 line of code on average) than us (70 lines of
code). We replicated their experiments on a 30 programmer subset of
our C++ data set, with eleven files containing 70 lines of code on
average and no comments. We
reach 76.67% accuracy with 6-grams, and 76.06% accuracy with 7-grams.
When we used a 6 and 7-gram feature set on 250 programmers with 9
files, we got 63.42%
accuracy. With our original feature set, we get 98% accuracy on 250 programmers.
The largest number of programmers studied in the related work was 46
programmers with 67.2% accuracy.
Ding and Samadzadeh [10] use statistical methods for
authorship attribution in Java. They show that among
lexical, keyword and layout properties, layout metrics
have a more important role than others which is not the
case in our analysis.
There are also a number of smaller scale, lower accuracy approaches in
the literature [9, 11, 18â€“21, 28],
shown in Table 9, all of which we significantly outperform. These
approaches use a combination of layout and
lexical features.
The only other work to explore structural features is
by Pellin [23], who used manually parsed abstract syntax
trees with an SVM that has a tree based kernel to classify
functions of two programmers. He obtains an average of
73% accuracy in a two class classification task. His approach
explained in the white paper can be extended to
our approach, so it is the closest to our work in the literature. This
work demonstrates that it is non-trivial to
use ASTs effectively. Our work is the first to use struc-
Related Work
Pellin [23]
MacDonell et al.[21]
Frantzeskou et al.[14]
Burrows et al. [9]
Elenbogen and Seliya [11]
Kothari et al. [18]
Lange and Mancoridis [20]
Krsul and Spafford [19]
Frantzeskou et al. [14]
Ding and Samadzadeh [10]
This work
This work
This work
This work
# of Programmers
Table 9: Comparison to Previous Results
7
Conclusion and Future Work
[1] The tigress diversifying c virtualizer, Source code stylometry has direct applications for privacy, security,
software forensics, plagiarism, copyright infringement disputes, and
authorship verification.
Source code stylometry is an immediate concern for programmers who
want to contribute code anonymously because de-anonymization is quite
possible. We introduce
the first principled use of syntactic features along with
lexical and layout features to investigate style in source
code. We can reach 94% accuracy in classifying 1,600
authors and 98% accuracy in classifying 250 authors
with eight training files per class. This is a significant
increase in accuracy and scale in source code authorship
attribution. In particular, it shows that source code authorship
attribution with the Code Stylometry Feature Set
scales even better than regular stylometric authorship attribution, as
these methods can only identify individuals
in sets of 50 authors with slightly over 90% accuracy [see
4]. Furthermore, this performance is achieved by training
on only 550 lines of code or eight solution files, whereas
classical stylometric analysis requires 5,000 words.
Additionally, our results raise a number of questions
that motivate future research. First, as malicious code
is often only available in binary format, it would be interesting to
investigate whether syntactic features can be
partially preserved in binaries. This may require our feature set to
be improved in order to incorporate information obtained from control
flow graphs.
Second, we would also like to see if classification accuracy can be
further increased. For example, we would
like to explore whether using features that have joint information
gain alongside features that have information
gain by themselves improve performance. Moreover, designing features
that capture larger fragments of the abstract syntax tree could
provide improvements. These
changes (along with adding lexical and layout features)
may provide significant improvements to the Python results and help
generalize the approach further.
Finally, we would like to investigate whether code can
be automatically normalized to remove stylistic information while
preserving functionality and readability.
[2] Google code jam,  2014.
[3] Stunnix,  November 2014.
[4] A BBASI , A., AND C HEN , H. Writeprints: A stylometric approach
to identity-level identification and similarity detection in
cyberspace. ACM Trans. Inf. Syst. 26, 2 (2008), 1â€“29.
[5] A FROZ , S., B RENNAN , M., AND G REENSTADT, R. Detecting
hoaxes, frauds, and deception in writing style online. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE,
pp. 461â€“475.
[6] A IKEN , A., ET AL . Moss: A system for detecting software
plagiarism. University of Californiaâ€“Berkeley. See www. cs.
berkeley. edu/aiken/moss. html 9 (2005).
[7] B REIMAN , L. Random forests. Machine Learning 45, 1 (2001),
[8] B URROWS , S., AND TAHAGHOGHI , S. M. Source code authorship
attribution using n-grams. In Proc. of the Australasian Document
Computing Symposium (2007).
[9] B URROWS , S., U ITDENBOGERD , A. L., AND T URPIN , A. Application
of information retrieval techniques for source code authorship
attribution. In Database Systems for Advanced Applications (2009),
Springer, pp. 699â€“713.
[10] D ING , H., AND S AMADZADEH , M. H. Extraction of java program
fingerprints for software authorship identification. Journal
of Systems and Software 72, 1 (2004), 49â€“57.
[11] E LENBOGEN , B. S., AND S ELIYA , N. Detecting outsourced student
programming assignments. Journal of Computing Sciences
in Colleges 23, 3 (2008), 50â€“57.
[12] F RANTZESKOU , G., M AC D ONELL , S., S TAMATATOS , E., AND
G RITZALIS , S. Examining the significance of high-level programming
features in source code author classification. Journal
of Systems and Software 81, 3 (2008), 447â€“460.
[13] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S.,
C HASKI , C. E., AND H OWALD , B. S. Identifying authorship
by byte-level n-grams: The source code author profile (scap)
method. International Journal of Digital Evidence 6, 1 (2007),
[14] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S., AND
K ATSIKAS , S. Effective identification of source code authors
using byte-level information. In Proceedings of the 28th International
Conference on Software Engineering (2006), ACM,
pp. 893â€“896.
This material is based on work supported by the ARO
(U.S. Army Research Office) Grant W911NF-14-10444, the DFG (German
Research Foundation) under the
project DEVIL (RI 2469/1-1), and AWS in Education
Research Grant award. Any opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not necessarily reflect
those of the ARO, DFG, and AWS.
[15] G RAY, A., S ALLIS , P., AND M AC D ONELL , S. Software
forensics: Extending authorship analysis techniques to computer
[16] H AYES , J. H., AND O FFUTT, J. Recognizing authors: an
examination of the consistent programmer hypothesis. Software Testing,
Verification and Reliability 20, 4 (2010), 329â€“356.
[17] I NOCENCIO , R. U.s. programmer outsources own job to china,
surfs cat videos, January 2013.
A
[18] KOTHARI , J., S HEVERTALOV, M., S TEHLE , E., AND M AN CORIDIS ,
S. A probabilistic approach to source code authorship
identification. In Information Technology, 2007. ITNGâ€™07. Fourth
International Conference on (2007), IEEE, pp. 243â€“248.
Appendix: Keywords and Node Types
[19] K RSUL , I., AND S PAFFORD , E. H. Authorship analysis:
Identifying the author of a program. Computers & Security 16, 3
(1997), 233â€“257.
[20] L ANGE , R. C., AND M ANCORIDIS , S. Using code metric histograms
and genetic algorithms to perform author identification
for software forensics. In Proceedings of the 9th Annual Conference on
Genetic and Evolutionary Computation (2007), ACM,
pp. 2082â€“2089.
[21] M AC D ONELL , S. G., G RAY, A. R., M AC L ENNAN , G., AND
S ALLIS , P. J. Software forensics for discriminating between
program authors using case-based reasoning, feedforward neural
networks and multiple discriminant analysis. In Neural Information
Processing, 1999. Proceedings. ICONIPâ€™99. 6th International
Conference on (1999), vol. 1, IEEE, pp. 66â€“71.
[22] NARAYANAN , A., PASKOV, H., G ONG , N. Z., B ETHENCOURT,
J., S TEFANOV, E., S HIN , E. C. R., AND S ONG , D. On the
feasibility of internet-scale author identification. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE, pp. 300â€“
[23] P ELLIN , B. N. Using classification techniques to determine
source code authorship. White Paper: Department of Computer
Science, University of Wisconsin (2000).
[24] P IKE , R. The sherlock plagiarism detector, 2011.
Table 10: Abstract syntax tree node types
[25] P RECHELT, L., M ALPOHL , G., AND P HILIPPSEN , M. Finding
plagiarisms among a set of programs with jplag. J. UCS 8, 11
(2002), 1016.
Table 10 lists the AST node types generated by Joern
that were incorporated to the feature set. Table 11 shows
the C++ keywords used in the feature set.
[26] Q UINLAN , J. Induction of decision trees. Machine learning 1, 1
(1986), 81â€“106.
[27] ROSENBLUM , N., Z HU , X., AND M ILLER , B. Who wrote this
code? identifying the authors of program binaries. Computer
Securityâ€“ESORICS 2011 (2011), 172â€“189.
[31] W IKIPEDIA. Saeed Malekpour, 2014. [Online; accessed 04November-2014].
[32] YAMAGUCHI , F., G OLDE , N., A RP, D., AND R IECK , K. Modeling
and discovering vulnerabilities with code property graphs. In
Proc of IEEE Symposium on Security and Privacy (S&P) (2014).
[33] YAMAGUCHI , F., W RESSNEGGER , C., G ASCON , H., AND
R IECK , K. Chucky: Exposing missing checks in source code
for vulnerability discovery. In Proceedings of the 2013 ACM
SIGSAC Conference on Computer & Communications Security
(2013), ACM, pp. 499â€“510.
[28] S HEVERTALOV, M., KOTHARI , J., S TEHLE , E., AND M AN CORIDIS ,
S. On the use of discretized source code metrics for author
identification. In Search Based Software Engineering, 2009
1st International Symposium on (2009), IEEE, pp. 69â€“78.
[29] S PAFFORD , E. H., AND W EEBER , S. A. Software forensics:
Can we track code to its authors? Computers & Security 12, 6
(1993), 585â€“595.
[30] S TOLERMAN , A., OVERDORF, R., A FROZ , S., AND G REEN STADT, R.
Classify, but verify: Breaking the closed-world assumption in
stylometric authorship attribution. In IFIP Working
Group 11.9 on Digital Forensics (2014), IFIP.
Table 11: C++ keywords
B
Appendix: Original vs Obfuscated Code
Figure 6: A code sample X
Figure 6 shows a source code sample X from our
dataset that is 21 lines long. After obfuscation with Tigress, sample
X became 537 lines long. Figure 7 shows
the first 13 lines of the obfuscated sample X.
Figure 7: Code sample X after obfuscation

@_date: unknown_date
@_author: i 
@_subject: unknown_subject 
programmers who had the solutions to the same set of
9 problems. We extracted only the features that had positive
information gain in 2014â€™s dataset that was used as
Where V j (i) = 1 if the jth tree voted for class i and
0 otherwise, and |T | f denotes the total number of trees
in forest f . Note that by construction, âˆ‘i P(Ci ) = 1 and
P(Ci ) â‰¥ 0 âˆ€ i, allowing us to treat P(Ci ) as a probability
There was one correct classification made with 13.7%
confidence. This suggests that we can use a threshold between 13.7%
and 15% confidence level for verification,
and manually analyze the classifications that did not pass
the confidence threshold or exclude them from results.
We picked an aggressive threshold of 15% and to validate it, we
trained a random forest classifier on the same
set of 30 programmers 270 code samples. We tested on
150 different files from the programmers in the training
set. There were 6 classifications below the 15% threshold
and two of them were misclassified. We took another set
of 420 test files from 30 programmers that were not in the
training set. All the files from the 30 programmers were
attributed to one of the 30 programmers in the training
set since this is a closed world classification task, however, the
highest confidence level in these classifications
was 14.7%. The 15% threshold catches all the instances
that do not belong to the programmers in the suspect set,
gets rid of 2 misclassifications and 4 correct classifications.
Consequently, when we see a classification with
less than a threshold value, we can reject the classification and
attribute the test instance to an unknown suspect.
the main dataset to implement the approach. The classification
accuracy was 96.83%, which is higher than the
95.07% accuracy obtained in 2014â€™s dataset.
The high accuracy of validation results in Table 5 show
that we identified the important features of code stylometry and found
a stable feature set. This feature set does
not necessarily represent the exact features for all possible
datasets. For a given dataset that has ground truth
information on authorship, following the same approach
should generate the most important features that represent coding
style in that particular dataset.
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 250 from 2014
A = 250 from 2012
A = 250 all years
F = 9 from 2014
F = 9 from 2014
F â‰¥ 9 all years
Average accuracy after 10 iterations with IG-CSFS features
Table 5: Validation Experiments
Mutliclass Open World Task
The experiments in this section can be used in software
forensics to find out the programmer of a piece of malware. In
software forensics, the analyst does not know if
source code belongs to one of the programmers in the
candidate set of programmers. In such cases, we can
classify the anonymous source code, and if the majority
number of votes of trees in the random forest is below a
certain threshold, we can reject the classification considering the
possibility that it might not belong to any of the
classes in the training data. By doing so, we can scale
our approach to an open world scenario, where we might
not have encountered the suspect before. As long as we
determine a confidence threshold based on training data
[30], we can calculate the probability that an instance
belongs to one of the programmers in the set and accordingly accept or
reject the classification.
We performed 270 classifications in a 30-class problem using all the
features to determine the confidence
threshold based on the training data. The accuracy was
96.67%. There were 9 misclassifications and all of them
were classified with less than 15% confidence by the
classifier. The class probability or classification confidence that
source code fragment C is of class i is calculated by taking the
percentage of trees in the random
forest that voted for that particular class, as follows2:
P(Ci ) =
âˆ‘ j V j (i)
Two-class Closed World Task
Source code author identification could automatically
deal with source code copyright disputes without requiring manual
analysis by an objective code investigator.
A copyright dispute on code ownership can be resolved
by comparing the styles of both parties claiming to have
generated the code. The style of the disputed code can
be compared to both partiesâ€™ other source code to aid in
the investigation. To imitate such a scenario, we took
60 different pairs of programmers, each with 9 solution
files. We used a random forest and 9-fold cross validation
to classify two programmersâ€™ source code. The average
classification accuracy using CSFS set is 100.00% and
100.00% with the information gain features.
Two-class/One-class Open World Task
Another two-class machine learning task can be formulated for
authorship verification. We suspect Mallory of
plagiarizing, so we mix in some code of hers with a large
sample of other people, test, and see if the disputed code
gets classified as hers or someone elseâ€™s. If it gets classified as
hers, then it was with high probability really
written by her. If it is classified as someone elseâ€™s, it
really was someone elseâ€™s code. This could be an open
world problem and the person that originally wrote the
code could be a previously unknown programmer.
This is a two-class problem with classes Mallory and
others. We train on Malloryâ€™s solutions to problems a,
b, c, d, e, f, g, h. We also train on programmer Aâ€™s solution to
problem a, programmer Bâ€™s solution to problem b,
programmer Câ€™s solution to problem c, programmer Dâ€™s
solution to problem d, programmer Eâ€™s solution to problem e,
programmer Fâ€™s solution to problem f, programmer Gâ€™s solution to
problem g, programmer Hâ€™s solution
to problem h and put them in one class called ABCDEFGH. We train a
random forest classifier with 300 trees
on classes Mallory and ABCDEFGH. We have 6 test instances from Mallory
and 6 test instances from another
programmer ZZZZZZ, who is not in the training set.
These experiments have been repeated in the exact same setting with 80
different sets of programmers
ABCDEFGH, ZZZZZZ and Mallorys. The average classification accuracy for
Mallory using the CSFS set is
100.00%. ZZZZZZâ€™s test instances are classified as programmer
ABCDEFGH 82.04% of the time, and classified as Mallory for the rest of
the time while using the
CSFS. Depending on the amount of false positives we
are willing to accept, we can change the operating point
on the ROC curve.
These results are also promising for use in cases where
a piece of code is suspected to be plagiarized. Following
the same approach, if the classification result of the piece
of code is someone other than Mallory, that piece of code
was with very high probability not written by Mallory.
Additional Insights
scales. We are able to de-anonymize 1,600 programmers
using 32GB memory within one hour. Alternately, we
can use 40 trees and get nearly the same accuracy (within
0.5%) in a few minutes.
Figure 3: Large Scale De-anonymization
Training Data and Features
We selected different sets of 62 programmers that had F
solution files, from 2 up to 14. Each dataset has the solutions to the
same set of F problems by different sets
of programmers. Each dataset consisted of programmers
that were able to solve exactly F problems. Such an experimental setup
makes it possible to investigate the effect of programmer skill set on
coding style. The size of
the datasets were limited to 62, because there were only
62 contestants with 14 files. There were a few contestants with up to
19 files but we had to exclude them since
there were not enough programmers to compare them.
The same set of F problems were used to ensure that
the coding style of the programmer is being classified
and not the properties of possible solutions of the problem itself. We
were able to capture personal programming style since all the
programmers are coding the same
functionality in their own ways.
Stratified F-fold cross validation was used by training
on everyoneâ€™s (F âˆ’ 1) solutions and testing on the F th
problem that did not appear in the training set. As a result, the
problems in the test files were encountered for
the first time by the classifier.
We used a random forest with 300 trees and (logM)+1
features with F-fold stratified cross validation, first with
the Code Stylometry Feature Set (CSFS) and then with
the CSFSâ€™s features that had information gain.
Figure 4 shows the accuracy from 13 different sets of
62 programmers with 2 to 14 solution files, and consequently 1 to 13
training files. The CSFS reaches an optimal training set size at 9
solution files, where the classifier trains on 8 (F âˆ’ 1) solutions.
In the datasets we constructed, as the number of files
increase and problems from more advanced rounds are
included, the average line of code (LOC) per file also
increases. The average lines of code per source code
in the dataset is 70. Increased number of lines of code
might have a positive effect on the accuracy but at the
same time it reveals programmerâ€™s choice of program
We collected a larger dataset of 1,600 programmers from
various years. Each of the programmers had 9 source
code samples. We created 7 subsets of this large dataset
in differing sizes, with 250, 500, 750, 1,000, 1,250,
1,500, and 1,600 programmers. These subsets are useful to understand
how well our approach scales. We extracted the specific features that
had information gain in
the main 250 programmer dataset from this large dataset.
In theory, we need to use more trees in the random forest as the
number of classes increase to decrease variance, but we used fewer
trees compared to smaller experiments. We used 300 trees in the random
forest to
run the experiments in a reasonable amount of time with
a reasonable amount of memory. The accuracy did not
decrease too much when increasing the number of programmers. This
result shows that information gain features are robust against changes
in class and are important properties of programmersâ€™ coding styles.
following Figure 3 demonstrates how well our method
detectable change in the performance of the classifier for
this sample. The results are summarized in Table 6.
We took the maximum number of programmers, 20,
that had solutions to 9 problems in C and obfuscated
the code (see example in Appendix B) using a much
more sophisticated open source obfuscator called Tigress
[1]. In particular, Tigress implements function virtualization, an
obfuscation technique that turns functions into
interpreters and converts the original program into corresponding
bytecode. After applying function virtualization, we were less able to
effectively de-anonymize
programmers, so it has potential as a countermeasure to
programmer de-anonymization. However, this obfuscation comes at a
cost. First of all, the obfuscated code is
neither readable nor maintainable, and is thus unsuitable
for an open source project. Second, the obfuscation adds
significant overhead (9 times slower) to the runtime of
the program, which is another disadvantage.
The accuracy with the information gain feature set on
the obfuscated dataset is reduced to 67.22%. When we
limit the feature set to AST node bigrams, we get 18.89%
accuracy, which demonstrates the need for all feature
types in certain scenarios. The accuracy on the same
dataset when the code is not obfuscated is 95.91%.
Figure 4: Training Data
length in implementing the same functionality. On the
other hand, the average line of code of the 7 easier (76
LOC) or difficult problems (83 LOC) taken from contestants that were
able to complete 14 problems, is higher
than the average line of code (68) of contestants that
were able to solve only 7 problems. This shows that
programmers with better skills tend to write longer code
to solve Google Code Jam problems. The mainstream
idea is that better programmers write shorter and cleaner
code which contradicts with line of code statistics in our
datasets. Google Code Jam contestants are supposed to
optimize their code to process large inputs with faster
performance. This implementation strategy might be
leading to advanced programmers implementing longer
solutions for the sake of optimization.
We took the dataset with 62 programmers each with
9 solutions. We get 97.67% accuracy with all the features and 99.28%
accuracy with the information gain features. We excluded all the
syntactic features and the accuracy dropped to 88.89% with all the
non-syntactic features and 88.35% with the information gain features
the non-syntactic feature set. We ran another experiment
using only the syntactic features and obtained 96.06%
with all the syntactic features and 96.96% with the information gain
features of the syntactic feature set. Most
of the classification power is preserved with the syntactic features,
and using non-syntactic features leads to a
significant decline in accuracy.
Results w/o
*Information gain features
Table 6: Effect of Obfuscation on De-anonymization
Relaxed Classification
The goal here is to determine whether it is possible to reduce the
number of suspects using code stylometry. Reducing the set of suspects
in challenging cases, such as
having too many suspects, would reduce the effort required to manually
find the actual programmer of the
In this section, we performed classification on the
main 250 programmer dataset from 2014 using the information gain
features. The classification was relaxed
to a set of top R suspects instead of exact classification
of the programmer. The relaxed factor R varied from 1
to 10. Instead of taking the highest majority vote of the
decisions trees in the random forest, the highest R majority vote
decisions were taken and the classification result
was considered correct if the programmer was in the set
of top R highest voted classes. The accuracy does not
improve much after the relaxed factor is larger than 5.
We took a dataset with 9 solution files and 20 programmers and
obfuscated the code using an off-the-shelf C++
obfuscator called stunnix [3]. The accuracy with the information gain
code stylometry feature set on the obfuscated dataset is 98.89%. The
accuracy on the same
dataset when the code is not obfuscated is 100.00%. The
obfuscator refactored function and variable names, as
well as comments, and stripped all the spaces, preserving the
functionality of code without changing the structure of the program.
Obfuscating the data produced little
programming languages by implementing the layout and
lexical features as well as using a fuzzy parser.
Classification IG
Top-5 IG
Figure 5: Relaxed Classification with 250 Programmers
Table 7: Generalizing to Other Programming Languages
Generalizing the Method
Features derived from ASTs can represent coding styles
in various languages. These features are applicable in
cases when lexical and layout features may be less discriminating due
to formatting standards and reliance on
whitespace and other â€˜lexicalâ€™ features as syntax, such
as Pythonâ€™s PEP8 formatting. To show that our method
generalizes, we collected source code of 229 Python programmers from
GCJâ€™s 2014 competition. 229 programmers had exactly 9 solutions.
Using only the Python
equivalents of syntactic features listed in Table 4 and
9-fold cross-validation, the average accuracy is 53.91%
for top-1 classification, 75.69% for top-5 relaxed attribution. The
largest set of programmers to all work on
the same set of 9 problems was 23 programmers. The
average accuracy in identifying these 23 programmers is
87.93% for top-1 and 99.52% for top-5 relaxed attribution. The same
classification tasks using the information
gain features are also listed in Table 7. The overall accuracy in
datasets composed of Python code are lower
than C++ datasets. In Python datasets, we only used
syntactic features from ASTs that were generated by a
parser that was not fuzzy. The lack of quantity and specificity of
features accounts for the decreased accuracy.
The Python datasetâ€™s information gain features are significantly
fewer in quantity, compared to C++ datasetâ€™s
information gain features. Information gain only keeps
features that have discriminative value all on their own.
If two features only provide discriminative value when
used together, then information gain will discard them.
So if a lot of the features for the Python set are only
jointly discriminative (and not individually discriminative), then the
information gain criteria may be removing
features that in combination could effectively discriminate between
authors. This might account for the decrease when using information
gain features. While in
the context of other results in this paper the results in Table 7
appear lackluster, it is worth noting that even this
preliminary test using only syntactic features has comparable
performance to other prior work at a similar scale
(see Section 6 and Table 9), demonstrating the utility
of syntactic features and the relative ease of generating
them for novel programming languages. Nevertheless, a
CSFS equivalent feature set can be generated for other
Software Engineering Insights
We wanted to investigate if programming style is consistent throughout
years. We found the contestants that had
the same username and country information both in 2012
and 2014. We assumed that these are the same people but
there is a chance that they might be different people. In
2014, someone else might have picked up the same username from the
same country and started using it. We are
going to ignore such a ground truth problem for now and
assume that they are the same people.
We took a set of 25 programmers from 2012 that were
also contestants in 2014â€™s competition. We took 8 files
from their submissions in 2012 and trained a random forest classifier
with 300 trees using CSFS. We had one instance from each one of the
contestants from 2014. The
correct classification of these test instances from 2014
is 96.00%. The accuracy dropped to 92.00% when using
only information gain features, which might be due to the
aggressive elimination of pairs of features that are jointly
discriminative. These 25 programmersâ€™ 9 files from 2014
had a correct classification accuracy of 98.04%. These
results indicate that coding style is preserved up to some
degree throughout years.
To investigate problem difficultyâ€™s effect on coding
style, we created two datasets from 62 programmers that
had exactly 14 solution files. Table 8 summarizes the
following results. A dataset with 7 of the easier problems out of 14
resulted in 95.62% accuracy. A dataset
with 7 of the more difficult problems out of 14 resulted
in 99.31% accuracy. This might imply that more difficult
coding tasks have a more prevalent reflection of coding
style. On the other hand, the dataset that had 62 programmers with
exactly 7 of the easier problems resulted
in 91.24% accuracy, which is a lot lower than the accuracy obtained
from the dataset whose programmers were
able to advance to solve 14 problems. This might indicate that,
programmers who are advanced enough to answer 14 problems likely have
more unique coding styles
compared to contestants that were only able to solve the
first 7 problems.
To investigate the possibility that contestants who are
able to advance further in the rounds have more unique
coding styles, we performed a second round of experiments on
comparable datasets. We took the dataset with
12 solution files and 62 programmers. A dataset with 6
of the easier problems out of 12 resulted in 91.39% accuracy. A
dataset with 6 of the more difficult problems
out of 12 resulted in 94.35% accuracy. These results are
higher than the dataset whose programmers were only
able to solve the easier 6 problems. The dataset that had
62 programmers with exactly 6 of the easier problems
resulted in 90.05% accuracy.
that coding style is reflected more through difficult programming
tasks. This might indicate that programmers
come up with unique solutions and preserve their coding style more
when problems get harder. On the other
hand, programmers with a better skill set have a prevalent
coding style which can be identified more easily compared to
contestants who were not able to advance as
far in the competition. This might indicate that as programmers become
more advanced, they build a stronger
coding style compared to novices. There is another possibility that
maybe better programmers start out with a
more unique coding style.
Effects of Obfuscation. A malware author or plagiarizing programmer
might deliberately try to hide his
source code by obfuscation. Our experiments indicate
that our method is resistant to simple off-the-shelf obfuscators such
as stunnix, that make code look cryptic while
preserving functionality. The reason for this success is
that the changes stunnix makes to the code have no effect
on syntactic features, e.g., removal of comments, changing of names,
and stripping of whitespace.
In contrast, sophisticated obfuscation techniques such
as function virtualization hinder de-anonymization to
some degree, however, at the cost of making code
unreadable and introducing a significant performance
penalty. Unfortunately, unreadability of code is not acceptable for
open-source projects, while it is no problem
for attackers interested in covering their tracks. Developing methods
to automatically remove stylometric information from source code
without sacrificing readability
is therefore a promising direction for future research.
Limitations. We have not considered the case where
a source file might be written by a different author than
the stated contestant, which is a ground truth problem
that we cannot control. Moreover, it is often the case that
code fragments are the work of multiple authors. We
plan to extend this work to study such datasets. To shed
light on the feasibility of classifying such code, we are
currently working with a dataset of git commits to open
source projects. Our parser works on code fragments
rather than complete code, consequently we believe this
analysis will be possible.
Another fundamental problem for machine learning
classifiers are mimicry attacks. For example, our classifier may be
evaded by an adversary by adding extra
dummy code to a file that closely resembles that of another
programmer, albeit without affecting the programâ€™s
behavior. This evasion is possible, but trivial to resolve
when an analysts verifies the decision.
Finally, we cannot be sure whether the original author is actually a
Google Code Jam contestant. In this
case, we can detect those by a classify and then verify
approach as explained in Stolerman et al.â€™s work [30].
Each classification could go through a verification step
A =  F = max  completed
N =  included in dataset (N â‰¤ F)
A = 62
F = 14
F = 12
Average accuracy after 10 iterations while using CSFS
Average accuracy after 10 iterations while using IG CSFS
1 Drop in accuracy due to programmer skill set.
2 Coding style is more distinct in more difficult tasks.
Table 8: Effect of Problem Difficulty on Coding Style
In this section, we discuss the conclusions we draw from
the experiments outlined in the previous section, limitations, as well
as questions raised by our results. In particular, we discuss the
difficulty of the different settings
considered, the effects of obfuscation, and limitations of
our current approach.
Problem Difficulty. The experiment with random
problems from random authors among seven years most
closely resembles a real world scenario. In such an experimental
setting, there is a chance that instead of only
identifying authors we are also identifying the properties
of a specific problemâ€™s solution, which results in a boost
in accuracy.
In contrast, our main experimental setting where all
authors have only answered the nine easiest problems is
possibly the hardest scenario, since we are training on the
same set of eight problems that all the authors have algorithmically
solved and try to identify the authors from
the test instances that are all solutions of the 9th problem. On the
upside, these test instances help us precisely
capture the differences between individual coding style
that represent the same functionality. We also see that
such a scenario is harder since the randomized dataset
has higher accuracy.
Classifying authors that have implemented the solution to a set of
difficult problems is easier than identifying authors with a set of
easier problems. This shows
to eliminate instances where the classifierâ€™s confidence is
below a threshold. After the verification step, instances
that do not belong to the set of known authors can be
separated from the dataset to be excluded or for further
manual analysis.
tural features to achieve higher accuracies at larger scales
and the first to study how code obfuscation affects code
There has also been some code stylometry work that
focused on manual analysis and case studies. Spafford
and Weeber [29] suggest that use of lexical features such
as variable names, formatting and comments, as well as
some syntactic features such as usage of keywords, scoping and
presence of bugs could aid in source code attribution but they do not
present results or a case study
experiment with a formal approach. Gray et al. [15]
identify three categories in code stylometry: the layout
of the code, variable and function naming conventions,
types of data structures being used and also the cyclomatic complexity
of the code obtained from the control
flow graph. They do not mention anything about the syntactic
characteristics of code, which could potentially be
a great marker of coding style that reveals the usage of
programming languageâ€™s grammar. Their case study is
based on a manual analysis of three worms, rather than
a statistical learning approach. Hayes and Offutt [16]
examine coding style in source code by their consistent
programmer hypothesis. They focused on lexical and
layout features, such as the occurrence of semicolons,
operators and constants. Their dataset consisted of 20
programmers and the analysis was not automated. They
concluded that coding style exists through some of their
features and professional programmers have a stronger
programming style compared to students. In our results
in Section 4.3.6, we also show that more advanced programmers have a
more identifying coding style.
There is also a great deal of research on plagiarism
detection which is carried out by identifying the similarities between
different programs. For example, there is a
widely used tool called Moss that originated from Stanford University
for detecting software plagiarism. Moss
[6] is able to analyze the similarities of code written by
different programmers. Rosenblum et al. [27] present a
novel program representation and techniques that automatically detect
the stylistic features of binary code.
Related Work
Our work is inspired by the research done on authorship
attribution of unstructured or semi-structured text [5, 22].
In this section, we discuss prior work on source code
authorship attribution. In general, such work (Table 9)
looks at smaller scale problems, does not use structural
features, and achieves lower accuracies than our work.
The highest accuracies in the related work are
achieved by Frantzeskou et al. [12, 14]. They used 1,500
7-grams to reach 97% accuracy with 30 programmers.
They investigated the high-level features that contribute
to source code authorship attribution in Java and Common Lisp. They
determined the importance of each feature by iteratively excluding one
of the features from the
feature set. They showed that comments, layout features
and naming patterns have a strong influence on the author
classification accuracy. They used more training
data (172 line of code on average) than us (70 lines of
code). We replicated their experiments on a 30 programmer subset of
our C++ data set, with eleven files containing 70 lines of code on
average and no comments. We
reach 76.67% accuracy with 6-grams, and 76.06% accuracy with 7-grams.
When we used a 6 and 7-gram feature set on 250 programmers with 9
files, we got 63.42%
accuracy. With our original feature set, we get 98% accuracy on 250 programmers.
The largest number of programmers studied in the related work was 46
programmers with 67.2% accuracy.
Ding and Samadzadeh [10] use statistical methods for
authorship attribution in Java. They show that among
lexical, keyword and layout properties, layout metrics
have a more important role than others which is not the
case in our analysis.
There are also a number of smaller scale, lower accuracy approaches in
the literature [9, 11, 18â€“21, 28],
shown in Table 9, all of which we significantly outperform. These
approaches use a combination of layout and
lexical features.
The only other work to explore structural features is
by Pellin [23], who used manually parsed abstract syntax
trees with an SVM that has a tree based kernel to classify
functions of two programmers. He obtains an average of
73% accuracy in a two class classification task. His approach
explained in the white paper can be extended to
our approach, so it is the closest to our work in the literature. This
work demonstrates that it is non-trivial to
use ASTs effectively. Our work is the first to use struc-
Related Work
Pellin [23]
MacDonell et al.[21]
Frantzeskou et al.[14]
Burrows et al. [9]
Elenbogen and Seliya [11]
Kothari et al. [18]
Lange and Mancoridis [20]
Krsul and Spafford [19]
Frantzeskou et al. [14]
Ding and Samadzadeh [10]
This work
This work
This work
This work
# of Programmers
Table 9: Comparison to Previous Results
7
Conclusion and Future Work
[1] The tigress diversifying c virtualizer, Source code stylometry has direct applications for privacy, security,
software forensics, plagiarism, copyright infringement disputes, and
authorship verification.
Source code stylometry is an immediate concern for programmers who
want to contribute code anonymously because de-anonymization is quite
possible. We introduce
the first principled use of syntactic features along with
lexical and layout features to investigate style in source
code. We can reach 94% accuracy in classifying 1,600
authors and 98% accuracy in classifying 250 authors
with eight training files per class. This is a significant
increase in accuracy and scale in source code authorship
attribution. In particular, it shows that source code authorship
attribution with the Code Stylometry Feature Set
scales even better than regular stylometric authorship attribution, as
these methods can only identify individuals
in sets of 50 authors with slightly over 90% accuracy [see
4]. Furthermore, this performance is achieved by training
on only 550 lines of code or eight solution files, whereas
classical stylometric analysis requires 5,000 words.
Additionally, our results raise a number of questions
that motivate future research. First, as malicious code
is often only available in binary format, it would be interesting to
investigate whether syntactic features can be
partially preserved in binaries. This may require our feature set to
be improved in order to incorporate information obtained from control
flow graphs.
Second, we would also like to see if classification accuracy can be
further increased. For example, we would
like to explore whether using features that have joint information
gain alongside features that have information
gain by themselves improve performance. Moreover, designing features
that capture larger fragments of the abstract syntax tree could
provide improvements. These
changes (along with adding lexical and layout features)
may provide significant improvements to the Python results and help
generalize the approach further.
Finally, we would like to investigate whether code can
be automatically normalized to remove stylistic information while
preserving functionality and readability.
[2] Google code jam,  2014.
[3] Stunnix,  November 2014.
[4] A BBASI , A., AND C HEN , H. Writeprints: A stylometric approach
to identity-level identification and similarity detection in
cyberspace. ACM Trans. Inf. Syst. 26, 2 (2008), 1â€“29.
[5] A FROZ , S., B RENNAN , M., AND G REENSTADT, R. Detecting
hoaxes, frauds, and deception in writing style online. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE,
pp. 461â€“475.
[6] A IKEN , A., ET AL . Moss: A system for detecting software
plagiarism. University of Californiaâ€“Berkeley. See www. cs.
berkeley. edu/aiken/moss. html 9 (2005).
[7] B REIMAN , L. Random forests. Machine Learning 45, 1 (2001),
[8] B URROWS , S., AND TAHAGHOGHI , S. M. Source code authorship
attribution using n-grams. In Proc. of the Australasian Document
Computing Symposium (2007).
[9] B URROWS , S., U ITDENBOGERD , A. L., AND T URPIN , A. Application
of information retrieval techniques for source code authorship
attribution. In Database Systems for Advanced Applications (2009),
Springer, pp. 699â€“713.
[10] D ING , H., AND S AMADZADEH , M. H. Extraction of java program
fingerprints for software authorship identification. Journal
of Systems and Software 72, 1 (2004), 49â€“57.
[11] E LENBOGEN , B. S., AND S ELIYA , N. Detecting outsourced student
programming assignments. Journal of Computing Sciences
in Colleges 23, 3 (2008), 50â€“57.
[12] F RANTZESKOU , G., M AC D ONELL , S., S TAMATATOS , E., AND
G RITZALIS , S. Examining the significance of high-level programming
features in source code author classification. Journal
of Systems and Software 81, 3 (2008), 447â€“460.
[13] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S.,
C HASKI , C. E., AND H OWALD , B. S. Identifying authorship
by byte-level n-grams: The source code author profile (scap)
method. International Journal of Digital Evidence 6, 1 (2007),
[14] F RANTZESKOU , G., S TAMATATOS , E., G RITZALIS , S., AND
K ATSIKAS , S. Effective identification of source code authors
using byte-level information. In Proceedings of the 28th International
Conference on Software Engineering (2006), ACM,
pp. 893â€“896.
This material is based on work supported by the ARO
(U.S. Army Research Office) Grant W911NF-14-10444, the DFG (German
Research Foundation) under the
project DEVIL (RI 2469/1-1), and AWS in Education
Research Grant award. Any opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not necessarily reflect
those of the ARO, DFG, and AWS.
[15] G RAY, A., S ALLIS , P., AND M AC D ONELL , S. Software
forensics: Extending authorship analysis techniques to computer
[16] H AYES , J. H., AND O FFUTT, J. Recognizing authors: an
examination of the consistent programmer hypothesis. Software Testing,
Verification and Reliability 20, 4 (2010), 329â€“356.
[17] I NOCENCIO , R. U.s. programmer outsources own job to china,
surfs cat videos, January 2013.
A
[18] KOTHARI , J., S HEVERTALOV, M., S TEHLE , E., AND M AN CORIDIS ,
S. A probabilistic approach to source code authorship
identification. In Information Technology, 2007. ITNGâ€™07. Fourth
International Conference on (2007), IEEE, pp. 243â€“248.
Appendix: Keywords and Node Types
[19] K RSUL , I., AND S PAFFORD , E. H. Authorship analysis:
Identifying the author of a program. Computers & Security 16, 3
(1997), 233â€“257.
[20] L ANGE , R. C., AND M ANCORIDIS , S. Using code metric histograms
and genetic algorithms to perform author identification
for software forensics. In Proceedings of the 9th Annual Conference on
Genetic and Evolutionary Computation (2007), ACM,
pp. 2082â€“2089.
[21] M AC D ONELL , S. G., G RAY, A. R., M AC L ENNAN , G., AND
S ALLIS , P. J. Software forensics for discriminating between
program authors using case-based reasoning, feedforward neural
networks and multiple discriminant analysis. In Neural Information
Processing, 1999. Proceedings. ICONIPâ€™99. 6th International
Conference on (1999), vol. 1, IEEE, pp. 66â€“71.
[22] NARAYANAN , A., PASKOV, H., G ONG , N. Z., B ETHENCOURT,
J., S TEFANOV, E., S HIN , E. C. R., AND S ONG , D. On the
feasibility of internet-scale author identification. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE, pp. 300â€“
[23] P ELLIN , B. N. Using classification techniques to determine
source code authorship. White Paper: Department of Computer
Science, University of Wisconsin (2000).
[24] P IKE , R. The sherlock plagiarism detector, 2011.
Table 10: Abstract syntax tree node types
[25] P RECHELT, L., M ALPOHL , G., AND P HILIPPSEN , M. Finding
plagiarisms among a set of programs with jplag. J. UCS 8, 11
(2002), 1016.
Table 10 lists the AST node types generated by Joern
that were incorporated to the feature set. Table 11 shows
the C++ keywords used in the feature set.
[26] Q UINLAN , J. Induction of decision trees. Machine learning 1, 1
(1986), 81â€“106.
[27] ROSENBLUM , N., Z HU , X., AND M ILLER , B. Who wrote this
code? identifying the authors of program binaries. Computer
Securityâ€“ESORICS 2011 (2011), 172â€“189.
[31] W IKIPEDIA. Saeed Malekpour, 2014. [Online; accessed 04November-2014].
[32] YAMAGUCHI , F., G OLDE , N., A RP, D., AND R IECK , K. Modeling
and discovering vulnerabilities with code property graphs. In
Proc of IEEE Symposium on Security and Privacy (S&P) (2014).
[33] YAMAGUCHI , F., W RESSNEGGER , C., G ASCON , H., AND
R IECK , K. Chucky: Exposing missing checks in source code
for vulnerability discovery. In Proceedings of the 2013 ACM
SIGSAC Conference on Computer & Communications Security
(2013), ACM, pp. 499â€“510.
[28] S HEVERTALOV, M., KOTHARI , J., S TEHLE , E., AND M AN CORIDIS ,
S. On the use of discretized source code metrics for author
identification. In Search Based Software Engineering, 2009
1st International Symposium on (2009), IEEE, pp. 69â€“78.
[29] S PAFFORD , E. H., AND W EEBER , S. A. Software forensics:
Can we track code to its authors? Computers & Security 12, 6
(1993), 585â€“595.
[30] S TOLERMAN , A., OVERDORF, R., A FROZ , S., AND G REEN STADT, R.
Classify, but verify: Breaking the closed-world assumption in
stylometric authorship attribution. In IFIP Working
Group 11.9 on Digital Forensics (2014), IFIP.
Table 11: C++ keywords
B
Appendix: Original vs Obfuscated Code
Figure 6: A code sample X
Figure 6 shows a source code sample X from our
dataset that is 21 lines long. After obfuscation with Tigress, sample
X became 537 lines long. Figure 7 shows
the first 13 lines of the obfuscated sample X.
Figure 7: Code sample X after obfuscation

@_date: unknown_date
@_author: eg parking fine 
@_subject: unknown_subject 
masters degree in mediation in germany
german primary edu
Some schools also have special mediators who are student volunteers trained
to resolve conflicts between their classmates or younger students.
i just know about it cause i know students there and they think adults are
crazy with all their conflicts all the time ... as there are easy solutions
they know

@_date: unknown_date
@_author: eg parking fine 
@_subject: unknown_subject 
masters degree in mediation in germany
german primary edu
Some schools also have special mediators who are student volunteers trained
to resolve conflicts between their classmates or younger students.
i just know about it cause i know students there and they think adults are
crazy with all their conflicts all the time ... as there are easy solutions
they know

@_date: 2019-12-10 19:45:29
@_author: Zach 
@_subject: No subject 
I am pleased to announce the availability of FSTC's "Understanding and
Countering the Phishing Threat," the summary whitepaper of findings and
recommendations of the FSTC Counter-Phishing Project. The whitepaper
contains valuable data, published here for the first time, including FSTC's
"Phishing Attack Life Cycle" and FSTC's "Taxonomy of Phishing Attacks." This
and all other project deliverables are located at
In addition to the whitepaper, the following deliverables are being made
available on the site, as follows:
TO ALL: "Results Summary: FSTC Counter-Phishing Solutions Survey": An
overview of the 60+ solutions currently offered on the marketplace, broken
down by where they map against the FSTC "Phishing Attack Life Cycle"
TO ALL: "Vocabulary of Phishing Terms": A glossary of terms used throughout
the project. The project team used these to "speak the same language" when
talking about the problem and potential solutions, whether internally, or
with vendors, or with customers
TO FSTC MEMBERS ONLY: "Results Summarized By Solution": identifies solutions
by company and product name as they map against the different phases of the
FSTC "Phishing Attack Life Cycle"
TO FSTC MEMBERS ONLY: "Directory of Survey Respondents": contact information
for each company/solution provider that responded to the survey
FOR PURCHASE: "Cost/Impact Spreadsheet Tool": a tool that provides a means
to estimate the direct and indirect costs/impacts of phishing to a financial
FSTC extends its gratitude to its member organizations for their efforts and
contributions in completing this important industry research, and to the
project's talented management team for helping our members realize their
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: 2019-12-10 19:45:29
@_author: Mike 
@_subject: No subject 
Dear "Joe":
So, you really DO want to become a member of Better A Millstone!
Well, on behalf of children everywhere, congratulations and thanks!
Please understand that to become a member of BAM a number of intrusive questions need to be asked because BAM must be in a position to vouch to law enforcement agencies to whom it provides intelligence and information that you have been checked out and that they can trust the information that you provide as valid and that you are of good character.
And after receiving and reviewing your application, to become a member of Better A Millstone, you will be required to go to your local law enforcement agency to be properly identified and to have them do a criminal history background check on you for forwarding to Better A Millstone.
And please know that we will only confirm your actual identity and location to law enforcement on an individual "need to know" basis for those cases that you are involved in helping them to solve.  And after receiving and reviewing your application, and becoming a member of Better A Millstone, we will only confirm your actual identity and location to law enforcement on an individual "need to know" basis for those cases that you are involved in helping them to solve or to another member of BAM with whom you are working.
Please answer the following questions accurately by typing in the requested information or responses and--when completed--simply return this e-mail in its entirety to  so that I and/or members of the Board of Directors of Better A Millstone may check out the information which you provide.
After reviewing your application--and within ten days or so--you will receive a phone call from me or one of the members of the Board of Directors to chat with you and get to know you.  And soon thereafter you will be notified as to your acceptance as a member of BAM.
Full Name:
Name/Nickname That You Like To Be Called By:
Date of Birth:
Place of Birth:
Social Security Number (if U.S. resident):
Citizen of What Country:
Married or Single:
Spouse's Name:
Any Children?
If So, Their Ages, Please:
Mail Address:
Zip Code or Postal Code:
Country, Area, &/or City Code & Phone Number:
Country, Area, &/or City Code & Fax Number:
Location of Principal Internet Access (Home or Office):
Local Hours You Are Usually on the Internet:
Main E-mail Address:
Secondary E-mail Address:
Your Occupation:
Education (high school, college, etc.):
What Languages Do You Speak?:
What Languages Do You Read?:
What Languages Do You Write?:
Please Detail All Professional Work with Children:
Please Detail All Volunteer Work with Children:
Any Felony Convictions of Any Kind?  (Yes or No):
If Yes, Please Provide Location, Date, Nature of Crime, and Sentence:
Any Misdemeanor Convictions Involving Children?  (Yes or No):
If Yes, Please Provide Location, Date, Nature of Crime, and Sentence:
Why Do You Wish to Become a Member of Better A Millstone?:
WHEN YOU HAVE COMPLETED FILLING OUT THIS FORM, PLEASE E-MAIL IT BACK TO BAM USING THE SAME E-MAIL ADDRESS FROM WHICH IT WAS SENT TO YOU -  - IN OTHER WORDS, CLICK ON "Reply", fill out the entire application (DO NOT DELETE ANYTHING!), and then click on "Send".
You will then be contacted by phone within approximately ten days.
And if you have any questions about any of this, please e-mail me, Mike Echols, at  and identify your questions in the subject line as follows:  "BAM Application Questions".
I thank you for your interest in becoming a member of Better A Millstone and for your concern about protecting children!
Most sincerely yours,
Mike Echols, Founder & Executive Director, Better A Millstone, Inc.
484-B Washington Street,  Monterey, California 93940  U.S.A.
Toll Free Voice Mail in the U.S.:  1-800-971-7818
Voice Mail from Other Countries:  area code 831-887-3362
Better A Millstone's Main Web Page: Help Kids Protect Themselves: Get your FREE download of MSN Explorer at

@_date: 2019-12-10 19:45:29
@_author: Government Management Best Practices Training Workshop 
@_subject: No subject 
exhibitors/sponsors/speakers.  For more information call Andrea Feinberg at
201-592-6477 or <>afeinberg at marketaccess.org **
To subscribe to your complimentary copy of Homeland Defense Journal, visit
our home page at Special Announcement: The Integrated Physical Security Handbook is now
available at  First print sold out. Second
printing just arrived. Published by Homeland Defense Journal and written by
security, physical security, and architects this book and on-line reference
library provide step-by-step guidance to building and facility managers in
government and private enterprise. Sample chapters and details on content
are at  .
Homeland Defense Journal Training Conference (R)
Credentialing and Identity Assurance Training Conference
Best Practices and Lessons Learned in HSPD-12, FIPS-201, REAL ID, WHTI
June 21, 2007
Capital Hilton
Washington, DC
Homeland Defense Journal is presenting the Physical Infrastructure
Technologies in Homeland Security: Evaluating and Implementing Technologies
& Techniques for Sustainability and Integration, a one-day conference on
June 20, 2007, at the same location, that will place special emphasis on IT
convergence, surveillance and inspections.
.- Register for both and save $100.
The following additional training courses are also available: (For details
go to - National Conference on Data Centers, May 9-10, 2007, Arlington, VA
- Crisis Management Plan Writing Workshop, May 30-31, 2007, Arlington, VA
- Emergency Preparedness for Government Facilities, June 4-5, 2007,
Arlington, VA
- CARVER Methodology - Target Analysis and Vulnerability Assessment: An
Interactive Three-Day Workshop on Conducting Facility Vulnerability
Assessments, June 12-14, 2007, Falls Church, VA
- Managing Today's Threats to Homeland Security Conference: With a Special
Focus on Chemical, Biological, Radiological, Nuclear, and Explosives
(CBRNE), June 27, 2007, Washington, DC
- Emergency Communication and Notification Solutions for Government and
Business Conference, June 28, 2007, Washington, DC
- Physical Security for Government Facilities, July 24-25, 2007, Memphis,
- COOP and Telework Training Conference, August 15, 2007, Arlington, VA
- Immigration and Enforcement Policies Conference, September 26, 2007,
Washington, DC About This Conference
Proper credentialing enables federal agencies to enhance government
efficiency, reduce identity fraud, protect personal privacy and comply with
government mandates.  The Credentialing and Identity Assurance Conference
will delve into the current political, logistical and technological
implications and issues surrounding Homeland Security Presidential
Directive 12 (HSPD-12) and the Federal Information Processing Standard 201
This conference will give federal and state employees, government
integrators and industry stakeholders insight to the interoperable and
future-proof solutions needed to ensure proper credentialing.  It will
cover compliance and interoperability issues between various components
including PKI, smartcards, middleware, card management systems and physical
access systems to minimize integration challenges.  It will highlight the
significant impact and challenges of government agencies' policies,
processes and technologies, as well as provide an overview on the status of
Attendees will learn about the latest technologies that are being utilized,
such as biometrics, and the technical specifications required integrating
these advancements into legacy, current and future credentialing systems.
Attendees will also be given an opportunity to get an overview of new
government credentialing programs such as the Western Hemisphere Traveler
Initiative (WHTI), also known as ePassports, and the REAL ID Act on
driver's license reform.  Attendees will come away from this conference
with insights into the timetables, opportunities and challenges ahead for
government issuers at both federal and state agencies, accreditation and
testing authorities, procurement programs and the industry as a whole.
This conference provides an ideal forum for attendees to share experiences
and challenges with their peers, receive an update on policy fronts, and
more importantly, learn proven methods, processes and approaches for
learning the latest on credentialing and achieving compliance.
Topics Include:
Industry trends represented in the credentialing market
Products and services available
Core security components needed for a comprehensive and compliant
identification solution
The importance for federal agencies to maximize their investments and
The impact of HSPD-12, FIPS-201, REAL ID and WHTI on the future of identity
management infrastructure and agency interoperability
Analysis of the next generation of government-wide security requirements
Implementation lessons learned and best practices
Agency compliancy updates
Policy updates
Who Should Attend
Executive level managers & administrators: network, systems,
infrastructure, and security
Executives responsible for implementing, planning or maintaining facility
access control systems, biometrics and smart cards
Managers and staff responsible for the compliance of HSPD-12/FIPS-201 mandates
Security personnel responsible for collecting and maintaining application
and identification source data
Decision makers that support HSPD-12, FIPS-201, REAL ID, WHTI resources
IT public sector systems integrators
Manufacturers of biometric technologies and scanners
Federal government technology decision-makers
Registration Charges
Industry: $495 per person
Small Business (<100 employees): $395 per person
Government: $295 per person
Registration Options
[1] Register on-line at [2] Phone Katie Smith at (703) 807-2758
[3] E-mail Katie Smith at ksmith at marketaccess.org
[5] Mail the Registration Form provided below to:
Homeland Defense Journal
4301 Wilson Blvd.  Arlington, VA 22203
Contact Us
* For registration information, contact Katie Smith, (703) 807-2758
* For government speaking and best practices presentation opportunities,
contact Brian Lake
(703) 807-2753
* For product and solutions companies interested in sponsorship information
and related speaking opportunities, contact Andrea Feinberg, (201) 592-6477.
Location Information
The conference will be held at the Capital Hilton, 1001 16th Street, NW,
Washington, DC  20036 (202) 393-1000.  The hotel is located only a few
blocks from 3 different Metro Stations: Farragut North (Red Line), Farragut
West (Blue & Orange Lines), McPherson Square (Blue & Orange Lines). Please note that the Capital Hilton is holding a block of guest rooms
available at the rate of $289 +tax until May 19th. To secure a room, you
must contact the Capital Hilton at 1-800-HILTON and mention Market*Access
International to get this rate.
REGISTRATION FORM
Credentialing and Identity Assurance Training Conference
June 21, 2007
Washington, DC
Attendee name:
City, State, and Zip Code:
Telephone Number:
Fax Number:
Attendee E-mail Address:
Training Coordinator E-mail Address:
Phone REGISTRATION CHARGES (CIRCLE ONE):
Industry: $495 per person
Small Business (<100 employees): $395 per person
Government: $295 per person
Method of Payment:
Company Check (payable to Homeland Defense Journal) - Tax ID: 01-0577059
Credit Card
Government P.O. (please attach)
Type of Credit Card (check one):
____Visa____MasterCard____American Express
Card Number: ____________________________________
Exp. Date:____________________
Name Printed on Card: ___________________________________________________
Signature (required): ___________________________________________________
Please fax this form, complete with payment information, to
(703) 807-2728 or mail it with your payment to:
Homeland Defense Journal, 4301 Wilson Blvd, Suite 1003, Arlington, VA 22203
If you have questions about registration/payment, please call Katie Smith
at (703) 807-2758. Thank you

@_date: 2019-12-10 19:45:29
@_author: Jim 
@_subject: No subject 
*** September Project Update ***
After a busy summer season of meetings and project development, a number of
FSTC projects are poised to launch, as well as a strong pipeline in
development.  Our Standing Committees (SCOMs), especially those in Business
Continuity, Security, and Check Imaging and Truncation, continue to broaden
their participation, and build upon a foundation of dialog and action that
leads to FSTC projects.  In the past few weeks, we issued two new calls for
participation: e-Authentication Proof-of-Concept, and Business Continuity
Compliance and Status Reporting.  See  .
In addition, we have recently completed projects in Image Quality and
Usability Assurance Phase I, Technology Recovery Best Practices, and
Survivability of Check Security Features.  Details on these recent projects
can be found at:  .
FSTC provides an action-oriented, collaborative forum for our members to
address shared business opportunities and challenges through technology
projects and knowledge-sharing.  We view our projects as our core activity,
and one of the key benefits of FSTC membership is eligibility to participate
in these projects.  In our efforts to keep our members and friends
up-to-date on the latest developments in these active and developing
initiatives, we provide our colleagues this periodic project update  As
always, please contact me or Zach Tumin, FSTC Executive Director, for more
information.  Or visit our website at Active Projects:
1.  Counter-Phishing Phase I
Projects in Formation:
1.  e-Authentication: Business and Technology Proof-of-Concept (call for
participation issued 9/8)
2.  Business Continuity: Compliance and Status Reporting (call for
participation issued 9/8)
Projects in Development:
1.  Image Quality and Usability Assurance Phase II
2.  Survivability of Check Security Features Phase II
3.  Treasury Services Integration: Data Exchange and Customer Connectivity
through Web Services
4.  Transformation to Open Mission Critical Systems
5.  Minimum Essential Finance (MEF)
ACTIVE PROJECTS:
1.  Counter-Phishing Phase I (launched July 2004, expected to complete in
FSTC has launched a phased initiative to address the problem of phishing and
related threats in financial services, as it affects the relationship
between customer and firm.  In collaboration with other industry groups,
FSTC will focus on defining the unique technical and operating requirements
of financial institutions (FIs) for counter-phishing measures; investigating
counter-phishing technical solutions, proving and piloting solution sets
enabled by technology to determine their fit against FI criteria and
requirements; and clarifying the infrastructure fit, requirements, and
impact of these technologies when deployed in concert with customer
education, enforcement, and other industry initiatives.  The benefits to
participants are: industry-vetted due diligence and scaling of the current
problem and its future evolution; insight into peer institution strategies
and assessments; and definition of an industry response that may be best
undertaken with collaboration between key industry segments.
12 financial institutions and over 15 technology companies are participating
in the 5-month first phase.  This project originates from the Security SCOM:
co-chaired by Mike McCormick of Wells Fargo, and Mike Versace of NEC.
Please contact FSTC Managing Executive Gene Neyer for more information
(gene.neyer at fstc.org).  (
PROJECTS IN FORMATION:
1.  e-Authentication: Business and Technology Proof-of-Concept (call for
participation issued 9/8/04)
This 5-month project will assess the viability of the potential business
opportunity that exists for financial institutions to leverage their online
customer relationships and provide an authentication service to government
agencies, and to integrate these services into financial institutions'
online applications. FSTC, jointly with the GSA's E-Authentication
Initiative Project Management Office (EAI PMO), propose to launch a
three-track project to ascertain the business model, legal framework, and
technical viability of using institutions' identity credentials to permit
consumers and businesses to access secure online government applications.
The GSA is funding the business track of the initiative. There is no cost to
financial institutions, and a $5,000 fee for associate and advisory members.
In addition, a resource commitment is required for all participants, as
outlined in the prospectus. Participation commitments are requested by Sept
24th, and the target kickoff is the week of October 4th.
2.  Business Continuity: Compliance and Status Reporting (call for
participation issued 9/8/04)
The FSTC Business Continuity Standing Committee proposes an initiative to
assist the financial industry in coming to a common understanding on the
meaning of continuity regulation, prioritization of compliance related
activities, and creating efficiencies in documenting regulatory compliance
status. To establish a clear understanding of the regulatory environment, a
list of continuity related guidance will be pulled together along with the
name of the agency responsible. Each regulation will be reviewed and a
clearly worded summary of the continuity requirements will be developed.
Where possible the regulatory agencies will be contacted for clarification
on specific points. Common themes and requirements will be documented and
which will allow a FI to provide or collect continuity compliance status.
The project will focus on providing straight forward interpretations of what
is needed for an FI to comply with current regulations.
This project is sponsored by the Business Continuity SCOM, co-chaired by Tom
Hirsch of US Bank, and Damian Walch of IBM.  Please contact FSTC Managing
Executive Charles Wallen for more information (charles.wallen at fstc.org).
PROJECTS IN DEVELOPMENT:
1.   Image Quality and Usability Assurance: Phase II (proposal being
In Phase I, more than 20 companies, representing 2/3 of US check volume,
most major vendors, and key industry associations, undertook a 90-day effort
to assess the impact of poor quality check images, and defined 16 technical
metrics and 4 usability levels that can be used to measure image quality and
usability in a standard and interoperable way.  The findings of the Phase I
project team justified further development, to test these metrics in a
real-world scenario, on millions of images, to determine the quantitative
thresholds for the 16 metrics that will define a minimum baseline "standard"
for acceptable quality images for the industry.
The business objectives are to maximize efficiencies, cost savings, and
ensure strong adoption of image exchange. The project will undertake a
robust, "real-world" analysis and test to provide actionable specifications
and direction to the industry to allow financial institutions, technology
vendors, standards organizations, and other key partners to collectively
implement baseline image quality and usability through industry
collaboration under the FSTC umbrella.
This project originates from the Check Truncation SIG
( co-chaired by Katrina
Brown, Wells Fargo; Glen Ulrich, US Bank; and Ian Goodall, NCR.  A call for
participation is expected during the month of September.
2.  Survivability of Check Security Features Phase II
As a follow-on to the recently completed Phase I
( this initiative will seek to develop
interoperability specifications for automated security feature verification
engines.  As a growing number of vendors offer security features targeted at
surviving the imaging process, institutions face a growing number of
proprietary verification engines that must be installed and configured to
validate these features during processing.  The objective of this initiative
is to make is less expensive and easier to manage the implementation of
these security feature verification products.
This project originates from the Check Truncation SIG
(  More information on this
project will be published in the next month or so.
3.  Treasury Services Integration: Data Exchange and Customer Connectivity
through Web Services (on hold)
As a potential Phase II following the previous Web Services for Corporate
Cash Management effort, a core group of FSTC institutions and technology
companies have defined key business objectives and deliverables for a
discovery phase, and subsequent pilot-level project utilizing Web Services
in the Treasury Services / Cash Management area.  The project, as it
currently stands, will seek to further develop the Phase I set of web
services and associated definitions to create new and open-standards-based
connectivity options between banks, and between banks and their customers.
The business goals are to enable standards-based "plug-and-play" integration
capabilities between institutions and customer platforms, whether ERP,
Treasury Work Station (TWS), or desktop.
A core group of financial institutions and technology companies has
committed to launching this initiative in the second half of 2004.  This
project is considered on-hold until later this year.
4.  Transformation to Open Mission Critical Systems
The transformation of systems from higher cost or proprietary delivery to
open systems is one of the most hotly debated and discussed topics in
financial services IT.  While there is great promise in the flexibility and
efficiencies gained, there is also risk and cost. An FSTC project will soon
form up to determine answers to such key questions as, "Are those
transformations viable?" and "What are the costs and processes by which a
successful transformation program will be run?" The vision of this
initiative is to bring together financial institutions to investigate the
needs, processes, best practices, technology issues, risk factors,
organizational issues and lessons-learned for transformation projects which
move core business processes from legacy IT assets to open systems.  We will
provide additional details shortly.  If you are interested in joining an
interest group around this topic, please contact us.
5.  Minimum Essential Finance (MEF)
In its early stages, FSTC and its members are in dialog with numerous
government and industry organizations to explore interest in an initiative
to identify the minimum essential elements of our financial system, and to
develop a plan and process to ensure that it remains operational in the
event of a disruption to normal operations.  A workshop is currently being
planned for this fall for multiple public and private sector organizations
to develop this concept further.  If you are interested in joining this
dialog, please contact Zach Tumin at zachary.tumin at fstc.org .
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: 2019-12-10 19:45:29
@_author: Jim 
@_subject: No subject 
*** January/February Project Update ***
Since our last update, we have launched two new projects (Business
Continuity Compliance and Status Reporting, Image Quality and Usability
Assurance Phase II), completed one project (Counter-Phishing Phase I), and
have added two new projects to our pipeline (Better Mutual Authentication,
Resiliency Maturity Model) in addition to Interoperable Verification of
Check Security Features.
[As a reminder, projects show up in this update only after it has a high
probability of launching.  We have a number of initiatives in earlier stages
of development.]
Our Standing Committees (SCOMs) and Special Interest Groups (SIGs) continue
to provide a forum for discussion that results in networking, knowledge
sharing, and action in the form of projects and workshops.  If you are not
yet active in one or more committees, please contact me or the committee's
Managing Executive.  SCOMs and SIGs are still open to non-members, however,
projects are members-only.
FSTC provides an action-oriented, collaborative forum for our members to
address shared business opportunities and challenges through technology
projects and knowledge-sharing.  We view our projects as our core activity,
and one of the key benefits of FSTC membership is eligibility to participate
in these projects.  In our efforts to keep our members and friends
up-to-date on the latest developments in these active and developing
initiatives, we provide our colleagues this periodic project update  As
always, please contact me or Zach Tumin, FSTC Executive Director, for more
information.  Or visit our website at Active Projects:
1.  Counter-Phishing Phase I (completed Dec 2004)
2.  e-Authentication: Business and Technology Proof-of-Concept (launched Oct
3.  Business Continuity: Compliance and Status Reporting (launched Nov 2004)
4.  Image Quality and Usability Assurance Phase II  (launched Nov 2004)
Projects in Formation (soliciting commitments):
[coming soon]
Projects in Development:
1.  Interoperable Verification of Check Security Features
2.  Resilience Maturity Model (RMM): Phase I
3.  Better Mutual Authentication: Phase I
ACTIVE PROJECTS:
1.  Counter-Phishing Phase I (completed Dec 2004)
FSTC has completed a first-phase initiative to address the problem of
phishing and related threats in financial services, as it affects the
relationship between customer and firm.  In collaboration with other
industry groups, the project team developed a suite of documents and tools
that allowed institutions to understand the comprehensive nature of the
problem, and understand the available solution options available to the
industry.  The project developed a detailed model of the problem, a
cost/impact model, the solution space, and a survey of over 60 solution
providers.  In addition, the project developed a next-phase proposal draft
for coordinated industry action to enable Better Mutual Authentication
(described below).
12 financial institutions and over 15 technology companies participated in
the initiative, and recently published the project's core findings and
recommendations to the public.  These documents are available from the FSTC
web site (link above).  A core group is currently developing a next-phase
initiative in Better Mutual Authentication, which is described below, and
other areas.  This project originated from the Security SCOM: co-chaired by
Mike McCormick of Wells Fargo, and Mike Versace of NEC.
2.  FSTC/GSA e-Authentication: Business and Technology Proof-of-Concept
(launched Oct 2004, to complete in late March)
This 5-month project is assessing the viability of the potential business
opportunity that exists for financial institutions to leverage their online
customer relationships and provide a federated identity-driven
authentication service to government agencies, and to integrate these
services into financial institutions' online applications. FSTC, jointly
with the GSA's E-Authentication Initiative Project Management Office (EAI
PMO), have launched a three-track project to ascertain the business model,
legal framework, and technical viability of using institutions' identity
credentials to permit consumers and businesses to access secure online
government applications through federation.
There are 7 financial institutions and 10 technology companies and other
organizations participating in the project.  An in-person meeting is
currently scheduled for mid-March in Atlanta, hosted by Bank of America. The
project should complete in late March.
3.  Business Continuity: Compliance and Status Reporting (launched Dec 2004)
The FSTC Business Continuity Standing Committee has launched an initiative
to assist the financial industry in coming to a common understanding on the
meaning of continuity regulation, prioritization of compliance related
activities, and creating efficiencies in documenting regulatory compliance
status. To establish a clear understanding of the regulatory environment, a
list of continuity related guidance will be pulled together along with the
name of the agency responsible. Each regulation will be reviewed and a
clearly worded summary of the continuity requirements will be developed.
Where possible the regulatory agencies will be contacted for clarification
on specific points. Common themes and requirements will be documented and
The project will focus on providing straight forward interpretations of what
is needed for an FI to comply with current regulations.
This project is sponsored by the Business Continuity SCOM, co-chaired by Tom
Hirsch of US Bank, and Damian Walch of IBM.  Please contact FSTC Managing
Executive Charles Wallen for more information  (charles.wallen at fstc.org).
4.   Image Quality and Usability Assurance: Phase II (launched Nov 2004)
In Phase I, more than 20 companies, representing 2/3 of US check volume,
most major vendors, and key industry associations, undertook a 90-day effort
to assess the impact of poor quality check images, and defined 16 technical
metrics and 4 usability levels that can be used to measure image quality and
usability in a standard and interoperable way.  The findings of the Phase I
project team justified further development, to test these metrics in a
real-world scenario, on millions of images, to determine the quantitative
thresholds for the 16 metrics that will define a minimum baseline "standard"
for acceptable quality images for the industry.
The business objectives are to maximize efficiencies, cost savings, and
ensure strong adoption of image exchange. The project will undertake a
robust, "real-world" analysis and test to provide actionable specifications
and direction to the industry to allow financial institutions, technology
vendors, standards organizations, and other key partners to collectively
implement baseline image quality and usability through industry
collaboration under the FSTC umbrella.
This project originates from the Check Truncation SIG
( co-chaired by James
Burroughs, Wells Fargo; Glen Ulrich, US Bank; and Ian Goodall, NCR. 7
financial institutions and 18 vendors and industry organizations are
PROJECTS IN DEVELOPMENT:
1.  Interoperable Verification of Check Security Features (IV-CSF)
As a follow-on to the recently completed Survivability of Check Security
Features project ( this initiative will seek
to develop the business and technology foundation to enable interoperable
verification of check security features.  As a growing number of banks offer
their customers security features targeted at surviving the imaging process,
interoperability becomes an important enabler.  The objective of this
initiative, through interoperability, is to mitigate fraud risk for all
stakeholders (banks, customers, merchants, etc.) by shortening the time
between a check being presented, and the check verification process, and to
enable any receiver of a check to verify it as close to the point of
presentment as possible.
This project originates from the Check Truncation SIG
(  A whiteboard session was
held January 26-27 in Tempe, AZ, hosted by Bank of America and co-hosted by
JPMorgan Chase.  A full draft proposal will be published to the Check
Truncation SIG in the coming week to ten days, reflecting the refined
objectives and deliverables that were developed in Tempe.  Potential project
launch is in the March/April timeframe.
2.  Resilience Maturity Model (RMM): Phase I
A group of FSTC member institutions and vendors met at the FSTC Technology
Recovery Roundtable, hosted by US Bank on October 6th in St. Paul.  At the
meeting, the group defined a potential project that would develop metrics to
evaluate an institution's resilience, much like the Carnegie Mellon CMM
model in software development.  Resilience in this context is an
institution's overall business continuity, disaster recovery, and crisis
management program.  The business objective of the project would be to allow
financial institutions to "rate" themselves and their key business partners
against industry-vetted definitions and metrics, and justify investment (or
not) where needed to achieve the desired level of resilience.
The group met again in New York on January 13th, hosted by JPMorgan Chase,
and further refined the concept with 7 of the top 10 institutions in the US
represented.  A proposal is currently being finalized, and will be published
in the next 7-10 days to the general public.  More than 8 firms have already
committed to participate.  If you are interested, please contact Charles
Wallen, Business Continuity SCOM Managing Executive, at
charles.wallen at fstc.org.
3.  Better Mutual Authentication: Phase I
As a next-phase concept coming out of the Counter-Phishing: Phase I project,
the initiative will focus on establishing a blueprint for the financial
industry to establish better mutual authentication between customers and
financial institutions.  The three components of better mutual
authentication include: customer to institution, institution to customer,
and email communications from the institution to customer.  The objective is
to create a framework that supports individual institutions' efforts, while
defining a "blueprint" of requirements to ensuring a level of consistency in
customer experience (if affected), leveraging customer education efforts,
and establishing interoperability wherever possible and prudent.
An in-person, large-institution-only meeting is currently being scheduled
for mid-late-March to create the charter, objectives, and deliverables for
such an initiative. More information will be available in the coming weeks
under the auspices of the Security Standing Committee.
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: 2019-12-10 19:45:29
@_author: Jim 
@_subject: No subject 
We are pleased to announce a "call for participation" for two new FSTC
projects.  Each has a committed core group of member institutions and
vendors who solicit further participation from fellow members:
1.  FSTC e-Authentication Initiative: Business and Technology
2.  Business Continuity: Compliance and Status Reporting Project
1.  FSTC e-Authentication Initiative: Business and Technology
Prospectus: This 5-month project will assess the viability of the potential business
opportunity that exists for financial institutions to leverage their online
customer relationships and provide an authentication service to government
agencies, and to integrate these services into their own online
applications.  FSTC, jointly with the GSA's E-Authentication Initiative
Project Management Office (EAI PMO), propose to launch a three-track project
to ascertain the business model, legal framework, and technical viability of
using FIs' identity credentials to permit consumers and businesses to access
secure online government applications.
The GSA is funding the business track of the initiative.  There is no cost
to financial institutions, and a $5,000 fee for associate members.   In
addition, a resource commitment is required for all participants, as
outlined in the prospectus.  Participation commitments are requested by Sept
24th, and the target kickoff is the week of October 4th.
An informational conference call has been scheduled for:
Wednesday Sept 15th, 2pm EDT
512-225-3050, 71782#
2.  Business Continuity: Compliance and Status Reporting Project
Prospectus: The FSTC Business Continuity Standing Committee proposes an initiative to
assist the financial industry in coming to a common understanding on the
meaning of continuity regulation, prioritization of compliance related
activities, and creating efficiencies in documenting regulatory compliance
status.    To establish a clear understanding of the regulatory environment,
a list of continuity related guidance will be pulled together along with the
name of the agency responsible.   Each regulation will be reviewed and a
clearly worded summary of the continuity requirements will be developed.
Where possible the regulatory agencies will be contacted for clarification
on specific points.  Common themes and requirements will be documented and
will be developed which will allow a FI to provide or collect continuity
compliance status.  The project will focus on providing straight forward
interpretations of what is needed for an FI to comply with current
The final product will provide:
.	A comprehensive list of all current regulatory guidance related to
business continuity, an overview of the requirements of the key components
and the agency that produced it.
.	Prioritized summary of the key continuity requirements that FIs must
address and the timing for implementing them.
.	Standardized questionnaire that FIs can utilize to collect or
provide status of compliance with continuity regulatory requirements.
.	An ongoing forum for discussion on regulatory direction and for
sharing feedback/experiences other FIs have had with regulators.
.	A documented process that can be utilized to allow FSTC and its
members to maintain the regulatory summary and questionnaire.
Project Fees:
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: unknown_date
@_author: Chris ( 
@_subject: No subject 
2011 10:45:13 -0400
law enforcement is effective enough to prevent the average
criminal from
having access to firearms, then the law-abiding population can
be compelled to
disarm. That day is coming through US force as "Operation Gun Runner" from
ATF allowed Mexican drug cartel straw purchasers to come in, purchase
5 or
so AK-47 rifles, and when the gun store owner had suspicions
about not selling
it - the ATF told the owner to "let the guns walk"
so the group could track
down the weapons. Unfortunately, those weapons were used to kill a DEA agent
in Mexico
and a Border Patrol agent who was only armed with bean bag rounds in
his shotgun then died trying to cycle out those rounds to put in live
Also with al-CIAda patsy Adam Gahdan inaccurately reporting in his
video to other jihadists about purchasing "automatic weapons"
from gun shows,
I believe the ball is rolling for everyone in the
United States to be disarmed
through force by new legislation to
outlaw weapons. I do not think the average
gun owner would ever disarm
because the gun culture in our country is so deep
and passionate in
any freedom loving citizen's blood. The Second
Amendment, in my opinion and most gun owners agree with,
was put in the Bill
of Rights for the average citizen to remove
tyrants if the process of
democracy does not work. At present, the average criminal in my area does not
have firearms, and so I
do not own one. Gun crime is on the increase, however,
so this situation may
change. Better get one before it's too late :-) --
--C "The dumber people think you are, the more surprised they're going to
be when you kill them." - Sir William Clayton

@_date: 2019-12-10 19:45:29
@_author: Michael 
@_subject: Quick USB question 
Double checking that the USBs that you sent were prepared as-is and no
different from any other versions, except updated through August 14 2015.
John Young sent back an accusatory email:

@_date: 2019-12-10 19:45:29
@_author: John 
@_subject: Quick USB question 
Don't know. Updates generated scratch. Prepare to be surprised if not
deceived by anything digital or analogue or intergalactic. Especially if
authenticated, signed, sealed, shipped through thickets of traps and
contaminants. You know that, though, and are just being humorously baiting
and entrapping. Like Archive.org and Wikipedia and gosh the whole mess
seething with malevolence.
I replied to John:

@_date: 2019-12-10 19:45:29
@_author: Michael 
@_subject: Quick USB question 
Don't mean to bait or entrap, but asking questions with too much context
can be leading. I'm not worried about hidden payloads or anything, I want
to make sure that it was (as far as you know) the vanilla version of the
August 2015 archive and you hadn't purposefully included any extra
information for me to peruse before I posted my findings publicly.
John did not respond.
Since John made a point out of the USBs being generated from scratch every
time, I couldn't be sure how long the data had been available. After some
digging, I found a copy of Cryptome's archive apparently uploaded by
coderman[at]gmail.com AKA bandmon. You can find that torrent here
 I
downloaded the torrent to a remote server, unzipped the files and confirmed
there were log files there as well.
It was my strong preference **not** to post this, but since Cryptome has
refused to validate the data, there is no other way to authenticate it than
to release it to the public along with how to find that information in the
Cryptome USBs/CDs and their various mirrors. It was not my intention to
humiliate Cryptome or expose their users, only to demonstrate that the
slide allegedly proving the GCHQ has spied on Cryptome.org could have come
from anywhere. Despite being accurate, the information is not proof of
surveillance or anything nefarious. In short, the alleged GCHQ could have
been produced by GCHQ as an internal mockup, or forged by anyone with
access to an internet connection.
In addition to the links below, you can also download a complete copy of
the dataset from Cryptome  as well as download a .zip of
all of the leaked logs
 and
peruse them in your own time.
Cryptome's leaked logs:
 (4)
 (3)
 (2)
 (1)
If the information is a mockup as Cryptome alleges, then it was created and
distributed by them as part of an insane piece of disinformation designed
to implicate users who are innocent of even visiting Cryptome.org. Far more
likely is that Cryptome has been unaware of these ongoing leaks, refused to
discuss them with me and then attempted to deny their reality.

@_date: 2019-12-10 19:45:29
@_author: Jim 
@_subject: No subject 
*** Feb/March Project Update ***
Since our last update, 22 companies have committed to participate and launch
the Image Quality and Usability Assurance: Phase I project.  Also, we
completed the SVPCo/FSTC Black and White vs. Gray-Scale in Bank Operations
project in February, and SVPCo is now utilizing that report as a key
component of its planning.  Lastly, our Minimum Required Practices for
Global Sourcing project has issued a call for participation, and commitments
are being made by both financial institutions and technology partners.
FSTC provides an action-oriented, collaborative forum for our members to
address shared business opportunities and challenges through technology
projects and knowledge-sharing.  We view our projects as our core activity,
and one of the key benefits of FSTC membership is eligibility to participate
in these projects.  In our efforts to keep our members and friends
up-to-date on the latest developments in these active and developing
projects, we provide our colleagues this periodic project update.  As
always, please contact me or Zach Tumin, FSTC Executive Director, for more
information.  Or visit our website at Active Projects:
1.  Business Continuity: Technology Best Practices Expertise Center
(launched Nov 2003)
2.  Survivability of Check Security Features in an Imaged Environment
(launched Oct 2003)
3.  Image Quality and Usability Assurance: Phase I (launching March 2004)
Projects in Formation:
1.  Minimum Required Practices for Global Sourcing (call for participation
2.  Phishing and Financial Services
3.  eBilling Self Service through Federated Identity
4.  Biometrics in Financial Services: Assessment and Action
5.  Treasury Services Integration: Data Exchange and Customer Connectivity
through Web Services (on hold)
6.  A Federated Identity Implementation Framework for Secure Email (on hold)
7.  Transformation to Open Mission Critical Systems
ACTIVE PROJECTS:
1.  Business Continuity: Technology Best Practices Expertise Center
The Technology Best Practices Expertise Center Phase I initiative has
brought industry leaders together to jointly develop consolidated,
industry-vetted best practices and actionable recommendations for technology
recovery in post-outage, remote recovery.  Regulatory compliance will be a
key requirement considered by the team.  The resulting documentation will
define best practices, identify key challenges and gaps in available
solutions, and identify recommendations for further actions (such as
testing) in future efforts.  The objective is to enable participating
companies to recover in a cost-effective manner, to validate and compare
their own recovery strategies with their peers, and to address regulatory
compliance in an industry forum.
The project launched on November 5, 2003 in New York, and is expected to
conclude in April 2004.  Project participants include: Bank One, Bank of
America, Comerica, JPMorgan Chase, Huntington, RBC Financial, US Bank,
Wachovia, and IBM.
This initiative originated in our Business Continuity SCOM.
2.  Survivability of Check Security Features in an Imaged Environment
In this project, participating financial institutions and partner technology
companies are selecting current and proposed (next-generation) check
security features.  Working with participating industry partners and
financial institutions, the project will test the survivability of these
check security features in high-speed and low-speed capture, in both gray
scale and black & white imaged environments. The result will be an
independent indication as to the viability of current and proposed
next-generation check security features in a truncated (imaged) environment.
This project launched in October 2003, held its first in-person meeting
November 18th in Charlotte, hosted by IBM, and is expected to conclude in
the April 2004 timeframe.  Project participants include Bank of America,
Canadian Payments Association, Comerica, Federal Reserve, First Citizens,
JPMorgan Chase, US Treasury, Wachovia, Wells Fargo, and Zions Bank; ASD
Corp, Cheque Guard, Clarke American, Deluxe, Fiserv, Harland, IBM, and SQN
Banking Systems.
This project originates from the Check Truncation SIG
3.   Image Quality and Usability Assurance: Phase I
A group of FSTC member institutions and technology companies convened
January 13th in Atlanta to develop an initial set of objectives and
deliverables for a Phase I FSTC image quality initiative.   The goal is to
bring financial institutions together with key technology partners to better
understand the current industry activities in the area of image quality,
identify critical challenges yet to be addressed, and leverage the FSTC
project environment as a place to undertake key collaborative development,
testing, prototyping, and specification required to ultimately ensure
minimum image quality assessment capabilities in centralized and distributed
capture points, regardless of vendor or institution.  The ultimate objective
is to prevent unusable check images (and their financial exposure) from
entering the payment system.
A call for participation was issued February 4, and since then 22 companies
have committed to pariticpate.  The first in-person project meeting will be
held April 2nd in San Francisco, following the FSTC/ABA/Federal
Reserve/SVPCo Image Assurance and Security event March 31-April 1 at the
Fairmont (  The project is expected to
run for 90 days, and conclude in June 2004.
PROJECTS IN FORMATION:
1.  Minimum Required Practices for Global Sourcing
This primary objective of this initiative is to develop a comprehensive set
of tactical minimum required practices for both financial institutions and
vendors, as a baseline for the industry.  With increasing regulatory
interest, and a desire to collaborate to improve the overall strength of the
financial industry, FSTC members have defined a series of activities for
2004 that address key gaps and opportunities shared by FSTC member
institutions.  Areas of interest include data privacy, business continuity,
governance, safeguarding intellectual property, and others.
An in-person meeting was held February 26th in Orlando, bringing our core
group together with a broader industry audience to share the group's vision,
as well as further refine the scope, objectives, and next steps.  A call for
participation has been issued, and a number of financial institutions and
technology partners have committed to participate.
Please contact Zach Tumin (zachary.tumin at fstc.org) for more information.
2.  Phishing in Financial Services
FSTC member-institutions have expressed an interest in understanding and
addressing, at a technical level, the complex problem of phishing via both
email and web sites. A core group is developing a strawman statement of
financial institution requirements regarding phishing that will address
issues of ease of use and acceptance, effectiveness, cost and complexity of
implementation, and required industry coordination. When validated with a
larger group of financial institutions and technology providers, it is the
intention of these members to inventory and evaluate current vendor
solutions against the set of known threat models and financial institution
requirements, and to work with industry groups to prove/test/validate those
solutions. For more information, please contact Zach Tumin
(zachary.tumin at fstc.org).
An FI-only project definition session is being held March 24th in New York
to begin this process.
3.  eBilling Self Service Through Federated Identity
This proposed project would seek to bring FSTC members together to define an
implementation framework for using federated identity standards such as SAML
to link financial institution sites with biller self-service sites.  The
proposition for billers is reduced identity management costs and increased
adoption, while financial institutions benefit from increased online
traffic, stronger customer service, and increased use of online services.
Customers will benefit from having a consolidated access point for disparate
billing sites and fewer usernames and passwords to remember.
A core group of financial institutions and technology companies are
currently developing this concept, and developing an initial set of use
cases.  Also, these companies are talking to billers and banks who might
participate in a pilot.  We expect to be able to share more information
about this project soon.
4.  Biometrics in Financial Services: Assessment and Action
Using as a basis the internal control objectives and practices of the ANSI
Standard X9.84 - 2003, Security and Management of Biometric Data, a core
group of interested FSTC members is developing a project concept to assist
financial institutions in determining the viability of biometric
technologies in several financial institution-specific use cases, including
account openings.  As currently conceived, this effort will culminate in the
assessment of the current state (2004) and desired future state of biometric
standardization, technologies, and business process efforts, and produce a
statement of financial institution requirements and recommendations on
issues of interoperability, security and management, and customer service
for critical business processes. The requirements for institutions to
utilize data in a standardized and privacy-aware fashion will be a important
performance metric. Ultimately, the project may include the development of a
reference implementation and the deployment of a pilot system to validate
the reference implementation.
5.  Treasury Services Integration: Data Exchange and Customer Connectivity
through Web Services (on hold)
As a potential Phase II following the previous Web Services for Corporate
Cash Management effort, a core group of FSTC institutions and technology
companies have defined key business objectives and deliverables for a
discovery phase, and subsequent pilot-level project utilizing Web Services
in the Treasury Services / Cash Management area.  The project, as it
currently stands, will seek to further develop the Phase I set of web
services and associated definitions to create new and open-standards-based
connectivity options between banks, and between banks and their customers.
The business goals are to enable standards-based "plug-and-play" integration
capabilities between institutions and customer platforms, whether ERP,
Treasury Work Station (TWS), or desktop.
A core group of financial institutions and technology companies has
committed to launching this initiative in the second half of 2004.  This
project is considered on-hold until later this year.
6.  A Federated Identity Implementation Framework for Secure Email (on hold)
Coming from discussions in the FSTC Advisory Council Security and
Infrastructure Standing Committee (SCOM)
( FSTC members are putting plans
together to create a federated identity implementation framework, with the
primary application being secure email.  The primary business objectives are
to create interoperability for shared customers and business partners, as
well as to reduce the cost of managing identity databases internally within
institutions.  The project would deliver a set of technical, business, and
legal/regulatory definitions to create a framework for the industry to
utilize in secure email and ultimately other applications.
An in-person session was held January 8th in Boston, hosted by Fidelity.  A
revised proposal was developed coming out of this meeting, however, given
the relative immaturity of secure email technology and solutions that have
been selected and implemented within FSTC member institutions, the team
agreed that this project should be put on hold while those decisions are
made.  It is expected that later in the year, a discussion about enhancing
secure email through federation will be started up again.
7.  Transformation to Open Mission Critical Systems
The transformation of systems from higher cost or proprietary delivery to
open systems is one of the most hotly debated and discussed topics in
financial services IT. While there is great promise in the flexibility and
efficiencies gained, there is also risk and cost. An FSTC project will soon
form up to determine answers to such key questions as, "Are those
transformations viable?" and "What are the costs and processes by which a
successful transformation program will be run?" The vision of this
initiative is to bring together financial institutions to investigate the
needs, processes, best practices, technology issues, risk factors,
organizational issues and lessons-learned for transformation projects which
move core business processes from legacy IT assets to open systems.  We will
provide additional details shortly.  If you are interested in joining an
interest group around this topic, please contact us.
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: unknown_date
@_author: Richard 
@_subject: DHS chief contemplating proactive cyber attacks (Steve Johnson) 
Begin forwarded message (via Dave Farber's IP distribution):
Steve Johnson, Homeland Security chief contemplating proactive cyber attacks
*San Jose Mercury News*, 16 Apr 2012 sjohnson at mercurynews.com,
Posted:   04/16/2012 07:35:38 PM PDT
Updated:   04/16/2012 09:08:36 PM PDT
Homeland Security Secretary Janet Napolitano said Monday she would consider
having tech companies participate with the government in "proactive" efforts
to combat hackers based in foreign countries.
Napolitano, who made the comments during a meeting at the *San Jose Mercury
News* with the editorial board and reporters, declined to say what steps
corporations and federal agencies might take against foreign cybercrooks,
who have been blamed for numerous computerized incursions against the United
States. She made the remarks in response to a question, and emphasized the
idea is merely one she would consider and that no decisions have been made.
In discussing the private partnerships she is promoting to combat
cyberattacks, Napolitano was asked if instead of just taking defensive
measures, the government and companies should be launching proactive
counterattacks against foreign-based culprits. "Should there be some aspect
that is in a way proactive instead of reactive?" she responded, and then
answered her own question with "yes." She added, "it is not something that
we haven't been thinking about," noting someone else had raised the subject
with her earlier Monday.
However, Napolitano said some restrictions might have to be placed on
businesses participating in such cyber activities because "what you are
doing is authorizing a private entity to do what might otherwise be
construed as an attack on another entity."
  [Long item truncated for RISKS. PGN]
