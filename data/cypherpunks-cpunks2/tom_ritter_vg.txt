
@_date: 2013-03-08 13:05:30
@_author: Tom Ritter 
@_subject: Summary of where we are right now 
I don't agree that the NRL funded Tor for this purpose, but I do agree
that our tools today (Tor, mixmaster/mixminion, PGP mail, RedPhone,
TextSecure, OTR, etc) are easily distinguishable in traffic streams,
and that this is a problem.  Just as Riseup collects a bunch of people
who care a lot about privacy onto one mailserver - people using these
tools are likely to be interesting.
Skype, Facebook, Gmail - for all their problems, they are ubiquitous,
and don't draw attention.
A friend I talked with recently told me he thought it was easy to set
up an anonymity system that worked great for you and your friends, and
near impossible to build one that worked well for everyone else.  Once
it got popular or you became a target of investigation, people would
put the effort into detecting it.  Otherwise, it would continue along,
looking like another TLS/SSH/Skype/whatever that just a little bit
odd...  Tor faces this problem immensely.
I don't see us as having won, I see us as now knowing how to fight.
We know the devices they will use to easily detect our traffic, and in
most cases we can get access to them.  We must make our protocols
indistinguishable on the wire. We know the ubiquitous services and
protocols that we must work within or disguise ourselves as.
We know (some of? most of?) the statistical attacks adversaries of the
future can conduct - we must make them as difficult and expensive as
possible for them to achieve.
We know how woefully inadequate the user interfaces and requirements
of the first generation of tools were, and we know where we must go:
to browsers, smartphones, tablets, and consumer operating systems.
We have a much better idea of how normal people will react to our
tools, and thus how much effort we must make to make them usable, and
push for ubiquity.
We know what requirements are unreasonable of us to make upon people,
and that we must design systems where those requirements are worked
around, dulled, or the single 'sharp edge' of the system.

@_date: 2013-03-25 11:57:16
@_author: Tom Ritter 
@_subject: [liberationtech] A tool for encrypted laptops 
Hi all - at the risk of shilling, my company has released an Open
Source tool called "You'll Never Take Me Alive".  If your encrypted
laptop has its screen locked, and is plugged into power or ethernet,
the tool will hibernate your laptop if either of those plugs are
removed.  So if you run out for lunch, or leave it unattended (but
plugged in) at starbucks, and someone grabs your laptop and runs,
it'll hibernate to try to thwart memory attacks to retrieve the disk
encryption key. Not foolproof, but something simple and easy.
It the moment it only supports Bitlocker, but support for Truecrypt is
coming[0].  If you have suggestions - add them to the github issues
[0] Too many emails? Unsubscribe, change to digest, or change password by emailing moderator at companys at stanford.edu or changing your settings at

@_date: 2013-10-14 17:30:09
@_author: Tom Ritter 
@_subject: An Interview with Simon Persson of CounterMail 
"You can delete the private key from our server (but we recommend this
only for advanced users, your private key is always encrypted on our
server anyway"
This sounds pretty similar to Lavabit. The server stores your emails
encrypted, but they're decrypted for you when you login, using your
password as the key to decrypt your private key.  The difference (I
think, I never used Lavabit) is that you can retrieve the private key
from Countermail and then ask them to delete it.  It would be even
nicer if they let you upload your public key so they never see the
private key.  You'd still have to trust them not to copy plaintext as
it's coming in, which depending on how you think about it might be
equivalent to them having a private key to your mail in the first
In all these 'secure email' providers, they all have the same problem:
they see incoming plaintext, and could be compelled to store it/record
it. It's not their fault, they do the best they can, it's just how
email works.

@_date: 2013-10-14 17:30:09
@_author: Tom Ritter 
@_subject: An Interview with Simon Persson of CounterMail 
"You can delete the private key from our server (but we recommend this
only for advanced users, your private key is always encrypted on our
server anyway"
This sounds pretty similar to Lavabit. The server stores your emails
encrypted, but they're decrypted for you when you login, using your
password as the key to decrypt your private key.  The difference (I
think, I never used Lavabit) is that you can retrieve the private key
from Countermail and then ask them to delete it.  It would be even
nicer if they let you upload your public key so they never see the
private key.  You'd still have to trust them not to copy plaintext as
it's coming in, which depending on how you think about it might be
equivalent to them having a private key to your mail in the first
In all these 'secure email' providers, they all have the same problem:
they see incoming plaintext, and could be compelled to store it/record
it. It's not their fault, they do the best they can, it's just how
email works.

@_date: 2013-10-14 17:30:09
@_author: Tom Ritter 
@_subject: An Interview with Simon Persson of CounterMail 
"You can delete the private key from our server (but we recommend this
only for advanced users, your private key is always encrypted on our
server anyway"
This sounds pretty similar to Lavabit. The server stores your emails
encrypted, but they're decrypted for you when you login, using your
password as the key to decrypt your private key.  The difference (I
think, I never used Lavabit) is that you can retrieve the private key
from Countermail and then ask them to delete it.  It would be even
nicer if they let you upload your public key so they never see the
private key.  You'd still have to trust them not to copy plaintext as
it's coming in, which depending on how you think about it might be
equivalent to them having a private key to your mail in the first
In all these 'secure email' providers, they all have the same problem:
they see incoming plaintext, and could be compelled to store it/record
it. It's not their fault, they do the best they can, it's just how
email works.

@_date: 2013-09-19 15:39:53
@_author: Tom Ritter 
@_subject: Linus Torvalds admits he was asked to insert a backdoor into 
Is there any indication he took the question seriously and wasn't just
making a joke?  This is a lot to conclude from a single sentence.

@_date: 2013-09-30 22:08:35
@_author: Tom Ritter 
@_subject: Surveillance 
I would say you are incorrect. The UK and the US cooperate very, very
closely. Likewise, the Echelon/Five Eyes program is a publicly
documented SIGINT sharing program

@_date: 2013-09-19 15:39:53
@_author: Tom Ritter 
@_subject: Linus Torvalds admits he was asked to insert a backdoor into 
Is there any indication he took the question seriously and wasn't just
making a joke?  This is a lot to conclude from a single sentence.

@_date: 2013-09-30 22:08:35
@_author: Tom Ritter 
@_subject: Surveillance 
I would say you are incorrect. The UK and the US cooperate very, very
closely. Likewise, the Echelon/Five Eyes program is a publicly
documented SIGINT sharing program

@_date: 2013-09-19 15:39:53
@_author: Tom Ritter 
@_subject: Linus Torvalds admits he was asked to insert a backdoor into 
Is there any indication he took the question seriously and wasn't just
making a joke?  This is a lot to conclude from a single sentence.

@_date: 2013-09-30 22:08:35
@_author: Tom Ritter 
@_subject: Surveillance 
I would say you are incorrect. The UK and the US cooperate very, very
closely. Likewise, the Echelon/Five Eyes program is a publicly
documented SIGINT sharing program

@_date: 2014-08-20 11:10:35
@_author: Tom Ritter 
@_subject: New end to end encrypted IM/VOIP web app focused on ease of use 
This is cool!  I love the combined distribution of providing a hosted
version, and encouraging people to host it themselves.
I looked into the code to understand more about how it works.  Is it
fair to say that you use WebRTC with SRTP for the transport
encryption, and then a homebaked AES-GCM-based protocol with RSA
public keys to do the encrypted chat/actions/invites, and also to
distribute/authenticate the WebRTC fingerprints?

@_date: 2014-08-20 11:10:35
@_author: Tom Ritter 
@_subject: New end to end encrypted IM/VOIP web app focused on ease of use 
This is cool!  I love the combined distribution of providing a hosted
version, and encouraging people to host it themselves.
I looked into the code to understand more about how it works.  Is it
fair to say that you use WebRTC with SRTP for the transport
encryption, and then a homebaked AES-GCM-based protocol with RSA
public keys to do the encrypted chat/actions/invites, and also to
distribute/authenticate the WebRTC fingerprints?

@_date: 2014-08-20 11:10:35
@_author: Tom Ritter 
@_subject: New end to end encrypted IM/VOIP web app focused on ease of use 
This is cool!  I love the combined distribution of providing a hosted
version, and encouraging people to host it themselves.
I looked into the code to understand more about how it works.  Is it
fair to say that you use WebRTC with SRTP for the transport
encryption, and then a homebaked AES-GCM-based protocol with RSA
public keys to do the encrypted chat/actions/invites, and also to
distribute/authenticate the WebRTC fingerprints?

@_date: 2014-02-07 23:02:18
@_author: Tom Ritter 
@_subject: FB's Conceal secure-storage API 
It's not like preventing root from getting the key is some attribute
they omitted by accident or incompetence - it's a significant design
change that changes the way the application would work.
It seems like everyone criticizing Facebook is angry that they're not
compromising their design principals for added security.  They have
very clear priorities: We are _going_ to benchmark and make sure any
code we add does not increase UI latency beyond an unacceptable limit.
 We are _going_ to cache some large MB of data on the phone, because
it makes the app faster. We are _not_ going to take up more space than
we need. We are _going_ to support old phones that have an SD Card,
and if that's where we cache the data, then so be it. We are _not_
going to require the user to enter a password or PIN on app startup.
We are _not_ going to require the phone to be online to used the
cached data.
With requirements like those, what you get is exactly this library. It
adds some small level of security against a very specific attack: data
stored on the SD Card and accessible to other programs. (It may even
be a way to get the security they need to permit themselves to store
cached data on the SD Card, which is a desirable situation because it
makes the app faster.)
If you relax some of those requirements, you can add security
features. Relax the latency or minimal storage requirement and you can
create an encrypted container, and hide metadata like filenames,
sizes, and times (like IOCipher). Relax the password requirement, and
you can have the user enter a password on app startup and prevent root
from getting the key unless it's in memory or entered.  Relax the
latency and offline requirement, and you can have the server send down
a key to decrypt the data.
Facebook is starting with the User Experience and adding as much
security as it allows.

@_date: 2014-02-07 23:02:18
@_author: Tom Ritter 
@_subject: FB's Conceal secure-storage API 
It's not like preventing root from getting the key is some attribute
they omitted by accident or incompetence - it's a significant design
change that changes the way the application would work.
It seems like everyone criticizing Facebook is angry that they're not
compromising their design principals for added security.  They have
very clear priorities: We are _going_ to benchmark and make sure any
code we add does not increase UI latency beyond an unacceptable limit.
 We are _going_ to cache some large MB of data on the phone, because
it makes the app faster. We are _not_ going to take up more space than
we need. We are _going_ to support old phones that have an SD Card,
and if that's where we cache the data, then so be it. We are _not_
going to require the user to enter a password or PIN on app startup.
We are _not_ going to require the phone to be online to used the
cached data.
With requirements like those, what you get is exactly this library. It
adds some small level of security against a very specific attack: data
stored on the SD Card and accessible to other programs. (It may even
be a way to get the security they need to permit themselves to store
cached data on the SD Card, which is a desirable situation because it
makes the app faster.)
If you relax some of those requirements, you can add security
features. Relax the latency or minimal storage requirement and you can
create an encrypted container, and hide metadata like filenames,
sizes, and times (like IOCipher). Relax the password requirement, and
you can have the user enter a password on app startup and prevent root
from getting the key unless it's in memory or entered.  Relax the
latency and offline requirement, and you can have the server send down
a key to decrypt the data.
Facebook is starting with the User Experience and adding as much
security as it allows.

@_date: 2014-02-07 23:02:18
@_author: Tom Ritter 
@_subject: FB's Conceal secure-storage API 
It's not like preventing root from getting the key is some attribute
they omitted by accident or incompetence - it's a significant design
change that changes the way the application would work.
It seems like everyone criticizing Facebook is angry that they're not
compromising their design principals for added security.  They have
very clear priorities: We are _going_ to benchmark and make sure any
code we add does not increase UI latency beyond an unacceptable limit.
 We are _going_ to cache some large MB of data on the phone, because
it makes the app faster. We are _not_ going to take up more space than
we need. We are _going_ to support old phones that have an SD Card,
and if that's where we cache the data, then so be it. We are _not_
going to require the user to enter a password or PIN on app startup.
We are _not_ going to require the phone to be online to used the
cached data.
With requirements like those, what you get is exactly this library. It
adds some small level of security against a very specific attack: data
stored on the SD Card and accessible to other programs. (It may even
be a way to get the security they need to permit themselves to store
cached data on the SD Card, which is a desirable situation because it
makes the app faster.)
If you relax some of those requirements, you can add security
features. Relax the latency or minimal storage requirement and you can
create an encrypted container, and hide metadata like filenames,
sizes, and times (like IOCipher). Relax the password requirement, and
you can have the user enter a password on app startup and prevent root
from getting the key unless it's in memory or entered.  Relax the
latency and offline requirement, and you can have the server send down
a key to decrypt the data.
Facebook is starting with the User Experience and adding as much
security as it allows.

@_date: 2014-01-23 00:47:48
@_author: Tom Ritter 
@_subject: and not a single Tor hacker was surprised... 
> About
this. Is there a way to serve 2 (or more) certificates for a given HTTPS
that do
There are a lot of things like this, but the big question is: how does the
user indicate to you which cert they want?
If it was via pubca.x.com or privca.x.com - that's easy just put the
different certs in the different sites.
But otherwise, you have to rely on quirks.
TLS allows you to send different certs to different users, but this is
based off the handshake and is for algorithm agility - not cert chaining.
EG I send ECDSA signed certs if I know you can handle them, and RSA if not.
You can also send two leaf certs, two cert chains, a cert and garbage, a
cert and a stego message - whatever. This is the closest to what you want,
but this is undefined behavior. Browsers may build a valid chain off the
public CA, and monkeysphere off the private* and it works perfect... Or the
browser may pop an invalid cert warning. It's undefined behavior. You'll
have to test, see what happens, and hope chrome doesn't break when it
updates every week.
* I realize monkey sphere doesn't use a private CA, just using it as an

@_date: 2014-01-23 00:47:48
@_author: Tom Ritter 
@_subject: and not a single Tor hacker was surprised... 
> About
this. Is there a way to serve 2 (or more) certificates for a given HTTPS
that do
There are a lot of things like this, but the big question is: how does the
user indicate to you which cert they want?
If it was via pubca.x.com or privca.x.com - that's easy just put the
different certs in the different sites.
But otherwise, you have to rely on quirks.
TLS allows you to send different certs to different users, but this is
based off the handshake and is for algorithm agility - not cert chaining.
EG I send ECDSA signed certs if I know you can handle them, and RSA if not.
You can also send two leaf certs, two cert chains, a cert and garbage, a
cert and a stego message - whatever. This is the closest to what you want,
but this is undefined behavior. Browsers may build a valid chain off the
public CA, and monkeysphere off the private* and it works perfect... Or the
browser may pop an invalid cert warning. It's undefined behavior. You'll
have to test, see what happens, and hope chrome doesn't break when it
updates every week.
* I realize monkey sphere doesn't use a private CA, just using it as an

@_date: 2014-01-23 00:47:48
@_author: Tom Ritter 
@_subject: and not a single Tor hacker was surprised... 
> About
this. Is there a way to serve 2 (or more) certificates for a given HTTPS
that do
There are a lot of things like this, but the big question is: how does the
user indicate to you which cert they want?
If it was via pubca.x.com or privca.x.com - that's easy just put the
different certs in the different sites.
But otherwise, you have to rely on quirks.
TLS allows you to send different certs to different users, but this is
based off the handshake and is for algorithm agility - not cert chaining.
EG I send ECDSA signed certs if I know you can handle them, and RSA if not.
You can also send two leaf certs, two cert chains, a cert and garbage, a
cert and a stego message - whatever. This is the closest to what you want,
but this is undefined behavior. Browsers may build a valid chain off the
public CA, and monkeysphere off the private* and it works perfect... Or the
browser may pop an invalid cert warning. It's undefined behavior. You'll
have to test, see what happens, and hope chrome doesn't break when it
updates every week.
* I realize monkey sphere doesn't use a private CA, just using it as an

@_date: 2014-07-07 08:52:59
@_author: Tom Ritter 
@_subject: US enhanced airport security checks target electronics 
The text in the email is satire/commentary and not actual reporting.

@_date: 2014-07-07 08:52:59
@_author: Tom Ritter 
@_subject: US enhanced airport security checks target electronics 
The text in the email is satire/commentary and not actual reporting.

@_date: 2014-07-07 08:52:59
@_author: Tom Ritter 
@_subject: US enhanced airport security checks target electronics 
The text in the email is satire/commentary and not actual reporting.

@_date: 2014-06-04 08:50:14
@_author: Tom Ritter 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
On 4 June 2014 01:54, Stephan Neuhaus Perhaps this is getting too far into nits and wording, but I audit software
for my day job (iSEC Partners).  I'm not speaking for my employer. But,
with very few exceptions (we have a compliance arm for example), one does
not 'Pass' or 'Fail' one of our audits.  (Perhaps they might be better
termed as 'security assessments' then, like we call them internally, but
we're speaking in common english, and people tend to use them synonymously.)
Our customers are (mostly) on board with that too.  They never ask us if
they 'passed' or failed' - I'm certain some of them look at a report where
we failed to 'steal the crown jewels' as a successful audit - but the
expectation we set with them, and they sign on with, is not one of
'Pass/Fail'. And engagements where they want a statement saying they're
secure, we turn down - we're not in the business of rubber stamps*.
Our goal is to review software, identify bugs, and provide recommendations
to fix that issue and prevent it from occurring again. AND, in addition to
the specific bugs, provide general recommendations for the team to make
their application and environment more secure - provide defense in depth.
Maybe I didn't find a bug that let me do X, but if there's a layer of
defense you can put in that would stop someone who did, and you're missing
that layer, I would recommend it.
Examples: I audited an application that had no Mass Assignment bugs - but
no defenses against it either. Blacklists preventing XSS instead of
whitelist approaches, and like Andy said, homebrew C-code parsing JSON. We
'flag'-ed all of that, and told them they should rewrite, rearchitect, or
add layered defenses - even if we couldn't find bugs or bypasses.
So the notion of 'Passing' or 'Failing' an audit is pretty foreign to me.
 Perhaps people mean a different type of work (compliance?) than the one I
* The closest we get is one where we say 'We tested X as of [Date] for Y
amount of time for the following classes of vulnerabilities, reported them,
retested them Z months later, and confirmed they were fixed.'  As we do
this very rarely, very selectively, for clients we've dealt with before.

@_date: 2014-06-29 19:11:31
@_author: Tom Ritter 
@_subject: [liberationtech] Nsa-observer: organising nsa leaks by attack 
liberationtech It is up for me.  The site itself is open source
( and the data ex
exportable (

@_date: 2014-06-04 08:50:14
@_author: Tom Ritter 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
On 4 June 2014 01:54, Stephan Neuhaus Perhaps this is getting too far into nits and wording, but I audit software
for my day job (iSEC Partners).  I'm not speaking for my employer. But,
with very few exceptions (we have a compliance arm for example), one does
not 'Pass' or 'Fail' one of our audits.  (Perhaps they might be better
termed as 'security assessments' then, like we call them internally, but
we're speaking in common english, and people tend to use them synonymously.)
Our customers are (mostly) on board with that too.  They never ask us if
they 'passed' or failed' - I'm certain some of them look at a report where
we failed to 'steal the crown jewels' as a successful audit - but the
expectation we set with them, and they sign on with, is not one of
'Pass/Fail'. And engagements where they want a statement saying they're
secure, we turn down - we're not in the business of rubber stamps*.
Our goal is to review software, identify bugs, and provide recommendations
to fix that issue and prevent it from occurring again. AND, in addition to
the specific bugs, provide general recommendations for the team to make
their application and environment more secure - provide defense in depth.
Maybe I didn't find a bug that let me do X, but if there's a layer of
defense you can put in that would stop someone who did, and you're missing
that layer, I would recommend it.
Examples: I audited an application that had no Mass Assignment bugs - but
no defenses against it either. Blacklists preventing XSS instead of
whitelist approaches, and like Andy said, homebrew C-code parsing JSON. We
'flag'-ed all of that, and told them they should rewrite, rearchitect, or
add layered defenses - even if we couldn't find bugs or bypasses.
So the notion of 'Passing' or 'Failing' an audit is pretty foreign to me.
 Perhaps people mean a different type of work (compliance?) than the one I
* The closest we get is one where we say 'We tested X as of [Date] for Y
amount of time for the following classes of vulnerabilities, reported them,
retested them Z months later, and confirmed they were fixed.'  As we do
this very rarely, very selectively, for clients we've dealt with before.

@_date: 2014-06-29 19:11:31
@_author: Tom Ritter 
@_subject: [liberationtech] Nsa-observer: organising nsa leaks by attack 
liberationtech It is up for me.  The site itself is open source
( and the data ex
exportable (

@_date: 2014-06-04 08:50:14
@_author: Tom Ritter 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
On 4 June 2014 01:54, Stephan Neuhaus Perhaps this is getting too far into nits and wording, but I audit software
for my day job (iSEC Partners).  I'm not speaking for my employer. But,
with very few exceptions (we have a compliance arm for example), one does
not 'Pass' or 'Fail' one of our audits.  (Perhaps they might be better
termed as 'security assessments' then, like we call them internally, but
we're speaking in common english, and people tend to use them synonymously.)
Our customers are (mostly) on board with that too.  They never ask us if
they 'passed' or failed' - I'm certain some of them look at a report where
we failed to 'steal the crown jewels' as a successful audit - but the
expectation we set with them, and they sign on with, is not one of
'Pass/Fail'. And engagements where they want a statement saying they're
secure, we turn down - we're not in the business of rubber stamps*.
Our goal is to review software, identify bugs, and provide recommendations
to fix that issue and prevent it from occurring again. AND, in addition to
the specific bugs, provide general recommendations for the team to make
their application and environment more secure - provide defense in depth.
Maybe I didn't find a bug that let me do X, but if there's a layer of
defense you can put in that would stop someone who did, and you're missing
that layer, I would recommend it.
Examples: I audited an application that had no Mass Assignment bugs - but
no defenses against it either. Blacklists preventing XSS instead of
whitelist approaches, and like Andy said, homebrew C-code parsing JSON. We
'flag'-ed all of that, and told them they should rewrite, rearchitect, or
add layered defenses - even if we couldn't find bugs or bypasses.
So the notion of 'Passing' or 'Failing' an audit is pretty foreign to me.
 Perhaps people mean a different type of work (compliance?) than the one I
* The closest we get is one where we say 'We tested X as of [Date] for Y
amount of time for the following classes of vulnerabilities, reported them,
retested them Z months later, and confirmed they were fixed.'  As we do
this very rarely, very selectively, for clients we've dealt with before.

@_date: 2014-06-29 19:11:31
@_author: Tom Ritter 
@_subject: [liberationtech] Nsa-observer: organising nsa leaks by attack 
It is up for me.  The site itself is open source
( and the data ex
exportable (

@_date: 2014-11-22 20:24:19
@_author: Tom Ritter 
@_subject: Microsoft Root Certificate Bundle, where? 
I don't know.
But I know some copy of it can be accessed here:
I don't know how it's generated, how complete it is, or how up to date
it is.  Depending on your needs to may be sufficient, or may be

@_date: 2014-11-22 20:24:19
@_author: Tom Ritter 
@_subject: Microsoft Root Certificate Bundle, where? 
I don't know.
But I know some copy of it can be accessed here:
I don't know how it's generated, how complete it is, or how up to date
it is.  Depending on your needs to may be sufficient, or may be

@_date: 2014-11-22 20:24:19
@_author: Tom Ritter 
@_subject: Microsoft Root Certificate Bundle, where? 
I don't know.
But I know some copy of it can be accessed here:
I don't know how it's generated, how complete it is, or how up to date
it is.  Depending on your needs to may be sufficient, or may be

@_date: 2015-02-03 19:52:52
@_author: Tom Ritter 
@_subject: What the fark is "TFC" 
TCB is usually Trusted Computing Base.
Some searching indicates TFC may be Traffic Flow Confidentiality.  (Or
less likely, TinFoil Chat, which appears to be some random chat app
plugin for encrypted messaging.)

@_date: 2015-02-03 19:52:52
@_author: Tom Ritter 
@_subject: What the fark is "TFC" 
TCB is usually Trusted Computing Base.
Some searching indicates TFC may be Traffic Flow Confidentiality.  (Or
less likely, TinFoil Chat, which appears to be some random chat app
plugin for encrypted messaging.)

@_date: 2015-02-03 19:52:52
@_author: Tom Ritter 
@_subject: What the fark is "TFC" 
TCB is usually Trusted Computing Base.
Some searching indicates TFC may be Traffic Flow Confidentiality.  (Or
less likely, TinFoil Chat, which appears to be some random chat app
plugin for encrypted messaging.)

@_date: 2015-07-05 17:44:21
@_author: Tom Ritter 
@_subject: progression of technologies 
I'm far from certain, but I think what you have wrong is the notion
that wavelength doesn't matter. I think the courts have decided it
does: Specifically, "most of the general public lacks the expertise to
intercept and decode payload data transmitted over a Wi-Fi network."
Therefore the notion that you can point whatever sort of 'camera' you
want at people to capture them isn't accurate.  (The other relevant
case is that the police do need a warrant to point infrared cameras at
people's houses.)

@_date: 2015-07-11 14:19:59
@_author: Tom Ritter 
@_subject: progression of technologies 
Yes! That's the case I was obliquely referring to. Sorry, I kind of
glazed over that part of your argument in the article.
I guess where we quibble is I'm skeptical that the general public (as
defined by the courts?) will (ever?) adopt the types of tools you
refer to (uniquely identifying individuals based on electromagnetics,
tracking tire pressure sensors.)  I don't think the 'general public'
has adopted thermal imagers.  These will make their way into
industry... (advertisers tracking WiFi probes in malls obviously).
So my wonder now is if industry adopting a technology is sufficient
for the courts to qualify as 'general public'. But this, at best, only
affects exotic technology.  We're already fighting this battle.
Automated license plate readers have never (?) been challenged
(successfully?). They are an extension of "a police officer just
watching a highway" which is legal.  And the courts like extensions of
things that are already done - see bulk collection of metadata!
You're right - collection of this data by personals or corporations,
and selling it, is indeed the right battleground. I'm don't think the
answer is correlation, but the collection, as you say in the last

@_date: 2015-07-05 17:44:21
@_author: Tom Ritter 
@_subject: progression of technologies 
I'm far from certain, but I think what you have wrong is the notion
that wavelength doesn't matter. I think the courts have decided it
does: Specifically, "most of the general public lacks the expertise to
intercept and decode payload data transmitted over a Wi-Fi network."
Therefore the notion that you can point whatever sort of 'camera' you
want at people to capture them isn't accurate.  (The other relevant
case is that the police do need a warrant to point infrared cameras at
people's houses.)

@_date: 2015-07-11 14:19:59
@_author: Tom Ritter 
@_subject: progression of technologies 
Yes! That's the case I was obliquely referring to. Sorry, I kind of
glazed over that part of your argument in the article.
I guess where we quibble is I'm skeptical that the general public (as
defined by the courts?) will (ever?) adopt the types of tools you
refer to (uniquely identifying individuals based on electromagnetics,
tracking tire pressure sensors.)  I don't think the 'general public'
has adopted thermal imagers.  These will make their way into
industry... (advertisers tracking WiFi probes in malls obviously).
So my wonder now is if industry adopting a technology is sufficient
for the courts to qualify as 'general public'. But this, at best, only
affects exotic technology.  We're already fighting this battle.
Automated license plate readers have never (?) been challenged
(successfully?). They are an extension of "a police officer just
watching a highway" which is legal.  And the courts like extensions of
things that are already done - see bulk collection of metadata!
You're right - collection of this data by personals or corporations,
and selling it, is indeed the right battleground. I'm don't think the
answer is correlation, but the collection, as you say in the last

@_date: 2015-07-05 17:44:21
@_author: Tom Ritter 
@_subject: progression of technologies 
I'm far from certain, but I think what you have wrong is the notion
that wavelength doesn't matter. I think the courts have decided it
does: Specifically, "most of the general public lacks the expertise to
intercept and decode payload data transmitted over a Wi-Fi network."
Therefore the notion that you can point whatever sort of 'camera' you
want at people to capture them isn't accurate.  (The other relevant
case is that the police do need a warrant to point infrared cameras at
people's houses.)

@_date: 2015-07-11 14:19:59
@_author: Tom Ritter 
@_subject: progression of technologies 
Yes! That's the case I was obliquely referring to. Sorry, I kind of
glazed over that part of your argument in the article.
I guess where we quibble is I'm skeptical that the general public (as
defined by the courts?) will (ever?) adopt the types of tools you
refer to (uniquely identifying individuals based on electromagnetics,
tracking tire pressure sensors.)  I don't think the 'general public'
has adopted thermal imagers.  These will make their way into
industry... (advertisers tracking WiFi probes in malls obviously).
So my wonder now is if industry adopting a technology is sufficient
for the courts to qualify as 'general public'. But this, at best, only
affects exotic technology.  We're already fighting this battle.
Automated license plate readers have never (?) been challenged
(successfully?). They are an extension of "a police officer just
watching a highway" which is legal.  And the courts like extensions of
things that are already done - see bulk collection of metadata!
You're right - collection of this data by personals or corporations,
and selling it, is indeed the right battleground. I'm don't think the
answer is correlation, but the collection, as you say in the last

@_date: 2015-03-24 06:51:44
@_author: Tom Ritter 
@_subject: Firefox 36+ listens on UDP:1900 
This is a close-to-but-not-exact recounting. His disclosure of his
employer was required by state law, and was neither a statement of
support by the company nor his attempt to make it so.

@_date: 2015-03-24 06:51:44
@_author: Tom Ritter 
@_subject: Firefox 36+ listens on UDP:1900 
This is a close-to-but-not-exact recounting. His disclosure of his
employer was required by state law, and was neither a statement of
support by the company nor his attempt to make it so.

@_date: 2015-03-24 06:51:44
@_author: Tom Ritter 
@_subject: Firefox 36+ listens on UDP:1900 
This is a close-to-but-not-exact recounting. His disclosure of his
employer was required by state law, and was neither a statement of
support by the company nor his attempt to make it so.

@_date: 2015-05-28 07:28:13
@_author: Tom Ritter 
@_subject: Firefox will scan your browsing history to suggest advertiser 
4) The browser fetches all available suggested tiles based on country
and language from Onyx without using cookies or other user tracking
5) User interactions, such as clicks, pins and blocks, are examples of
data that may be measured and processed. View Mozilla’s Privacy Policy
or our Data Privacy Principles for more information.
6) Onyx submits the interaction data to Disco, a restricted access
database for largescale analysis.
7) Disco aggregates all Firefox tiles interactions, anonymizing
personally identifiable data before sending to Redshift for reporting.
8) Charts and reports are pulled from Redshift using Zenko, a Content
Services reporting tool, for analysis by Mozilla.
9) Mozilla sends this report to the partner shortly after the campaign ends.
""" [0]
How do you determine user interests?
For Suggested Tiles, we know whether users are interested in your
market category by matching a list of defined URLs (domains, or
subdomains) with their most frequently and recently visited URLs in
Firefox. In this way, we are able to preserve users’ anonymity while
providing a high level of confidence about their interest in different
site categories.
What input do I have over the interest categories?
We work with all our Suggested Tiles partners to define the most
effective interest categories. Partners may provide suggestions for
what URLs should be include. Mozilla’s Content Services Team will
actually define those categories.
""" [1]
I'm most curious about what 'User Interactions' are reported.  Clicks,
pins, and blocks all reveal which tile a user saw, and therefore
something about their browsing history. But they're also pretty
fundamental to advertising.  I'm more worried about Firefox reporting
"Views" or "Mouseovers" or other things that are not clear,
user-initiated actions.
[0] [1]

@_date: 2015-05-28 07:28:13
@_author: Tom Ritter 
@_subject: Firefox will scan your browsing history to suggest advertiser 
4) The browser fetches all available suggested tiles based on country
and language from Onyx without using cookies or other user tracking
5) User interactions, such as clicks, pins and blocks, are examples of
data that may be measured and processed. View Mozilla’s Privacy Policy
or our Data Privacy Principles for more information.
6) Onyx submits the interaction data to Disco, a restricted access
database for largescale analysis.
7) Disco aggregates all Firefox tiles interactions, anonymizing
personally identifiable data before sending to Redshift for reporting.
8) Charts and reports are pulled from Redshift using Zenko, a Content
Services reporting tool, for analysis by Mozilla.
9) Mozilla sends this report to the partner shortly after the campaign ends.
""" [0]
How do you determine user interests?
For Suggested Tiles, we know whether users are interested in your
market category by matching a list of defined URLs (domains, or
subdomains) with their most frequently and recently visited URLs in
Firefox. In this way, we are able to preserve users’ anonymity while
providing a high level of confidence about their interest in different
site categories.
What input do I have over the interest categories?
We work with all our Suggested Tiles partners to define the most
effective interest categories. Partners may provide suggestions for
what URLs should be include. Mozilla’s Content Services Team will
actually define those categories.
""" [1]
I'm most curious about what 'User Interactions' are reported.  Clicks,
pins, and blocks all reveal which tile a user saw, and therefore
something about their browsing history. But they're also pretty
fundamental to advertising.  I'm more worried about Firefox reporting
"Views" or "Mouseovers" or other things that are not clear,
user-initiated actions.
[0] [1]

@_date: 2015-05-28 07:28:13
@_author: Tom Ritter 
@_subject: Firefox will scan your browsing history to suggest advertiser 
4) The browser fetches all available suggested tiles based on country
and language from Onyx without using cookies or other user tracking
5) User interactions, such as clicks, pins and blocks, are examples of
data that may be measured and processed. View Mozilla’s Privacy Policy
or our Data Privacy Principles for more information.
6) Onyx submits the interaction data to Disco, a restricted access
database for largescale analysis.
7) Disco aggregates all Firefox tiles interactions, anonymizing
personally identifiable data before sending to Redshift for reporting.
8) Charts and reports are pulled from Redshift using Zenko, a Content
Services reporting tool, for analysis by Mozilla.
9) Mozilla sends this report to the partner shortly after the campaign ends.
""" [0]
How do you determine user interests?
For Suggested Tiles, we know whether users are interested in your
market category by matching a list of defined URLs (domains, or
subdomains) with their most frequently and recently visited URLs in
Firefox. In this way, we are able to preserve users’ anonymity while
providing a high level of confidence about their interest in different
site categories.
What input do I have over the interest categories?
We work with all our Suggested Tiles partners to define the most
effective interest categories. Partners may provide suggestions for
what URLs should be include. Mozilla’s Content Services Team will
actually define those categories.
""" [1]
I'm most curious about what 'User Interactions' are reported.  Clicks,
pins, and blocks all reveal which tile a user saw, and therefore
something about their browsing history. But they're also pretty
fundamental to advertising.  I'm more worried about Firefox reporting
"Views" or "Mouseovers" or other things that are not clear,
user-initiated actions.
[0] [1]
