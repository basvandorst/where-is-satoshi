
@_date: 2013-11-07 20:34:56
@_author: Hannes Frederic Sowa 
@_subject: fuck these guys 
Also large scale encryption deployments mostly use hardware acclerated
crypto offloading which (I think for historical reasons) are not as easy
to audit and recompile as open-source code (at least currently). I guess
some companies can work around that and do their own ASIC designs but
most companies don't have the resources to do that.
I wonder how Google deals with the encryption of their links between
datacenters.  Either this could be done on a per node basis, i.e.
opportunistic encryption, or centralize encryption to their border routers.
My guess is that per-flow ipsec state resolving is too costly, processing
and memory wise, because either packets get dropped or get buffered
(leading to a waste of memory in case of a high peer count) before keys
could be resolved leading to degeneration in performance or having impacts
to the programs error handling, thus not being transparent. Maybe this
can be dealt with in some time but is certainly no drop-in replacement.
This makes me believe that centralizing approaches are mostly in use today
which use unverifiable crypto implementations in hardware and it depends
on how far we trust these implementations to protect us from goverment
spying activities. IMHO target dispersal is something one should strive
for especially when encryption is in use, but this is difficult and I
don't think it is possible to realize this currently in the scale it
would be needed.
Thus large-scale interception programs must become illegal, otherwise
it is just a matter of how much the intelligence services can throw
at it to technically break down such easier to implement centralized
encryption approaches. Certainly there are other subsystems on such a
router to exploit on those routers to make the encryption meaningless.
  Hannes

@_date: 2013-11-16 20:47:21
@_author: Hannes Frederic Sowa 
@_subject: RetroShare 
Of course it is possible to add memory/type unsafe implementations later.
Actually, RetroShare is developed in C++/QT.

@_date: 2013-11-07 20:34:56
@_author: Hannes Frederic Sowa 
@_subject: fuck these guys 
Also large scale encryption deployments mostly use hardware acclerated
crypto offloading which (I think for historical reasons) are not as easy
to audit and recompile as open-source code (at least currently). I guess
some companies can work around that and do their own ASIC designs but
most companies don't have the resources to do that.
I wonder how Google deals with the encryption of their links between
datacenters.  Either this could be done on a per node basis, i.e.
opportunistic encryption, or centralize encryption to their border routers.
My guess is that per-flow ipsec state resolving is too costly, processing
and memory wise, because either packets get dropped or get buffered
(leading to a waste of memory in case of a high peer count) before keys
could be resolved leading to degeneration in performance or having impacts
to the programs error handling, thus not being transparent. Maybe this
can be dealt with in some time but is certainly no drop-in replacement.
This makes me believe that centralizing approaches are mostly in use today
which use unverifiable crypto implementations in hardware and it depends
on how far we trust these implementations to protect us from goverment
spying activities. IMHO target dispersal is something one should strive
for especially when encryption is in use, but this is difficult and I
don't think it is possible to realize this currently in the scale it
would be needed.
Thus large-scale interception programs must become illegal, otherwise
it is just a matter of how much the intelligence services can throw
at it to technically break down such easier to implement centralized
encryption approaches. Certainly there are other subsystems on such a
router to exploit on those routers to make the encryption meaningless.
  Hannes

@_date: 2013-11-16 20:47:21
@_author: Hannes Frederic Sowa 
@_subject: RetroShare 
Of course it is possible to add memory/type unsafe implementations later.
Actually, RetroShare is developed in C++/QT.

@_date: 2013-11-07 20:34:56
@_author: Hannes Frederic Sowa 
@_subject: fuck these guys 
Also large scale encryption deployments mostly use hardware acclerated
crypto offloading which (I think for historical reasons) are not as easy
to audit and recompile as open-source code (at least currently). I guess
some companies can work around that and do their own ASIC designs but
most companies don't have the resources to do that.
I wonder how Google deals with the encryption of their links between
datacenters.  Either this could be done on a per node basis, i.e.
opportunistic encryption, or centralize encryption to their border routers.
My guess is that per-flow ipsec state resolving is too costly, processing
and memory wise, because either packets get dropped or get buffered
(leading to a waste of memory in case of a high peer count) before keys
could be resolved leading to degeneration in performance or having impacts
to the programs error handling, thus not being transparent. Maybe this
can be dealt with in some time but is certainly no drop-in replacement.
This makes me believe that centralizing approaches are mostly in use today
which use unverifiable crypto implementations in hardware and it depends
on how far we trust these implementations to protect us from goverment
spying activities. IMHO target dispersal is something one should strive
for especially when encryption is in use, but this is difficult and I
don't think it is possible to realize this currently in the scale it
would be needed.
Thus large-scale interception programs must become illegal, otherwise
it is just a matter of how much the intelligence services can throw
at it to technically break down such easier to implement centralized
encryption approaches. Certainly there are other subsystems on such a
router to exploit on those routers to make the encryption meaningless.
  Hannes

@_date: 2013-11-16 20:47:21
@_author: Hannes Frederic Sowa 
@_subject: RetroShare 
Of course it is possible to add memory/type unsafe implementations later.
Actually, RetroShare is developed in C++/QT.

@_date: 2014-01-01 20:33:28
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
I was just referring to the Snowden documents.
Ok, CryptoAG is a story of its own, I agree. But they are not that much of a
major hardware vendor, either. Depends on which customer base you consider.
Agreed, but in the end it is important how they act in the long term. But
that needs more time to come until conclusions can be drawn. It is much more
difficult for hardware vendors to strike such good PR stunts as Google did.
Also, I guess, Google had this change in the works for a longer time,
otherwise I don't know if they could make the switch to crypto for
their internal cross-DC links so rapidly. It still seems a lot of work
+ testing and their services seem highly depending on good latency.
  Hannes

@_date: 2014-01-19 19:43:43
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Hi Hannes! :)
MPX was already committed to gcc trunk, so I hope this situation could
improve in future (it is reverted for 4.9 but I think it will come back
after the release in March).
(I am still not sure how this will be rolled out, maybe by switching
some software back to 32 bit to reduce the load on the pointer length
lookup tables.)
Maybe you can comment a bit on the code extraction process into compilable
There seems to be a semantic differences between the proofable
language and the language the extraction process targets in e.g. array
handling(e.g.  ocaml code) or just overflow handling in integers.
I guess Idris does not have this problem?
I always wondered if ats-lang would be the most suitable language for
writing more typesafe code?
  Hannes

@_date: 2014-01-20 14:42:34
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
MPX uses CPU managed out-of-band (but in application memory)
page-table-alike structures to store the bound table entries. They get
updated via specific cpu instructions, which result in nops on todays
cpus (so you can execute mpx code on non-mpx cpus and just won't have the
bounds checking). They also made sure that non-MPX code that is linked
against MPX code can propagate unbounded pointers, so you don't need to
switch your whole operating system to MPX enabled code at once (I guess
that would aslo be a problem memory-wise, but Intel entered the DRAM
business again, too :) ). The x64 linux ABI has also been updated.
While passing parameters and returning, MPX will introduce new registers
to pass those bounds checks automatically between function calls. I
guess this enables faster function calls because the cpu does not need
to store those pointer bounds in the permanent pointer bound tables thus
eliminating the stress on the cpu caches.
You can find details here:
What would be interesting, especially for the linux kernel, is to restrict
jmp and callq addresses so it is impossible for an attacker to get control
over them and e.g. dispatch own code on network packet dismantling without
needing whole pointer checking infrastructure e.g.
IIRC this was already addressed in the talk.
I would also like to point to Software Foundations from Benjamin Pierce here,
as it also has some great material to learn Coq:
Code generation without heavy runtime would also be nice.
ATS uses plain C as an intermediate language and the whole language feels
pretty low-level, too. So it seems it is easily possible to compile these
programs freestanding and also link those to other programs, which is
quite a nice feature, especially if one wants to make incrementally use
of more checked languages.
Thanks for your additional remarks,
  Hannes

@_date: 2014-01-20 17:03:22
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Just remembered there was some research on this already:
  Hannes

@_date: 2014-01-01 20:33:28
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
I was just referring to the Snowden documents.
Ok, CryptoAG is a story of its own, I agree. But they are not that much of a
major hardware vendor, either. Depends on which customer base you consider.
Agreed, but in the end it is important how they act in the long term. But
that needs more time to come until conclusions can be drawn. It is much more
difficult for hardware vendors to strike such good PR stunts as Google did.
Also, I guess, Google had this change in the works for a longer time,
otherwise I don't know if they could make the switch to crypto for
their internal cross-DC links so rapidly. It still seems a lot of work
+ testing and their services seem highly depending on good latency.
  Hannes

@_date: 2014-01-19 19:43:43
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Hi Hannes! :)
MPX was already committed to gcc trunk, so I hope this situation could
improve in future (it is reverted for 4.9 but I think it will come back
after the release in March).
(I am still not sure how this will be rolled out, maybe by switching
some software back to 32 bit to reduce the load on the pointer length
lookup tables.)
Maybe you can comment a bit on the code extraction process into compilable
There seems to be a semantic differences between the proofable
language and the language the extraction process targets in e.g. array
handling(e.g.  ocaml code) or just overflow handling in integers.
I guess Idris does not have this problem?
I always wondered if ats-lang would be the most suitable language for
writing more typesafe code?
  Hannes

@_date: 2014-01-20 14:42:34
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
MPX uses CPU managed out-of-band (but in application memory)
page-table-alike structures to store the bound table entries. They get
updated via specific cpu instructions, which result in nops on todays
cpus (so you can execute mpx code on non-mpx cpus and just won't have the
bounds checking). They also made sure that non-MPX code that is linked
against MPX code can propagate unbounded pointers, so you don't need to
switch your whole operating system to MPX enabled code at once (I guess
that would aslo be a problem memory-wise, but Intel entered the DRAM
business again, too :) ). The x64 linux ABI has also been updated.
While passing parameters and returning, MPX will introduce new registers
to pass those bounds checks automatically between function calls. I
guess this enables faster function calls because the cpu does not need
to store those pointer bounds in the permanent pointer bound tables thus
eliminating the stress on the cpu caches.
You can find details here:
What would be interesting, especially for the linux kernel, is to restrict
jmp and callq addresses so it is impossible for an attacker to get control
over them and e.g. dispatch own code on network packet dismantling without
needing whole pointer checking infrastructure e.g.
IIRC this was already addressed in the talk.
I would also like to point to Software Foundations from Benjamin Pierce here,
as it also has some great material to learn Coq:
Code generation without heavy runtime would also be nice.
ATS uses plain C as an intermediate language and the whole language feels
pretty low-level, too. So it seems it is easily possible to compile these
programs freestanding and also link those to other programs, which is
quite a nice feature, especially if one wants to make incrementally use
of more checked languages.
Thanks for your additional remarks,
  Hannes

@_date: 2014-01-20 17:03:22
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Just remembered there was some research on this already:
  Hannes

@_date: 2014-01-01 20:33:28
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
I was just referring to the Snowden documents.
Ok, CryptoAG is a story of its own, I agree. But they are not that much of a
major hardware vendor, either. Depends on which customer base you consider.
Agreed, but in the end it is important how they act in the long term. But
that needs more time to come until conclusions can be drawn. It is much more
difficult for hardware vendors to strike such good PR stunts as Google did.
Also, I guess, Google had this change in the works for a longer time,
otherwise I don't know if they could make the switch to crypto for
their internal cross-DC links so rapidly. It still seems a lot of work
+ testing and their services seem highly depending on good latency.
  Hannes

@_date: 2014-01-19 19:43:43
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Hi Hannes! :)
MPX was already committed to gcc trunk, so I hope this situation could
improve in future (it is reverted for 4.9 but I think it will come back
after the release in March).
(I am still not sure how this will be rolled out, maybe by switching
some software back to 32 bit to reduce the load on the pointer length
lookup tables.)
Maybe you can comment a bit on the code extraction process into compilable
There seems to be a semantic differences between the proofable
language and the language the extraction process targets in e.g. array
handling(e.g.  ocaml code) or just overflow handling in integers.
I guess Idris does not have this problem?
I always wondered if ats-lang would be the most suitable language for
writing more typesafe code?
  Hannes

@_date: 2014-01-20 14:42:34
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
MPX uses CPU managed out-of-band (but in application memory)
page-table-alike structures to store the bound table entries. They get
updated via specific cpu instructions, which result in nops on todays
cpus (so you can execute mpx code on non-mpx cpus and just won't have the
bounds checking). They also made sure that non-MPX code that is linked
against MPX code can propagate unbounded pointers, so you don't need to
switch your whole operating system to MPX enabled code at once (I guess
that would aslo be a problem memory-wise, but Intel entered the DRAM
business again, too :) ). The x64 linux ABI has also been updated.
While passing parameters and returning, MPX will introduce new registers
to pass those bounds checks automatically between function calls. I
guess this enables faster function calls because the cpu does not need
to store those pointer bounds in the permanent pointer bound tables thus
eliminating the stress on the cpu caches.
You can find details here:
What would be interesting, especially for the linux kernel, is to restrict
jmp and callq addresses so it is impossible for an attacker to get control
over them and e.g. dispatch own code on network packet dismantling without
needing whole pointer checking infrastructure e.g.
IIRC this was already addressed in the talk.
I would also like to point to Software Foundations from Benjamin Pierce here,
as it also has some great material to learn Coq:
Code generation without heavy runtime would also be nice.
ATS uses plain C as an intermediate language and the whole language feels
pretty low-level, too. So it seems it is easily possible to compile these
programs freestanding and also link those to other programs, which is
quite a nice feature, especially if one wants to make incrementally use
of more checked languages.
Thanks for your additional remarks,
  Hannes

@_date: 2014-01-20 17:03:22
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
Just remembered there was some research on this already:
  Hannes
