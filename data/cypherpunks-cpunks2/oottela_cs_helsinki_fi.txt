
@_date: 2014-07-26 11:32:26
@_author: oottela 
@_subject: Fwd: [Cryptography] hard to trust all those root CAs 
Are there any sources to the procedure how NSL's and other subpoenas / gag orders could be used to coerce certificate authorities to hand out their private keys?
My guess is the risk for using root certificate of different company for MITM is too high: EFF's SSL observatory would detect it. I'm suprised there has been no leaks about such attacks: It's fairly easy to mitigate, transparent, long term, and  extremely effective, even against Does anyone have guesses or information about how CA's handle their private keys? Are all certificates they sign for companies done on airgapped computers? How high are the security standards of these

@_date: 2014-07-26 11:32:26
@_author: oottela 
@_subject: Fwd: [Cryptography] hard to trust all those root CAs 
Are there any sources to the procedure how NSL's and other subpoenas / gag orders could be used to coerce certificate authorities to hand out their private keys?
My guess is the risk for using root certificate of different company for MITM is too high: EFF's SSL observatory would detect it. I'm suprised there has been no leaks about such attacks: It's fairly easy to mitigate, transparent, long term, and  extremely effective, even against Does anyone have guesses or information about how CA's handle their private keys? Are all certificates they sign for companies done on airgapped computers? How high are the security standards of these

@_date: 2014-07-26 11:32:26
@_author: oottela 
@_subject: Fwd: [Cryptography] hard to trust all those root CAs 
Are there any sources to the procedure how NSL's and other subpoenas / gag orders could be used to coerce certificate authorities to hand out their private keys?
My guess is the risk for using root certificate of different company for MITM is too high: EFF's SSL observatory would detect it. I'm suprised there has been no leaks about such attacks: It's fairly easy to mitigate, transparent, long term, and  extremely effective, even against Does anyone have guesses or information about how CA's handle their private keys? Are all certificates they sign for companies done on airgapped computers? How high are the security standards of these

@_date: 2014-06-08 05:33:39
@_author: oottela 
@_subject: Tinfoil Chat 
I'd like to share a project I came up with back in spring 2012 and begun working after the Snowden leaks started.
Highlights are
-OTP encryption for perfect secrecy
-OTP encrypted Keccak HMACs to prevent undetectable message tampering
-HW TRNG to generate truly random keys (Von Neumann whitened)
-HW Data diodes to provide immunity against message exfiltration attacks originating from network.
Source code and links to whitepaper and manual are available from
Regards, Markus

@_date: 2014-06-13 19:28:10
@_author: oottela 
@_subject: Tinfoil Chat 
Hi Cathal. Thanks for your review!
The RxM that does the MAC verification doesn't leak data to network connected computer so attacker would have to have line of sight to display of RxM and through trial and error send tampered packets until one doesn't show the warning about tampering. By then user should have pretty good idea about what is going on. Recovering key material shouldn't be a problem since key is not reused in signing of new messages. The implementation however has now been changed while fixing more critical vulnerability with hash function input size.
Regarding second issue, LibPurple does not contribute to privacy of messages in any way. The only thing the computer that LibPurple is running on handles is OTP encrypted data. OTP keys and plaintext messages never touch that computer and malicious functionality of that computer can not possibly request the actual keys from waterfall secured TxM or RxM. The whitepaper explains why the system is secure even if attacker gains root access to computer where Pidgin is running.
I did not write the program that samples entropy via GPIO. Programs written in C generally work faster so I didn't feel the need to redo the part. Also, the installer takes care of compiling. I've done some testing and according to Ent /dev/urandom is more random (7.999995 bits somewhat concerned about seeding, pseudo randomness and effect of modern hwrng devices - can seeding of urandom be compromised by /dev/hwrng input from processor with dopant trojan even if mixing entropy from functional TRNG is done. I'll have to read about the issue before I'll change the implementation. After all, with the TRNG there should be no auto-correlation if sampling speed is slow enough, and no bias when Von Neumann correction is used, despite what statistical tests say.
Also, I'll look into the ctypes library and see about the difference in sampling speed. As user has access to source I suppose compiling doesn't endanger the security in any notable way.

@_date: 2014-06-08 05:33:39
@_author: oottela 
@_subject: Tinfoil Chat 
I'd like to share a project I came up with back in spring 2012 and begun working after the Snowden leaks started.
Highlights are
-OTP encryption for perfect secrecy
-OTP encrypted Keccak HMACs to prevent undetectable message tampering
-HW TRNG to generate truly random keys (Von Neumann whitened)
-HW Data diodes to provide immunity against message exfiltration attacks originating from network.
Source code and links to whitepaper and manual are available from
Regards, Markus

@_date: 2014-06-13 19:28:10
@_author: oottela 
@_subject: Tinfoil Chat 
Hi Cathal. Thanks for your review!
The RxM that does the MAC verification doesn't leak data to network connected computer so attacker would have to have line of sight to display of RxM and through trial and error send tampered packets until one doesn't show the warning about tampering. By then user should have pretty good idea about what is going on. Recovering key material shouldn't be a problem since key is not reused in signing of new messages. The implementation however has now been changed while fixing more critical vulnerability with hash function input size.
Regarding second issue, LibPurple does not contribute to privacy of messages in any way. The only thing the computer that LibPurple is running on handles is OTP encrypted data. OTP keys and plaintext messages never touch that computer and malicious functionality of that computer can not possibly request the actual keys from waterfall secured TxM or RxM. The whitepaper explains why the system is secure even if attacker gains root access to computer where Pidgin is running.
I did not write the program that samples entropy via GPIO. Programs written in C generally work faster so I didn't feel the need to redo the part. Also, the installer takes care of compiling. I've done some testing and according to Ent /dev/urandom is more random (7.999995 bits somewhat concerned about seeding, pseudo randomness and effect of modern hwrng devices - can seeding of urandom be compromised by /dev/hwrng input from processor with dopant trojan even if mixing entropy from functional TRNG is done. I'll have to read about the issue before I'll change the implementation. After all, with the TRNG there should be no auto-correlation if sampling speed is slow enough, and no bias when Von Neumann correction is used, despite what statistical tests say.
Also, I'll look into the ctypes library and see about the difference in sampling speed. As user has access to source I suppose compiling doesn't endanger the security in any notable way.

@_date: 2014-06-08 05:33:39
@_author: oottela 
@_subject: Tinfoil Chat 
I'd like to share a project I came up with back in spring 2012 and begun working after the Snowden leaks started.
Highlights are
-OTP encryption for perfect secrecy
-OTP encrypted Keccak HMACs to prevent undetectable message tampering
-HW TRNG to generate truly random keys (Von Neumann whitened)
-HW Data diodes to provide immunity against message exfiltration attacks originating from network.
Source code and links to whitepaper and manual are available from
Regards, Markus

@_date: 2014-06-13 19:28:10
@_author: oottela 
@_subject: Tinfoil Chat 
Hi Cathal. Thanks for your review!
The RxM that does the MAC verification doesn't leak data to network connected computer so attacker would have to have line of sight to display of RxM and through trial and error send tampered packets until one doesn't show the warning about tampering. By then user should have pretty good idea about what is going on. Recovering key material shouldn't be a problem since key is not reused in signing of new messages. The implementation however has now been changed while fixing more critical vulnerability with hash function input size.
Regarding second issue, LibPurple does not contribute to privacy of messages in any way. The only thing the computer that LibPurple is running on handles is OTP encrypted data. OTP keys and plaintext messages never touch that computer and malicious functionality of that computer can not possibly request the actual keys from waterfall secured TxM or RxM. The whitepaper explains why the system is secure even if attacker gains root access to computer where Pidgin is running.
I did not write the program that samples entropy via GPIO. Programs written in C generally work faster so I didn't feel the need to redo the part. Also, the installer takes care of compiling. I've done some testing and according to Ent /dev/urandom is more random (7.999995 bits somewhat concerned about seeding, pseudo randomness and effect of modern hwrng devices - can seeding of urandom be compromised by /dev/hwrng input from processor with dopant trojan even if mixing entropy from functional TRNG is done. I'll have to read about the issue before I'll change the implementation. After all, with the TRNG there should be no auto-correlation if sampling speed is slow enough, and no bias when Von Neumann correction is used, despite what statistical tests say.
Also, I'll look into the ctypes library and see about the difference in sampling speed. As user has access to source I suppose compiling doesn't endanger the security in any notable way.

@_date: 2015-04-12 16:11:52
@_author: Markus Ottela 
@_subject: Briar 
NSA gets massive amounts of text messages through it's Dishfire program.
Users should not assume they're excluded just because the program had
limited scope of 200,000,000 SMS per day -- four years ago.
The content is unavailable in both data channels, yet you get better
protection against metadata analysis by routing TextSecure traffic
through Tor.

@_date: 2015-04-18 04:22:32
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Now that I think of it, SW implementations with the CEV version that
cascades symmetric ciphers are very very slow if SoCs such as RPi are
used. OTP and one time MAC is naturally very fast but I'm not sure how
large key storages can be added for micro controllers: users should
probably use OTF-encrypted HDDs to protect key data and avoid wear
levelling issues of flash memory.
Anyway, I pushed out 0.5.4. of TFC out yesterday. Lot's of fixes for
stability and usability, signed installer that checks SHA512 hashes of
other files. Probably the most important feature is hiding 'when' and
'how much' communication takes place. This is done by sending a constant
stream of noise messages and commands from the transmitter unit the
receivers transparently discard. This exhausts OTP keyfiles very quickly
so I'd recommend using the CEV version.

@_date: 2015-04-12 16:11:52
@_author: Markus Ottela 
@_subject: Briar 
NSA gets massive amounts of text messages through it's Dishfire program.
Users should not assume they're excluded just because the program had
limited scope of 200,000,000 SMS per day -- four years ago.
The content is unavailable in both data channels, yet you get better
protection against metadata analysis by routing TextSecure traffic
through Tor.

@_date: 2015-04-18 04:22:32
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Now that I think of it, SW implementations with the CEV version that
cascades symmetric ciphers are very very slow if SoCs such as RPi are
used. OTP and one time MAC is naturally very fast but I'm not sure how
large key storages can be added for micro controllers: users should
probably use OTF-encrypted HDDs to protect key data and avoid wear
levelling issues of flash memory.
Anyway, I pushed out 0.5.4. of TFC out yesterday. Lot's of fixes for
stability and usability, signed installer that checks SHA512 hashes of
other files. Probably the most important feature is hiding 'when' and
'how much' communication takes place. This is done by sending a constant
stream of noise messages and commands from the transmitter unit the
receivers transparently discard. This exhausts OTP keyfiles very quickly
so I'd recommend using the CEV version.

@_date: 2015-04-12 16:11:52
@_author: Markus Ottela 
@_subject: Briar 
NSA gets massive amounts of text messages through it's Dishfire program.
Users should not assume they're excluded just because the program had
limited scope of 200,000,000 SMS per day -- four years ago.
The content is unavailable in both data channels, yet you get better
protection against metadata analysis by routing TextSecure traffic
through Tor.

@_date: 2015-04-18 04:22:32
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Now that I think of it, SW implementations with the CEV version that
cascades symmetric ciphers are very very slow if SoCs such as RPi are
used. OTP and one time MAC is naturally very fast but I'm not sure how
large key storages can be added for micro controllers: users should
probably use OTF-encrypted HDDs to protect key data and avoid wear
levelling issues of flash memory.
Anyway, I pushed out 0.5.4. of TFC out yesterday. Lot's of fixes for
stability and usability, signed installer that checks SHA512 hashes of
other files. Probably the most important feature is hiding 'when' and
'how much' communication takes place. This is done by sending a constant
stream of noise messages and commands from the transmitter unit the
receivers transparently discard. This exhausts OTP keyfiles very quickly
so I'd recommend using the CEV version.

@_date: 2015-02-03 19:28:01
@_author: Markus Ottela 
@_subject: Tox.im 
From the PoW of Stef's seven rules of thumb to detect snake oil:
*1. Not free software *
"/Licenced the code under the GPL for now./" (Free software? Good. But, "for now" ? Is it going to change?)
*2. Runs in a browser *
*3. Runs on a smartphone *
Has been suggested but not yet implemented.
*4. The user doesn't generate, or exclusively own the private encryption The user is in control, yet the source of randomness and crypto implementation are
  not explained properly. The wiki talks about public keys and PFS without explaining
the relation between the two.
*5. There is no threat model*
"/With the rise of government monitoring programs/" implies it's designed to be secure against state surveillance.
"Tox does not cloak IP addresses when communicating with other users"
In disclaimer it is also just stated that
"/Tox prevents message contents from being read or altered by third parties, or anyone else other than the intended recipient/", yet it doesn't even bother to evaluate the system against HSAs or MSAs.
Instead, the threat model seems to revolve around developer anonymity ( "/Potential harassment by the government and trolls/" seems to include people pointing out issues with the software as well.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It doesn't, although it does say "/leading-class encryption/", and the logo is yet another unnecessary lock.
*7. Neglects general sad state of host security *
This. The developers think it is obvious for every user, that if the endpoint device is compromised, there is no security. This is horrible since average computer user is still mainly occupied with thoughts "I need a firewall" or "I might get a virus" -- not "The government might exploit unpatched OS or exploit a 0-day" or "The company behind my proprietary OS might be issued a subpoena to include a backdoor". It's not the job of Tox developers to patch OS, but it's their job to warn users there are attack vectors the developers are not in control of. They have refused to do so, which limits the users ability to make informed choices depending on their threat model.
For some time I've wanted to evaluate TFC from these perspectives as well:
1. Not free software *
It is, and it will always be.
*2. Runs in a browser
*It doesn't, and never will.
*3. Runs on a smartphone
*Only the handler of encrypted messages might in future run on smartphone (or proprietary OS), the TCB's never will.
*4. The user doesn't generate, or exclusively own the private encryption *The user does, and the user is also in control of the circuit that generates the encryption keys.
*5. There is no threat model
*The whitepaper has a five-page dissection about different attack vectors, what TFC is secure against and what it is not.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It never has and it never will.
7. Neglects general sad state of host security*
This was the starting point. Key-exfiltration-wise, it is immune against post exploitation of TCB-modules.

@_date: 2015-02-03 21:55:07
@_author: Markus Ottela 
@_subject: Tox.im 
Just because it could be worse doesn't mean it couldn't be better.
Thanks for the whitepaper, I'll have a look when I've the time.
IIRC the DH signing keys are bound the the account ID. Appelbaum recommended in his 31c3 talk 'Reconstructing Narratives' that users rotate their OTR keys often and verify the hash using off-band channel.
I'm not sure it's a convenient thing users have to re-add their contacts every time the DH signing key needs to be refreshed. It's sort of good thing users are immediately using the public signing key (Tox ID) but the issue is, while the Tox ID doesn't have to be secret, it must be authentic: so users unaware of this can be subjected to MITM attack.
This only changes the type of attack: a keylogger has to be used along the private key exfiltration tool.
I would argue the current OTR/PGP/ZRTP implementation has limited lifespan regardless of execution, given the fact intelligence community is expanding end-point exploitation to mass surveillance levels: methodology is changing, not the scale:
There's a lot of misconception on 0-days being expensive 'one-time-hacks' that must be used only when necessary. How many anti-virus programs detect and report these? What percentage of users are running some sort of IDS? How many users assume sudden system crash is due to malfunctioning exploit/payload? A 0-day is more like a master key for given OS with average lifespan of 300 days (  )
I agree it should be separate. I tried to keep that section short and the intention
was to provide contrast and show each of these can be addressed

@_date: 2015-02-04 00:54:07
@_author: Markus Ottela 
@_subject: Tox.im 
They are ignoring the criticism they should be warning users about constant issues in endpoint security: Subrosa, Ricochet, TextSecure, Cryptocat and Threema have all included a threat model/warning, Tox should do so too. Notifying users about risks is what keeps them safe, not moving to slightly more secure products they assume are impenetrable. Conscious ignoring of this on the developers part equals selling "bullshit".
Lets assume they put the warning on the web page. Now every user who reads the security warning begins to think "Ok, so given my contacts, reputation and opsec, my private key is compromised with probability P. Am I still going to write this or do I upgrade my tools? Am I under constant monitoring? Do I need to regenerate my keys?". In the beginning of Citizenfour, Snowden gives a warning to Poitras about private keys: even though PGP encrypts private key at rest. After that, Laura bought an airgapped machine and created new PGP keypair.
Again, security is a process, not a product: unless the implementation of crypto is secure and users know how to use it, properly written Salsa20 implementation isn't going to do much good. Writing a good manual is the responsibility of the developer.
Here's a Metasploit payload Meterpreter. How hard do you think it's for me to automate the two Armitage GUI functionalities of browsing files and logging keystrokes once I buy a 0-day from Vupen with tax money?
Now think about Fox Acid, Metasploit, with a budget. Then think of things like Quantuminsert that automate this process on mass scale. Your seat belt is a bad analogy. There are no bulletproof solutions but there are better ones.
No, the point here is, don't put TCB on a computer that does networking. Why are you putting emphasis on the word 'might' when Snowden says NSA bypasses encryption *every day*:
No, let's not assume. I've a small desk but it's still able to handle the three laptops in a configuration that does not have the issue.
The community has already accepted the host security as part of snake oil check. What on earth is the check doing here if we should accept OS vulnerabilities as a "baseline"? If the product isn't going to address it, it better not neglect it at least, Tox doesn't do even that.
I'm not trying to hijack this Tox discussion to say TFC is the solution. I'm trying to say it's pointless to create anything secure on a setup the features of which are limited(/rigged) to begin with. That's why smartphone is part of the snake oil checklist. The very first step says the product has to be FOSS, without free OS, no encryption software stands a chance. Without endpoint security, it's the same. The community is already praising $1,300 Novena laptops - I'm saying we can achieve higher security with set of three $200 COTS laptops and a few extra

@_date: 2015-02-04 03:40:02
@_author: Markus Ottela 
@_subject: Tox.im 
Tox developer team were not interested in implementing it in similar fashion. Using three computers was the main obstruction: A successor for Skype that makes the headlines is the one that you get everyone to use because it's easy to setup. It wouldn't get any attention nor media coverage if it wasn't free as in 'next, yes, next, next, install'.
I'd rather not meddle with Tox source: to quote the Norton's article you "C is good for two things: being beautiful and creating catastrophic 0days in memory management."
Tox is written in C, by people who seem to have limited understanding on computer security and programming. I do too, but a least I selected an approach that doesn't require 0-day free code, or OS.
TFC stands for Tinfoil Chat.
cs.helsinki.fi/u/oottela/tfc.pdf  // pages 9 and 10 explain how why there is no key exfiltration risk.
TCB is the Trusted Computing Base, the system responsible for cryptographic operations.
It depends on your threat model and how technically skilled your adversary is.
If adversarial government decides to buy malware from say, Hacking Team that automatically
replaces Tox IDs inside unencrypted emails to those owned by the state, it'll still get you killed unless you know what you're doing.
Just telling the user to meet the contact and exchange Tox ID in person is enough not to get MITM'd.
Just warning the user about not saying the most sensitive stuff on Tox might be enough to not to get killed.
If you're not in control of the laptop, you shouldn't be trusting your life on it; Tox does very little if there's a keylogger present, neither does TFC if you're not in control of the two TCB computers.
That part sounds like infomercial trying to overcomplicate a problem.
You need one device to store the (a)symmetric encryption keys (TCB 1)
You need another      to store the (a)symmetric decryption keys (TCB 2)
You need third one to transmit encrypted messages.
You need data diodes to enforce unidirectional communication between the devices. That's all.
The issue is global whether it's occupy movement fighting against economic segregation in the West,
or dissidents in 3rd world countries. The difference is the threat model. In west it's HSAs, in poor countries,
MSAs at top, unless it's the US doing surveillance against Afghans etc.
I get what you mean. You're trying to evaluate the skillset of developers in terms of
how things are implemented and programmed. I'm trying to say they've a bigger job
to do and so far they have failed at it.

@_date: 2015-02-04 06:39:48
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
Dingdingding. And we have a winner: Tinfoil Chat it is. Though I liked Gutmann's answer the most.

@_date: 2015-02-04 20:46:16
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
To summarise the general classess of side channels mentioned in Wikipedia:
Power analysis should be tackled with running the TCBs on batteries. I introduced the issue of electromagnetic and acoustic leaks but it's a very complex issue and I'm not an expert dealing with them. The RxM is the only device attacker can introduce faulty data to compute on. However, no feedback is available due to the implementation thus unless pre-compromised, the hardware should not have back channel. TFC does it's best effort to overwrite and verify overwriting after key material has been used. Each of these is mentioned in white paper. More work is needed to create high-assurance physical/close proximity security but again, user is informed about the issues and the main threat is automated remote exploitation.
The purpose of Pidgin is to transmit the messages. To simplify, TFC is a plugin for Pidgin that automates you doing encryption in a secure environment and typing the ciphertext to OTR encrypted Pidgin window with your keyboard. It also automates decryption of ciphertexts you receive when OTR-plugin of Pidgin decrypts the outer layer of message.
So for TFC the encryption is SSL( OTR( OTP(Message)||MAC ))
and for TFC-CEV you replace OTP(Message)||MAC with The pages 9 and 10 of whitepaper explains this in more detail. Please let me know if there's anything that needs to be clarified.

@_date: 2015-02-03 19:28:01
@_author: Markus Ottela 
@_subject: Tox.im 
From the PoW of Stef's seven rules of thumb to detect snake oil:
*1. Not free software *
"/Licenced the code under the GPL for now./" (Free software? Good. But, "for now" ? Is it going to change?)
*2. Runs in a browser *
*3. Runs on a smartphone *
Has been suggested but not yet implemented.
*4. The user doesn't generate, or exclusively own the private encryption The user is in control, yet the source of randomness and crypto implementation are
  not explained properly. The wiki talks about public keys and PFS without explaining
the relation between the two.
*5. There is no threat model*
"/With the rise of government monitoring programs/" implies it's designed to be secure against state surveillance.
"Tox does not cloak IP addresses when communicating with other users"
In disclaimer it is also just stated that
"/Tox prevents message contents from being read or altered by third parties, or anyone else other than the intended recipient/", yet it doesn't even bother to evaluate the system against HSAs or MSAs.
Instead, the threat model seems to revolve around developer anonymity ( "/Potential harassment by the government and trolls/" seems to include people pointing out issues with the software as well.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It doesn't, although it does say "/leading-class encryption/", and the logo is yet another unnecessary lock.
*7. Neglects general sad state of host security *
This. The developers think it is obvious for every user, that if the endpoint device is compromised, there is no security. This is horrible since average computer user is still mainly occupied with thoughts "I need a firewall" or "I might get a virus" -- not "The government might exploit unpatched OS or exploit a 0-day" or "The company behind my proprietary OS might be issued a subpoena to include a backdoor". It's not the job of Tox developers to patch OS, but it's their job to warn users there are attack vectors the developers are not in control of. They have refused to do so, which limits the users ability to make informed choices depending on their threat model.
For some time I've wanted to evaluate TFC from these perspectives as well:
1. Not free software *
It is, and it will always be.
*2. Runs in a browser
*It doesn't, and never will.
*3. Runs on a smartphone
*Only the handler of encrypted messages might in future run on smartphone (or proprietary OS), the TCB's never will.
*4. The user doesn't generate, or exclusively own the private encryption *The user does, and the user is also in control of the circuit that generates the encryption keys.
*5. There is no threat model
*The whitepaper has a five-page dissection about different attack vectors, what TFC is secure against and what it is not.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It never has and it never will.
7. Neglects general sad state of host security*
This was the starting point. Key-exfiltration-wise, it is immune against post exploitation of TCB-modules.

@_date: 2015-02-03 21:55:07
@_author: Markus Ottela 
@_subject: Tox.im 
Just because it could be worse doesn't mean it couldn't be better.
Thanks for the whitepaper, I'll have a look when I've the time.
IIRC the DH signing keys are bound the the account ID. Appelbaum recommended in his 31c3 talk 'Reconstructing Narratives' that users rotate their OTR keys often and verify the hash using off-band channel.
I'm not sure it's a convenient thing users have to re-add their contacts every time the DH signing key needs to be refreshed. It's sort of good thing users are immediately using the public signing key (Tox ID) but the issue is, while the Tox ID doesn't have to be secret, it must be authentic: so users unaware of this can be subjected to MITM attack.
This only changes the type of attack: a keylogger has to be used along the private key exfiltration tool.
I would argue the current OTR/PGP/ZRTP implementation has limited lifespan regardless of execution, given the fact intelligence community is expanding end-point exploitation to mass surveillance levels: methodology is changing, not the scale:
There's a lot of misconception on 0-days being expensive 'one-time-hacks' that must be used only when necessary. How many anti-virus programs detect and report these? What percentage of users are running some sort of IDS? How many users assume sudden system crash is due to malfunctioning exploit/payload? A 0-day is more like a master key for given OS with average lifespan of 300 days (  )
I agree it should be separate. I tried to keep that section short and the intention
was to provide contrast and show each of these can be addressed

@_date: 2015-02-04 00:54:07
@_author: Markus Ottela 
@_subject: Tox.im 
They are ignoring the criticism they should be warning users about constant issues in endpoint security: Subrosa, Ricochet, TextSecure, Cryptocat and Threema have all included a threat model/warning, Tox should do so too. Notifying users about risks is what keeps them safe, not moving to slightly more secure products they assume are impenetrable. Conscious ignoring of this on the developers part equals selling "bullshit".
Lets assume they put the warning on the web page. Now every user who reads the security warning begins to think "Ok, so given my contacts, reputation and opsec, my private key is compromised with probability P. Am I still going to write this or do I upgrade my tools? Am I under constant monitoring? Do I need to regenerate my keys?". In the beginning of Citizenfour, Snowden gives a warning to Poitras about private keys: even though PGP encrypts private key at rest. After that, Laura bought an airgapped machine and created new PGP keypair.
Again, security is a process, not a product: unless the implementation of crypto is secure and users know how to use it, properly written Salsa20 implementation isn't going to do much good. Writing a good manual is the responsibility of the developer.
Here's a Metasploit payload Meterpreter. How hard do you think it's for me to automate the two Armitage GUI functionalities of browsing files and logging keystrokes once I buy a 0-day from Vupen with tax money?
Now think about Fox Acid, Metasploit, with a budget. Then think of things like Quantuminsert that automate this process on mass scale. Your seat belt is a bad analogy. There are no bulletproof solutions but there are better ones.
No, the point here is, don't put TCB on a computer that does networking. Why are you putting emphasis on the word 'might' when Snowden says NSA bypasses encryption *every day*:
No, let's not assume. I've a small desk but it's still able to handle the three laptops in a configuration that does not have the issue.
The community has already accepted the host security as part of snake oil check. What on earth is the check doing here if we should accept OS vulnerabilities as a "baseline"? If the product isn't going to address it, it better not neglect it at least, Tox doesn't do even that.
I'm not trying to hijack this Tox discussion to say TFC is the solution. I'm trying to say it's pointless to create anything secure on a setup the features of which are limited(/rigged) to begin with. That's why smartphone is part of the snake oil checklist. The very first step says the product has to be FOSS, without free OS, no encryption software stands a chance. Without endpoint security, it's the same. The community is already praising $1,300 Novena laptops - I'm saying we can achieve higher security with set of three $200 COTS laptops and a few extra

@_date: 2015-02-04 03:40:02
@_author: Markus Ottela 
@_subject: Tox.im 
Tox developer team were not interested in implementing it in similar fashion. Using three computers was the main obstruction: A successor for Skype that makes the headlines is the one that you get everyone to use because it's easy to setup. It wouldn't get any attention nor media coverage if it wasn't free as in 'next, yes, next, next, install'.
I'd rather not meddle with Tox source: to quote the Norton's article you "C is good for two things: being beautiful and creating catastrophic 0days in memory management."
Tox is written in C, by people who seem to have limited understanding on computer security and programming. I do too, but a least I selected an approach that doesn't require 0-day free code, or OS.
TFC stands for Tinfoil Chat.
cs.helsinki.fi/u/oottela/tfc.pdf  // pages 9 and 10 explain how why there is no key exfiltration risk.
TCB is the Trusted Computing Base, the system responsible for cryptographic operations.
It depends on your threat model and how technically skilled your adversary is.
If adversarial government decides to buy malware from say, Hacking Team that automatically
replaces Tox IDs inside unencrypted emails to those owned by the state, it'll still get you killed unless you know what you're doing.
Just telling the user to meet the contact and exchange Tox ID in person is enough not to get MITM'd.
Just warning the user about not saying the most sensitive stuff on Tox might be enough to not to get killed.
If you're not in control of the laptop, you shouldn't be trusting your life on it; Tox does very little if there's a keylogger present, neither does TFC if you're not in control of the two TCB computers.
That part sounds like infomercial trying to overcomplicate a problem.
You need one device to store the (a)symmetric encryption keys (TCB 1)
You need another      to store the (a)symmetric decryption keys (TCB 2)
You need third one to transmit encrypted messages.
You need data diodes to enforce unidirectional communication between the devices. That's all.
The issue is global whether it's occupy movement fighting against economic segregation in the West,
or dissidents in 3rd world countries. The difference is the threat model. In west it's HSAs, in poor countries,
MSAs at top, unless it's the US doing surveillance against Afghans etc.
I get what you mean. You're trying to evaluate the skillset of developers in terms of
how things are implemented and programmed. I'm trying to say they've a bigger job
to do and so far they have failed at it.

@_date: 2015-02-04 06:39:48
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
Dingdingding. And we have a winner: Tinfoil Chat it is. Though I liked Gutmann's answer the most.

@_date: 2015-02-04 20:46:16
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
To summarise the general classess of side channels mentioned in Wikipedia:
Power analysis should be tackled with running the TCBs on batteries. I introduced the issue of electromagnetic and acoustic leaks but it's a very complex issue and I'm not an expert dealing with them. The RxM is the only device attacker can introduce faulty data to compute on. However, no feedback is available due to the implementation thus unless pre-compromised, the hardware should not have back channel. TFC does it's best effort to overwrite and verify overwriting after key material has been used. Each of these is mentioned in white paper. More work is needed to create high-assurance physical/close proximity security but again, user is informed about the issues and the main threat is automated remote exploitation.
The purpose of Pidgin is to transmit the messages. To simplify, TFC is a plugin for Pidgin that automates you doing encryption in a secure environment and typing the ciphertext to OTR encrypted Pidgin window with your keyboard. It also automates decryption of ciphertexts you receive when OTR-plugin of Pidgin decrypts the outer layer of message.
So for TFC the encryption is SSL( OTR( OTP(Message)||MAC ))
and for TFC-CEV you replace OTP(Message)||MAC with The pages 9 and 10 of whitepaper explains this in more detail. Please let me know if there's anything that needs to be clarified.

@_date: 2015-02-03 19:28:01
@_author: Markus Ottela 
@_subject: Tox.im 
From the PoW of Stef's seven rules of thumb to detect snake oil:
*1. Not free software *
"/Licenced the code under the GPL for now./" (Free software? Good. But, "for now" ? Is it going to change?)
*2. Runs in a browser *
*3. Runs on a smartphone *
Has been suggested but not yet implemented.
*4. The user doesn't generate, or exclusively own the private encryption The user is in control, yet the source of randomness and crypto implementation are
  not explained properly. The wiki talks about public keys and PFS without explaining
the relation between the two.
*5. There is no threat model*
"/With the rise of government monitoring programs/" implies it's designed to be secure against state surveillance.
"Tox does not cloak IP addresses when communicating with other users"
In disclaimer it is also just stated that
"/Tox prevents message contents from being read or altered by third parties, or anyone else other than the intended recipient/", yet it doesn't even bother to evaluate the system against HSAs or MSAs.
Instead, the threat model seems to revolve around developer anonymity ( "/Potential harassment by the government and trolls/" seems to include people pointing out issues with the software as well.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It doesn't, although it does say "/leading-class encryption/", and the logo is yet another unnecessary lock.
*7. Neglects general sad state of host security *
This. The developers think it is obvious for every user, that if the endpoint device is compromised, there is no security. This is horrible since average computer user is still mainly occupied with thoughts "I need a firewall" or "I might get a virus" -- not "The government might exploit unpatched OS or exploit a 0-day" or "The company behind my proprietary OS might be issued a subpoena to include a backdoor". It's not the job of Tox developers to patch OS, but it's their job to warn users there are attack vectors the developers are not in control of. They have refused to do so, which limits the users ability to make informed choices depending on their threat model.
For some time I've wanted to evaluate TFC from these perspectives as well:
1. Not free software *
It is, and it will always be.
*2. Runs in a browser
*It doesn't, and never will.
*3. Runs on a smartphone
*Only the handler of encrypted messages might in future run on smartphone (or proprietary OS), the TCB's never will.
*4. The user doesn't generate, or exclusively own the private encryption *The user does, and the user is also in control of the circuit that generates the encryption keys.
*5. There is no threat model
*The whitepaper has a five-page dissection about different attack vectors, what TFC is secure against and what it is not.
*6. Uses marketing-terminology like "cyber", "military-grade"*
It never has and it never will.
7. Neglects general sad state of host security*
This was the starting point. Key-exfiltration-wise, it is immune against post exploitation of TCB-modules.

@_date: 2015-02-03 21:55:07
@_author: Markus Ottela 
@_subject: Tox.im 
Just because it could be worse doesn't mean it couldn't be better.
Thanks for the whitepaper, I'll have a look when I've the time.
IIRC the DH signing keys are bound the the account ID. Appelbaum recommended in his 31c3 talk 'Reconstructing Narratives' that users rotate their OTR keys often and verify the hash using off-band channel.
I'm not sure it's a convenient thing users have to re-add their contacts every time the DH signing key needs to be refreshed. It's sort of good thing users are immediately using the public signing key (Tox ID) but the issue is, while the Tox ID doesn't have to be secret, it must be authentic: so users unaware of this can be subjected to MITM attack.
This only changes the type of attack: a keylogger has to be used along the private key exfiltration tool.
I would argue the current OTR/PGP/ZRTP implementation has limited lifespan regardless of execution, given the fact intelligence community is expanding end-point exploitation to mass surveillance levels: methodology is changing, not the scale:
There's a lot of misconception on 0-days being expensive 'one-time-hacks' that must be used only when necessary. How many anti-virus programs detect and report these? What percentage of users are running some sort of IDS? How many users assume sudden system crash is due to malfunctioning exploit/payload? A 0-day is more like a master key for given OS with average lifespan of 300 days (  )
I agree it should be separate. I tried to keep that section short and the intention
was to provide contrast and show each of these can be addressed

@_date: 2015-02-04 00:54:07
@_author: Markus Ottela 
@_subject: Tox.im 
They are ignoring the criticism they should be warning users about constant issues in endpoint security: Subrosa, Ricochet, TextSecure, Cryptocat and Threema have all included a threat model/warning, Tox should do so too. Notifying users about risks is what keeps them safe, not moving to slightly more secure products they assume are impenetrable. Conscious ignoring of this on the developers part equals selling "bullshit".
Lets assume they put the warning on the web page. Now every user who reads the security warning begins to think "Ok, so given my contacts, reputation and opsec, my private key is compromised with probability P. Am I still going to write this or do I upgrade my tools? Am I under constant monitoring? Do I need to regenerate my keys?". In the beginning of Citizenfour, Snowden gives a warning to Poitras about private keys: even though PGP encrypts private key at rest. After that, Laura bought an airgapped machine and created new PGP keypair.
Again, security is a process, not a product: unless the implementation of crypto is secure and users know how to use it, properly written Salsa20 implementation isn't going to do much good. Writing a good manual is the responsibility of the developer.
Here's a Metasploit payload Meterpreter. How hard do you think it's for me to automate the two Armitage GUI functionalities of browsing files and logging keystrokes once I buy a 0-day from Vupen with tax money?
Now think about Fox Acid, Metasploit, with a budget. Then think of things like Quantuminsert that automate this process on mass scale. Your seat belt is a bad analogy. There are no bulletproof solutions but there are better ones.
No, the point here is, don't put TCB on a computer that does networking. Why are you putting emphasis on the word 'might' when Snowden says NSA bypasses encryption *every day*:
No, let's not assume. I've a small desk but it's still able to handle the three laptops in a configuration that does not have the issue.
The community has already accepted the host security as part of snake oil check. What on earth is the check doing here if we should accept OS vulnerabilities as a "baseline"? If the product isn't going to address it, it better not neglect it at least, Tox doesn't do even that.
I'm not trying to hijack this Tox discussion to say TFC is the solution. I'm trying to say it's pointless to create anything secure on a setup the features of which are limited(/rigged) to begin with. That's why smartphone is part of the snake oil checklist. The very first step says the product has to be FOSS, without free OS, no encryption software stands a chance. Without endpoint security, it's the same. The community is already praising $1,300 Novena laptops - I'm saying we can achieve higher security with set of three $200 COTS laptops and a few extra

@_date: 2015-02-04 03:40:02
@_author: Markus Ottela 
@_subject: Tox.im 
Tox developer team were not interested in implementing it in similar fashion. Using three computers was the main obstruction: A successor for Skype that makes the headlines is the one that you get everyone to use because it's easy to setup. It wouldn't get any attention nor media coverage if it wasn't free as in 'next, yes, next, next, install'.
I'd rather not meddle with Tox source: to quote the Norton's article you "C is good for two things: being beautiful and creating catastrophic 0days in memory management."
Tox is written in C, by people who seem to have limited understanding on computer security and programming. I do too, but a least I selected an approach that doesn't require 0-day free code, or OS.
TFC stands for Tinfoil Chat.
cs.helsinki.fi/u/oottela/tfc.pdf  // pages 9 and 10 explain how why there is no key exfiltration risk.
TCB is the Trusted Computing Base, the system responsible for cryptographic operations.
It depends on your threat model and how technically skilled your adversary is.
If adversarial government decides to buy malware from say, Hacking Team that automatically
replaces Tox IDs inside unencrypted emails to those owned by the state, it'll still get you killed unless you know what you're doing.
Just telling the user to meet the contact and exchange Tox ID in person is enough not to get MITM'd.
Just warning the user about not saying the most sensitive stuff on Tox might be enough to not to get killed.
If you're not in control of the laptop, you shouldn't be trusting your life on it; Tox does very little if there's a keylogger present, neither does TFC if you're not in control of the two TCB computers.
That part sounds like infomercial trying to overcomplicate a problem.
You need one device to store the (a)symmetric encryption keys (TCB 1)
You need another      to store the (a)symmetric decryption keys (TCB 2)
You need third one to transmit encrypted messages.
You need data diodes to enforce unidirectional communication between the devices. That's all.
The issue is global whether it's occupy movement fighting against economic segregation in the West,
or dissidents in 3rd world countries. The difference is the threat model. In west it's HSAs, in poor countries,
MSAs at top, unless it's the US doing surveillance against Afghans etc.
I get what you mean. You're trying to evaluate the skillset of developers in terms of
how things are implemented and programmed. I'm trying to say they've a bigger job
to do and so far they have failed at it.

@_date: 2015-02-04 06:39:48
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
Dingdingding. And we have a winner: Tinfoil Chat it is. Though I liked Gutmann's answer the most.

@_date: 2015-02-04 20:46:16
@_author: Markus Ottela 
@_subject: What the fark is "TFC" 
To summarise the general classess of side channels mentioned in Wikipedia:
Power analysis should be tackled with running the TCBs on batteries. I introduced the issue of electromagnetic and acoustic leaks but it's a very complex issue and I'm not an expert dealing with them. The RxM is the only device attacker can introduce faulty data to compute on. However, no feedback is available due to the implementation thus unless pre-compromised, the hardware should not have back channel. TFC does it's best effort to overwrite and verify overwriting after key material has been used. Each of these is mentioned in white paper. More work is needed to create high-assurance physical/close proximity security but again, user is informed about the issues and the main threat is automated remote exploitation.
The purpose of Pidgin is to transmit the messages. To simplify, TFC is a plugin for Pidgin that automates you doing encryption in a secure environment and typing the ciphertext to OTR encrypted Pidgin window with your keyboard. It also automates decryption of ciphertexts you receive when OTR-plugin of Pidgin decrypts the outer layer of message.
So for TFC the encryption is SSL( OTR( OTP(Message)||MAC ))
and for TFC-CEV you replace OTP(Message)||MAC with The pages 9 and 10 of whitepaper explains this in more detail. Please let me know if there's anything that needs to be clarified.

@_date: 2015-07-24 18:17:31
@_author: Markus Ottela 
@_subject: True Crypt is Not Secure 
What the warning by developers means is, in the event a vulnerability would be found in Truecrypt, no one would be there to fix it.
Except for the entire open source community who would publish dozens of "here's how to fix the vulnerability in source code before recompiling it" accompanied with
warnings "doing this breaks the truecrypt licence". This is true, and you might be in trouble
in case the anonymous developers want to stop being anonymous and prosecute you across
different jurisdictions. So, unlikely. Also read these:
I heard TC uses insufficiently low iteration count for PBKDF2? -- this doesn't change the fact
a high entropy passphrase (>128bits) remains unbreakable in feasible time.

@_date: 2015-07-24 18:17:31
@_author: Markus Ottela 
@_subject: True Crypt is Not Secure 
What the warning by developers means is, in the event a vulnerability would be found in Truecrypt, no one would be there to fix it.
Except for the entire open source community who would publish dozens of "here's how to fix the vulnerability in source code before recompiling it" accompanied with
warnings "doing this breaks the truecrypt licence". This is true, and you might be in trouble
in case the anonymous developers want to stop being anonymous and prosecute you across
different jurisdictions. So, unlikely. Also read these:
I heard TC uses insufficiently low iteration count for PBKDF2? -- this doesn't change the fact
a high entropy passphrase (>128bits) remains unbreakable in feasible time.

@_date: 2015-07-24 18:17:31
@_author: Markus Ottela 
@_subject: True Crypt is Not Secure 
What the warning by developers means is, in the event a vulnerability would be found in Truecrypt, no one would be there to fix it.
Except for the entire open source community who would publish dozens of "here's how to fix the vulnerability in source code before recompiling it" accompanied with
warnings "doing this breaks the truecrypt licence". This is true, and you might be in trouble
in case the anonymous developers want to stop being anonymous and prosecute you across
different jurisdictions. So, unlikely. Also read these:
I heard TC uses insufficiently low iteration count for PBKDF2? -- this doesn't change the fact
a high entropy passphrase (>128bits) remains unbreakable in feasible time.

@_date: 2015-03-29 00:46:08
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
A microcontroller as TCB doing OTP with HWRNG-generated keys. Sounds a
lot like the OTP-version of Tinfoil Chat ( github.com/maqp/tfc ). It
doesn't take a spy or terrorist to create something like this: TFC was a
hobby of a CS-student.
Distribution of key material isn't the big problem, keeping the keys
secure from end-point exploitation is as TAO, ANT-implants, COMMONDEER,
VALIDATOR, QUANTUM w/ UNITEDRAKE/SALVAGERABBIT etc. make it hard. But
even these could be addressed in TFC - enforcing the need for close
access operations, close proximity malware injection or retro reflectors
and other HW implants is the only way to avoid untasked targeting from
becoming the mass surveillance of next generation; It's the sweet spot
of security, as the attack can not be automated, and the cost increases
linearly with the number of targets.

@_date: 2015-03-31 18:47:01
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Feel free to choose your own devices / micro controllers as long as it
supports the data diode and doesn't have wireless / audio devices that
provide covert channels to HSAs. Wide range of platforms makes
compromise of COTS hardware much more difficult. Netbooks are not
significantly more expensive than if one were to buy separate batteries,
chargers, displays, cables and peripherals -- It's also more convenient.
Two netbooks pushes the system price around that of a Blackphone.

@_date: 2015-03-29 00:46:08
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
A microcontroller as TCB doing OTP with HWRNG-generated keys. Sounds a
lot like the OTP-version of Tinfoil Chat ( github.com/maqp/tfc ). It
doesn't take a spy or terrorist to create something like this: TFC was a
hobby of a CS-student.
Distribution of key material isn't the big problem, keeping the keys
secure from end-point exploitation is as TAO, ANT-implants, COMMONDEER,
VALIDATOR, QUANTUM w/ UNITEDRAKE/SALVAGERABBIT etc. make it hard. But
even these could be addressed in TFC - enforcing the need for close
access operations, close proximity malware injection or retro reflectors
and other HW implants is the only way to avoid untasked targeting from
becoming the mass surveillance of next generation; It's the sweet spot
of security, as the attack can not be automated, and the cost increases
linearly with the number of targets.

@_date: 2015-03-31 18:47:01
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Feel free to choose your own devices / micro controllers as long as it
supports the data diode and doesn't have wireless / audio devices that
provide covert channels to HSAs. Wide range of platforms makes
compromise of COTS hardware much more difficult. Netbooks are not
significantly more expensive than if one were to buy separate batteries,
chargers, displays, cables and peripherals -- It's also more convenient.
Two netbooks pushes the system price around that of a Blackphone.

@_date: 2015-03-29 00:46:08
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
A microcontroller as TCB doing OTP with HWRNG-generated keys. Sounds a
lot like the OTP-version of Tinfoil Chat ( github.com/maqp/tfc ). It
doesn't take a spy or terrorist to create something like this: TFC was a
hobby of a CS-student.
Distribution of key material isn't the big problem, keeping the keys
secure from end-point exploitation is as TAO, ANT-implants, COMMONDEER,
VALIDATOR, QUANTUM w/ UNITEDRAKE/SALVAGERABBIT etc. make it hard. But
even these could be addressed in TFC - enforcing the need for close
access operations, close proximity malware injection or retro reflectors
and other HW implants is the only way to avoid untasked targeting from
becoming the mass surveillance of next generation; It's the sweet spot
of security, as the attack can not be automated, and the cost increases
linearly with the number of targets.

@_date: 2015-03-31 18:47:01
@_author: Markus Ottela 
@_subject: One Laptop Per Terrorist 
Feel free to choose your own devices / micro controllers as long as it
supports the data diode and doesn't have wireless / audio devices that
provide covert channels to HSAs. Wide range of platforms makes
compromise of COTS hardware much more difficult. Netbooks are not
significantly more expensive than if one were to buy separate batteries,
chargers, displays, cables and peripherals -- It's also more convenient.
Two netbooks pushes the system price around that of a Blackphone.

@_date: 2015-05-24 21:43:52
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
TFC 0.5.5 is now out
I found another vulnerability in the constant transmission feature; between each long message only one command would be sent at most. This issue has now been fixed.
After a request, I added further message authentication for CEV: In addition to GMAC it now does encrypt-then-MAC style authentication using HMAC-SHA2-512 (512-bit key) and SHA3-512 MAC (1144-bit key) before GCM authenticates and decrypts the ciphertext.
I upgraded all keys to 512-bit ones so cyclic hashing of keys won't reduce security over long period of time (there is no key negotiation in I also upgraded hashes from Keccak-256 to Keccak-512 and at the same time the Keccak-CTR key size was upgraded to 512-bits. So 1280-bits of symmetric key security in total.
As for key generation, CEV now has constant 2kHz sampling speed (1.5M samples are loaded in total), three vN whitening passes and between each of those, Keccak-512 compression with 2:1 ratio (1024 bits in, 512 out). User can now input different entropy from keyboard for each of the eight 512-bit keys generated.
Also fixed lots of bugs and typos, and as usual, updated whitepaper and manual where necessary.
UI was tweaked slightly: I added cleaner completion messages and some startup animation for the lulz (disable option available in settings).
Full update log:

@_date: 2015-05-25 14:54:56
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
You don't lose one bit for each message. The idea is that the average length of cycle for a good 512 bit hash function is 2^256. So after 2^256 messages, keys might start to repeat. 2^256 keys last for 10^73 messages, or 10^56 years with 1TB/s continuous transfer speed.
This however doesn't mean the ciphertext will repeat. That would require that all four encryption keys are the same (Probability for that is 1 / (256^4)) and that all nonces are same as well (even less likely).
You can't guarantee all messages make it through, and there is no return channel from either RxM to sender's TxM to tell if some message has not been received. If more entropy would be transferred inside messages, drop of packets could lead to keys getting out of sync. But since the keyspace of current implementation effectively never runs out, this is not necessary.
I think the local testing version comes very close to the "microservice" model you described. The local testing version runs all three programs on same computer and messages are transmitted unidirectionally via files. But whatever you can exploit on the single system, can lead to exfiltration of keys so the HW data-diode model is infinite times more secure. Malware isn't going to break the laws of physics inside data diodes (removing other covert channels from audio to heat between TCB units is of course required).
Pidgin is currently the ideal client, mainly because it was fairly easy to implement (readily available code) and because it's bundled with Tails. If the constant transmission feature of TFC is combined with hidden service XMPP server, the amount of metadata should be about as low as you can make it.

@_date: 2015-05-31 22:51:25
@_author: Markus Ottela 
@_subject: Threat model: Parents 
Has the kid been told about his/her legal right to privacy from his/her The most useful suggestion up until now has been use of Tails LiveUSB w/ I'm not sure if Truecrypt is still bundled with Tails. If not, keeping the installer inside persistent volume isn't that inconvenient and use of steganographic volumes helps with the 5-dollar wrench problem.
The kid has the right to be curious about computing, programming and whatnot, so it should be straightforward to explain why the distro needs to be installed. Avoiding the privacy side of discussion might also be

@_date: 2015-05-24 21:43:52
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
TFC 0.5.5 is now out
I found another vulnerability in the constant transmission feature; between each long message only one command would be sent at most. This issue has now been fixed.
After a request, I added further message authentication for CEV: In addition to GMAC it now does encrypt-then-MAC style authentication using HMAC-SHA2-512 (512-bit key) and SHA3-512 MAC (1144-bit key) before GCM authenticates and decrypts the ciphertext.
I upgraded all keys to 512-bit ones so cyclic hashing of keys won't reduce security over long period of time (there is no key negotiation in I also upgraded hashes from Keccak-256 to Keccak-512 and at the same time the Keccak-CTR key size was upgraded to 512-bits. So 1280-bits of symmetric key security in total.
As for key generation, CEV now has constant 2kHz sampling speed (1.5M samples are loaded in total), three vN whitening passes and between each of those, Keccak-512 compression with 2:1 ratio (1024 bits in, 512 out). User can now input different entropy from keyboard for each of the eight 512-bit keys generated.
Also fixed lots of bugs and typos, and as usual, updated whitepaper and manual where necessary.
UI was tweaked slightly: I added cleaner completion messages and some startup animation for the lulz (disable option available in settings).
Full update log:

@_date: 2015-05-25 14:54:56
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
You don't lose one bit for each message. The idea is that the average length of cycle for a good 512 bit hash function is 2^256. So after 2^256 messages, keys might start to repeat. 2^256 keys last for 10^73 messages, or 10^56 years with 1TB/s continuous transfer speed.
This however doesn't mean the ciphertext will repeat. That would require that all four encryption keys are the same (Probability for that is 1 / (256^4)) and that all nonces are same as well (even less likely).
You can't guarantee all messages make it through, and there is no return channel from either RxM to sender's TxM to tell if some message has not been received. If more entropy would be transferred inside messages, drop of packets could lead to keys getting out of sync. But since the keyspace of current implementation effectively never runs out, this is not necessary.
I think the local testing version comes very close to the "microservice" model you described. The local testing version runs all three programs on same computer and messages are transmitted unidirectionally via files. But whatever you can exploit on the single system, can lead to exfiltration of keys so the HW data-diode model is infinite times more secure. Malware isn't going to break the laws of physics inside data diodes (removing other covert channels from audio to heat between TCB units is of course required).
Pidgin is currently the ideal client, mainly because it was fairly easy to implement (readily available code) and because it's bundled with Tails. If the constant transmission feature of TFC is combined with hidden service XMPP server, the amount of metadata should be about as low as you can make it.

@_date: 2015-05-31 22:51:25
@_author: Markus Ottela 
@_subject: Threat model: Parents 
Has the kid been told about his/her legal right to privacy from his/her The most useful suggestion up until now has been use of Tails LiveUSB w/ I'm not sure if Truecrypt is still bundled with Tails. If not, keeping the installer inside persistent volume isn't that inconvenient and use of steganographic volumes helps with the 5-dollar wrench problem.
The kid has the right to be curious about computing, programming and whatnot, so it should be straightforward to explain why the distro needs to be installed. Avoiding the privacy side of discussion might also be

@_date: 2015-05-24 21:43:52
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
TFC 0.5.5 is now out
I found another vulnerability in the constant transmission feature; between each long message only one command would be sent at most. This issue has now been fixed.
After a request, I added further message authentication for CEV: In addition to GMAC it now does encrypt-then-MAC style authentication using HMAC-SHA2-512 (512-bit key) and SHA3-512 MAC (1144-bit key) before GCM authenticates and decrypts the ciphertext.
I upgraded all keys to 512-bit ones so cyclic hashing of keys won't reduce security over long period of time (there is no key negotiation in I also upgraded hashes from Keccak-256 to Keccak-512 and at the same time the Keccak-CTR key size was upgraded to 512-bits. So 1280-bits of symmetric key security in total.
As for key generation, CEV now has constant 2kHz sampling speed (1.5M samples are loaded in total), three vN whitening passes and between each of those, Keccak-512 compression with 2:1 ratio (1024 bits in, 512 out). User can now input different entropy from keyboard for each of the eight 512-bit keys generated.
Also fixed lots of bugs and typos, and as usual, updated whitepaper and manual where necessary.
UI was tweaked slightly: I added cleaner completion messages and some startup animation for the lulz (disable option available in settings).
Full update log:

@_date: 2015-05-25 14:54:56
@_author: Markus Ottela 
@_subject: TFC 0.5.5 out 
You don't lose one bit for each message. The idea is that the average length of cycle for a good 512 bit hash function is 2^256. So after 2^256 messages, keys might start to repeat. 2^256 keys last for 10^73 messages, or 10^56 years with 1TB/s continuous transfer speed.
This however doesn't mean the ciphertext will repeat. That would require that all four encryption keys are the same (Probability for that is 1 / (256^4)) and that all nonces are same as well (even less likely).
You can't guarantee all messages make it through, and there is no return channel from either RxM to sender's TxM to tell if some message has not been received. If more entropy would be transferred inside messages, drop of packets could lead to keys getting out of sync. But since the keyspace of current implementation effectively never runs out, this is not necessary.
I think the local testing version comes very close to the "microservice" model you described. The local testing version runs all three programs on same computer and messages are transmitted unidirectionally via files. But whatever you can exploit on the single system, can lead to exfiltration of keys so the HW data-diode model is infinite times more secure. Malware isn't going to break the laws of physics inside data diodes (removing other covert channels from audio to heat between TCB units is of course required).
Pidgin is currently the ideal client, mainly because it was fairly easy to implement (readily available code) and because it's bundled with Tails. If the constant transmission feature of TFC is combined with hidden service XMPP server, the amount of metadata should be about as low as you can make it.

@_date: 2015-05-31 22:51:25
@_author: Markus Ottela 
@_subject: Threat model: Parents 
Has the kid been told about his/her legal right to privacy from his/her The most useful suggestion up until now has been use of Tails LiveUSB w/ I'm not sure if Truecrypt is still bundled with Tails. If not, keeping the installer inside persistent volume isn't that inconvenient and use of steganographic volumes helps with the 5-dollar wrench problem.
The kid has the right to be curious about computing, programming and whatnot, so it should be straightforward to explain why the distro needs to be installed. Avoiding the privacy side of discussion might also be
