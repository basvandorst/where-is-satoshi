
@_date: 2014-01-11 12:24:59
@_author: Jesse Taylor 
@_subject: Twister: P2P microblogging platform 
I came across this the other day:
    /"This paper presents a proposal of a new P2P microblogging platform
    that is scalable, resilient to failures and attacks, does not depend
    on any central authority for user registration, provides easy-to-use
    encrypted private communication and authenticated public posts. The
    architecture tries to leverage from existing and proven P2P
    technologies such as Bittorrent and Bitcoin as much possible.
    Privacy is also one of the primary design concerns, no one should be
    able to see the user's IP or their followers unless he explicitly
    shares such information. The proposed platform is comprised of three
    mostly independent overlay networks. The first provides distributed
    user registration and authentication and is based on the Bitcoin
    protocol. The second one is a Distributed Hash Table (DHT) overlay
    network providing key/value storage for user resources and tracker
    location for the third network. The last network is a collection of
    possibly disjoint "swarms" of followers, based on the Bittorrent
    protocol, which can be used for efficient near-instant notification
    delivery to many users. [...] "/
    -- "twister - a P2P microblogging platform
    ", Miguel Freitas
Personally, I'm impressed by the creative application of Bitcoin/Bitorrent/DHT protocols to the problem of private/anonymous communication ... and the software looks very clean and usable as well. I also like that it's based on P2P technology unlike solutions such as Diaspora, which still require trusting third parties with personal data ...
--Jesse Taylor

@_date: 2014-01-12 18:53:11
@_author: Jesse Taylor 
@_subject: Replacing corporate search engines with anonymous/decentralized 
Thanks for your comments on this folks ... lots of food for thought.
coderman  said:
    /the longer discussion is how to make decentralized search useful.
    "Google style" search has a terrific performance advantage over
    decentralized designs by brute force. however, take advantage of
    massive endpoint / peer processing and resources combined with
    implicit observational metrics for reputation and recommendation,
    inside a well integrated framework for resource discovery in usable
    software, and you have something more robust and more effective than
    "Google style" could ever provide./
Yes, it does seem like the speed advantage of centralized search will be
a barrier to adoption of decentralized search. This is analogous to the
difficulty getting people adopt systems like Tor because it is slow. But
I think that as more people become aware of the extent of
state/corporate surveillance, they will become more inclined to accept
solutions that are slower in exchange for not having their search habits
monitored, and also being able to receive uncensored search results. As
long as decentralized search is (a) usable/simple and (b) provides
quality results, I feel like speed is somewhat of a secondary concern.
The key question to me is: "How do we build a search engine that is
simple enough for Grandma to use
, that
produces quality results without massive centralized indexing servers?"
Standalone P2P search applications (e.g. Yacy) don't really make sense
from a usability perspective. It's unrealistic to expect hundreds of
millions of users to download a standalone Java app, and configure a P2P
search node. What would make more sense, and would lead to much more
rapid/widespread adoption, is to use protocols like WebSockets
 / WebRTC
 to facilitate
P2P connectivity in the web browser
so that everything can be done via a simple browser plugin that can be
installed by anyone with few clicks, and would then just allow people to
use the browser search bar as usual. This browser integration would also
have the bonus of simplifying the choice of what to index -- it could
just default to indexing bookmarked and frequently-visited pages, and
then be optionally customized by more advanced users to create custom
indexes (i.e. all of the complexity of setting up indexing could be
hidden from the user, unless they choose to look for it).
To help bootstrap the WebRTC nodes into the P2P network, and to deal
with some of the instability inherent in P2P networks (i.e. by creating
stable "super-peer
" indexing
nodes), I like cathalgarvey's suggestion of utilizing something like a
Wordpress//plugin that would use the same index/search standard as the
WebRTC clients, but could additionally bootstrap the web-based clients.
As cathalgarvey said:/
    /A standard rather than a codebase. But there's a huge advantage to
    this line of thought, if you'll bear with me. A two-digit fraction
    of the web right now is powered by Wordpress.org, who explicitly
    advocate open/free culture. If you can convince them to include a
    social search/index standard of this type, which is virtually free
    in terms of computer resources, then you'd have it deployed across
    the web in days as the next update rolled out. Indeed, even if
    Wordpress seemed reluctant, a wordpress plugin could probably be
    written quickly enough to enable such a thing and make it available
    for casual use. Suddenly, a bunch of PHP-powered sites around the
    web start committing small bits and pieces of resources to a social
    search engine based on human-curated attestations of trust that flow
    through a web, helping to confine spammers to the fringes and to
    users with stupid taste. /
What would also be interesting is if this standard enabled some kind of
"pingback" mechanism whereby WebRTC nodes could be associated with
specific super-peer nodes (e.g. maybe people who have bookmarked the
super-peer site in their browser, or subscribe to its feed), so that in
addition to broad/random queries that target the entire P2P network,
clients could also create more targeted custom searches that say
something like "start the search with the nodes that are clustered
around these super-peers". This would create an enormous diversity of
search possibilities -- hundreds of thousands (millions) of different
"search engines", each of which would return different results for the
same query, depending on where you start your search ... This diversity
is another reason I find P2P search interesting, in addition to the
benefits re: censorship, traffic shaping, and surveillance.
I've been looking around for some kind of WebRTC P2P search engine and
haven't found anything yet ... maybe I've found a programming project
for this summer :)

@_date: 2014-01-11 12:24:59
@_author: Jesse Taylor 
@_subject: Twister: P2P microblogging platform 
I came across this the other day:
    /"This paper presents a proposal of a new P2P microblogging platform
    that is scalable, resilient to failures and attacks, does not depend
    on any central authority for user registration, provides easy-to-use
    encrypted private communication and authenticated public posts. The
    architecture tries to leverage from existing and proven P2P
    technologies such as Bittorrent and Bitcoin as much possible.
    Privacy is also one of the primary design concerns, no one should be
    able to see the user's IP or their followers unless he explicitly
    shares such information. The proposed platform is comprised of three
    mostly independent overlay networks. The first provides distributed
    user registration and authentication and is based on the Bitcoin
    protocol. The second one is a Distributed Hash Table (DHT) overlay
    network providing key/value storage for user resources and tracker
    location for the third network. The last network is a collection of
    possibly disjoint "swarms" of followers, based on the Bittorrent
    protocol, which can be used for efficient near-instant notification
    delivery to many users. [...] "/
    -- "twister - a P2P microblogging platform
    ", Miguel Freitas
Personally, I'm impressed by the creative application of Bitcoin/Bitorrent/DHT protocols to the problem of private/anonymous communication ... and the software looks very clean and usable as well. I also like that it's based on P2P technology unlike solutions such as Diaspora, which still require trusting third parties with personal data ...
--Jesse Taylor

@_date: 2014-01-12 18:53:11
@_author: Jesse Taylor 
@_subject: Replacing corporate search engines with anonymous/decentralized 
Thanks for your comments on this folks ... lots of food for thought.
coderman at gmail.com  said:
    /the longer discussion is how to make decentralized search useful.
    "Google style" search has a terrific performance advantage over
    decentralized designs by brute force. however, take advantage of
    massive endpoint / peer processing and resources combined with
    implicit observational metrics for reputation and recommendation,
    inside a well integrated framework for resource discovery in usable
    software, and you have something more robust and more effective than
    "Google style" could ever provide./
Yes, it does seem like the speed advantage of centralized search will be
a barrier to adoption of decentralized search. This is analogous to the
difficulty getting people adopt systems like Tor because it is slow. But
I think that as more people become aware of the extent of
state/corporate surveillance, they will become more inclined to accept
solutions that are slower in exchange for not having their search habits
monitored, and also being able to receive uncensored search results. As
long as decentralized search is (a) usable/simple and (b) provides
quality results, I feel like speed is somewhat of a secondary concern.
The key question to me is: "How do we build a search engine that is
simple enough for Grandma to use
, that
produces quality results without massive centralized indexing servers?"
Standalone P2P search applications (e.g. Yacy) don't really make sense
from a usability perspective. It's unrealistic to expect hundreds of
millions of users to download a standalone Java app, and configure a P2P
search node. What would make more sense, and would lead to much more
rapid/widespread adoption, is to use protocols like WebSockets
 / WebRTC
 to facilitate
P2P connectivity in the web browser
so that everything can be done via a simple browser plugin that can be
installed by anyone with few clicks, and would then just allow people to
use the browser search bar as usual. This browser integration would also
have the bonus of simplifying the choice of what to index -- it could
just default to indexing bookmarked and frequently-visited pages, and
then be optionally customized by more advanced users to create custom
indexes (i.e. all of the complexity of setting up indexing could be
hidden from the user, unless they choose to look for it).
To help bootstrap the WebRTC nodes into the P2P network, and to deal
with some of the instability inherent in P2P networks (i.e. by creating
stable "super-peer
" indexing
nodes), I like cathalgarvey's suggestion of utilizing something like a
Wordpress//plugin that would use the same index/search standard as the
WebRTC clients, but could additionally bootstrap the web-based clients.
As cathalgarvey said:/
    /A standard rather than a codebase. But there's a huge advantage to
    this line of thought, if you'll bear with me. A two-digit fraction
    of the web right now is powered by Wordpress.org, who explicitly
    advocate open/free culture. If you can convince them to include a
    social search/index standard of this type, which is virtually free
    in terms of computer resources, then you'd have it deployed across
    the web in days as the next update rolled out. Indeed, even if
    Wordpress seemed reluctant, a wordpress plugin could probably be
    written quickly enough to enable such a thing and make it available
    for casual use. Suddenly, a bunch of PHP-powered sites around the
    web start committing small bits and pieces of resources to a social
    search engine based on human-curated attestations of trust that flow
    through a web, helping to confine spammers to the fringes and to
    users with stupid taste. /
What would also be interesting is if this standard enabled some kind of
"pingback" mechanism whereby WebRTC nodes could be associated with
specific super-peer nodes (e.g. maybe people who have bookmarked the
super-peer site in their browser, or subscribe to its feed), so that in
addition to broad/random queries that target the entire P2P network,
clients could also create more targeted custom searches that say
something like "start the search with the nodes that are clustered
around these super-peers". This would create an enormous diversity of
search possibilities -- hundreds of thousands (millions) of different
"search engines", each of which would return different results for the
same query, depending on where you start your search ... This diversity
is another reason I find P2P search interesting, in addition to the
benefits re: censorship, traffic shaping, and surveillance.
I've been looking around for some kind of WebRTC P2P search engine and
haven't found anything yet ... maybe I've found a programming project
for this summer :)

@_date: 2014-01-11 12:24:59
@_author: Jesse Taylor 
@_subject: Twister: P2P microblogging platform 
I came across this the other day:
    /"This paper presents a proposal of a new P2P microblogging platform
    that is scalable, resilient to failures and attacks, does not depend
    on any central authority for user registration, provides easy-to-use
    encrypted private communication and authenticated public posts. The
    architecture tries to leverage from existing and proven P2P
    technologies such as Bittorrent and Bitcoin as much possible.
    Privacy is also one of the primary design concerns, no one should be
    able to see the user's IP or their followers unless he explicitly
    shares such information. The proposed platform is comprised of three
    mostly independent overlay networks. The first provides distributed
    user registration and authentication and is based on the Bitcoin
    protocol. The second one is a Distributed Hash Table (DHT) overlay
    network providing key/value storage for user resources and tracker
    location for the third network. The last network is a collection of
    possibly disjoint "swarms" of followers, based on the Bittorrent
    protocol, which can be used for efficient near-instant notification
    delivery to many users. [...] "/
    -- "twister - a P2P microblogging platform
    ", Miguel Freitas
Personally, I'm impressed by the creative application of Bitcoin/Bitorrent/DHT protocols to the problem of private/anonymous communication ... and the software looks very clean and usable as well. I also like that it's based on P2P technology unlike solutions such as Diaspora, which still require trusting third parties with personal data ...
--Jesse Taylor

@_date: 2014-01-12 18:53:11
@_author: Jesse Taylor 
@_subject: Replacing corporate search engines with anonymous/decentralized 
Thanks for your comments on this folks ... lots of food for thought.
coderman at gmail.com  said:
    /the longer discussion is how to make decentralized search useful.
    "Google style" search has a terrific performance advantage over
    decentralized designs by brute force. however, take advantage of
    massive endpoint / peer processing and resources combined with
    implicit observational metrics for reputation and recommendation,
    inside a well integrated framework for resource discovery in usable
    software, and you have something more robust and more effective than
    "Google style" could ever provide./
Yes, it does seem like the speed advantage of centralized search will be
a barrier to adoption of decentralized search. This is analogous to the
difficulty getting people adopt systems like Tor because it is slow. But
I think that as more people become aware of the extent of
state/corporate surveillance, they will become more inclined to accept
solutions that are slower in exchange for not having their search habits
monitored, and also being able to receive uncensored search results. As
long as decentralized search is (a) usable/simple and (b) provides
quality results, I feel like speed is somewhat of a secondary concern.
The key question to me is: "How do we build a search engine that is
simple enough for Grandma to use
, that
produces quality results without massive centralized indexing servers?"
Standalone P2P search applications (e.g. Yacy) don't really make sense
from a usability perspective. It's unrealistic to expect hundreds of
millions of users to download a standalone Java app, and configure a P2P
search node. What would make more sense, and would lead to much more
rapid/widespread adoption, is to use protocols like WebSockets
 / WebRTC
 to facilitate
P2P connectivity in the web browser
so that everything can be done via a simple browser plugin that can be
installed by anyone with few clicks, and would then just allow people to
use the browser search bar as usual. This browser integration would also
have the bonus of simplifying the choice of what to index -- it could
just default to indexing bookmarked and frequently-visited pages, and
then be optionally customized by more advanced users to create custom
indexes (i.e. all of the complexity of setting up indexing could be
hidden from the user, unless they choose to look for it).
To help bootstrap the WebRTC nodes into the P2P network, and to deal
with some of the instability inherent in P2P networks (i.e. by creating
stable "super-peer
" indexing
nodes), I like cathalgarvey's suggestion of utilizing something like a
Wordpress//plugin that would use the same index/search standard as the
WebRTC clients, but could additionally bootstrap the web-based clients.
As cathalgarvey said:/
    /A standard rather than a codebase. But there's a huge advantage to
    this line of thought, if you'll bear with me. A two-digit fraction
    of the web right now is powered by Wordpress.org, who explicitly
    advocate open/free culture. If you can convince them to include a
    social search/index standard of this type, which is virtually free
    in terms of computer resources, then you'd have it deployed across
    the web in days as the next update rolled out. Indeed, even if
    Wordpress seemed reluctant, a wordpress plugin could probably be
    written quickly enough to enable such a thing and make it available
    for casual use. Suddenly, a bunch of PHP-powered sites around the
    web start committing small bits and pieces of resources to a social
    search engine based on human-curated attestations of trust that flow
    through a web, helping to confine spammers to the fringes and to
    users with stupid taste. /
What would also be interesting is if this standard enabled some kind of
"pingback" mechanism whereby WebRTC nodes could be associated with
specific super-peer nodes (e.g. maybe people who have bookmarked the
super-peer site in their browser, or subscribe to its feed), so that in
addition to broad/random queries that target the entire P2P network,
clients could also create more targeted custom searches that say
something like "start the search with the nodes that are clustered
around these super-peers". This would create an enormous diversity of
search possibilities -- hundreds of thousands (millions) of different
"search engines", each of which would return different results for the
same query, depending on where you start your search ... This diversity
is another reason I find P2P search interesting, in addition to the
benefits re: censorship, traffic shaping, and surveillance.
I've been looking around for some kind of WebRTC P2P search engine and
haven't found anything yet ... maybe I've found a programming project
for this summer :)

@_date: 2014-10-25 19:06:01
@_author: Jesse Taylor 
@_subject: CITIZENFOUR 
After reading the Salon article you mentioned, I filed FOIA requests regarding "Main Core" with NSA, FBI, DHS, DSS, NORTHCOM, STRATCOM, CYBERCOM, and DODIIS. All I got was responses saying that there were no responsive records. Of course, this is what happens when almost any information about classified programs is requested via FOIA, so I wasn't surprised (FOIA is really just a smokescreen to make people waste their time begging for records they're never going to get, and to promote the illusion that citizens have some sort of oversight over "their" government).
I sent a letter to the editorial staff at Salon and Democracy Now, which both ran stories on this based on "anonymous sources", and pointed out to them that pretty much every intelligence agency is responding to FOIA requests saying the topic of one of their stories doesn't actually exist. Neither of them responded.
    /1) Who might have access to a list known as Main Core? This is such
    an old story that it would seem that some kind of list would now be
    available, but I haven't found it. Has it ever been leaked, FOIA'd,
    successfully released in partially redacted form in some other
    mechanism, or made searchable somewhere? //
    //
    //[[ Main Core notes / background: Salon reported on Main Core in
    July of 2008 with an article by Tim Shorrock. Apparently, William
    Hamilton, a former NSA intelligence officer who left the agency in
    the 1970s, had heard of Main Core at some point in 1992, according
    to the Salon article. Hamilton, who (was then, and still is)
    president of Inslaw Inc., a computer services firm that includes
    clients in government, indicated that the Bush administration's
    domestic surveillance operations used Main Core - it is not known if
    it is still used today in 2014. Main Core was first widely reported
    on in May 2008 by Christopher Ketcham and in July 2008 by Tim
    Shorrock, which included in July of 2008 an interview by Amy Goodman
    of Tim Shorrock. However, I am unaware of any release of names,
    e-mails, etc. which might be on this list, and it seemed kind of
    obvious that those who were reporting on it probably had never seen
    the Main Core list. This may involve use of PROMIS software, and
    according to Adm. Dan Murphy (a former military advisor to Elliot
    Richardson who later served under President George H.W. Bush as
    deputy director of the CIA, who 'died' shortly after his meeting in
    2001 with William Hamilton), did not specifically mention Main Core.
    But he informed Hamilton that the NSA's use of PROMIS involved
    something "so seriously wrong that money alone cannot cure the
    problem." ]]/

@_date: 2014-10-25 19:06:01
@_author: Jesse Taylor 
@_subject: CITIZENFOUR 
After reading the Salon article you mentioned, I filed FOIA requests regarding "Main Core" with NSA, FBI, DHS, DSS, NORTHCOM, STRATCOM, CYBERCOM, and DODIIS. All I got was responses saying that there were no responsive records. Of course, this is what happens when almost any information about classified programs is requested via FOIA, so I wasn't surprised (FOIA is really just a smokescreen to make people waste their time begging for records they're never going to get, and to promote the illusion that citizens have some sort of oversight over "their" government).
I sent a letter to the editorial staff at Salon and Democracy Now, which both ran stories on this based on "anonymous sources", and pointed out to them that pretty much every intelligence agency is responding to FOIA requests saying the topic of one of their stories doesn't actually exist. Neither of them responded.
    /1) Who might have access to a list known as Main Core? This is such
    an old story that it would seem that some kind of list would now be
    available, but I haven't found it. Has it ever been leaked, FOIA'd,
    successfully released in partially redacted form in some other
    mechanism, or made searchable somewhere? //
    //
    //[[ Main Core notes / background: Salon reported on Main Core in
    July of 2008 with an article by Tim Shorrock. Apparently, William
    Hamilton, a former NSA intelligence officer who left the agency in
    the 1970s, had heard of Main Core at some point in 1992, according
    to the Salon article. Hamilton, who (was then, and still is)
    president of Inslaw Inc., a computer services firm that includes
    clients in government, indicated that the Bush administration's
    domestic surveillance operations used Main Core - it is not known if
    it is still used today in 2014. Main Core was first widely reported
    on in May 2008 by Christopher Ketcham and in July 2008 by Tim
    Shorrock, which included in July of 2008 an interview by Amy Goodman
    of Tim Shorrock. However, I am unaware of any release of names,
    e-mails, etc. which might be on this list, and it seemed kind of
    obvious that those who were reporting on it probably had never seen
    the Main Core list. This may involve use of PROMIS software, and
    according to Adm. Dan Murphy (a former military advisor to Elliot
    Richardson who later served under President George H.W. Bush as
    deputy director of the CIA, who 'died' shortly after his meeting in
    2001 with William Hamilton), did not specifically mention Main Core.
    But he informed Hamilton that the NSA's use of PROMIS involved
    something "so seriously wrong that money alone cannot cure the
    problem." ]]/

@_date: 2014-10-25 19:06:01
@_author: Jesse Taylor 
@_subject: CITIZENFOUR 
After reading the Salon article you mentioned, I filed FOIA requests regarding "Main Core" with NSA, FBI, DHS, DSS, NORTHCOM, STRATCOM, CYBERCOM, and DODIIS. All I got was responses saying that there were no responsive records. Of course, this is what happens when almost any information about classified programs is requested via FOIA, so I wasn't surprised (FOIA is really just a smokescreen to make people waste their time begging for records they're never going to get, and to promote the illusion that citizens have some sort of oversight over "their" government).
I sent a letter to the editorial staff at Salon and Democracy Now, which both ran stories on this based on "anonymous sources", and pointed out to them that pretty much every intelligence agency is responding to FOIA requests saying the topic of one of their stories doesn't actually exist. Neither of them responded.
    /1) Who might have access to a list known as Main Core? This is such
    an old story that it would seem that some kind of list would now be
    available, but I haven't found it. Has it ever been leaked, FOIA'd,
    successfully released in partially redacted form in some other
    mechanism, or made searchable somewhere? //
    //
    //[[ Main Core notes / background: Salon reported on Main Core in
    July of 2008 with an article by Tim Shorrock. Apparently, William
    Hamilton, a former NSA intelligence officer who left the agency in
    the 1970s, had heard of Main Core at some point in 1992, according
    to the Salon article. Hamilton, who (was then, and still is)
    president of Inslaw Inc., a computer services firm that includes
    clients in government, indicated that the Bush administration's
    domestic surveillance operations used Main Core - it is not known if
    it is still used today in 2014. Main Core was first widely reported
    on in May 2008 by Christopher Ketcham and in July 2008 by Tim
    Shorrock, which included in July of 2008 an interview by Amy Goodman
    of Tim Shorrock. However, I am unaware of any release of names,
    e-mails, etc. which might be on this list, and it seemed kind of
    obvious that those who were reporting on it probably had never seen
    the Main Core list. This may involve use of PROMIS software, and
    according to Adm. Dan Murphy (a former military advisor to Elliot
    Richardson who later served under President George H.W. Bush as
    deputy director of the CIA, who 'died' shortly after his meeting in
    2001 with William Hamilton), did not specifically mention Main Core.
    But he informed Hamilton that the NSA's use of PROMIS involved
    something "so seriously wrong that money alone cannot cure the
    problem." ]]/

@_date: 2015-05-22 23:22:03
@_author: Jesse Taylor 
@_subject: Astoria - new Tor client designed to be more resistant to timing 
Curious to hear your thoughts on this a new////Tor ////client called////Astoria eavesdropping harder for the world's richest, most aggressive, and most capable spies.//
user fires up the client and connects to the network through what's called an entry node. To reach a website anonymously, the user’s Internet traffic is then passed encrypted through a so-called middle relay and then an exit relay (and back again). That user-relay connection is called a circuit. The website on the receiving end doesn’t know who is visiting, only that a faceless Tor user has connected.////An eavesdropper shouldn’t be able to know who the Tor user is either, thanks to the encrypted traffic being routed through 6,000 nodes in the network.////But something called "timing attacks" change the situation. When an adversary takes control of both the entry and exit relays, research shows they can potentially deanonymize Tor users //within minutes //.//
attackers, such as the NSA or Britain’s Government Communications Headquarters (GCHQ), when they access popular websites, according to new////research ////from American and Israeli academics. Chinese users are the most vulnerable of all to these kinds of attacks, with researchers finding 85.7 percent of all Tor circuits from the country to be vulnerable.//
users, the NSA’s position means they can potentially see and measure both traffic entering the Tor network and the traffic that comes out. When an intelligence agency can see both,//simple statistics system at their control match the data up in a timing attack and discover the identity of the sender.////Anonymity over.////This kind of threat has been known to Tor developers for over a decade. They’ve been trying to make eavesdropping difficult for spy agencies for just as long.//
new Tor client focused on defeating autonomous systems that can break Tor’s anonymity.////Astoria reduces the number of vulnerable circuits from 58 percent to 5.8 percent, the researchers say. The new solution is the first designed to beat even the most//recently attacks on Tor.//
Tor's default client in how it selects the circuits that connect a user to the network and then to the outside Internet. The tool, at its foundation, is an algorithm designed to more accurately predict attacks and then securely select relays that mitigate timing attack opportunities for top-tier adversaries.//
researchers, be made “when there are no safe possibilities,” how to safely balance the growing bandwidth load across the Tor network, and how to keep Tor’s performance “reasonable” and relatively fast even when Astoria is in its most secure configuration.////All this while under the unblinking gaze of the world’s best intel services.////Defeating timing attacks against Tor completely isn’t possible because of how Tor is built, but making the attacks more costly and less likely to succeed is a pastime that Tor developers have dedicated a decade to. Astoria follows in those footsteps.////By choosing relays based on lowering the threat of eavesdropping by autonomous systems and then choosing randomly if no safe passage is possible, Astoria aims to minimize the information gained by an adversary watching an entire circuit.//
attacks, Astoria also has performance that is within a reasonable distance from the current Tor client,” the researchers wrote. “Unlike other AS-aware Tor clients, Astoria also considers how circuits should be built in the worst case—i.e., when there are no safe relays that are available. Further, Astoria is a good network citizen and works to ensure that the all circuits created by it are load-balanced across the volunteer driven Tor network.”//
person, the newest Tor Browser allows a sliding scale of security that balances speed and usability with strong security preferences.////Similarly, Astoria provides multiple security options. However, it's both most effective and most usable when at its highest security level, the researchers say, so "Astoria is a usable substitute for the vanilla Tor client only in scenarios where security is a high Source:

@_date: 2015-05-22 23:22:03
@_author: Jesse Taylor 
@_subject: Astoria - new Tor client designed to be more resistant to timing 
Curious to hear your thoughts on this a new////Tor ////client called////Astoria eavesdropping harder for the world's richest, most aggressive, and most capable spies.//
user fires up the client and connects to the network through what's called an entry node. To reach a website anonymously, the user’s Internet traffic is then passed encrypted through a so-called middle relay and then an exit relay (and back again). That user-relay connection is called a circuit. The website on the receiving end doesn’t know who is visiting, only that a faceless Tor user has connected.////An eavesdropper shouldn’t be able to know who the Tor user is either, thanks to the encrypted traffic being routed through 6,000 nodes in the network.////But something called "timing attacks" change the situation. When an adversary takes control of both the entry and exit relays, research shows they can potentially deanonymize Tor users //within minutes //.//
attackers, such as the NSA or Britain’s Government Communications Headquarters (GCHQ), when they access popular websites, according to new////research ////from American and Israeli academics. Chinese users are the most vulnerable of all to these kinds of attacks, with researchers finding 85.7 percent of all Tor circuits from the country to be vulnerable.//
users, the NSA’s position means they can potentially see and measure both traffic entering the Tor network and the traffic that comes out. When an intelligence agency can see both,//simple statistics system at their control match the data up in a timing attack and discover the identity of the sender.////Anonymity over.////This kind of threat has been known to Tor developers for over a decade. They’ve been trying to make eavesdropping difficult for spy agencies for just as long.//
new Tor client focused on defeating autonomous systems that can break Tor’s anonymity.////Astoria reduces the number of vulnerable circuits from 58 percent to 5.8 percent, the researchers say. The new solution is the first designed to beat even the most//recently attacks on Tor.//
Tor's default client in how it selects the circuits that connect a user to the network and then to the outside Internet. The tool, at its foundation, is an algorithm designed to more accurately predict attacks and then securely select relays that mitigate timing attack opportunities for top-tier adversaries.//
researchers, be made “when there are no safe possibilities,” how to safely balance the growing bandwidth load across the Tor network, and how to keep Tor’s performance “reasonable” and relatively fast even when Astoria is in its most secure configuration.////All this while under the unblinking gaze of the world’s best intel services.////Defeating timing attacks against Tor completely isn’t possible because of how Tor is built, but making the attacks more costly and less likely to succeed is a pastime that Tor developers have dedicated a decade to. Astoria follows in those footsteps.////By choosing relays based on lowering the threat of eavesdropping by autonomous systems and then choosing randomly if no safe passage is possible, Astoria aims to minimize the information gained by an adversary watching an entire circuit.//
attacks, Astoria also has performance that is within a reasonable distance from the current Tor client,” the researchers wrote. “Unlike other AS-aware Tor clients, Astoria also considers how circuits should be built in the worst case—i.e., when there are no safe relays that are available. Further, Astoria is a good network citizen and works to ensure that the all circuits created by it are load-balanced across the volunteer driven Tor network.”//
person, the newest Tor Browser allows a sliding scale of security that balances speed and usability with strong security preferences.////Similarly, Astoria provides multiple security options. However, it's both most effective and most usable when at its highest security level, the researchers say, so "Astoria is a usable substitute for the vanilla Tor client only in scenarios where security is a high Source:

@_date: 2015-05-22 23:22:03
@_author: Jesse Taylor 
@_subject: Astoria - new Tor client designed to be more resistant to timing 
Curious to hear your thoughts on this a new////Tor ////client called////Astoria eavesdropping harder for the world's richest, most aggressive, and most capable spies.//
user fires up the client and connects to the network through what's called an entry node. To reach a website anonymously, the user’s Internet traffic is then passed encrypted through a so-called middle relay and then an exit relay (and back again). That user-relay connection is called a circuit. The website on the receiving end doesn’t know who is visiting, only that a faceless Tor user has connected.////An eavesdropper shouldn’t be able to know who the Tor user is either, thanks to the encrypted traffic being routed through 6,000 nodes in the network.////But something called "timing attacks" change the situation. When an adversary takes control of both the entry and exit relays, research shows they can potentially deanonymize Tor users //within minutes //.//
attackers, such as the NSA or Britain’s Government Communications Headquarters (GCHQ), when they access popular websites, according to new////research ////from American and Israeli academics. Chinese users are the most vulnerable of all to these kinds of attacks, with researchers finding 85.7 percent of all Tor circuits from the country to be vulnerable.//
users, the NSA’s position means they can potentially see and measure both traffic entering the Tor network and the traffic that comes out. When an intelligence agency can see both,//simple statistics system at their control match the data up in a timing attack and discover the identity of the sender.////Anonymity over.////This kind of threat has been known to Tor developers for over a decade. They’ve been trying to make eavesdropping difficult for spy agencies for just as long.//
new Tor client focused on defeating autonomous systems that can break Tor’s anonymity.////Astoria reduces the number of vulnerable circuits from 58 percent to 5.8 percent, the researchers say. The new solution is the first designed to beat even the most//recently attacks on Tor.//
Tor's default client in how it selects the circuits that connect a user to the network and then to the outside Internet. The tool, at its foundation, is an algorithm designed to more accurately predict attacks and then securely select relays that mitigate timing attack opportunities for top-tier adversaries.//
researchers, be made “when there are no safe possibilities,” how to safely balance the growing bandwidth load across the Tor network, and how to keep Tor’s performance “reasonable” and relatively fast even when Astoria is in its most secure configuration.////All this while under the unblinking gaze of the world’s best intel services.////Defeating timing attacks against Tor completely isn’t possible because of how Tor is built, but making the attacks more costly and less likely to succeed is a pastime that Tor developers have dedicated a decade to. Astoria follows in those footsteps.////By choosing relays based on lowering the threat of eavesdropping by autonomous systems and then choosing randomly if no safe passage is possible, Astoria aims to minimize the information gained by an adversary watching an entire circuit.//
attacks, Astoria also has performance that is within a reasonable distance from the current Tor client,” the researchers wrote. “Unlike other AS-aware Tor clients, Astoria also considers how circuits should be built in the worst case—i.e., when there are no safe relays that are available. Further, Astoria is a good network citizen and works to ensure that the all circuits created by it are load-balanced across the volunteer driven Tor network.”//
person, the newest Tor Browser allows a sliding scale of security that balances speed and usability with strong security preferences.////Similarly, Astoria provides multiple security options. However, it's both most effective and most usable when at its highest security level, the researchers say, so "Astoria is a usable substitute for the vanilla Tor client only in scenarios where security is a high Source:
